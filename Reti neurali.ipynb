{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import zscore\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import itertools\n",
    "import os\n",
    "from datetime import datetime\n",
    "from phik.report import plot_correlation_matrix\n",
    "from patsy import dmatrices\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from featurewiz import featurewiz\n",
    "from optbinning import BinningProcess\n",
    "from sklearn.metrics import classification_report\n",
    "import openpyxl\n",
    "from pycaret.classification import *\n",
    "import re\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import confusion_matrix, f1_score, roc_auc_score, precision_score, recall_score, roc_curve, auc\n",
    "import pickle\n",
    "import jenkspy\n",
    "from openpyxl.drawing.image import Image\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "import itertools\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "#pd.set_option('display.max_rows', None)\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"Input_Sviluppo_Prestitempo_custom_con_più_colonne.parquet\"\n",
    "df = pd.read_parquet(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clienti = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Chiave</th>\n",
       "      <th>PRATICA</th>\n",
       "      <th>DELIN_HISTORY</th>\n",
       "      <th>MOD_PAGAM</th>\n",
       "      <th>MOD_PAG</th>\n",
       "      <th>TIPO_GARANTE</th>\n",
       "      <th>Denominazione Regione</th>\n",
       "      <th>CODICE_PHONIA</th>\n",
       "      <th>DES_PRODOTTO</th>\n",
       "      <th>DES_BENE</th>\n",
       "      <th>DATA_AFFIDO</th>\n",
       "      <th>DATA_FINE_AFFIDO</th>\n",
       "      <th>DATA_FINANZIAM</th>\n",
       "      <th>DT_VAL_ULT_PAGAM</th>\n",
       "      <th>DATA_ESTINZ</th>\n",
       "      <th>DATA_AGGIORNAMENTO</th>\n",
       "      <th>P_INST1_DUE_DATE</th>\n",
       "      <th>P_INST2_DUE_DATE</th>\n",
       "      <th>P_INST3_DUE_DATE</th>\n",
       "      <th>DT_VAL_RAT_IMP1</th>\n",
       "      <th>DT_VAL_RAT_IMP2</th>\n",
       "      <th>P_INST1_VAL_DATE</th>\n",
       "      <th>P_INST3_VAL_DATE</th>\n",
       "      <th>SCAD_MAXIRATA</th>\n",
       "      <th>SCAD_RAT_IMP2</th>\n",
       "      <th>SCAD_RAT_IMP1</th>\n",
       "      <th>IMP_FINANZIATO</th>\n",
       "      <th>IMP_MAXIRATA</th>\n",
       "      <th>CURRENT_BALANCE</th>\n",
       "      <th>P_INST1_AMOUNT_PAID</th>\n",
       "      <th>P_INST2_AMOUNT_PAID</th>\n",
       "      <th>P_INST3_AMOUNT_PAID</th>\n",
       "      <th>RATE_TOTALI</th>\n",
       "      <th>NUM_RATE_RIFI</th>\n",
       "      <th>NUM_RAT_IMP1</th>\n",
       "      <th>NUM_RAT_IMP2</th>\n",
       "      <th>P_INST1_NUM</th>\n",
       "      <th>P_INST2_NUM</th>\n",
       "      <th>P_INST3_NUM</th>\n",
       "      <th>NUMERO RATE</th>\n",
       "      <th>PROVINCIA</th>\n",
       "      <th>P_INST1_AMOUNT</th>\n",
       "      <th>P_INST2_AMOUNT</th>\n",
       "      <th>P_INST3_AMOUNT</th>\n",
       "      <th>IMP_RAT_IMP1</th>\n",
       "      <th>IMP_RAT_IMP2</th>\n",
       "      <th>IMP_PAG_RAT_IMP1</th>\n",
       "      <th>IMP_PAG_RAT_IMP2</th>\n",
       "      <th>TOT_RATE_AFF_INIZIO_MAND</th>\n",
       "      <th>TOT_MORA_AFF_INIZIO_MAND</th>\n",
       "      <th>TOT_ONERI_AFF_INIZIO_MAND</th>\n",
       "      <th>TOT_AFF_INIZIO_MAND</th>\n",
       "      <th>TOT_RATE_PAG</th>\n",
       "      <th>TOT_MORA_PAG</th>\n",
       "      <th>TOT_ONERI_PAG</th>\n",
       "      <th>TOT_PAG</th>\n",
       "      <th>TOT_RATE_AFF</th>\n",
       "      <th>TOT_MORA_AFF</th>\n",
       "      <th>TOT_ONERI_AFF</th>\n",
       "      <th>TOT_AFF</th>\n",
       "      <th>Flag_Riciclo_SDD</th>\n",
       "      <th>TOT REC</th>\n",
       "      <th>TOT AFF</th>\n",
       "      <th>Flag_Rifinanziamento</th>\n",
       "      <th>Flag_Garante</th>\n",
       "      <th>Flag_Cointestazione</th>\n",
       "      <th>Metodo_pagamento</th>\n",
       "      <th>Ratio_Rate_Imp1</th>\n",
       "      <th>Ratio_Rate_Imp2</th>\n",
       "      <th>DistanzaAffidoUltimoPagamento</th>\n",
       "      <th>Sesso</th>\n",
       "      <th>Eta_Debitore</th>\n",
       "      <th>NOSTART12M</th>\n",
       "      <th>NOSTART6M</th>\n",
       "      <th>DurataFinanziamento</th>\n",
       "      <th>Flag_Galleggiamento_3M</th>\n",
       "      <th>Flag_Galleggiamento_6M</th>\n",
       "      <th>Flag_Gestione_Prec</th>\n",
       "      <th>Flag_InsolvenzaGrave_3M</th>\n",
       "      <th>Flag_InsolvenzaGrave_6M</th>\n",
       "      <th>Flag_Miglioramento_3M</th>\n",
       "      <th>Flag_Rate_piu_uno</th>\n",
       "      <th>Flag_rientrototale_6M</th>\n",
       "      <th>Flag_Scivolamento_3M</th>\n",
       "      <th>Flag_Scivolamento_6M</th>\n",
       "      <th>Numero_mesi_rec</th>\n",
       "      <th>Severity_12M</th>\n",
       "      <th>Severity_12M_pesata</th>\n",
       "      <th>Flag_InsolvenzaGrave_12M</th>\n",
       "      <th>Target_95</th>\n",
       "      <th>FlagRecOver100</th>\n",
       "      <th>AgeingGestioneGg</th>\n",
       "      <th>AgeingErogazioneGg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00000854016301_08_2021</td>\n",
       "      <td>00000854016301</td>\n",
       "      <td>13221222111111143332432332111100000000</td>\n",
       "      <td>P</td>\n",
       "      <td>BP</td>\n",
       "      <td>C</td>\n",
       "      <td>Lazio</td>\n",
       "      <td>ALPTEL</td>\n",
       "      <td>PRESTITO PERSONALE</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-08-02</td>\n",
       "      <td>2021-09-05</td>\n",
       "      <td>2018-06-12</td>\n",
       "      <td>2021-07-05</td>\n",
       "      <td>None</td>\n",
       "      <td>01/01/0001</td>\n",
       "      <td>2021-04-12</td>\n",
       "      <td>2021-05-12</td>\n",
       "      <td>2021-06-12</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-06-16</td>\n",
       "      <td>2021-07-05</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-08-12</td>\n",
       "      <td>2021-07-12</td>\n",
       "      <td>15,200.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9,770.58</td>\n",
       "      <td>0000000023700</td>\n",
       "      <td>0000000023700</td>\n",
       "      <td>0000000023700</td>\n",
       "      <td>84.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>37.00</td>\n",
       "      <td>38.00</td>\n",
       "      <td>34.00</td>\n",
       "      <td>35.00</td>\n",
       "      <td>36.00</td>\n",
       "      <td>2</td>\n",
       "      <td>LT</td>\n",
       "      <td>237.00</td>\n",
       "      <td>237.00</td>\n",
       "      <td>237.00</td>\n",
       "      <td>237.00</td>\n",
       "      <td>237.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>237.00</td>\n",
       "      <td>28.24</td>\n",
       "      <td>12.00</td>\n",
       "      <td>277.24</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>237.00</td>\n",
       "      <td>28.24</td>\n",
       "      <td>12.00</td>\n",
       "      <td>277.24</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00</td>\n",
       "      <td>486.00</td>\n",
       "      <td>NO</td>\n",
       "      <td>SI</td>\n",
       "      <td>SI</td>\n",
       "      <td>BP</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.45</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>7.00</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>SI</td>\n",
       "      <td>SI</td>\n",
       "      <td>NO</td>\n",
       "      <td>SI</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>12.00</td>\n",
       "      <td>1.58</td>\n",
       "      <td>0.95</td>\n",
       "      <td>SI</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21.00</td>\n",
       "      <td>1,126.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00006003826200_08_2021</td>\n",
       "      <td>00006003826200</td>\n",
       "      <td>1001000000000000000000000000000000001100100110...</td>\n",
       "      <td>P</td>\n",
       "      <td>BP</td>\n",
       "      <td>None</td>\n",
       "      <td>Liguria</td>\n",
       "      <td>ALPTEL</td>\n",
       "      <td>REPEAT BUSINESS</td>\n",
       "      <td>FIDELIZ.OTTIMI</td>\n",
       "      <td>2021-08-10</td>\n",
       "      <td>2021-09-05</td>\n",
       "      <td>2016-05-12</td>\n",
       "      <td>2021-06-09</td>\n",
       "      <td>None</td>\n",
       "      <td>01/01/0001</td>\n",
       "      <td>2021-04-12</td>\n",
       "      <td>2021-05-12</td>\n",
       "      <td>2021-06-12</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-04-16</td>\n",
       "      <td>2021-06-09</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-08-12</td>\n",
       "      <td>2021-07-12</td>\n",
       "      <td>14,234.40</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8,636.46</td>\n",
       "      <td>0000000018350</td>\n",
       "      <td>0000000018350</td>\n",
       "      <td>0000000018350</td>\n",
       "      <td>120.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>62.00</td>\n",
       "      <td>63.00</td>\n",
       "      <td>59.00</td>\n",
       "      <td>60.00</td>\n",
       "      <td>61.00</td>\n",
       "      <td>2</td>\n",
       "      <td>IM</td>\n",
       "      <td>183.50</td>\n",
       "      <td>183.50</td>\n",
       "      <td>183.50</td>\n",
       "      <td>183.50</td>\n",
       "      <td>183.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>183.50</td>\n",
       "      <td>1.56</td>\n",
       "      <td>12.00</td>\n",
       "      <td>197.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>183.50</td>\n",
       "      <td>1.56</td>\n",
       "      <td>12.00</td>\n",
       "      <td>197.06</td>\n",
       "      <td>Y</td>\n",
       "      <td>0.00</td>\n",
       "      <td>379.00</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>BP</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.53</td>\n",
       "      <td>-2.00</td>\n",
       "      <td>Donna</td>\n",
       "      <td>59.00</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>10.00</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>SI</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>SI</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.15</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29.00</td>\n",
       "      <td>1,887.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00002286163301_08_2021</td>\n",
       "      <td>00002286163301</td>\n",
       "      <td>111010</td>\n",
       "      <td>P</td>\n",
       "      <td>BP</td>\n",
       "      <td>None</td>\n",
       "      <td>Calabria</td>\n",
       "      <td>ALPTEL</td>\n",
       "      <td>ELETTRONICA &amp; ELETTRODOME</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-08-10</td>\n",
       "      <td>2021-09-05</td>\n",
       "      <td>2021-02-10</td>\n",
       "      <td>2021-06-14</td>\n",
       "      <td>None</td>\n",
       "      <td>01/01/0001</td>\n",
       "      <td>2021-04-12</td>\n",
       "      <td>2021-05-12</td>\n",
       "      <td>2021-06-12</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-04-12</td>\n",
       "      <td>2021-06-14</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-08-12</td>\n",
       "      <td>2021-07-12</td>\n",
       "      <td>1,500.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1,329.19</td>\n",
       "      <td>0000000005650</td>\n",
       "      <td>0000000005650</td>\n",
       "      <td>0000000005650</td>\n",
       "      <td>30.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>2</td>\n",
       "      <td>KR</td>\n",
       "      <td>56.50</td>\n",
       "      <td>56.50</td>\n",
       "      <td>56.50</td>\n",
       "      <td>56.50</td>\n",
       "      <td>56.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>56.50</td>\n",
       "      <td>0.58</td>\n",
       "      <td>15.97</td>\n",
       "      <td>73.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>56.50</td>\n",
       "      <td>0.58</td>\n",
       "      <td>15.97</td>\n",
       "      <td>73.05</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00</td>\n",
       "      <td>144.94</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>BP</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-2.00</td>\n",
       "      <td>Donna</td>\n",
       "      <td>57.00</td>\n",
       "      <td>SI</td>\n",
       "      <td>SI</td>\n",
       "      <td>2.50</td>\n",
       "      <td>SI</td>\n",
       "      <td>NO</td>\n",
       "      <td>SI</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>SI</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.57</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29.00</td>\n",
       "      <td>152.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00001576642301_08_2021</td>\n",
       "      <td>00001576642301</td>\n",
       "      <td>10012101111101110000000</td>\n",
       "      <td>P</td>\n",
       "      <td>BP</td>\n",
       "      <td>None</td>\n",
       "      <td>Sicilia</td>\n",
       "      <td>ALPTEL</td>\n",
       "      <td>REPEAT BUSINESS</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-08-10</td>\n",
       "      <td>2021-09-05</td>\n",
       "      <td>2019-09-12</td>\n",
       "      <td>2021-05-27</td>\n",
       "      <td>None</td>\n",
       "      <td>01/01/0001</td>\n",
       "      <td>2021-04-12</td>\n",
       "      <td>2021-05-12</td>\n",
       "      <td>2021-06-12</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-04-26</td>\n",
       "      <td>2021-05-27</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-08-12</td>\n",
       "      <td>2021-07-12</td>\n",
       "      <td>14,280.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10,954.69</td>\n",
       "      <td>0000000026000</td>\n",
       "      <td>0000000026000</td>\n",
       "      <td>0000000026000</td>\n",
       "      <td>72.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>22.00</td>\n",
       "      <td>23.00</td>\n",
       "      <td>19.00</td>\n",
       "      <td>20.00</td>\n",
       "      <td>21.00</td>\n",
       "      <td>2</td>\n",
       "      <td>EN</td>\n",
       "      <td>260.00</td>\n",
       "      <td>260.00</td>\n",
       "      <td>260.00</td>\n",
       "      <td>260.00</td>\n",
       "      <td>260.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>260.00</td>\n",
       "      <td>16.20</td>\n",
       "      <td>12.00</td>\n",
       "      <td>288.20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>260.00</td>\n",
       "      <td>16.20</td>\n",
       "      <td>12.00</td>\n",
       "      <td>288.20</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00</td>\n",
       "      <td>532.00</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>BP</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.32</td>\n",
       "      <td>-2.00</td>\n",
       "      <td>Uomo</td>\n",
       "      <td>76.00</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>6.00</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>SI</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>SI</td>\n",
       "      <td>SI</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.41</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29.00</td>\n",
       "      <td>669.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00002230493301_08_2021</td>\n",
       "      <td>00002230493301</td>\n",
       "      <td>2111210</td>\n",
       "      <td>P</td>\n",
       "      <td>BP</td>\n",
       "      <td>None</td>\n",
       "      <td>Lazio</td>\n",
       "      <td>ALPTEL</td>\n",
       "      <td>SPESE MEDICHE</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-08-09</td>\n",
       "      <td>2021-09-05</td>\n",
       "      <td>2021-01-13</td>\n",
       "      <td>2021-07-27</td>\n",
       "      <td>None</td>\n",
       "      <td>01/01/0001</td>\n",
       "      <td>2021-04-12</td>\n",
       "      <td>2021-05-12</td>\n",
       "      <td>2021-06-12</td>\n",
       "      <td>2021-07-27</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-04-14</td>\n",
       "      <td>2021-07-27</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-08-12</td>\n",
       "      <td>2021-07-12</td>\n",
       "      <td>6,800.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6,214.51</td>\n",
       "      <td>0000000016600</td>\n",
       "      <td>0000000016600</td>\n",
       "      <td>0000000016600</td>\n",
       "      <td>48.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>7.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>2</td>\n",
       "      <td>RM</td>\n",
       "      <td>166.00</td>\n",
       "      <td>166.00</td>\n",
       "      <td>166.00</td>\n",
       "      <td>166.00</td>\n",
       "      <td>166.00</td>\n",
       "      <td>2.34</td>\n",
       "      <td>0.00</td>\n",
       "      <td>163.66</td>\n",
       "      <td>3.51</td>\n",
       "      <td>32.40</td>\n",
       "      <td>199.57</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>163.66</td>\n",
       "      <td>3.51</td>\n",
       "      <td>32.40</td>\n",
       "      <td>199.57</td>\n",
       "      <td>None</td>\n",
       "      <td>2.34</td>\n",
       "      <td>396.80</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>BP</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.15</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>Uomo</td>\n",
       "      <td>57.00</td>\n",
       "      <td>SI</td>\n",
       "      <td>SI</td>\n",
       "      <td>4.00</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>SI</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>6.00</td>\n",
       "      <td>1.14</td>\n",
       "      <td>0.92</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28.00</td>\n",
       "      <td>180.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148458</th>\n",
       "      <td>00002785945301_06_2023</td>\n",
       "      <td>00002785945301</td>\n",
       "      <td>11011101100000</td>\n",
       "      <td>R</td>\n",
       "      <td>RI</td>\n",
       "      <td>None</td>\n",
       "      <td>Lazio</td>\n",
       "      <td>ALPTEX</td>\n",
       "      <td>DISINTERMEDIATO AUTOMOTIV</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-06-29</td>\n",
       "      <td>None</td>\n",
       "      <td>2022-04-14</td>\n",
       "      <td>2023-05-24</td>\n",
       "      <td>None</td>\n",
       "      <td>01/01/0001</td>\n",
       "      <td>2023-03-14</td>\n",
       "      <td>2023-04-14</td>\n",
       "      <td>2023-05-14</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-03-24</td>\n",
       "      <td>2023-05-24</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-07-14</td>\n",
       "      <td>2023-06-14</td>\n",
       "      <td>4,120.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3,169.02</td>\n",
       "      <td>0000000010600</td>\n",
       "      <td>0000000010600</td>\n",
       "      <td>0000000010600</td>\n",
       "      <td>48.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>14.00</td>\n",
       "      <td>15.00</td>\n",
       "      <td>11.00</td>\n",
       "      <td>12.00</td>\n",
       "      <td>13.00</td>\n",
       "      <td>2</td>\n",
       "      <td>RM</td>\n",
       "      <td>106.00</td>\n",
       "      <td>106.00</td>\n",
       "      <td>106.00</td>\n",
       "      <td>106.00</td>\n",
       "      <td>106.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>106.00</td>\n",
       "      <td>2.17</td>\n",
       "      <td>0.00</td>\n",
       "      <td>108.17</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>106.00</td>\n",
       "      <td>2.17</td>\n",
       "      <td>0.00</td>\n",
       "      <td>108.17</td>\n",
       "      <td>Y</td>\n",
       "      <td>0.00</td>\n",
       "      <td>246.40</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>SI</td>\n",
       "      <td>RI</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.31</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>4.00</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>SI</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>SI</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.39</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15.00</td>\n",
       "      <td>426.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148459</th>\n",
       "      <td>00002471469301_06_2023</td>\n",
       "      <td>00002471469301</td>\n",
       "      <td>110000000000000000000000</td>\n",
       "      <td>R</td>\n",
       "      <td>RI</td>\n",
       "      <td>None</td>\n",
       "      <td>Umbria</td>\n",
       "      <td>ALPTEX</td>\n",
       "      <td>PRESTITO PERSONALE</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-06-29</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-07-15</td>\n",
       "      <td>2023-05-24</td>\n",
       "      <td>None</td>\n",
       "      <td>01/01/0001</td>\n",
       "      <td>2023-03-14</td>\n",
       "      <td>2023-04-14</td>\n",
       "      <td>2023-05-14</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-03-14</td>\n",
       "      <td>2023-05-24</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-07-14</td>\n",
       "      <td>2023-06-14</td>\n",
       "      <td>6,250.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4,725.40</td>\n",
       "      <td>0000000011600</td>\n",
       "      <td>0000000011600</td>\n",
       "      <td>0000000011600</td>\n",
       "      <td>72.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>23.00</td>\n",
       "      <td>24.00</td>\n",
       "      <td>20.00</td>\n",
       "      <td>21.00</td>\n",
       "      <td>22.00</td>\n",
       "      <td>2</td>\n",
       "      <td>TR</td>\n",
       "      <td>116.00</td>\n",
       "      <td>116.00</td>\n",
       "      <td>116.00</td>\n",
       "      <td>116.00</td>\n",
       "      <td>116.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>116.00</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>116.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>116.00</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>116.85</td>\n",
       "      <td>Y</td>\n",
       "      <td>116.00</td>\n",
       "      <td>116.00</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>SI</td>\n",
       "      <td>RI</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.33</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>6.00</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>SI</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.16</td>\n",
       "      <td>NO</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>15.00</td>\n",
       "      <td>699.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148460</th>\n",
       "      <td>00002324126301_06_2023</td>\n",
       "      <td>00002324126301</td>\n",
       "      <td>1000100000000110000000000000</td>\n",
       "      <td>R</td>\n",
       "      <td>RI</td>\n",
       "      <td>None</td>\n",
       "      <td>Lombardia</td>\n",
       "      <td>ALPTEX</td>\n",
       "      <td>DIPENDENTI AZIENDE DOC -</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-06-29</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-03-15</td>\n",
       "      <td>2022-05-05</td>\n",
       "      <td>None</td>\n",
       "      <td>01/01/0001</td>\n",
       "      <td>2023-03-14</td>\n",
       "      <td>2023-04-14</td>\n",
       "      <td>2023-05-14</td>\n",
       "      <td>2022-05-05</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-03-14</td>\n",
       "      <td>2023-05-14</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-07-14</td>\n",
       "      <td>2023-06-14</td>\n",
       "      <td>14,492.80</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9,057.10</td>\n",
       "      <td>0000000030300</td>\n",
       "      <td>0000000030300</td>\n",
       "      <td>0000000030300</td>\n",
       "      <td>60.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>27.00</td>\n",
       "      <td>28.00</td>\n",
       "      <td>24.00</td>\n",
       "      <td>25.00</td>\n",
       "      <td>26.00</td>\n",
       "      <td>2</td>\n",
       "      <td>VA</td>\n",
       "      <td>303.00</td>\n",
       "      <td>303.00</td>\n",
       "      <td>303.00</td>\n",
       "      <td>303.00</td>\n",
       "      <td>303.00</td>\n",
       "      <td>1.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>301.75</td>\n",
       "      <td>3.66</td>\n",
       "      <td>0.00</td>\n",
       "      <td>305.41</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>301.75</td>\n",
       "      <td>3.66</td>\n",
       "      <td>0.00</td>\n",
       "      <td>305.41</td>\n",
       "      <td>Y</td>\n",
       "      <td>1.25</td>\n",
       "      <td>303.00</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>SI</td>\n",
       "      <td>RI</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.47</td>\n",
       "      <td>-14.00</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>5.00</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>SI</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>SI</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.14</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15.00</td>\n",
       "      <td>821.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148461</th>\n",
       "      <td>00001908346301_06_2023</td>\n",
       "      <td>00001908346301</td>\n",
       "      <td>10000000000000010000000000000000111000</td>\n",
       "      <td>R</td>\n",
       "      <td>RI</td>\n",
       "      <td>None</td>\n",
       "      <td>Lombardia</td>\n",
       "      <td>ALPTEX</td>\n",
       "      <td>SPESE MEDICHE</td>\n",
       "      <td>CURE ODONTOIATRICHE</td>\n",
       "      <td>2023-06-29</td>\n",
       "      <td>None</td>\n",
       "      <td>2020-05-15</td>\n",
       "      <td>2020-10-16</td>\n",
       "      <td>None</td>\n",
       "      <td>01/01/0001</td>\n",
       "      <td>2023-03-14</td>\n",
       "      <td>2023-04-14</td>\n",
       "      <td>2023-05-14</td>\n",
       "      <td>2020-10-16</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-03-14</td>\n",
       "      <td>2023-05-14</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-07-14</td>\n",
       "      <td>2023-06-14</td>\n",
       "      <td>6,654.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3,828.21</td>\n",
       "      <td>0000000012200</td>\n",
       "      <td>0000000012200</td>\n",
       "      <td>0000000012200</td>\n",
       "      <td>70.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>35.00</td>\n",
       "      <td>36.00</td>\n",
       "      <td>32.00</td>\n",
       "      <td>33.00</td>\n",
       "      <td>34.00</td>\n",
       "      <td>2</td>\n",
       "      <td>MI</td>\n",
       "      <td>122.00</td>\n",
       "      <td>122.00</td>\n",
       "      <td>122.00</td>\n",
       "      <td>122.00</td>\n",
       "      <td>122.00</td>\n",
       "      <td>26.24</td>\n",
       "      <td>0.00</td>\n",
       "      <td>95.76</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>95.76</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>95.76</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>95.76</td>\n",
       "      <td>Y</td>\n",
       "      <td>26.24</td>\n",
       "      <td>122.00</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>RI</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.51</td>\n",
       "      <td>-32.00</td>\n",
       "      <td>Uomo</td>\n",
       "      <td>61.00</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>5.83</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>SI</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.08</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15.00</td>\n",
       "      <td>1,125.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148462</th>\n",
       "      <td>00002905333301_06_2023</td>\n",
       "      <td>00002905333301</td>\n",
       "      <td>10000000000</td>\n",
       "      <td>R</td>\n",
       "      <td>RI</td>\n",
       "      <td>None</td>\n",
       "      <td>Lazio</td>\n",
       "      <td>ALPTEX</td>\n",
       "      <td>AUTOMOTIVE USATO</td>\n",
       "      <td>forfour 2as. (W453) FW098</td>\n",
       "      <td>2023-06-29</td>\n",
       "      <td>None</td>\n",
       "      <td>2022-07-15</td>\n",
       "      <td>2023-05-14</td>\n",
       "      <td>None</td>\n",
       "      <td>01/01/0001</td>\n",
       "      <td>2023-03-14</td>\n",
       "      <td>2023-04-14</td>\n",
       "      <td>2023-05-14</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-03-14</td>\n",
       "      <td>2023-05-14</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-07-14</td>\n",
       "      <td>2023-06-14</td>\n",
       "      <td>7,009.40</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6,037.66</td>\n",
       "      <td>0000000014300</td>\n",
       "      <td>0000000014300</td>\n",
       "      <td>0000000014300</td>\n",
       "      <td>60.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11.00</td>\n",
       "      <td>12.00</td>\n",
       "      <td>8.00</td>\n",
       "      <td>9.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>2</td>\n",
       "      <td>RM</td>\n",
       "      <td>143.00</td>\n",
       "      <td>143.00</td>\n",
       "      <td>143.00</td>\n",
       "      <td>143.00</td>\n",
       "      <td>143.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>143.00</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.00</td>\n",
       "      <td>143.48</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>143.00</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.00</td>\n",
       "      <td>143.48</td>\n",
       "      <td>Y</td>\n",
       "      <td>0.00</td>\n",
       "      <td>171.95</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>RI</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-2.00</td>\n",
       "      <td>Donna</td>\n",
       "      <td>53.00</td>\n",
       "      <td>SI</td>\n",
       "      <td>NO</td>\n",
       "      <td>5.00</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>SI</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.09</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15.00</td>\n",
       "      <td>334.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>148463 rows × 93 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Chiave         PRATICA  \\\n",
       "0       00000854016301_08_2021  00000854016301   \n",
       "1       00006003826200_08_2021  00006003826200   \n",
       "2       00002286163301_08_2021  00002286163301   \n",
       "3       00001576642301_08_2021  00001576642301   \n",
       "4       00002230493301_08_2021  00002230493301   \n",
       "...                        ...             ...   \n",
       "148458  00002785945301_06_2023  00002785945301   \n",
       "148459  00002471469301_06_2023  00002471469301   \n",
       "148460  00002324126301_06_2023  00002324126301   \n",
       "148461  00001908346301_06_2023  00001908346301   \n",
       "148462  00002905333301_06_2023  00002905333301   \n",
       "\n",
       "                                            DELIN_HISTORY MOD_PAGAM MOD_PAG  \\\n",
       "0                  13221222111111143332432332111100000000         P      BP   \n",
       "1       1001000000000000000000000000000000001100100110...         P      BP   \n",
       "2                                                  111010         P      BP   \n",
       "3                                 10012101111101110000000         P      BP   \n",
       "4                                                 2111210         P      BP   \n",
       "...                                                   ...       ...     ...   \n",
       "148458                                     11011101100000         R      RI   \n",
       "148459                           110000000000000000000000         R      RI   \n",
       "148460                       1000100000000110000000000000         R      RI   \n",
       "148461             10000000000000010000000000000000111000         R      RI   \n",
       "148462                                        10000000000         R      RI   \n",
       "\n",
       "       TIPO_GARANTE Denominazione Regione CODICE_PHONIA  \\\n",
       "0                 C                 Lazio        ALPTEL   \n",
       "1              None               Liguria        ALPTEL   \n",
       "2              None              Calabria        ALPTEL   \n",
       "3              None               Sicilia        ALPTEL   \n",
       "4              None                 Lazio        ALPTEL   \n",
       "...             ...                   ...           ...   \n",
       "148458         None                 Lazio        ALPTEX   \n",
       "148459         None                Umbria        ALPTEX   \n",
       "148460         None             Lombardia        ALPTEX   \n",
       "148461         None             Lombardia        ALPTEX   \n",
       "148462         None                 Lazio        ALPTEX   \n",
       "\n",
       "                     DES_PRODOTTO                   DES_BENE DATA_AFFIDO  \\\n",
       "0              PRESTITO PERSONALE                       None  2021-08-02   \n",
       "1                 REPEAT BUSINESS             FIDELIZ.OTTIMI  2021-08-10   \n",
       "2       ELETTRONICA & ELETTRODOME                       None  2021-08-10   \n",
       "3                 REPEAT BUSINESS                       None  2021-08-10   \n",
       "4                   SPESE MEDICHE                       None  2021-08-09   \n",
       "...                           ...                        ...         ...   \n",
       "148458  DISINTERMEDIATO AUTOMOTIV                       None  2023-06-29   \n",
       "148459         PRESTITO PERSONALE                       None  2023-06-29   \n",
       "148460   DIPENDENTI AZIENDE DOC -                       None  2023-06-29   \n",
       "148461              SPESE MEDICHE        CURE ODONTOIATRICHE  2023-06-29   \n",
       "148462           AUTOMOTIVE USATO  forfour 2as. (W453) FW098  2023-06-29   \n",
       "\n",
       "       DATA_FINE_AFFIDO DATA_FINANZIAM DT_VAL_ULT_PAGAM DATA_ESTINZ  \\\n",
       "0            2021-09-05     2018-06-12       2021-07-05        None   \n",
       "1            2021-09-05     2016-05-12       2021-06-09        None   \n",
       "2            2021-09-05     2021-02-10       2021-06-14        None   \n",
       "3            2021-09-05     2019-09-12       2021-05-27        None   \n",
       "4            2021-09-05     2021-01-13       2021-07-27        None   \n",
       "...                 ...            ...              ...         ...   \n",
       "148458             None     2022-04-14       2023-05-24        None   \n",
       "148459             None     2021-07-15       2023-05-24        None   \n",
       "148460             None     2021-03-15       2022-05-05        None   \n",
       "148461             None     2020-05-15       2020-10-16        None   \n",
       "148462             None     2022-07-15       2023-05-14        None   \n",
       "\n",
       "       DATA_AGGIORNAMENTO P_INST1_DUE_DATE P_INST2_DUE_DATE P_INST3_DUE_DATE  \\\n",
       "0              01/01/0001       2021-04-12       2021-05-12       2021-06-12   \n",
       "1              01/01/0001       2021-04-12       2021-05-12       2021-06-12   \n",
       "2              01/01/0001       2021-04-12       2021-05-12       2021-06-12   \n",
       "3              01/01/0001       2021-04-12       2021-05-12       2021-06-12   \n",
       "4              01/01/0001       2021-04-12       2021-05-12       2021-06-12   \n",
       "...                   ...              ...              ...              ...   \n",
       "148458         01/01/0001       2023-03-14       2023-04-14       2023-05-14   \n",
       "148459         01/01/0001       2023-03-14       2023-04-14       2023-05-14   \n",
       "148460         01/01/0001       2023-03-14       2023-04-14       2023-05-14   \n",
       "148461         01/01/0001       2023-03-14       2023-04-14       2023-05-14   \n",
       "148462         01/01/0001       2023-03-14       2023-04-14       2023-05-14   \n",
       "\n",
       "       DT_VAL_RAT_IMP1 DT_VAL_RAT_IMP2 P_INST1_VAL_DATE P_INST3_VAL_DATE  \\\n",
       "0                 None            None       2021-06-16       2021-07-05   \n",
       "1                 None            None       2021-04-16       2021-06-09   \n",
       "2                 None            None       2021-04-12       2021-06-14   \n",
       "3                 None            None       2021-04-26       2021-05-27   \n",
       "4           2021-07-27            None       2021-04-14       2021-07-27   \n",
       "...                ...             ...              ...              ...   \n",
       "148458            None            None       2023-03-24       2023-05-24   \n",
       "148459            None            None       2023-03-14       2023-05-24   \n",
       "148460      2022-05-05            None       2023-03-14       2023-05-14   \n",
       "148461      2020-10-16            None       2023-03-14       2023-05-14   \n",
       "148462            None            None       2023-03-14       2023-05-14   \n",
       "\n",
       "       SCAD_MAXIRATA SCAD_RAT_IMP2 SCAD_RAT_IMP1  IMP_FINANZIATO  \\\n",
       "0               None    2021-08-12    2021-07-12       15,200.00   \n",
       "1               None    2021-08-12    2021-07-12       14,234.40   \n",
       "2               None    2021-08-12    2021-07-12        1,500.00   \n",
       "3               None    2021-08-12    2021-07-12       14,280.00   \n",
       "4               None    2021-08-12    2021-07-12        6,800.00   \n",
       "...              ...           ...           ...             ...   \n",
       "148458          None    2023-07-14    2023-06-14        4,120.88   \n",
       "148459          None    2023-07-14    2023-06-14        6,250.00   \n",
       "148460          None    2023-07-14    2023-06-14       14,492.80   \n",
       "148461          None    2023-07-14    2023-06-14        6,654.00   \n",
       "148462          None    2023-07-14    2023-06-14        7,009.40   \n",
       "\n",
       "        IMP_MAXIRATA  CURRENT_BALANCE P_INST1_AMOUNT_PAID P_INST2_AMOUNT_PAID  \\\n",
       "0               0.00         9,770.58       0000000023700       0000000023700   \n",
       "1               0.00         8,636.46       0000000018350       0000000018350   \n",
       "2               0.00         1,329.19       0000000005650       0000000005650   \n",
       "3               0.00        10,954.69       0000000026000       0000000026000   \n",
       "4               0.00         6,214.51       0000000016600       0000000016600   \n",
       "...              ...              ...                 ...                 ...   \n",
       "148458          0.00         3,169.02       0000000010600       0000000010600   \n",
       "148459          0.00         4,725.40       0000000011600       0000000011600   \n",
       "148460          0.00         9,057.10       0000000030300       0000000030300   \n",
       "148461          0.00         3,828.21       0000000012200       0000000012200   \n",
       "148462          0.00         6,037.66       0000000014300       0000000014300   \n",
       "\n",
       "       P_INST3_AMOUNT_PAID  RATE_TOTALI  NUM_RATE_RIFI  NUM_RAT_IMP1  \\\n",
       "0            0000000023700        84.00           0.00         37.00   \n",
       "1            0000000018350       120.00           0.00         62.00   \n",
       "2            0000000005650        30.00           0.00          5.00   \n",
       "3            0000000026000        72.00           0.00         22.00   \n",
       "4            0000000016600        48.00           0.00          6.00   \n",
       "...                    ...          ...            ...           ...   \n",
       "148458       0000000010600        48.00           0.00         14.00   \n",
       "148459       0000000011600        72.00           0.00         23.00   \n",
       "148460       0000000030300        60.00           0.00         27.00   \n",
       "148461       0000000012200        70.00           0.00         35.00   \n",
       "148462       0000000014300        60.00           0.00         11.00   \n",
       "\n",
       "        NUM_RAT_IMP2  P_INST1_NUM  P_INST2_NUM  P_INST3_NUM  NUMERO RATE  \\\n",
       "0              38.00        34.00        35.00        36.00            2   \n",
       "1              63.00        59.00        60.00        61.00            2   \n",
       "2               6.00         2.00         3.00         4.00            2   \n",
       "3              23.00        19.00        20.00        21.00            2   \n",
       "4               7.00         3.00         4.00         5.00            2   \n",
       "...              ...          ...          ...          ...          ...   \n",
       "148458         15.00        11.00        12.00        13.00            2   \n",
       "148459         24.00        20.00        21.00        22.00            2   \n",
       "148460         28.00        24.00        25.00        26.00            2   \n",
       "148461         36.00        32.00        33.00        34.00            2   \n",
       "148462         12.00         8.00         9.00        10.00            2   \n",
       "\n",
       "       PROVINCIA  P_INST1_AMOUNT  P_INST2_AMOUNT  P_INST3_AMOUNT  \\\n",
       "0             LT          237.00          237.00          237.00   \n",
       "1             IM          183.50          183.50          183.50   \n",
       "2             KR           56.50           56.50           56.50   \n",
       "3             EN          260.00          260.00          260.00   \n",
       "4             RM          166.00          166.00          166.00   \n",
       "...          ...             ...             ...             ...   \n",
       "148458        RM          106.00          106.00          106.00   \n",
       "148459        TR          116.00          116.00          116.00   \n",
       "148460        VA          303.00          303.00          303.00   \n",
       "148461        MI          122.00          122.00          122.00   \n",
       "148462        RM          143.00          143.00          143.00   \n",
       "\n",
       "        IMP_RAT_IMP1  IMP_RAT_IMP2  IMP_PAG_RAT_IMP1  IMP_PAG_RAT_IMP2  \\\n",
       "0             237.00        237.00              0.00              0.00   \n",
       "1             183.50        183.50              0.00              0.00   \n",
       "2              56.50         56.50              0.00              0.00   \n",
       "3             260.00        260.00              0.00              0.00   \n",
       "4             166.00        166.00              2.34              0.00   \n",
       "...              ...           ...               ...               ...   \n",
       "148458        106.00        106.00              0.00              0.00   \n",
       "148459        116.00        116.00              0.00              0.00   \n",
       "148460        303.00        303.00              1.25              0.00   \n",
       "148461        122.00        122.00             26.24              0.00   \n",
       "148462        143.00        143.00              0.00              0.00   \n",
       "\n",
       "        TOT_RATE_AFF_INIZIO_MAND  TOT_MORA_AFF_INIZIO_MAND  \\\n",
       "0                         237.00                     28.24   \n",
       "1                         183.50                      1.56   \n",
       "2                          56.50                      0.58   \n",
       "3                         260.00                     16.20   \n",
       "4                         163.66                      3.51   \n",
       "...                          ...                       ...   \n",
       "148458                    106.00                      2.17   \n",
       "148459                    116.00                      0.85   \n",
       "148460                    301.75                      3.66   \n",
       "148461                     95.76                      0.00   \n",
       "148462                    143.00                      0.48   \n",
       "\n",
       "        TOT_ONERI_AFF_INIZIO_MAND  TOT_AFF_INIZIO_MAND  TOT_RATE_PAG  \\\n",
       "0                           12.00               277.24          0.00   \n",
       "1                           12.00               197.06          0.00   \n",
       "2                           15.97                73.05          0.00   \n",
       "3                           12.00               288.20          0.00   \n",
       "4                           32.40               199.57          0.00   \n",
       "...                           ...                  ...           ...   \n",
       "148458                       0.00               108.17          0.00   \n",
       "148459                       0.00               116.85          0.00   \n",
       "148460                       0.00               305.41          0.00   \n",
       "148461                       0.00                95.76          0.00   \n",
       "148462                       0.00               143.48          0.00   \n",
       "\n",
       "        TOT_MORA_PAG  TOT_ONERI_PAG  TOT_PAG  TOT_RATE_AFF  TOT_MORA_AFF  \\\n",
       "0               0.00           0.00     0.00        237.00         28.24   \n",
       "1               0.00           0.00     0.00        183.50          1.56   \n",
       "2               0.00           0.00     0.00         56.50          0.58   \n",
       "3               0.00           0.00     0.00        260.00         16.20   \n",
       "4               0.00           0.00     0.00        163.66          3.51   \n",
       "...              ...            ...      ...           ...           ...   \n",
       "148458          0.00           0.00     0.00        106.00          2.17   \n",
       "148459          0.00           0.00     0.00        116.00          0.85   \n",
       "148460          0.00           0.00     0.00        301.75          3.66   \n",
       "148461          0.00           0.00     0.00         95.76          0.00   \n",
       "148462          0.00           0.00     0.00        143.00          0.48   \n",
       "\n",
       "        TOT_ONERI_AFF  TOT_AFF Flag_Riciclo_SDD  TOT REC  TOT AFF  \\\n",
       "0               12.00   277.24             None     0.00   486.00   \n",
       "1               12.00   197.06                Y     0.00   379.00   \n",
       "2               15.97    73.05             None     0.00   144.94   \n",
       "3               12.00   288.20             None     0.00   532.00   \n",
       "4               32.40   199.57             None     2.34   396.80   \n",
       "...               ...      ...              ...      ...      ...   \n",
       "148458           0.00   108.17                Y     0.00   246.40   \n",
       "148459           0.00   116.85                Y   116.00   116.00   \n",
       "148460           0.00   305.41                Y     1.25   303.00   \n",
       "148461           0.00    95.76                Y    26.24   122.00   \n",
       "148462           0.00   143.48                Y     0.00   171.95   \n",
       "\n",
       "       Flag_Rifinanziamento Flag_Garante Flag_Cointestazione Metodo_pagamento  \\\n",
       "0                        NO           SI                  SI               BP   \n",
       "1                        NO           NO                  NO               BP   \n",
       "2                        NO           NO                  NO               BP   \n",
       "3                        NO           NO                  NO               BP   \n",
       "4                        NO           NO                  NO               BP   \n",
       "...                     ...          ...                 ...              ...   \n",
       "148458                   NO           NO                  SI               RI   \n",
       "148459                   NO           NO                  SI               RI   \n",
       "148460                   NO           NO                  SI               RI   \n",
       "148461                   NO           NO                  NO               RI   \n",
       "148462                   NO           NO                  NO               RI   \n",
       "\n",
       "        Ratio_Rate_Imp1  Ratio_Rate_Imp2  DistanzaAffidoUltimoPagamento  \\\n",
       "0                  0.44             0.45                          -1.00   \n",
       "1                  0.52             0.53                          -2.00   \n",
       "2                  0.17             0.20                          -2.00   \n",
       "3                  0.31             0.32                          -2.00   \n",
       "4                  0.12             0.15                          -0.00   \n",
       "...                 ...              ...                            ...   \n",
       "148458             0.29             0.31                          -1.00   \n",
       "148459             0.32             0.33                          -1.00   \n",
       "148460             0.45             0.47                         -14.00   \n",
       "148461             0.50             0.51                         -32.00   \n",
       "148462             0.18             0.20                          -2.00   \n",
       "\n",
       "        Sesso  Eta_Debitore NOSTART12M NOSTART6M  DurataFinanziamento  \\\n",
       "0        None           NaN         NO        NO                 7.00   \n",
       "1       Donna         59.00         NO        NO                10.00   \n",
       "2       Donna         57.00         SI        SI                 2.50   \n",
       "3        Uomo         76.00         NO        NO                 6.00   \n",
       "4        Uomo         57.00         SI        SI                 4.00   \n",
       "...       ...           ...        ...       ...                  ...   \n",
       "148458   None           NaN         NO        NO                 4.00   \n",
       "148459   None           NaN         NO        NO                 6.00   \n",
       "148460   None           NaN         NO        NO                 5.00   \n",
       "148461   Uomo         61.00         NO        NO                 5.83   \n",
       "148462  Donna         53.00         SI        NO                 5.00   \n",
       "\n",
       "       Flag_Galleggiamento_3M Flag_Galleggiamento_6M Flag_Gestione_Prec  \\\n",
       "0                          NO                     NO                 NO   \n",
       "1                          NO                     NO                 SI   \n",
       "2                          SI                     NO                 SI   \n",
       "3                          NO                     NO                 SI   \n",
       "4                          NO                     NO                 NO   \n",
       "...                       ...                    ...                ...   \n",
       "148458                     NO                     NO                 SI   \n",
       "148459                     NO                     NO                 NO   \n",
       "148460                     NO                     NO                 SI   \n",
       "148461                     NO                     NO                 NO   \n",
       "148462                     NO                     NO                 NO   \n",
       "\n",
       "       Flag_InsolvenzaGrave_3M Flag_InsolvenzaGrave_6M Flag_Miglioramento_3M  \\\n",
       "0                           SI                      SI                    NO   \n",
       "1                           NO                      NO                    NO   \n",
       "2                           NO                      NO                    NO   \n",
       "3                           NO                      NO                    NO   \n",
       "4                           NO                      NO                    NO   \n",
       "...                        ...                     ...                   ...   \n",
       "148458                      NO                      NO                    NO   \n",
       "148459                      NO                      NO                    NO   \n",
       "148460                      NO                      NO                    NO   \n",
       "148461                      NO                      NO                    NO   \n",
       "148462                      NO                      NO                    NO   \n",
       "\n",
       "       Flag_Rate_piu_uno Flag_rientrototale_6M Flag_Scivolamento_3M  \\\n",
       "0                     SI                    NO                   NO   \n",
       "1                     NO                    SI                   NO   \n",
       "2                     NO                    SI                   NO   \n",
       "3                     SI                    SI                   NO   \n",
       "4                     SI                    NO                   NO   \n",
       "...                  ...                   ...                  ...   \n",
       "148458                NO                    SI                   NO   \n",
       "148459                NO                    SI                   NO   \n",
       "148460                NO                    SI                   NO   \n",
       "148461                NO                    SI                   NO   \n",
       "148462                NO                    SI                   NO   \n",
       "\n",
       "       Flag_Scivolamento_6M  Numero_mesi_rec  Severity_12M  \\\n",
       "0                        NO            12.00          1.58   \n",
       "1                        NO             1.00          0.17   \n",
       "2                        NO             3.00          0.67   \n",
       "3                        NO             1.00          0.83   \n",
       "4                        NO             6.00          1.14   \n",
       "...                     ...              ...           ...   \n",
       "148458                   NO             2.00          0.58   \n",
       "148459                   NO             2.00          0.17   \n",
       "148460                   NO             1.00          0.17   \n",
       "148461                   NO             1.00          0.08   \n",
       "148462                   NO             1.00          0.09   \n",
       "\n",
       "        Severity_12M_pesata Flag_InsolvenzaGrave_12M Target_95 FlagRecOver100  \\\n",
       "0                      0.95                       SI         0              0   \n",
       "1                      0.15                       NO         0              0   \n",
       "2                      0.57                       NO         0              0   \n",
       "3                      0.41                       NO         0              0   \n",
       "4                      0.92                       NO         0              0   \n",
       "...                     ...                      ...       ...            ...   \n",
       "148458                 0.39                       NO         0              0   \n",
       "148459                 0.16                       NO         1              0   \n",
       "148460                 0.14                       NO         0              0   \n",
       "148461                 0.08                       NO         0              0   \n",
       "148462                 0.09                       NO         0              0   \n",
       "\n",
       "        AgeingGestioneGg  AgeingErogazioneGg  \n",
       "0                  21.00            1,126.00  \n",
       "1                  29.00            1,887.00  \n",
       "2                  29.00              152.00  \n",
       "3                  29.00              669.00  \n",
       "4                  28.00              180.00  \n",
       "...                  ...                 ...  \n",
       "148458             15.00              426.00  \n",
       "148459             15.00              699.00  \n",
       "148460             15.00              821.00  \n",
       "148461             15.00            1,125.00  \n",
       "148462             15.00              334.00  \n",
       "\n",
       "[148463 rows x 93 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clienti"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6> Ricalcolo la colonna Target_95 perchè ha dei valori mancanti </h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clienti['Target_95'] = (df_clienti['TOT REC'] / df_clienti['TOT AFF'] >= 0.95).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AgeingErogazioneGg',\n",
       " 'AgeingGestioneGg',\n",
       " 'CODICE_PHONIA',\n",
       " 'CURRENT_BALANCE',\n",
       " 'Chiave',\n",
       " 'DATA_AFFIDO',\n",
       " 'DATA_AGGIORNAMENTO',\n",
       " 'DATA_ESTINZ',\n",
       " 'DATA_FINANZIAM',\n",
       " 'DATA_FINE_AFFIDO',\n",
       " 'DELIN_HISTORY',\n",
       " 'DES_BENE',\n",
       " 'DES_PRODOTTO',\n",
       " 'DT_VAL_RAT_IMP1',\n",
       " 'DT_VAL_RAT_IMP2',\n",
       " 'DT_VAL_ULT_PAGAM',\n",
       " 'Denominazione Regione',\n",
       " 'DistanzaAffidoUltimoPagamento',\n",
       " 'DurataFinanziamento',\n",
       " 'Eta_Debitore',\n",
       " 'FlagRecOver100',\n",
       " 'Flag_Cointestazione',\n",
       " 'Flag_Galleggiamento_3M',\n",
       " 'Flag_Galleggiamento_6M',\n",
       " 'Flag_Garante',\n",
       " 'Flag_Gestione_Prec',\n",
       " 'Flag_InsolvenzaGrave_12M',\n",
       " 'Flag_InsolvenzaGrave_3M',\n",
       " 'Flag_InsolvenzaGrave_6M',\n",
       " 'Flag_Miglioramento_3M',\n",
       " 'Flag_Rate_piu_uno',\n",
       " 'Flag_Riciclo_SDD',\n",
       " 'Flag_Rifinanziamento',\n",
       " 'Flag_Scivolamento_3M',\n",
       " 'Flag_Scivolamento_6M',\n",
       " 'Flag_rientrototale_6M',\n",
       " 'IMP_FINANZIATO',\n",
       " 'IMP_MAXIRATA',\n",
       " 'IMP_PAG_RAT_IMP1',\n",
       " 'IMP_PAG_RAT_IMP2',\n",
       " 'IMP_RAT_IMP1',\n",
       " 'IMP_RAT_IMP2',\n",
       " 'MOD_PAG',\n",
       " 'MOD_PAGAM',\n",
       " 'Metodo_pagamento',\n",
       " 'NOSTART12M',\n",
       " 'NOSTART6M',\n",
       " 'NUMERO RATE',\n",
       " 'NUM_RATE_RIFI',\n",
       " 'NUM_RAT_IMP1',\n",
       " 'NUM_RAT_IMP2',\n",
       " 'Numero_mesi_rec',\n",
       " 'PRATICA',\n",
       " 'PROVINCIA',\n",
       " 'P_INST1_AMOUNT',\n",
       " 'P_INST1_AMOUNT_PAID',\n",
       " 'P_INST1_DUE_DATE',\n",
       " 'P_INST1_NUM',\n",
       " 'P_INST1_VAL_DATE',\n",
       " 'P_INST2_AMOUNT',\n",
       " 'P_INST2_AMOUNT_PAID',\n",
       " 'P_INST2_DUE_DATE',\n",
       " 'P_INST2_NUM',\n",
       " 'P_INST3_AMOUNT',\n",
       " 'P_INST3_AMOUNT_PAID',\n",
       " 'P_INST3_DUE_DATE',\n",
       " 'P_INST3_NUM',\n",
       " 'P_INST3_VAL_DATE',\n",
       " 'RATE_TOTALI',\n",
       " 'Ratio_Rate_Imp1',\n",
       " 'Ratio_Rate_Imp2',\n",
       " 'SCAD_MAXIRATA',\n",
       " 'SCAD_RAT_IMP1',\n",
       " 'SCAD_RAT_IMP2',\n",
       " 'Sesso',\n",
       " 'Severity_12M',\n",
       " 'Severity_12M_pesata',\n",
       " 'TIPO_GARANTE',\n",
       " 'TOT AFF',\n",
       " 'TOT REC',\n",
       " 'TOT_AFF',\n",
       " 'TOT_AFF_INIZIO_MAND',\n",
       " 'TOT_MORA_AFF',\n",
       " 'TOT_MORA_AFF_INIZIO_MAND',\n",
       " 'TOT_MORA_PAG',\n",
       " 'TOT_ONERI_AFF',\n",
       " 'TOT_ONERI_AFF_INIZIO_MAND',\n",
       " 'TOT_ONERI_PAG',\n",
       " 'TOT_PAG',\n",
       " 'TOT_RATE_AFF',\n",
       " 'TOT_RATE_AFF_INIZIO_MAND',\n",
       " 'TOT_RATE_PAG',\n",
       " 'Target_95']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(df_clienti.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clienti['Target_95'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(148463, 93)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clienti.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definiamo funzione per spostare le colonne all'inizio del dataset\n",
    "def move_column_inplace(df, col, pos):\n",
    "    col = df.pop(col)\n",
    "    df.insert(pos, col.name, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spostiamo il Target_95 sulla prima colonna\n",
    "move_column_inplace(df_clienti,'Target_95',0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target_95</th>\n",
       "      <th>Chiave</th>\n",
       "      <th>PRATICA</th>\n",
       "      <th>DELIN_HISTORY</th>\n",
       "      <th>MOD_PAGAM</th>\n",
       "      <th>MOD_PAG</th>\n",
       "      <th>TIPO_GARANTE</th>\n",
       "      <th>Denominazione Regione</th>\n",
       "      <th>CODICE_PHONIA</th>\n",
       "      <th>DES_PRODOTTO</th>\n",
       "      <th>DES_BENE</th>\n",
       "      <th>DATA_AFFIDO</th>\n",
       "      <th>DATA_FINE_AFFIDO</th>\n",
       "      <th>DATA_FINANZIAM</th>\n",
       "      <th>DT_VAL_ULT_PAGAM</th>\n",
       "      <th>DATA_ESTINZ</th>\n",
       "      <th>DATA_AGGIORNAMENTO</th>\n",
       "      <th>P_INST1_DUE_DATE</th>\n",
       "      <th>P_INST2_DUE_DATE</th>\n",
       "      <th>P_INST3_DUE_DATE</th>\n",
       "      <th>DT_VAL_RAT_IMP1</th>\n",
       "      <th>DT_VAL_RAT_IMP2</th>\n",
       "      <th>P_INST1_VAL_DATE</th>\n",
       "      <th>P_INST3_VAL_DATE</th>\n",
       "      <th>SCAD_MAXIRATA</th>\n",
       "      <th>SCAD_RAT_IMP2</th>\n",
       "      <th>SCAD_RAT_IMP1</th>\n",
       "      <th>IMP_FINANZIATO</th>\n",
       "      <th>IMP_MAXIRATA</th>\n",
       "      <th>CURRENT_BALANCE</th>\n",
       "      <th>P_INST1_AMOUNT_PAID</th>\n",
       "      <th>P_INST2_AMOUNT_PAID</th>\n",
       "      <th>P_INST3_AMOUNT_PAID</th>\n",
       "      <th>RATE_TOTALI</th>\n",
       "      <th>NUM_RATE_RIFI</th>\n",
       "      <th>NUM_RAT_IMP1</th>\n",
       "      <th>NUM_RAT_IMP2</th>\n",
       "      <th>P_INST1_NUM</th>\n",
       "      <th>P_INST2_NUM</th>\n",
       "      <th>P_INST3_NUM</th>\n",
       "      <th>NUMERO RATE</th>\n",
       "      <th>PROVINCIA</th>\n",
       "      <th>P_INST1_AMOUNT</th>\n",
       "      <th>P_INST2_AMOUNT</th>\n",
       "      <th>P_INST3_AMOUNT</th>\n",
       "      <th>IMP_RAT_IMP1</th>\n",
       "      <th>IMP_RAT_IMP2</th>\n",
       "      <th>IMP_PAG_RAT_IMP1</th>\n",
       "      <th>IMP_PAG_RAT_IMP2</th>\n",
       "      <th>TOT_RATE_AFF_INIZIO_MAND</th>\n",
       "      <th>TOT_MORA_AFF_INIZIO_MAND</th>\n",
       "      <th>TOT_ONERI_AFF_INIZIO_MAND</th>\n",
       "      <th>TOT_AFF_INIZIO_MAND</th>\n",
       "      <th>TOT_RATE_PAG</th>\n",
       "      <th>TOT_MORA_PAG</th>\n",
       "      <th>TOT_ONERI_PAG</th>\n",
       "      <th>TOT_PAG</th>\n",
       "      <th>TOT_RATE_AFF</th>\n",
       "      <th>TOT_MORA_AFF</th>\n",
       "      <th>TOT_ONERI_AFF</th>\n",
       "      <th>TOT_AFF</th>\n",
       "      <th>Flag_Riciclo_SDD</th>\n",
       "      <th>TOT REC</th>\n",
       "      <th>TOT AFF</th>\n",
       "      <th>Flag_Rifinanziamento</th>\n",
       "      <th>Flag_Garante</th>\n",
       "      <th>Flag_Cointestazione</th>\n",
       "      <th>Metodo_pagamento</th>\n",
       "      <th>Ratio_Rate_Imp1</th>\n",
       "      <th>Ratio_Rate_Imp2</th>\n",
       "      <th>DistanzaAffidoUltimoPagamento</th>\n",
       "      <th>Sesso</th>\n",
       "      <th>Eta_Debitore</th>\n",
       "      <th>NOSTART12M</th>\n",
       "      <th>NOSTART6M</th>\n",
       "      <th>DurataFinanziamento</th>\n",
       "      <th>Flag_Galleggiamento_3M</th>\n",
       "      <th>Flag_Galleggiamento_6M</th>\n",
       "      <th>Flag_Gestione_Prec</th>\n",
       "      <th>Flag_InsolvenzaGrave_3M</th>\n",
       "      <th>Flag_InsolvenzaGrave_6M</th>\n",
       "      <th>Flag_Miglioramento_3M</th>\n",
       "      <th>Flag_Rate_piu_uno</th>\n",
       "      <th>Flag_rientrototale_6M</th>\n",
       "      <th>Flag_Scivolamento_3M</th>\n",
       "      <th>Flag_Scivolamento_6M</th>\n",
       "      <th>Numero_mesi_rec</th>\n",
       "      <th>Severity_12M</th>\n",
       "      <th>Severity_12M_pesata</th>\n",
       "      <th>Flag_InsolvenzaGrave_12M</th>\n",
       "      <th>FlagRecOver100</th>\n",
       "      <th>AgeingGestioneGg</th>\n",
       "      <th>AgeingErogazioneGg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>00000854016301_08_2021</td>\n",
       "      <td>00000854016301</td>\n",
       "      <td>13221222111111143332432332111100000000</td>\n",
       "      <td>P</td>\n",
       "      <td>BP</td>\n",
       "      <td>C</td>\n",
       "      <td>Lazio</td>\n",
       "      <td>ALPTEL</td>\n",
       "      <td>PRESTITO PERSONALE</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-08-02</td>\n",
       "      <td>2021-09-05</td>\n",
       "      <td>2018-06-12</td>\n",
       "      <td>2021-07-05</td>\n",
       "      <td>None</td>\n",
       "      <td>01/01/0001</td>\n",
       "      <td>2021-04-12</td>\n",
       "      <td>2021-05-12</td>\n",
       "      <td>2021-06-12</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-06-16</td>\n",
       "      <td>2021-07-05</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-08-12</td>\n",
       "      <td>2021-07-12</td>\n",
       "      <td>15,200.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9,770.58</td>\n",
       "      <td>0000000023700</td>\n",
       "      <td>0000000023700</td>\n",
       "      <td>0000000023700</td>\n",
       "      <td>84.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>37.00</td>\n",
       "      <td>38.00</td>\n",
       "      <td>34.00</td>\n",
       "      <td>35.00</td>\n",
       "      <td>36.00</td>\n",
       "      <td>2</td>\n",
       "      <td>LT</td>\n",
       "      <td>237.00</td>\n",
       "      <td>237.00</td>\n",
       "      <td>237.00</td>\n",
       "      <td>237.00</td>\n",
       "      <td>237.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>237.00</td>\n",
       "      <td>28.24</td>\n",
       "      <td>12.00</td>\n",
       "      <td>277.24</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>237.00</td>\n",
       "      <td>28.24</td>\n",
       "      <td>12.00</td>\n",
       "      <td>277.24</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00</td>\n",
       "      <td>486.00</td>\n",
       "      <td>NO</td>\n",
       "      <td>SI</td>\n",
       "      <td>SI</td>\n",
       "      <td>BP</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.45</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>7.00</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>SI</td>\n",
       "      <td>SI</td>\n",
       "      <td>NO</td>\n",
       "      <td>SI</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>12.00</td>\n",
       "      <td>1.58</td>\n",
       "      <td>0.95</td>\n",
       "      <td>SI</td>\n",
       "      <td>0</td>\n",
       "      <td>21.00</td>\n",
       "      <td>1,126.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>00006003826200_08_2021</td>\n",
       "      <td>00006003826200</td>\n",
       "      <td>1001000000000000000000000000000000001100100110...</td>\n",
       "      <td>P</td>\n",
       "      <td>BP</td>\n",
       "      <td>None</td>\n",
       "      <td>Liguria</td>\n",
       "      <td>ALPTEL</td>\n",
       "      <td>REPEAT BUSINESS</td>\n",
       "      <td>FIDELIZ.OTTIMI</td>\n",
       "      <td>2021-08-10</td>\n",
       "      <td>2021-09-05</td>\n",
       "      <td>2016-05-12</td>\n",
       "      <td>2021-06-09</td>\n",
       "      <td>None</td>\n",
       "      <td>01/01/0001</td>\n",
       "      <td>2021-04-12</td>\n",
       "      <td>2021-05-12</td>\n",
       "      <td>2021-06-12</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-04-16</td>\n",
       "      <td>2021-06-09</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-08-12</td>\n",
       "      <td>2021-07-12</td>\n",
       "      <td>14,234.40</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8,636.46</td>\n",
       "      <td>0000000018350</td>\n",
       "      <td>0000000018350</td>\n",
       "      <td>0000000018350</td>\n",
       "      <td>120.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>62.00</td>\n",
       "      <td>63.00</td>\n",
       "      <td>59.00</td>\n",
       "      <td>60.00</td>\n",
       "      <td>61.00</td>\n",
       "      <td>2</td>\n",
       "      <td>IM</td>\n",
       "      <td>183.50</td>\n",
       "      <td>183.50</td>\n",
       "      <td>183.50</td>\n",
       "      <td>183.50</td>\n",
       "      <td>183.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>183.50</td>\n",
       "      <td>1.56</td>\n",
       "      <td>12.00</td>\n",
       "      <td>197.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>183.50</td>\n",
       "      <td>1.56</td>\n",
       "      <td>12.00</td>\n",
       "      <td>197.06</td>\n",
       "      <td>Y</td>\n",
       "      <td>0.00</td>\n",
       "      <td>379.00</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>BP</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.53</td>\n",
       "      <td>-2.00</td>\n",
       "      <td>Donna</td>\n",
       "      <td>59.00</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>10.00</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>SI</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>SI</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.15</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>29.00</td>\n",
       "      <td>1,887.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>00002286163301_08_2021</td>\n",
       "      <td>00002286163301</td>\n",
       "      <td>111010</td>\n",
       "      <td>P</td>\n",
       "      <td>BP</td>\n",
       "      <td>None</td>\n",
       "      <td>Calabria</td>\n",
       "      <td>ALPTEL</td>\n",
       "      <td>ELETTRONICA &amp; ELETTRODOME</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-08-10</td>\n",
       "      <td>2021-09-05</td>\n",
       "      <td>2021-02-10</td>\n",
       "      <td>2021-06-14</td>\n",
       "      <td>None</td>\n",
       "      <td>01/01/0001</td>\n",
       "      <td>2021-04-12</td>\n",
       "      <td>2021-05-12</td>\n",
       "      <td>2021-06-12</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-04-12</td>\n",
       "      <td>2021-06-14</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-08-12</td>\n",
       "      <td>2021-07-12</td>\n",
       "      <td>1,500.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1,329.19</td>\n",
       "      <td>0000000005650</td>\n",
       "      <td>0000000005650</td>\n",
       "      <td>0000000005650</td>\n",
       "      <td>30.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>2</td>\n",
       "      <td>KR</td>\n",
       "      <td>56.50</td>\n",
       "      <td>56.50</td>\n",
       "      <td>56.50</td>\n",
       "      <td>56.50</td>\n",
       "      <td>56.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>56.50</td>\n",
       "      <td>0.58</td>\n",
       "      <td>15.97</td>\n",
       "      <td>73.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>56.50</td>\n",
       "      <td>0.58</td>\n",
       "      <td>15.97</td>\n",
       "      <td>73.05</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00</td>\n",
       "      <td>144.94</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>BP</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-2.00</td>\n",
       "      <td>Donna</td>\n",
       "      <td>57.00</td>\n",
       "      <td>SI</td>\n",
       "      <td>SI</td>\n",
       "      <td>2.50</td>\n",
       "      <td>SI</td>\n",
       "      <td>NO</td>\n",
       "      <td>SI</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>SI</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.57</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>29.00</td>\n",
       "      <td>152.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>00001576642301_08_2021</td>\n",
       "      <td>00001576642301</td>\n",
       "      <td>10012101111101110000000</td>\n",
       "      <td>P</td>\n",
       "      <td>BP</td>\n",
       "      <td>None</td>\n",
       "      <td>Sicilia</td>\n",
       "      <td>ALPTEL</td>\n",
       "      <td>REPEAT BUSINESS</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-08-10</td>\n",
       "      <td>2021-09-05</td>\n",
       "      <td>2019-09-12</td>\n",
       "      <td>2021-05-27</td>\n",
       "      <td>None</td>\n",
       "      <td>01/01/0001</td>\n",
       "      <td>2021-04-12</td>\n",
       "      <td>2021-05-12</td>\n",
       "      <td>2021-06-12</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-04-26</td>\n",
       "      <td>2021-05-27</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-08-12</td>\n",
       "      <td>2021-07-12</td>\n",
       "      <td>14,280.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10,954.69</td>\n",
       "      <td>0000000026000</td>\n",
       "      <td>0000000026000</td>\n",
       "      <td>0000000026000</td>\n",
       "      <td>72.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>22.00</td>\n",
       "      <td>23.00</td>\n",
       "      <td>19.00</td>\n",
       "      <td>20.00</td>\n",
       "      <td>21.00</td>\n",
       "      <td>2</td>\n",
       "      <td>EN</td>\n",
       "      <td>260.00</td>\n",
       "      <td>260.00</td>\n",
       "      <td>260.00</td>\n",
       "      <td>260.00</td>\n",
       "      <td>260.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>260.00</td>\n",
       "      <td>16.20</td>\n",
       "      <td>12.00</td>\n",
       "      <td>288.20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>260.00</td>\n",
       "      <td>16.20</td>\n",
       "      <td>12.00</td>\n",
       "      <td>288.20</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00</td>\n",
       "      <td>532.00</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>BP</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.32</td>\n",
       "      <td>-2.00</td>\n",
       "      <td>Uomo</td>\n",
       "      <td>76.00</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>6.00</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>SI</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>SI</td>\n",
       "      <td>SI</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.41</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>29.00</td>\n",
       "      <td>669.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>00002230493301_08_2021</td>\n",
       "      <td>00002230493301</td>\n",
       "      <td>2111210</td>\n",
       "      <td>P</td>\n",
       "      <td>BP</td>\n",
       "      <td>None</td>\n",
       "      <td>Lazio</td>\n",
       "      <td>ALPTEL</td>\n",
       "      <td>SPESE MEDICHE</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-08-09</td>\n",
       "      <td>2021-09-05</td>\n",
       "      <td>2021-01-13</td>\n",
       "      <td>2021-07-27</td>\n",
       "      <td>None</td>\n",
       "      <td>01/01/0001</td>\n",
       "      <td>2021-04-12</td>\n",
       "      <td>2021-05-12</td>\n",
       "      <td>2021-06-12</td>\n",
       "      <td>2021-07-27</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-04-14</td>\n",
       "      <td>2021-07-27</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-08-12</td>\n",
       "      <td>2021-07-12</td>\n",
       "      <td>6,800.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6,214.51</td>\n",
       "      <td>0000000016600</td>\n",
       "      <td>0000000016600</td>\n",
       "      <td>0000000016600</td>\n",
       "      <td>48.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>7.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>2</td>\n",
       "      <td>RM</td>\n",
       "      <td>166.00</td>\n",
       "      <td>166.00</td>\n",
       "      <td>166.00</td>\n",
       "      <td>166.00</td>\n",
       "      <td>166.00</td>\n",
       "      <td>2.34</td>\n",
       "      <td>0.00</td>\n",
       "      <td>163.66</td>\n",
       "      <td>3.51</td>\n",
       "      <td>32.40</td>\n",
       "      <td>199.57</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>163.66</td>\n",
       "      <td>3.51</td>\n",
       "      <td>32.40</td>\n",
       "      <td>199.57</td>\n",
       "      <td>None</td>\n",
       "      <td>2.34</td>\n",
       "      <td>396.80</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>BP</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.15</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>Uomo</td>\n",
       "      <td>57.00</td>\n",
       "      <td>SI</td>\n",
       "      <td>SI</td>\n",
       "      <td>4.00</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>SI</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>6.00</td>\n",
       "      <td>1.14</td>\n",
       "      <td>0.92</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>28.00</td>\n",
       "      <td>180.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148458</th>\n",
       "      <td>0</td>\n",
       "      <td>00002785945301_06_2023</td>\n",
       "      <td>00002785945301</td>\n",
       "      <td>11011101100000</td>\n",
       "      <td>R</td>\n",
       "      <td>RI</td>\n",
       "      <td>None</td>\n",
       "      <td>Lazio</td>\n",
       "      <td>ALPTEX</td>\n",
       "      <td>DISINTERMEDIATO AUTOMOTIV</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-06-29</td>\n",
       "      <td>None</td>\n",
       "      <td>2022-04-14</td>\n",
       "      <td>2023-05-24</td>\n",
       "      <td>None</td>\n",
       "      <td>01/01/0001</td>\n",
       "      <td>2023-03-14</td>\n",
       "      <td>2023-04-14</td>\n",
       "      <td>2023-05-14</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-03-24</td>\n",
       "      <td>2023-05-24</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-07-14</td>\n",
       "      <td>2023-06-14</td>\n",
       "      <td>4,120.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3,169.02</td>\n",
       "      <td>0000000010600</td>\n",
       "      <td>0000000010600</td>\n",
       "      <td>0000000010600</td>\n",
       "      <td>48.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>14.00</td>\n",
       "      <td>15.00</td>\n",
       "      <td>11.00</td>\n",
       "      <td>12.00</td>\n",
       "      <td>13.00</td>\n",
       "      <td>2</td>\n",
       "      <td>RM</td>\n",
       "      <td>106.00</td>\n",
       "      <td>106.00</td>\n",
       "      <td>106.00</td>\n",
       "      <td>106.00</td>\n",
       "      <td>106.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>106.00</td>\n",
       "      <td>2.17</td>\n",
       "      <td>0.00</td>\n",
       "      <td>108.17</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>106.00</td>\n",
       "      <td>2.17</td>\n",
       "      <td>0.00</td>\n",
       "      <td>108.17</td>\n",
       "      <td>Y</td>\n",
       "      <td>0.00</td>\n",
       "      <td>246.40</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>SI</td>\n",
       "      <td>RI</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.31</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>4.00</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>SI</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>SI</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.39</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>15.00</td>\n",
       "      <td>426.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148459</th>\n",
       "      <td>1</td>\n",
       "      <td>00002471469301_06_2023</td>\n",
       "      <td>00002471469301</td>\n",
       "      <td>110000000000000000000000</td>\n",
       "      <td>R</td>\n",
       "      <td>RI</td>\n",
       "      <td>None</td>\n",
       "      <td>Umbria</td>\n",
       "      <td>ALPTEX</td>\n",
       "      <td>PRESTITO PERSONALE</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-06-29</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-07-15</td>\n",
       "      <td>2023-05-24</td>\n",
       "      <td>None</td>\n",
       "      <td>01/01/0001</td>\n",
       "      <td>2023-03-14</td>\n",
       "      <td>2023-04-14</td>\n",
       "      <td>2023-05-14</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-03-14</td>\n",
       "      <td>2023-05-24</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-07-14</td>\n",
       "      <td>2023-06-14</td>\n",
       "      <td>6,250.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4,725.40</td>\n",
       "      <td>0000000011600</td>\n",
       "      <td>0000000011600</td>\n",
       "      <td>0000000011600</td>\n",
       "      <td>72.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>23.00</td>\n",
       "      <td>24.00</td>\n",
       "      <td>20.00</td>\n",
       "      <td>21.00</td>\n",
       "      <td>22.00</td>\n",
       "      <td>2</td>\n",
       "      <td>TR</td>\n",
       "      <td>116.00</td>\n",
       "      <td>116.00</td>\n",
       "      <td>116.00</td>\n",
       "      <td>116.00</td>\n",
       "      <td>116.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>116.00</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>116.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>116.00</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>116.85</td>\n",
       "      <td>Y</td>\n",
       "      <td>116.00</td>\n",
       "      <td>116.00</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>SI</td>\n",
       "      <td>RI</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.33</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>6.00</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>SI</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.16</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>15.00</td>\n",
       "      <td>699.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148460</th>\n",
       "      <td>0</td>\n",
       "      <td>00002324126301_06_2023</td>\n",
       "      <td>00002324126301</td>\n",
       "      <td>1000100000000110000000000000</td>\n",
       "      <td>R</td>\n",
       "      <td>RI</td>\n",
       "      <td>None</td>\n",
       "      <td>Lombardia</td>\n",
       "      <td>ALPTEX</td>\n",
       "      <td>DIPENDENTI AZIENDE DOC -</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-06-29</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-03-15</td>\n",
       "      <td>2022-05-05</td>\n",
       "      <td>None</td>\n",
       "      <td>01/01/0001</td>\n",
       "      <td>2023-03-14</td>\n",
       "      <td>2023-04-14</td>\n",
       "      <td>2023-05-14</td>\n",
       "      <td>2022-05-05</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-03-14</td>\n",
       "      <td>2023-05-14</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-07-14</td>\n",
       "      <td>2023-06-14</td>\n",
       "      <td>14,492.80</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9,057.10</td>\n",
       "      <td>0000000030300</td>\n",
       "      <td>0000000030300</td>\n",
       "      <td>0000000030300</td>\n",
       "      <td>60.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>27.00</td>\n",
       "      <td>28.00</td>\n",
       "      <td>24.00</td>\n",
       "      <td>25.00</td>\n",
       "      <td>26.00</td>\n",
       "      <td>2</td>\n",
       "      <td>VA</td>\n",
       "      <td>303.00</td>\n",
       "      <td>303.00</td>\n",
       "      <td>303.00</td>\n",
       "      <td>303.00</td>\n",
       "      <td>303.00</td>\n",
       "      <td>1.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>301.75</td>\n",
       "      <td>3.66</td>\n",
       "      <td>0.00</td>\n",
       "      <td>305.41</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>301.75</td>\n",
       "      <td>3.66</td>\n",
       "      <td>0.00</td>\n",
       "      <td>305.41</td>\n",
       "      <td>Y</td>\n",
       "      <td>1.25</td>\n",
       "      <td>303.00</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>SI</td>\n",
       "      <td>RI</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.47</td>\n",
       "      <td>-14.00</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>5.00</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>SI</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>SI</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.14</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>15.00</td>\n",
       "      <td>821.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148461</th>\n",
       "      <td>0</td>\n",
       "      <td>00001908346301_06_2023</td>\n",
       "      <td>00001908346301</td>\n",
       "      <td>10000000000000010000000000000000111000</td>\n",
       "      <td>R</td>\n",
       "      <td>RI</td>\n",
       "      <td>None</td>\n",
       "      <td>Lombardia</td>\n",
       "      <td>ALPTEX</td>\n",
       "      <td>SPESE MEDICHE</td>\n",
       "      <td>CURE ODONTOIATRICHE</td>\n",
       "      <td>2023-06-29</td>\n",
       "      <td>None</td>\n",
       "      <td>2020-05-15</td>\n",
       "      <td>2020-10-16</td>\n",
       "      <td>None</td>\n",
       "      <td>01/01/0001</td>\n",
       "      <td>2023-03-14</td>\n",
       "      <td>2023-04-14</td>\n",
       "      <td>2023-05-14</td>\n",
       "      <td>2020-10-16</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-03-14</td>\n",
       "      <td>2023-05-14</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-07-14</td>\n",
       "      <td>2023-06-14</td>\n",
       "      <td>6,654.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3,828.21</td>\n",
       "      <td>0000000012200</td>\n",
       "      <td>0000000012200</td>\n",
       "      <td>0000000012200</td>\n",
       "      <td>70.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>35.00</td>\n",
       "      <td>36.00</td>\n",
       "      <td>32.00</td>\n",
       "      <td>33.00</td>\n",
       "      <td>34.00</td>\n",
       "      <td>2</td>\n",
       "      <td>MI</td>\n",
       "      <td>122.00</td>\n",
       "      <td>122.00</td>\n",
       "      <td>122.00</td>\n",
       "      <td>122.00</td>\n",
       "      <td>122.00</td>\n",
       "      <td>26.24</td>\n",
       "      <td>0.00</td>\n",
       "      <td>95.76</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>95.76</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>95.76</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>95.76</td>\n",
       "      <td>Y</td>\n",
       "      <td>26.24</td>\n",
       "      <td>122.00</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>RI</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.51</td>\n",
       "      <td>-32.00</td>\n",
       "      <td>Uomo</td>\n",
       "      <td>61.00</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>5.83</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>SI</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.08</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>15.00</td>\n",
       "      <td>1,125.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148462</th>\n",
       "      <td>0</td>\n",
       "      <td>00002905333301_06_2023</td>\n",
       "      <td>00002905333301</td>\n",
       "      <td>10000000000</td>\n",
       "      <td>R</td>\n",
       "      <td>RI</td>\n",
       "      <td>None</td>\n",
       "      <td>Lazio</td>\n",
       "      <td>ALPTEX</td>\n",
       "      <td>AUTOMOTIVE USATO</td>\n",
       "      <td>forfour 2as. (W453) FW098</td>\n",
       "      <td>2023-06-29</td>\n",
       "      <td>None</td>\n",
       "      <td>2022-07-15</td>\n",
       "      <td>2023-05-14</td>\n",
       "      <td>None</td>\n",
       "      <td>01/01/0001</td>\n",
       "      <td>2023-03-14</td>\n",
       "      <td>2023-04-14</td>\n",
       "      <td>2023-05-14</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-03-14</td>\n",
       "      <td>2023-05-14</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-07-14</td>\n",
       "      <td>2023-06-14</td>\n",
       "      <td>7,009.40</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6,037.66</td>\n",
       "      <td>0000000014300</td>\n",
       "      <td>0000000014300</td>\n",
       "      <td>0000000014300</td>\n",
       "      <td>60.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11.00</td>\n",
       "      <td>12.00</td>\n",
       "      <td>8.00</td>\n",
       "      <td>9.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>2</td>\n",
       "      <td>RM</td>\n",
       "      <td>143.00</td>\n",
       "      <td>143.00</td>\n",
       "      <td>143.00</td>\n",
       "      <td>143.00</td>\n",
       "      <td>143.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>143.00</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.00</td>\n",
       "      <td>143.48</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>143.00</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.00</td>\n",
       "      <td>143.48</td>\n",
       "      <td>Y</td>\n",
       "      <td>0.00</td>\n",
       "      <td>171.95</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>RI</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-2.00</td>\n",
       "      <td>Donna</td>\n",
       "      <td>53.00</td>\n",
       "      <td>SI</td>\n",
       "      <td>NO</td>\n",
       "      <td>5.00</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>SI</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.09</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>15.00</td>\n",
       "      <td>334.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>148463 rows × 93 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Target_95                  Chiave         PRATICA  \\\n",
       "0               0  00000854016301_08_2021  00000854016301   \n",
       "1               0  00006003826200_08_2021  00006003826200   \n",
       "2               0  00002286163301_08_2021  00002286163301   \n",
       "3               0  00001576642301_08_2021  00001576642301   \n",
       "4               0  00002230493301_08_2021  00002230493301   \n",
       "...           ...                     ...             ...   \n",
       "148458          0  00002785945301_06_2023  00002785945301   \n",
       "148459          1  00002471469301_06_2023  00002471469301   \n",
       "148460          0  00002324126301_06_2023  00002324126301   \n",
       "148461          0  00001908346301_06_2023  00001908346301   \n",
       "148462          0  00002905333301_06_2023  00002905333301   \n",
       "\n",
       "                                            DELIN_HISTORY MOD_PAGAM MOD_PAG  \\\n",
       "0                  13221222111111143332432332111100000000         P      BP   \n",
       "1       1001000000000000000000000000000000001100100110...         P      BP   \n",
       "2                                                  111010         P      BP   \n",
       "3                                 10012101111101110000000         P      BP   \n",
       "4                                                 2111210         P      BP   \n",
       "...                                                   ...       ...     ...   \n",
       "148458                                     11011101100000         R      RI   \n",
       "148459                           110000000000000000000000         R      RI   \n",
       "148460                       1000100000000110000000000000         R      RI   \n",
       "148461             10000000000000010000000000000000111000         R      RI   \n",
       "148462                                        10000000000         R      RI   \n",
       "\n",
       "       TIPO_GARANTE Denominazione Regione CODICE_PHONIA  \\\n",
       "0                 C                 Lazio        ALPTEL   \n",
       "1              None               Liguria        ALPTEL   \n",
       "2              None              Calabria        ALPTEL   \n",
       "3              None               Sicilia        ALPTEL   \n",
       "4              None                 Lazio        ALPTEL   \n",
       "...             ...                   ...           ...   \n",
       "148458         None                 Lazio        ALPTEX   \n",
       "148459         None                Umbria        ALPTEX   \n",
       "148460         None             Lombardia        ALPTEX   \n",
       "148461         None             Lombardia        ALPTEX   \n",
       "148462         None                 Lazio        ALPTEX   \n",
       "\n",
       "                     DES_PRODOTTO                   DES_BENE DATA_AFFIDO  \\\n",
       "0              PRESTITO PERSONALE                       None  2021-08-02   \n",
       "1                 REPEAT BUSINESS             FIDELIZ.OTTIMI  2021-08-10   \n",
       "2       ELETTRONICA & ELETTRODOME                       None  2021-08-10   \n",
       "3                 REPEAT BUSINESS                       None  2021-08-10   \n",
       "4                   SPESE MEDICHE                       None  2021-08-09   \n",
       "...                           ...                        ...         ...   \n",
       "148458  DISINTERMEDIATO AUTOMOTIV                       None  2023-06-29   \n",
       "148459         PRESTITO PERSONALE                       None  2023-06-29   \n",
       "148460   DIPENDENTI AZIENDE DOC -                       None  2023-06-29   \n",
       "148461              SPESE MEDICHE        CURE ODONTOIATRICHE  2023-06-29   \n",
       "148462           AUTOMOTIVE USATO  forfour 2as. (W453) FW098  2023-06-29   \n",
       "\n",
       "       DATA_FINE_AFFIDO DATA_FINANZIAM DT_VAL_ULT_PAGAM DATA_ESTINZ  \\\n",
       "0            2021-09-05     2018-06-12       2021-07-05        None   \n",
       "1            2021-09-05     2016-05-12       2021-06-09        None   \n",
       "2            2021-09-05     2021-02-10       2021-06-14        None   \n",
       "3            2021-09-05     2019-09-12       2021-05-27        None   \n",
       "4            2021-09-05     2021-01-13       2021-07-27        None   \n",
       "...                 ...            ...              ...         ...   \n",
       "148458             None     2022-04-14       2023-05-24        None   \n",
       "148459             None     2021-07-15       2023-05-24        None   \n",
       "148460             None     2021-03-15       2022-05-05        None   \n",
       "148461             None     2020-05-15       2020-10-16        None   \n",
       "148462             None     2022-07-15       2023-05-14        None   \n",
       "\n",
       "       DATA_AGGIORNAMENTO P_INST1_DUE_DATE P_INST2_DUE_DATE P_INST3_DUE_DATE  \\\n",
       "0              01/01/0001       2021-04-12       2021-05-12       2021-06-12   \n",
       "1              01/01/0001       2021-04-12       2021-05-12       2021-06-12   \n",
       "2              01/01/0001       2021-04-12       2021-05-12       2021-06-12   \n",
       "3              01/01/0001       2021-04-12       2021-05-12       2021-06-12   \n",
       "4              01/01/0001       2021-04-12       2021-05-12       2021-06-12   \n",
       "...                   ...              ...              ...              ...   \n",
       "148458         01/01/0001       2023-03-14       2023-04-14       2023-05-14   \n",
       "148459         01/01/0001       2023-03-14       2023-04-14       2023-05-14   \n",
       "148460         01/01/0001       2023-03-14       2023-04-14       2023-05-14   \n",
       "148461         01/01/0001       2023-03-14       2023-04-14       2023-05-14   \n",
       "148462         01/01/0001       2023-03-14       2023-04-14       2023-05-14   \n",
       "\n",
       "       DT_VAL_RAT_IMP1 DT_VAL_RAT_IMP2 P_INST1_VAL_DATE P_INST3_VAL_DATE  \\\n",
       "0                 None            None       2021-06-16       2021-07-05   \n",
       "1                 None            None       2021-04-16       2021-06-09   \n",
       "2                 None            None       2021-04-12       2021-06-14   \n",
       "3                 None            None       2021-04-26       2021-05-27   \n",
       "4           2021-07-27            None       2021-04-14       2021-07-27   \n",
       "...                ...             ...              ...              ...   \n",
       "148458            None            None       2023-03-24       2023-05-24   \n",
       "148459            None            None       2023-03-14       2023-05-24   \n",
       "148460      2022-05-05            None       2023-03-14       2023-05-14   \n",
       "148461      2020-10-16            None       2023-03-14       2023-05-14   \n",
       "148462            None            None       2023-03-14       2023-05-14   \n",
       "\n",
       "       SCAD_MAXIRATA SCAD_RAT_IMP2 SCAD_RAT_IMP1  IMP_FINANZIATO  \\\n",
       "0               None    2021-08-12    2021-07-12       15,200.00   \n",
       "1               None    2021-08-12    2021-07-12       14,234.40   \n",
       "2               None    2021-08-12    2021-07-12        1,500.00   \n",
       "3               None    2021-08-12    2021-07-12       14,280.00   \n",
       "4               None    2021-08-12    2021-07-12        6,800.00   \n",
       "...              ...           ...           ...             ...   \n",
       "148458          None    2023-07-14    2023-06-14        4,120.88   \n",
       "148459          None    2023-07-14    2023-06-14        6,250.00   \n",
       "148460          None    2023-07-14    2023-06-14       14,492.80   \n",
       "148461          None    2023-07-14    2023-06-14        6,654.00   \n",
       "148462          None    2023-07-14    2023-06-14        7,009.40   \n",
       "\n",
       "        IMP_MAXIRATA  CURRENT_BALANCE P_INST1_AMOUNT_PAID P_INST2_AMOUNT_PAID  \\\n",
       "0               0.00         9,770.58       0000000023700       0000000023700   \n",
       "1               0.00         8,636.46       0000000018350       0000000018350   \n",
       "2               0.00         1,329.19       0000000005650       0000000005650   \n",
       "3               0.00        10,954.69       0000000026000       0000000026000   \n",
       "4               0.00         6,214.51       0000000016600       0000000016600   \n",
       "...              ...              ...                 ...                 ...   \n",
       "148458          0.00         3,169.02       0000000010600       0000000010600   \n",
       "148459          0.00         4,725.40       0000000011600       0000000011600   \n",
       "148460          0.00         9,057.10       0000000030300       0000000030300   \n",
       "148461          0.00         3,828.21       0000000012200       0000000012200   \n",
       "148462          0.00         6,037.66       0000000014300       0000000014300   \n",
       "\n",
       "       P_INST3_AMOUNT_PAID  RATE_TOTALI  NUM_RATE_RIFI  NUM_RAT_IMP1  \\\n",
       "0            0000000023700        84.00           0.00         37.00   \n",
       "1            0000000018350       120.00           0.00         62.00   \n",
       "2            0000000005650        30.00           0.00          5.00   \n",
       "3            0000000026000        72.00           0.00         22.00   \n",
       "4            0000000016600        48.00           0.00          6.00   \n",
       "...                    ...          ...            ...           ...   \n",
       "148458       0000000010600        48.00           0.00         14.00   \n",
       "148459       0000000011600        72.00           0.00         23.00   \n",
       "148460       0000000030300        60.00           0.00         27.00   \n",
       "148461       0000000012200        70.00           0.00         35.00   \n",
       "148462       0000000014300        60.00           0.00         11.00   \n",
       "\n",
       "        NUM_RAT_IMP2  P_INST1_NUM  P_INST2_NUM  P_INST3_NUM  NUMERO RATE  \\\n",
       "0              38.00        34.00        35.00        36.00            2   \n",
       "1              63.00        59.00        60.00        61.00            2   \n",
       "2               6.00         2.00         3.00         4.00            2   \n",
       "3              23.00        19.00        20.00        21.00            2   \n",
       "4               7.00         3.00         4.00         5.00            2   \n",
       "...              ...          ...          ...          ...          ...   \n",
       "148458         15.00        11.00        12.00        13.00            2   \n",
       "148459         24.00        20.00        21.00        22.00            2   \n",
       "148460         28.00        24.00        25.00        26.00            2   \n",
       "148461         36.00        32.00        33.00        34.00            2   \n",
       "148462         12.00         8.00         9.00        10.00            2   \n",
       "\n",
       "       PROVINCIA  P_INST1_AMOUNT  P_INST2_AMOUNT  P_INST3_AMOUNT  \\\n",
       "0             LT          237.00          237.00          237.00   \n",
       "1             IM          183.50          183.50          183.50   \n",
       "2             KR           56.50           56.50           56.50   \n",
       "3             EN          260.00          260.00          260.00   \n",
       "4             RM          166.00          166.00          166.00   \n",
       "...          ...             ...             ...             ...   \n",
       "148458        RM          106.00          106.00          106.00   \n",
       "148459        TR          116.00          116.00          116.00   \n",
       "148460        VA          303.00          303.00          303.00   \n",
       "148461        MI          122.00          122.00          122.00   \n",
       "148462        RM          143.00          143.00          143.00   \n",
       "\n",
       "        IMP_RAT_IMP1  IMP_RAT_IMP2  IMP_PAG_RAT_IMP1  IMP_PAG_RAT_IMP2  \\\n",
       "0             237.00        237.00              0.00              0.00   \n",
       "1             183.50        183.50              0.00              0.00   \n",
       "2              56.50         56.50              0.00              0.00   \n",
       "3             260.00        260.00              0.00              0.00   \n",
       "4             166.00        166.00              2.34              0.00   \n",
       "...              ...           ...               ...               ...   \n",
       "148458        106.00        106.00              0.00              0.00   \n",
       "148459        116.00        116.00              0.00              0.00   \n",
       "148460        303.00        303.00              1.25              0.00   \n",
       "148461        122.00        122.00             26.24              0.00   \n",
       "148462        143.00        143.00              0.00              0.00   \n",
       "\n",
       "        TOT_RATE_AFF_INIZIO_MAND  TOT_MORA_AFF_INIZIO_MAND  \\\n",
       "0                         237.00                     28.24   \n",
       "1                         183.50                      1.56   \n",
       "2                          56.50                      0.58   \n",
       "3                         260.00                     16.20   \n",
       "4                         163.66                      3.51   \n",
       "...                          ...                       ...   \n",
       "148458                    106.00                      2.17   \n",
       "148459                    116.00                      0.85   \n",
       "148460                    301.75                      3.66   \n",
       "148461                     95.76                      0.00   \n",
       "148462                    143.00                      0.48   \n",
       "\n",
       "        TOT_ONERI_AFF_INIZIO_MAND  TOT_AFF_INIZIO_MAND  TOT_RATE_PAG  \\\n",
       "0                           12.00               277.24          0.00   \n",
       "1                           12.00               197.06          0.00   \n",
       "2                           15.97                73.05          0.00   \n",
       "3                           12.00               288.20          0.00   \n",
       "4                           32.40               199.57          0.00   \n",
       "...                           ...                  ...           ...   \n",
       "148458                       0.00               108.17          0.00   \n",
       "148459                       0.00               116.85          0.00   \n",
       "148460                       0.00               305.41          0.00   \n",
       "148461                       0.00                95.76          0.00   \n",
       "148462                       0.00               143.48          0.00   \n",
       "\n",
       "        TOT_MORA_PAG  TOT_ONERI_PAG  TOT_PAG  TOT_RATE_AFF  TOT_MORA_AFF  \\\n",
       "0               0.00           0.00     0.00        237.00         28.24   \n",
       "1               0.00           0.00     0.00        183.50          1.56   \n",
       "2               0.00           0.00     0.00         56.50          0.58   \n",
       "3               0.00           0.00     0.00        260.00         16.20   \n",
       "4               0.00           0.00     0.00        163.66          3.51   \n",
       "...              ...            ...      ...           ...           ...   \n",
       "148458          0.00           0.00     0.00        106.00          2.17   \n",
       "148459          0.00           0.00     0.00        116.00          0.85   \n",
       "148460          0.00           0.00     0.00        301.75          3.66   \n",
       "148461          0.00           0.00     0.00         95.76          0.00   \n",
       "148462          0.00           0.00     0.00        143.00          0.48   \n",
       "\n",
       "        TOT_ONERI_AFF  TOT_AFF Flag_Riciclo_SDD  TOT REC  TOT AFF  \\\n",
       "0               12.00   277.24             None     0.00   486.00   \n",
       "1               12.00   197.06                Y     0.00   379.00   \n",
       "2               15.97    73.05             None     0.00   144.94   \n",
       "3               12.00   288.20             None     0.00   532.00   \n",
       "4               32.40   199.57             None     2.34   396.80   \n",
       "...               ...      ...              ...      ...      ...   \n",
       "148458           0.00   108.17                Y     0.00   246.40   \n",
       "148459           0.00   116.85                Y   116.00   116.00   \n",
       "148460           0.00   305.41                Y     1.25   303.00   \n",
       "148461           0.00    95.76                Y    26.24   122.00   \n",
       "148462           0.00   143.48                Y     0.00   171.95   \n",
       "\n",
       "       Flag_Rifinanziamento Flag_Garante Flag_Cointestazione Metodo_pagamento  \\\n",
       "0                        NO           SI                  SI               BP   \n",
       "1                        NO           NO                  NO               BP   \n",
       "2                        NO           NO                  NO               BP   \n",
       "3                        NO           NO                  NO               BP   \n",
       "4                        NO           NO                  NO               BP   \n",
       "...                     ...          ...                 ...              ...   \n",
       "148458                   NO           NO                  SI               RI   \n",
       "148459                   NO           NO                  SI               RI   \n",
       "148460                   NO           NO                  SI               RI   \n",
       "148461                   NO           NO                  NO               RI   \n",
       "148462                   NO           NO                  NO               RI   \n",
       "\n",
       "        Ratio_Rate_Imp1  Ratio_Rate_Imp2  DistanzaAffidoUltimoPagamento  \\\n",
       "0                  0.44             0.45                          -1.00   \n",
       "1                  0.52             0.53                          -2.00   \n",
       "2                  0.17             0.20                          -2.00   \n",
       "3                  0.31             0.32                          -2.00   \n",
       "4                  0.12             0.15                          -0.00   \n",
       "...                 ...              ...                            ...   \n",
       "148458             0.29             0.31                          -1.00   \n",
       "148459             0.32             0.33                          -1.00   \n",
       "148460             0.45             0.47                         -14.00   \n",
       "148461             0.50             0.51                         -32.00   \n",
       "148462             0.18             0.20                          -2.00   \n",
       "\n",
       "        Sesso  Eta_Debitore NOSTART12M NOSTART6M  DurataFinanziamento  \\\n",
       "0        None           NaN         NO        NO                 7.00   \n",
       "1       Donna         59.00         NO        NO                10.00   \n",
       "2       Donna         57.00         SI        SI                 2.50   \n",
       "3        Uomo         76.00         NO        NO                 6.00   \n",
       "4        Uomo         57.00         SI        SI                 4.00   \n",
       "...       ...           ...        ...       ...                  ...   \n",
       "148458   None           NaN         NO        NO                 4.00   \n",
       "148459   None           NaN         NO        NO                 6.00   \n",
       "148460   None           NaN         NO        NO                 5.00   \n",
       "148461   Uomo         61.00         NO        NO                 5.83   \n",
       "148462  Donna         53.00         SI        NO                 5.00   \n",
       "\n",
       "       Flag_Galleggiamento_3M Flag_Galleggiamento_6M Flag_Gestione_Prec  \\\n",
       "0                          NO                     NO                 NO   \n",
       "1                          NO                     NO                 SI   \n",
       "2                          SI                     NO                 SI   \n",
       "3                          NO                     NO                 SI   \n",
       "4                          NO                     NO                 NO   \n",
       "...                       ...                    ...                ...   \n",
       "148458                     NO                     NO                 SI   \n",
       "148459                     NO                     NO                 NO   \n",
       "148460                     NO                     NO                 SI   \n",
       "148461                     NO                     NO                 NO   \n",
       "148462                     NO                     NO                 NO   \n",
       "\n",
       "       Flag_InsolvenzaGrave_3M Flag_InsolvenzaGrave_6M Flag_Miglioramento_3M  \\\n",
       "0                           SI                      SI                    NO   \n",
       "1                           NO                      NO                    NO   \n",
       "2                           NO                      NO                    NO   \n",
       "3                           NO                      NO                    NO   \n",
       "4                           NO                      NO                    NO   \n",
       "...                        ...                     ...                   ...   \n",
       "148458                      NO                      NO                    NO   \n",
       "148459                      NO                      NO                    NO   \n",
       "148460                      NO                      NO                    NO   \n",
       "148461                      NO                      NO                    NO   \n",
       "148462                      NO                      NO                    NO   \n",
       "\n",
       "       Flag_Rate_piu_uno Flag_rientrototale_6M Flag_Scivolamento_3M  \\\n",
       "0                     SI                    NO                   NO   \n",
       "1                     NO                    SI                   NO   \n",
       "2                     NO                    SI                   NO   \n",
       "3                     SI                    SI                   NO   \n",
       "4                     SI                    NO                   NO   \n",
       "...                  ...                   ...                  ...   \n",
       "148458                NO                    SI                   NO   \n",
       "148459                NO                    SI                   NO   \n",
       "148460                NO                    SI                   NO   \n",
       "148461                NO                    SI                   NO   \n",
       "148462                NO                    SI                   NO   \n",
       "\n",
       "       Flag_Scivolamento_6M  Numero_mesi_rec  Severity_12M  \\\n",
       "0                        NO            12.00          1.58   \n",
       "1                        NO             1.00          0.17   \n",
       "2                        NO             3.00          0.67   \n",
       "3                        NO             1.00          0.83   \n",
       "4                        NO             6.00          1.14   \n",
       "...                     ...              ...           ...   \n",
       "148458                   NO             2.00          0.58   \n",
       "148459                   NO             2.00          0.17   \n",
       "148460                   NO             1.00          0.17   \n",
       "148461                   NO             1.00          0.08   \n",
       "148462                   NO             1.00          0.09   \n",
       "\n",
       "        Severity_12M_pesata Flag_InsolvenzaGrave_12M FlagRecOver100  \\\n",
       "0                      0.95                       SI              0   \n",
       "1                      0.15                       NO              0   \n",
       "2                      0.57                       NO              0   \n",
       "3                      0.41                       NO              0   \n",
       "4                      0.92                       NO              0   \n",
       "...                     ...                      ...            ...   \n",
       "148458                 0.39                       NO              0   \n",
       "148459                 0.16                       NO              0   \n",
       "148460                 0.14                       NO              0   \n",
       "148461                 0.08                       NO              0   \n",
       "148462                 0.09                       NO              0   \n",
       "\n",
       "        AgeingGestioneGg  AgeingErogazioneGg  \n",
       "0                  21.00            1,126.00  \n",
       "1                  29.00            1,887.00  \n",
       "2                  29.00              152.00  \n",
       "3                  29.00              669.00  \n",
       "4                  28.00              180.00  \n",
       "...                  ...                 ...  \n",
       "148458             15.00              426.00  \n",
       "148459             15.00              699.00  \n",
       "148460             15.00              821.00  \n",
       "148461             15.00            1,125.00  \n",
       "148462             15.00              334.00  \n",
       "\n",
       "[148463 rows x 93 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clienti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 148463 entries, 0 to 148462\n",
      "Data columns (total 93 columns):\n",
      " #   Column                         Non-Null Count   Dtype  \n",
      "---  ------                         --------------   -----  \n",
      " 0   Target_95                      148463 non-null  int32  \n",
      " 1   Chiave                         148463 non-null  object \n",
      " 2   PRATICA                        148463 non-null  object \n",
      " 3   DELIN_HISTORY                  148463 non-null  object \n",
      " 4   MOD_PAGAM                      147932 non-null  object \n",
      " 5   MOD_PAG                        148463 non-null  object \n",
      " 6   TIPO_GARANTE                   16976 non-null   object \n",
      " 7   Denominazione Regione          148462 non-null  object \n",
      " 8   CODICE_PHONIA                  148463 non-null  object \n",
      " 9   DES_PRODOTTO                   148463 non-null  object \n",
      " 10  DES_BENE                       75973 non-null   object \n",
      " 11  DATA_AFFIDO                    148463 non-null  object \n",
      " 12  DATA_FINE_AFFIDO               135897 non-null  object \n",
      " 13  DATA_FINANZIAM                 148463 non-null  object \n",
      " 14  DT_VAL_ULT_PAGAM               148463 non-null  object \n",
      " 15  DATA_ESTINZ                    120 non-null     object \n",
      " 16  DATA_AGGIORNAMENTO             148463 non-null  object \n",
      " 17  P_INST1_DUE_DATE               141880 non-null  object \n",
      " 18  P_INST2_DUE_DATE               143827 non-null  object \n",
      " 19  P_INST3_DUE_DATE               145780 non-null  object \n",
      " 20  DT_VAL_RAT_IMP1                43724 non-null   object \n",
      " 21  DT_VAL_RAT_IMP2                4402 non-null    object \n",
      " 22  P_INST1_VAL_DATE               141878 non-null  object \n",
      " 23  P_INST3_VAL_DATE               145775 non-null  object \n",
      " 24  SCAD_MAXIRATA                  546 non-null     object \n",
      " 25  SCAD_RAT_IMP2                  144834 non-null  object \n",
      " 26  SCAD_RAT_IMP1                  148463 non-null  object \n",
      " 27  IMP_FINANZIATO                 148463 non-null  float64\n",
      " 28  IMP_MAXIRATA                   148463 non-null  float64\n",
      " 29  CURRENT_BALANCE                148463 non-null  float64\n",
      " 30  P_INST1_AMOUNT_PAID            148463 non-null  object \n",
      " 31  P_INST2_AMOUNT_PAID            148463 non-null  object \n",
      " 32  P_INST3_AMOUNT_PAID            148463 non-null  object \n",
      " 33  RATE_TOTALI                    148463 non-null  float64\n",
      " 34  NUM_RATE_RIFI                  148463 non-null  float64\n",
      " 35  NUM_RAT_IMP1                   148463 non-null  float64\n",
      " 36  NUM_RAT_IMP2                   148463 non-null  float64\n",
      " 37  P_INST1_NUM                    148463 non-null  float64\n",
      " 38  P_INST2_NUM                    148463 non-null  float64\n",
      " 39  P_INST3_NUM                    148463 non-null  float64\n",
      " 40  NUMERO RATE                    148463 non-null  int32  \n",
      " 41  PROVINCIA                      148463 non-null  object \n",
      " 42  P_INST1_AMOUNT                 148463 non-null  float64\n",
      " 43  P_INST2_AMOUNT                 148463 non-null  float64\n",
      " 44  P_INST3_AMOUNT                 148463 non-null  float64\n",
      " 45  IMP_RAT_IMP1                   148463 non-null  float64\n",
      " 46  IMP_RAT_IMP2                   148463 non-null  float64\n",
      " 47  IMP_PAG_RAT_IMP1               148463 non-null  float64\n",
      " 48  IMP_PAG_RAT_IMP2               148463 non-null  float64\n",
      " 49  TOT_RATE_AFF_INIZIO_MAND       148463 non-null  float64\n",
      " 50  TOT_MORA_AFF_INIZIO_MAND       148463 non-null  float64\n",
      " 51  TOT_ONERI_AFF_INIZIO_MAND      148463 non-null  float64\n",
      " 52  TOT_AFF_INIZIO_MAND            148463 non-null  float64\n",
      " 53  TOT_RATE_PAG                   148463 non-null  float64\n",
      " 54  TOT_MORA_PAG                   148463 non-null  float64\n",
      " 55  TOT_ONERI_PAG                  148463 non-null  float64\n",
      " 56  TOT_PAG                        148463 non-null  float64\n",
      " 57  TOT_RATE_AFF                   148463 non-null  float64\n",
      " 58  TOT_MORA_AFF                   148463 non-null  float64\n",
      " 59  TOT_ONERI_AFF                  148463 non-null  float64\n",
      " 60  TOT_AFF                        148463 non-null  float64\n",
      " 61  Flag_Riciclo_SDD               109547 non-null  object \n",
      " 62  TOT REC                        148463 non-null  float64\n",
      " 63  TOT AFF                        148463 non-null  float64\n",
      " 64  Flag_Rifinanziamento           148463 non-null  object \n",
      " 65  Flag_Garante                   148463 non-null  object \n",
      " 66  Flag_Cointestazione            148463 non-null  object \n",
      " 67  Metodo_pagamento               148463 non-null  object \n",
      " 68  Ratio_Rate_Imp1                148463 non-null  float64\n",
      " 69  Ratio_Rate_Imp2                148463 non-null  float64\n",
      " 70  DistanzaAffidoUltimoPagamento  148463 non-null  float64\n",
      " 71  Sesso                          113805 non-null  object \n",
      " 72  Eta_Debitore                   113805 non-null  float64\n",
      " 73  NOSTART12M                     148463 non-null  object \n",
      " 74  NOSTART6M                      148463 non-null  object \n",
      " 75  DurataFinanziamento            148463 non-null  float64\n",
      " 76  Flag_Galleggiamento_3M         148463 non-null  object \n",
      " 77  Flag_Galleggiamento_6M         148463 non-null  object \n",
      " 78  Flag_Gestione_Prec             148463 non-null  object \n",
      " 79  Flag_InsolvenzaGrave_3M        148463 non-null  object \n",
      " 80  Flag_InsolvenzaGrave_6M        148463 non-null  object \n",
      " 81  Flag_Miglioramento_3M          148463 non-null  object \n",
      " 82  Flag_Rate_piu_uno              148463 non-null  object \n",
      " 83  Flag_rientrototale_6M          148463 non-null  object \n",
      " 84  Flag_Scivolamento_3M           148463 non-null  object \n",
      " 85  Flag_Scivolamento_6M           148463 non-null  object \n",
      " 86  Numero_mesi_rec                148463 non-null  float64\n",
      " 87  Severity_12M                   147425 non-null  float64\n",
      " 88  Severity_12M_pesata            147425 non-null  float64\n",
      " 89  Flag_InsolvenzaGrave_12M       148463 non-null  object \n",
      " 90  FlagRecOver100                 148463 non-null  object \n",
      " 91  AgeingGestioneGg               148463 non-null  float64\n",
      " 92  AgeingErogazioneGg             148463 non-null  float64\n",
      "dtypes: float64(41), int32(2), object(50)\n",
      "memory usage: 104.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df_clienti.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering di alcune colonne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cluster = df_clienti.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ALTRI BENI E SERVIZI',\n",
       " 'ALTRO FINALIZZATO PER AZI',\n",
       " 'ARREDAMENTO',\n",
       " 'AUTOMOTIVE AZIENDE NUOVO',\n",
       " 'AUTOMOTIVE NUOVO',\n",
       " 'AUTOMOTIVE USATO',\n",
       " 'CASA GRANDI INTERVENTI',\n",
       " 'CASA PICCOLI INTERVENTI',\n",
       " 'CONSOLIDAMENTO BANCOPOSTA',\n",
       " 'CONSOLIDAMENTO DEL DEBITO',\n",
       " 'DIPENDENTI & PARENTI GRUP',\n",
       " 'DIPENDENTI AZIENDE DOC -',\n",
       " 'DISINTERMEDIATO ALTRI BEN',\n",
       " 'DISINTERMEDIATO AUTOMOTIV',\n",
       " 'ELETTRONICA & ELETTRODOME',\n",
       " 'GESTIONE STOCK REWRITE',\n",
       " 'MOTO E CICLOMOTORI NUOVO',\n",
       " 'MOTO E CICLOMOTORI USATO',\n",
       " 'PRESTITI CON TRATTENUTA',\n",
       " 'PRESTITO BANCOPOSTA',\n",
       " 'PRESTITO PERSONALE',\n",
       " 'REPEAT BUSINESS',\n",
       " 'REWRITE',\n",
       " 'SPESE MEDICHE',\n",
       " 'TEMPO LIBERO',\n",
       " 'VEICOLI NON TARGATI']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(df_cluster['DES_PRODOTTO'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clusterizzo la colonna 'DES_PRODOTTO' sulla base di regole scelte da noi\n",
    "dict_prodotti = {'AUTOMOTIVE AZIENDE NUOVO' : 'AUTOMOTIVE NUOVO',\n",
    "                 'AUTOMOTIVE NUOVO' : 'AUTOMOTIVE NUOVO',\n",
    "                 'VEICOLI NON TARGATI' : 'AUTOMOTIVE NUOVO',\n",
    "                 'DISINTERMEDIATO AUTOMOTIV' : 'AUTOMOTIVE NUOVO',\n",
    "                 'PRESTITO BANCOPOSTA' : 'ALTRI BENI E SERVIZI',\n",
    "                 'CONSOLIDAMENTO BANCOPOSTA' : 'ALTRI BENI E SERVIZI',\n",
    "                 'REWRITE' : 'ALTRI BENI E SERVIZI',\n",
    "                 'GESTIONE STOCK REWRITE' : 'ALTRI BENI E SERVIZI',\n",
    "                 'DISINTERMEDIATO ALTRI BEN' : 'ALTRI BENI E SERVIZI',\n",
    "                 'PRESTITI CON TRATTENUTA' : 'PRESTITI CON TRATTENUTA',\n",
    "                 'DIPENDENTI AZIENDE DOC -' : 'PRESTITI CON TRATTENUTA',\n",
    "                 'DIPENDENTI & PARENTI GRUP' : 'PRESTITI CON TRATTENUTA',\n",
    "                 'REPEAT BUSINESS' : 'FINANZIAMENTI AZIENDE',\n",
    "                 'ALTRO FINALIZZATO PER AZI' : 'FINANZIAMENTI AZIENDE',\n",
    "                 'CASA GRANDI INTERVENTI' : 'INTERVENTI CASA',\n",
    "                 'CASA PICCOLI INTERVENTI': 'INTERVENTI CASA'\n",
    "                 }\n",
    "df_cluster['DESCRIZIONE PRODOTTO'] = df_cluster['DES_PRODOTTO'].map(dict_prodotti)\n",
    "\n",
    "# Ora ci sono dei valori NaN per tutti i valori che non sono stati mappati da nessuna parte, e quindi per quelli rimetto il valore originale\n",
    "df_cluster['DESCRIZIONE PRODOTTO'] = df_cluster['DESCRIZIONE PRODOTTO'].fillna(df_cluster['DES_PRODOTTO'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target_95</th>\n",
       "      <th>Chiave</th>\n",
       "      <th>PRATICA</th>\n",
       "      <th>DELIN_HISTORY</th>\n",
       "      <th>MOD_PAGAM</th>\n",
       "      <th>MOD_PAG</th>\n",
       "      <th>TIPO_GARANTE</th>\n",
       "      <th>Denominazione Regione</th>\n",
       "      <th>CODICE_PHONIA</th>\n",
       "      <th>DES_PRODOTTO</th>\n",
       "      <th>DES_BENE</th>\n",
       "      <th>DATA_AFFIDO</th>\n",
       "      <th>DATA_FINE_AFFIDO</th>\n",
       "      <th>DATA_FINANZIAM</th>\n",
       "      <th>DT_VAL_ULT_PAGAM</th>\n",
       "      <th>DATA_ESTINZ</th>\n",
       "      <th>DATA_AGGIORNAMENTO</th>\n",
       "      <th>P_INST1_DUE_DATE</th>\n",
       "      <th>P_INST2_DUE_DATE</th>\n",
       "      <th>P_INST3_DUE_DATE</th>\n",
       "      <th>DT_VAL_RAT_IMP1</th>\n",
       "      <th>DT_VAL_RAT_IMP2</th>\n",
       "      <th>P_INST1_VAL_DATE</th>\n",
       "      <th>P_INST3_VAL_DATE</th>\n",
       "      <th>SCAD_MAXIRATA</th>\n",
       "      <th>SCAD_RAT_IMP2</th>\n",
       "      <th>SCAD_RAT_IMP1</th>\n",
       "      <th>IMP_FINANZIATO</th>\n",
       "      <th>IMP_MAXIRATA</th>\n",
       "      <th>CURRENT_BALANCE</th>\n",
       "      <th>P_INST1_AMOUNT_PAID</th>\n",
       "      <th>P_INST2_AMOUNT_PAID</th>\n",
       "      <th>P_INST3_AMOUNT_PAID</th>\n",
       "      <th>RATE_TOTALI</th>\n",
       "      <th>NUM_RATE_RIFI</th>\n",
       "      <th>NUM_RAT_IMP1</th>\n",
       "      <th>NUM_RAT_IMP2</th>\n",
       "      <th>P_INST1_NUM</th>\n",
       "      <th>P_INST2_NUM</th>\n",
       "      <th>P_INST3_NUM</th>\n",
       "      <th>NUMERO RATE</th>\n",
       "      <th>PROVINCIA</th>\n",
       "      <th>P_INST1_AMOUNT</th>\n",
       "      <th>P_INST2_AMOUNT</th>\n",
       "      <th>P_INST3_AMOUNT</th>\n",
       "      <th>IMP_RAT_IMP1</th>\n",
       "      <th>IMP_RAT_IMP2</th>\n",
       "      <th>IMP_PAG_RAT_IMP1</th>\n",
       "      <th>IMP_PAG_RAT_IMP2</th>\n",
       "      <th>TOT_RATE_AFF_INIZIO_MAND</th>\n",
       "      <th>TOT_MORA_AFF_INIZIO_MAND</th>\n",
       "      <th>TOT_ONERI_AFF_INIZIO_MAND</th>\n",
       "      <th>TOT_AFF_INIZIO_MAND</th>\n",
       "      <th>TOT_RATE_PAG</th>\n",
       "      <th>TOT_MORA_PAG</th>\n",
       "      <th>TOT_ONERI_PAG</th>\n",
       "      <th>TOT_PAG</th>\n",
       "      <th>TOT_RATE_AFF</th>\n",
       "      <th>TOT_MORA_AFF</th>\n",
       "      <th>TOT_ONERI_AFF</th>\n",
       "      <th>TOT_AFF</th>\n",
       "      <th>Flag_Riciclo_SDD</th>\n",
       "      <th>TOT REC</th>\n",
       "      <th>TOT AFF</th>\n",
       "      <th>Flag_Rifinanziamento</th>\n",
       "      <th>Flag_Garante</th>\n",
       "      <th>Flag_Cointestazione</th>\n",
       "      <th>Metodo_pagamento</th>\n",
       "      <th>Ratio_Rate_Imp1</th>\n",
       "      <th>Ratio_Rate_Imp2</th>\n",
       "      <th>DistanzaAffidoUltimoPagamento</th>\n",
       "      <th>Sesso</th>\n",
       "      <th>Eta_Debitore</th>\n",
       "      <th>NOSTART12M</th>\n",
       "      <th>NOSTART6M</th>\n",
       "      <th>DurataFinanziamento</th>\n",
       "      <th>Flag_Galleggiamento_3M</th>\n",
       "      <th>Flag_Galleggiamento_6M</th>\n",
       "      <th>Flag_Gestione_Prec</th>\n",
       "      <th>Flag_InsolvenzaGrave_3M</th>\n",
       "      <th>Flag_InsolvenzaGrave_6M</th>\n",
       "      <th>Flag_Miglioramento_3M</th>\n",
       "      <th>Flag_Rate_piu_uno</th>\n",
       "      <th>Flag_rientrototale_6M</th>\n",
       "      <th>Flag_Scivolamento_3M</th>\n",
       "      <th>Flag_Scivolamento_6M</th>\n",
       "      <th>Numero_mesi_rec</th>\n",
       "      <th>Severity_12M</th>\n",
       "      <th>Severity_12M_pesata</th>\n",
       "      <th>Flag_InsolvenzaGrave_12M</th>\n",
       "      <th>FlagRecOver100</th>\n",
       "      <th>AgeingGestioneGg</th>\n",
       "      <th>AgeingErogazioneGg</th>\n",
       "      <th>DESCRIZIONE PRODOTTO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>00000854016301_08_2021</td>\n",
       "      <td>00000854016301</td>\n",
       "      <td>13221222111111143332432332111100000000</td>\n",
       "      <td>P</td>\n",
       "      <td>BP</td>\n",
       "      <td>C</td>\n",
       "      <td>Lazio</td>\n",
       "      <td>ALPTEL</td>\n",
       "      <td>PRESTITO PERSONALE</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-08-02</td>\n",
       "      <td>2021-09-05</td>\n",
       "      <td>2018-06-12</td>\n",
       "      <td>2021-07-05</td>\n",
       "      <td>None</td>\n",
       "      <td>01/01/0001</td>\n",
       "      <td>2021-04-12</td>\n",
       "      <td>2021-05-12</td>\n",
       "      <td>2021-06-12</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-06-16</td>\n",
       "      <td>2021-07-05</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-08-12</td>\n",
       "      <td>2021-07-12</td>\n",
       "      <td>15,200.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9,770.58</td>\n",
       "      <td>0000000023700</td>\n",
       "      <td>0000000023700</td>\n",
       "      <td>0000000023700</td>\n",
       "      <td>84.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>37.00</td>\n",
       "      <td>38.00</td>\n",
       "      <td>34.00</td>\n",
       "      <td>35.00</td>\n",
       "      <td>36.00</td>\n",
       "      <td>2</td>\n",
       "      <td>LT</td>\n",
       "      <td>237.00</td>\n",
       "      <td>237.00</td>\n",
       "      <td>237.00</td>\n",
       "      <td>237.00</td>\n",
       "      <td>237.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>237.00</td>\n",
       "      <td>28.24</td>\n",
       "      <td>12.00</td>\n",
       "      <td>277.24</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>237.00</td>\n",
       "      <td>28.24</td>\n",
       "      <td>12.00</td>\n",
       "      <td>277.24</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00</td>\n",
       "      <td>486.00</td>\n",
       "      <td>NO</td>\n",
       "      <td>SI</td>\n",
       "      <td>SI</td>\n",
       "      <td>BP</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.45</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>7.00</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>SI</td>\n",
       "      <td>SI</td>\n",
       "      <td>NO</td>\n",
       "      <td>SI</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>12.00</td>\n",
       "      <td>1.58</td>\n",
       "      <td>0.95</td>\n",
       "      <td>SI</td>\n",
       "      <td>0</td>\n",
       "      <td>21.00</td>\n",
       "      <td>1,126.00</td>\n",
       "      <td>PRESTITO PERSONALE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>00006003826200_08_2021</td>\n",
       "      <td>00006003826200</td>\n",
       "      <td>1001000000000000000000000000000000001100100110...</td>\n",
       "      <td>P</td>\n",
       "      <td>BP</td>\n",
       "      <td>None</td>\n",
       "      <td>Liguria</td>\n",
       "      <td>ALPTEL</td>\n",
       "      <td>REPEAT BUSINESS</td>\n",
       "      <td>FIDELIZ.OTTIMI</td>\n",
       "      <td>2021-08-10</td>\n",
       "      <td>2021-09-05</td>\n",
       "      <td>2016-05-12</td>\n",
       "      <td>2021-06-09</td>\n",
       "      <td>None</td>\n",
       "      <td>01/01/0001</td>\n",
       "      <td>2021-04-12</td>\n",
       "      <td>2021-05-12</td>\n",
       "      <td>2021-06-12</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-04-16</td>\n",
       "      <td>2021-06-09</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-08-12</td>\n",
       "      <td>2021-07-12</td>\n",
       "      <td>14,234.40</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8,636.46</td>\n",
       "      <td>0000000018350</td>\n",
       "      <td>0000000018350</td>\n",
       "      <td>0000000018350</td>\n",
       "      <td>120.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>62.00</td>\n",
       "      <td>63.00</td>\n",
       "      <td>59.00</td>\n",
       "      <td>60.00</td>\n",
       "      <td>61.00</td>\n",
       "      <td>2</td>\n",
       "      <td>IM</td>\n",
       "      <td>183.50</td>\n",
       "      <td>183.50</td>\n",
       "      <td>183.50</td>\n",
       "      <td>183.50</td>\n",
       "      <td>183.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>183.50</td>\n",
       "      <td>1.56</td>\n",
       "      <td>12.00</td>\n",
       "      <td>197.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>183.50</td>\n",
       "      <td>1.56</td>\n",
       "      <td>12.00</td>\n",
       "      <td>197.06</td>\n",
       "      <td>Y</td>\n",
       "      <td>0.00</td>\n",
       "      <td>379.00</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>BP</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.53</td>\n",
       "      <td>-2.00</td>\n",
       "      <td>Donna</td>\n",
       "      <td>59.00</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>10.00</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>SI</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>SI</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.15</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>29.00</td>\n",
       "      <td>1,887.00</td>\n",
       "      <td>FINANZIAMENTI AZIENDE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>00002286163301_08_2021</td>\n",
       "      <td>00002286163301</td>\n",
       "      <td>111010</td>\n",
       "      <td>P</td>\n",
       "      <td>BP</td>\n",
       "      <td>None</td>\n",
       "      <td>Calabria</td>\n",
       "      <td>ALPTEL</td>\n",
       "      <td>ELETTRONICA &amp; ELETTRODOME</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-08-10</td>\n",
       "      <td>2021-09-05</td>\n",
       "      <td>2021-02-10</td>\n",
       "      <td>2021-06-14</td>\n",
       "      <td>None</td>\n",
       "      <td>01/01/0001</td>\n",
       "      <td>2021-04-12</td>\n",
       "      <td>2021-05-12</td>\n",
       "      <td>2021-06-12</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-04-12</td>\n",
       "      <td>2021-06-14</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-08-12</td>\n",
       "      <td>2021-07-12</td>\n",
       "      <td>1,500.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1,329.19</td>\n",
       "      <td>0000000005650</td>\n",
       "      <td>0000000005650</td>\n",
       "      <td>0000000005650</td>\n",
       "      <td>30.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>2</td>\n",
       "      <td>KR</td>\n",
       "      <td>56.50</td>\n",
       "      <td>56.50</td>\n",
       "      <td>56.50</td>\n",
       "      <td>56.50</td>\n",
       "      <td>56.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>56.50</td>\n",
       "      <td>0.58</td>\n",
       "      <td>15.97</td>\n",
       "      <td>73.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>56.50</td>\n",
       "      <td>0.58</td>\n",
       "      <td>15.97</td>\n",
       "      <td>73.05</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00</td>\n",
       "      <td>144.94</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>BP</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-2.00</td>\n",
       "      <td>Donna</td>\n",
       "      <td>57.00</td>\n",
       "      <td>SI</td>\n",
       "      <td>SI</td>\n",
       "      <td>2.50</td>\n",
       "      <td>SI</td>\n",
       "      <td>NO</td>\n",
       "      <td>SI</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>SI</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.57</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>29.00</td>\n",
       "      <td>152.00</td>\n",
       "      <td>ELETTRONICA &amp; ELETTRODOME</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>00001576642301_08_2021</td>\n",
       "      <td>00001576642301</td>\n",
       "      <td>10012101111101110000000</td>\n",
       "      <td>P</td>\n",
       "      <td>BP</td>\n",
       "      <td>None</td>\n",
       "      <td>Sicilia</td>\n",
       "      <td>ALPTEL</td>\n",
       "      <td>REPEAT BUSINESS</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-08-10</td>\n",
       "      <td>2021-09-05</td>\n",
       "      <td>2019-09-12</td>\n",
       "      <td>2021-05-27</td>\n",
       "      <td>None</td>\n",
       "      <td>01/01/0001</td>\n",
       "      <td>2021-04-12</td>\n",
       "      <td>2021-05-12</td>\n",
       "      <td>2021-06-12</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-04-26</td>\n",
       "      <td>2021-05-27</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-08-12</td>\n",
       "      <td>2021-07-12</td>\n",
       "      <td>14,280.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10,954.69</td>\n",
       "      <td>0000000026000</td>\n",
       "      <td>0000000026000</td>\n",
       "      <td>0000000026000</td>\n",
       "      <td>72.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>22.00</td>\n",
       "      <td>23.00</td>\n",
       "      <td>19.00</td>\n",
       "      <td>20.00</td>\n",
       "      <td>21.00</td>\n",
       "      <td>2</td>\n",
       "      <td>EN</td>\n",
       "      <td>260.00</td>\n",
       "      <td>260.00</td>\n",
       "      <td>260.00</td>\n",
       "      <td>260.00</td>\n",
       "      <td>260.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>260.00</td>\n",
       "      <td>16.20</td>\n",
       "      <td>12.00</td>\n",
       "      <td>288.20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>260.00</td>\n",
       "      <td>16.20</td>\n",
       "      <td>12.00</td>\n",
       "      <td>288.20</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00</td>\n",
       "      <td>532.00</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>BP</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.32</td>\n",
       "      <td>-2.00</td>\n",
       "      <td>Uomo</td>\n",
       "      <td>76.00</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>6.00</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>SI</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>SI</td>\n",
       "      <td>SI</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.41</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>29.00</td>\n",
       "      <td>669.00</td>\n",
       "      <td>FINANZIAMENTI AZIENDE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>00002230493301_08_2021</td>\n",
       "      <td>00002230493301</td>\n",
       "      <td>2111210</td>\n",
       "      <td>P</td>\n",
       "      <td>BP</td>\n",
       "      <td>None</td>\n",
       "      <td>Lazio</td>\n",
       "      <td>ALPTEL</td>\n",
       "      <td>SPESE MEDICHE</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-08-09</td>\n",
       "      <td>2021-09-05</td>\n",
       "      <td>2021-01-13</td>\n",
       "      <td>2021-07-27</td>\n",
       "      <td>None</td>\n",
       "      <td>01/01/0001</td>\n",
       "      <td>2021-04-12</td>\n",
       "      <td>2021-05-12</td>\n",
       "      <td>2021-06-12</td>\n",
       "      <td>2021-07-27</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-04-14</td>\n",
       "      <td>2021-07-27</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-08-12</td>\n",
       "      <td>2021-07-12</td>\n",
       "      <td>6,800.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6,214.51</td>\n",
       "      <td>0000000016600</td>\n",
       "      <td>0000000016600</td>\n",
       "      <td>0000000016600</td>\n",
       "      <td>48.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>7.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>2</td>\n",
       "      <td>RM</td>\n",
       "      <td>166.00</td>\n",
       "      <td>166.00</td>\n",
       "      <td>166.00</td>\n",
       "      <td>166.00</td>\n",
       "      <td>166.00</td>\n",
       "      <td>2.34</td>\n",
       "      <td>0.00</td>\n",
       "      <td>163.66</td>\n",
       "      <td>3.51</td>\n",
       "      <td>32.40</td>\n",
       "      <td>199.57</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>163.66</td>\n",
       "      <td>3.51</td>\n",
       "      <td>32.40</td>\n",
       "      <td>199.57</td>\n",
       "      <td>None</td>\n",
       "      <td>2.34</td>\n",
       "      <td>396.80</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>BP</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.15</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>Uomo</td>\n",
       "      <td>57.00</td>\n",
       "      <td>SI</td>\n",
       "      <td>SI</td>\n",
       "      <td>4.00</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>SI</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>6.00</td>\n",
       "      <td>1.14</td>\n",
       "      <td>0.92</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>28.00</td>\n",
       "      <td>180.00</td>\n",
       "      <td>SPESE MEDICHE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148458</th>\n",
       "      <td>0</td>\n",
       "      <td>00002785945301_06_2023</td>\n",
       "      <td>00002785945301</td>\n",
       "      <td>11011101100000</td>\n",
       "      <td>R</td>\n",
       "      <td>RI</td>\n",
       "      <td>None</td>\n",
       "      <td>Lazio</td>\n",
       "      <td>ALPTEX</td>\n",
       "      <td>DISINTERMEDIATO AUTOMOTIV</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-06-29</td>\n",
       "      <td>None</td>\n",
       "      <td>2022-04-14</td>\n",
       "      <td>2023-05-24</td>\n",
       "      <td>None</td>\n",
       "      <td>01/01/0001</td>\n",
       "      <td>2023-03-14</td>\n",
       "      <td>2023-04-14</td>\n",
       "      <td>2023-05-14</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-03-24</td>\n",
       "      <td>2023-05-24</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-07-14</td>\n",
       "      <td>2023-06-14</td>\n",
       "      <td>4,120.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3,169.02</td>\n",
       "      <td>0000000010600</td>\n",
       "      <td>0000000010600</td>\n",
       "      <td>0000000010600</td>\n",
       "      <td>48.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>14.00</td>\n",
       "      <td>15.00</td>\n",
       "      <td>11.00</td>\n",
       "      <td>12.00</td>\n",
       "      <td>13.00</td>\n",
       "      <td>2</td>\n",
       "      <td>RM</td>\n",
       "      <td>106.00</td>\n",
       "      <td>106.00</td>\n",
       "      <td>106.00</td>\n",
       "      <td>106.00</td>\n",
       "      <td>106.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>106.00</td>\n",
       "      <td>2.17</td>\n",
       "      <td>0.00</td>\n",
       "      <td>108.17</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>106.00</td>\n",
       "      <td>2.17</td>\n",
       "      <td>0.00</td>\n",
       "      <td>108.17</td>\n",
       "      <td>Y</td>\n",
       "      <td>0.00</td>\n",
       "      <td>246.40</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>SI</td>\n",
       "      <td>RI</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.31</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>4.00</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>SI</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>SI</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.39</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>15.00</td>\n",
       "      <td>426.00</td>\n",
       "      <td>AUTOMOTIVE NUOVO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148459</th>\n",
       "      <td>1</td>\n",
       "      <td>00002471469301_06_2023</td>\n",
       "      <td>00002471469301</td>\n",
       "      <td>110000000000000000000000</td>\n",
       "      <td>R</td>\n",
       "      <td>RI</td>\n",
       "      <td>None</td>\n",
       "      <td>Umbria</td>\n",
       "      <td>ALPTEX</td>\n",
       "      <td>PRESTITO PERSONALE</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-06-29</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-07-15</td>\n",
       "      <td>2023-05-24</td>\n",
       "      <td>None</td>\n",
       "      <td>01/01/0001</td>\n",
       "      <td>2023-03-14</td>\n",
       "      <td>2023-04-14</td>\n",
       "      <td>2023-05-14</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-03-14</td>\n",
       "      <td>2023-05-24</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-07-14</td>\n",
       "      <td>2023-06-14</td>\n",
       "      <td>6,250.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4,725.40</td>\n",
       "      <td>0000000011600</td>\n",
       "      <td>0000000011600</td>\n",
       "      <td>0000000011600</td>\n",
       "      <td>72.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>23.00</td>\n",
       "      <td>24.00</td>\n",
       "      <td>20.00</td>\n",
       "      <td>21.00</td>\n",
       "      <td>22.00</td>\n",
       "      <td>2</td>\n",
       "      <td>TR</td>\n",
       "      <td>116.00</td>\n",
       "      <td>116.00</td>\n",
       "      <td>116.00</td>\n",
       "      <td>116.00</td>\n",
       "      <td>116.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>116.00</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>116.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>116.00</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>116.85</td>\n",
       "      <td>Y</td>\n",
       "      <td>116.00</td>\n",
       "      <td>116.00</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>SI</td>\n",
       "      <td>RI</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.33</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>6.00</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>SI</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.16</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>15.00</td>\n",
       "      <td>699.00</td>\n",
       "      <td>PRESTITO PERSONALE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148460</th>\n",
       "      <td>0</td>\n",
       "      <td>00002324126301_06_2023</td>\n",
       "      <td>00002324126301</td>\n",
       "      <td>1000100000000110000000000000</td>\n",
       "      <td>R</td>\n",
       "      <td>RI</td>\n",
       "      <td>None</td>\n",
       "      <td>Lombardia</td>\n",
       "      <td>ALPTEX</td>\n",
       "      <td>DIPENDENTI AZIENDE DOC -</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-06-29</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-03-15</td>\n",
       "      <td>2022-05-05</td>\n",
       "      <td>None</td>\n",
       "      <td>01/01/0001</td>\n",
       "      <td>2023-03-14</td>\n",
       "      <td>2023-04-14</td>\n",
       "      <td>2023-05-14</td>\n",
       "      <td>2022-05-05</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-03-14</td>\n",
       "      <td>2023-05-14</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-07-14</td>\n",
       "      <td>2023-06-14</td>\n",
       "      <td>14,492.80</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9,057.10</td>\n",
       "      <td>0000000030300</td>\n",
       "      <td>0000000030300</td>\n",
       "      <td>0000000030300</td>\n",
       "      <td>60.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>27.00</td>\n",
       "      <td>28.00</td>\n",
       "      <td>24.00</td>\n",
       "      <td>25.00</td>\n",
       "      <td>26.00</td>\n",
       "      <td>2</td>\n",
       "      <td>VA</td>\n",
       "      <td>303.00</td>\n",
       "      <td>303.00</td>\n",
       "      <td>303.00</td>\n",
       "      <td>303.00</td>\n",
       "      <td>303.00</td>\n",
       "      <td>1.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>301.75</td>\n",
       "      <td>3.66</td>\n",
       "      <td>0.00</td>\n",
       "      <td>305.41</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>301.75</td>\n",
       "      <td>3.66</td>\n",
       "      <td>0.00</td>\n",
       "      <td>305.41</td>\n",
       "      <td>Y</td>\n",
       "      <td>1.25</td>\n",
       "      <td>303.00</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>SI</td>\n",
       "      <td>RI</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.47</td>\n",
       "      <td>-14.00</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>5.00</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>SI</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>SI</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.14</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>15.00</td>\n",
       "      <td>821.00</td>\n",
       "      <td>PRESTITI CON TRATTENUTA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148461</th>\n",
       "      <td>0</td>\n",
       "      <td>00001908346301_06_2023</td>\n",
       "      <td>00001908346301</td>\n",
       "      <td>10000000000000010000000000000000111000</td>\n",
       "      <td>R</td>\n",
       "      <td>RI</td>\n",
       "      <td>None</td>\n",
       "      <td>Lombardia</td>\n",
       "      <td>ALPTEX</td>\n",
       "      <td>SPESE MEDICHE</td>\n",
       "      <td>CURE ODONTOIATRICHE</td>\n",
       "      <td>2023-06-29</td>\n",
       "      <td>None</td>\n",
       "      <td>2020-05-15</td>\n",
       "      <td>2020-10-16</td>\n",
       "      <td>None</td>\n",
       "      <td>01/01/0001</td>\n",
       "      <td>2023-03-14</td>\n",
       "      <td>2023-04-14</td>\n",
       "      <td>2023-05-14</td>\n",
       "      <td>2020-10-16</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-03-14</td>\n",
       "      <td>2023-05-14</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-07-14</td>\n",
       "      <td>2023-06-14</td>\n",
       "      <td>6,654.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3,828.21</td>\n",
       "      <td>0000000012200</td>\n",
       "      <td>0000000012200</td>\n",
       "      <td>0000000012200</td>\n",
       "      <td>70.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>35.00</td>\n",
       "      <td>36.00</td>\n",
       "      <td>32.00</td>\n",
       "      <td>33.00</td>\n",
       "      <td>34.00</td>\n",
       "      <td>2</td>\n",
       "      <td>MI</td>\n",
       "      <td>122.00</td>\n",
       "      <td>122.00</td>\n",
       "      <td>122.00</td>\n",
       "      <td>122.00</td>\n",
       "      <td>122.00</td>\n",
       "      <td>26.24</td>\n",
       "      <td>0.00</td>\n",
       "      <td>95.76</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>95.76</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>95.76</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>95.76</td>\n",
       "      <td>Y</td>\n",
       "      <td>26.24</td>\n",
       "      <td>122.00</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>RI</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.51</td>\n",
       "      <td>-32.00</td>\n",
       "      <td>Uomo</td>\n",
       "      <td>61.00</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>5.83</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>SI</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.08</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>15.00</td>\n",
       "      <td>1,125.00</td>\n",
       "      <td>SPESE MEDICHE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148462</th>\n",
       "      <td>0</td>\n",
       "      <td>00002905333301_06_2023</td>\n",
       "      <td>00002905333301</td>\n",
       "      <td>10000000000</td>\n",
       "      <td>R</td>\n",
       "      <td>RI</td>\n",
       "      <td>None</td>\n",
       "      <td>Lazio</td>\n",
       "      <td>ALPTEX</td>\n",
       "      <td>AUTOMOTIVE USATO</td>\n",
       "      <td>forfour 2as. (W453) FW098</td>\n",
       "      <td>2023-06-29</td>\n",
       "      <td>None</td>\n",
       "      <td>2022-07-15</td>\n",
       "      <td>2023-05-14</td>\n",
       "      <td>None</td>\n",
       "      <td>01/01/0001</td>\n",
       "      <td>2023-03-14</td>\n",
       "      <td>2023-04-14</td>\n",
       "      <td>2023-05-14</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-03-14</td>\n",
       "      <td>2023-05-14</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-07-14</td>\n",
       "      <td>2023-06-14</td>\n",
       "      <td>7,009.40</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6,037.66</td>\n",
       "      <td>0000000014300</td>\n",
       "      <td>0000000014300</td>\n",
       "      <td>0000000014300</td>\n",
       "      <td>60.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11.00</td>\n",
       "      <td>12.00</td>\n",
       "      <td>8.00</td>\n",
       "      <td>9.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>2</td>\n",
       "      <td>RM</td>\n",
       "      <td>143.00</td>\n",
       "      <td>143.00</td>\n",
       "      <td>143.00</td>\n",
       "      <td>143.00</td>\n",
       "      <td>143.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>143.00</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.00</td>\n",
       "      <td>143.48</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>143.00</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.00</td>\n",
       "      <td>143.48</td>\n",
       "      <td>Y</td>\n",
       "      <td>0.00</td>\n",
       "      <td>171.95</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>RI</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-2.00</td>\n",
       "      <td>Donna</td>\n",
       "      <td>53.00</td>\n",
       "      <td>SI</td>\n",
       "      <td>NO</td>\n",
       "      <td>5.00</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>SI</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.09</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>15.00</td>\n",
       "      <td>334.00</td>\n",
       "      <td>AUTOMOTIVE USATO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>148463 rows × 94 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Target_95                  Chiave         PRATICA  \\\n",
       "0               0  00000854016301_08_2021  00000854016301   \n",
       "1               0  00006003826200_08_2021  00006003826200   \n",
       "2               0  00002286163301_08_2021  00002286163301   \n",
       "3               0  00001576642301_08_2021  00001576642301   \n",
       "4               0  00002230493301_08_2021  00002230493301   \n",
       "...           ...                     ...             ...   \n",
       "148458          0  00002785945301_06_2023  00002785945301   \n",
       "148459          1  00002471469301_06_2023  00002471469301   \n",
       "148460          0  00002324126301_06_2023  00002324126301   \n",
       "148461          0  00001908346301_06_2023  00001908346301   \n",
       "148462          0  00002905333301_06_2023  00002905333301   \n",
       "\n",
       "                                            DELIN_HISTORY MOD_PAGAM MOD_PAG  \\\n",
       "0                  13221222111111143332432332111100000000         P      BP   \n",
       "1       1001000000000000000000000000000000001100100110...         P      BP   \n",
       "2                                                  111010         P      BP   \n",
       "3                                 10012101111101110000000         P      BP   \n",
       "4                                                 2111210         P      BP   \n",
       "...                                                   ...       ...     ...   \n",
       "148458                                     11011101100000         R      RI   \n",
       "148459                           110000000000000000000000         R      RI   \n",
       "148460                       1000100000000110000000000000         R      RI   \n",
       "148461             10000000000000010000000000000000111000         R      RI   \n",
       "148462                                        10000000000         R      RI   \n",
       "\n",
       "       TIPO_GARANTE Denominazione Regione CODICE_PHONIA  \\\n",
       "0                 C                 Lazio        ALPTEL   \n",
       "1              None               Liguria        ALPTEL   \n",
       "2              None              Calabria        ALPTEL   \n",
       "3              None               Sicilia        ALPTEL   \n",
       "4              None                 Lazio        ALPTEL   \n",
       "...             ...                   ...           ...   \n",
       "148458         None                 Lazio        ALPTEX   \n",
       "148459         None                Umbria        ALPTEX   \n",
       "148460         None             Lombardia        ALPTEX   \n",
       "148461         None             Lombardia        ALPTEX   \n",
       "148462         None                 Lazio        ALPTEX   \n",
       "\n",
       "                     DES_PRODOTTO                   DES_BENE DATA_AFFIDO  \\\n",
       "0              PRESTITO PERSONALE                       None  2021-08-02   \n",
       "1                 REPEAT BUSINESS             FIDELIZ.OTTIMI  2021-08-10   \n",
       "2       ELETTRONICA & ELETTRODOME                       None  2021-08-10   \n",
       "3                 REPEAT BUSINESS                       None  2021-08-10   \n",
       "4                   SPESE MEDICHE                       None  2021-08-09   \n",
       "...                           ...                        ...         ...   \n",
       "148458  DISINTERMEDIATO AUTOMOTIV                       None  2023-06-29   \n",
       "148459         PRESTITO PERSONALE                       None  2023-06-29   \n",
       "148460   DIPENDENTI AZIENDE DOC -                       None  2023-06-29   \n",
       "148461              SPESE MEDICHE        CURE ODONTOIATRICHE  2023-06-29   \n",
       "148462           AUTOMOTIVE USATO  forfour 2as. (W453) FW098  2023-06-29   \n",
       "\n",
       "       DATA_FINE_AFFIDO DATA_FINANZIAM DT_VAL_ULT_PAGAM DATA_ESTINZ  \\\n",
       "0            2021-09-05     2018-06-12       2021-07-05        None   \n",
       "1            2021-09-05     2016-05-12       2021-06-09        None   \n",
       "2            2021-09-05     2021-02-10       2021-06-14        None   \n",
       "3            2021-09-05     2019-09-12       2021-05-27        None   \n",
       "4            2021-09-05     2021-01-13       2021-07-27        None   \n",
       "...                 ...            ...              ...         ...   \n",
       "148458             None     2022-04-14       2023-05-24        None   \n",
       "148459             None     2021-07-15       2023-05-24        None   \n",
       "148460             None     2021-03-15       2022-05-05        None   \n",
       "148461             None     2020-05-15       2020-10-16        None   \n",
       "148462             None     2022-07-15       2023-05-14        None   \n",
       "\n",
       "       DATA_AGGIORNAMENTO P_INST1_DUE_DATE P_INST2_DUE_DATE P_INST3_DUE_DATE  \\\n",
       "0              01/01/0001       2021-04-12       2021-05-12       2021-06-12   \n",
       "1              01/01/0001       2021-04-12       2021-05-12       2021-06-12   \n",
       "2              01/01/0001       2021-04-12       2021-05-12       2021-06-12   \n",
       "3              01/01/0001       2021-04-12       2021-05-12       2021-06-12   \n",
       "4              01/01/0001       2021-04-12       2021-05-12       2021-06-12   \n",
       "...                   ...              ...              ...              ...   \n",
       "148458         01/01/0001       2023-03-14       2023-04-14       2023-05-14   \n",
       "148459         01/01/0001       2023-03-14       2023-04-14       2023-05-14   \n",
       "148460         01/01/0001       2023-03-14       2023-04-14       2023-05-14   \n",
       "148461         01/01/0001       2023-03-14       2023-04-14       2023-05-14   \n",
       "148462         01/01/0001       2023-03-14       2023-04-14       2023-05-14   \n",
       "\n",
       "       DT_VAL_RAT_IMP1 DT_VAL_RAT_IMP2 P_INST1_VAL_DATE P_INST3_VAL_DATE  \\\n",
       "0                 None            None       2021-06-16       2021-07-05   \n",
       "1                 None            None       2021-04-16       2021-06-09   \n",
       "2                 None            None       2021-04-12       2021-06-14   \n",
       "3                 None            None       2021-04-26       2021-05-27   \n",
       "4           2021-07-27            None       2021-04-14       2021-07-27   \n",
       "...                ...             ...              ...              ...   \n",
       "148458            None            None       2023-03-24       2023-05-24   \n",
       "148459            None            None       2023-03-14       2023-05-24   \n",
       "148460      2022-05-05            None       2023-03-14       2023-05-14   \n",
       "148461      2020-10-16            None       2023-03-14       2023-05-14   \n",
       "148462            None            None       2023-03-14       2023-05-14   \n",
       "\n",
       "       SCAD_MAXIRATA SCAD_RAT_IMP2 SCAD_RAT_IMP1  IMP_FINANZIATO  \\\n",
       "0               None    2021-08-12    2021-07-12       15,200.00   \n",
       "1               None    2021-08-12    2021-07-12       14,234.40   \n",
       "2               None    2021-08-12    2021-07-12        1,500.00   \n",
       "3               None    2021-08-12    2021-07-12       14,280.00   \n",
       "4               None    2021-08-12    2021-07-12        6,800.00   \n",
       "...              ...           ...           ...             ...   \n",
       "148458          None    2023-07-14    2023-06-14        4,120.88   \n",
       "148459          None    2023-07-14    2023-06-14        6,250.00   \n",
       "148460          None    2023-07-14    2023-06-14       14,492.80   \n",
       "148461          None    2023-07-14    2023-06-14        6,654.00   \n",
       "148462          None    2023-07-14    2023-06-14        7,009.40   \n",
       "\n",
       "        IMP_MAXIRATA  CURRENT_BALANCE P_INST1_AMOUNT_PAID P_INST2_AMOUNT_PAID  \\\n",
       "0               0.00         9,770.58       0000000023700       0000000023700   \n",
       "1               0.00         8,636.46       0000000018350       0000000018350   \n",
       "2               0.00         1,329.19       0000000005650       0000000005650   \n",
       "3               0.00        10,954.69       0000000026000       0000000026000   \n",
       "4               0.00         6,214.51       0000000016600       0000000016600   \n",
       "...              ...              ...                 ...                 ...   \n",
       "148458          0.00         3,169.02       0000000010600       0000000010600   \n",
       "148459          0.00         4,725.40       0000000011600       0000000011600   \n",
       "148460          0.00         9,057.10       0000000030300       0000000030300   \n",
       "148461          0.00         3,828.21       0000000012200       0000000012200   \n",
       "148462          0.00         6,037.66       0000000014300       0000000014300   \n",
       "\n",
       "       P_INST3_AMOUNT_PAID  RATE_TOTALI  NUM_RATE_RIFI  NUM_RAT_IMP1  \\\n",
       "0            0000000023700        84.00           0.00         37.00   \n",
       "1            0000000018350       120.00           0.00         62.00   \n",
       "2            0000000005650        30.00           0.00          5.00   \n",
       "3            0000000026000        72.00           0.00         22.00   \n",
       "4            0000000016600        48.00           0.00          6.00   \n",
       "...                    ...          ...            ...           ...   \n",
       "148458       0000000010600        48.00           0.00         14.00   \n",
       "148459       0000000011600        72.00           0.00         23.00   \n",
       "148460       0000000030300        60.00           0.00         27.00   \n",
       "148461       0000000012200        70.00           0.00         35.00   \n",
       "148462       0000000014300        60.00           0.00         11.00   \n",
       "\n",
       "        NUM_RAT_IMP2  P_INST1_NUM  P_INST2_NUM  P_INST3_NUM  NUMERO RATE  \\\n",
       "0              38.00        34.00        35.00        36.00            2   \n",
       "1              63.00        59.00        60.00        61.00            2   \n",
       "2               6.00         2.00         3.00         4.00            2   \n",
       "3              23.00        19.00        20.00        21.00            2   \n",
       "4               7.00         3.00         4.00         5.00            2   \n",
       "...              ...          ...          ...          ...          ...   \n",
       "148458         15.00        11.00        12.00        13.00            2   \n",
       "148459         24.00        20.00        21.00        22.00            2   \n",
       "148460         28.00        24.00        25.00        26.00            2   \n",
       "148461         36.00        32.00        33.00        34.00            2   \n",
       "148462         12.00         8.00         9.00        10.00            2   \n",
       "\n",
       "       PROVINCIA  P_INST1_AMOUNT  P_INST2_AMOUNT  P_INST3_AMOUNT  \\\n",
       "0             LT          237.00          237.00          237.00   \n",
       "1             IM          183.50          183.50          183.50   \n",
       "2             KR           56.50           56.50           56.50   \n",
       "3             EN          260.00          260.00          260.00   \n",
       "4             RM          166.00          166.00          166.00   \n",
       "...          ...             ...             ...             ...   \n",
       "148458        RM          106.00          106.00          106.00   \n",
       "148459        TR          116.00          116.00          116.00   \n",
       "148460        VA          303.00          303.00          303.00   \n",
       "148461        MI          122.00          122.00          122.00   \n",
       "148462        RM          143.00          143.00          143.00   \n",
       "\n",
       "        IMP_RAT_IMP1  IMP_RAT_IMP2  IMP_PAG_RAT_IMP1  IMP_PAG_RAT_IMP2  \\\n",
       "0             237.00        237.00              0.00              0.00   \n",
       "1             183.50        183.50              0.00              0.00   \n",
       "2              56.50         56.50              0.00              0.00   \n",
       "3             260.00        260.00              0.00              0.00   \n",
       "4             166.00        166.00              2.34              0.00   \n",
       "...              ...           ...               ...               ...   \n",
       "148458        106.00        106.00              0.00              0.00   \n",
       "148459        116.00        116.00              0.00              0.00   \n",
       "148460        303.00        303.00              1.25              0.00   \n",
       "148461        122.00        122.00             26.24              0.00   \n",
       "148462        143.00        143.00              0.00              0.00   \n",
       "\n",
       "        TOT_RATE_AFF_INIZIO_MAND  TOT_MORA_AFF_INIZIO_MAND  \\\n",
       "0                         237.00                     28.24   \n",
       "1                         183.50                      1.56   \n",
       "2                          56.50                      0.58   \n",
       "3                         260.00                     16.20   \n",
       "4                         163.66                      3.51   \n",
       "...                          ...                       ...   \n",
       "148458                    106.00                      2.17   \n",
       "148459                    116.00                      0.85   \n",
       "148460                    301.75                      3.66   \n",
       "148461                     95.76                      0.00   \n",
       "148462                    143.00                      0.48   \n",
       "\n",
       "        TOT_ONERI_AFF_INIZIO_MAND  TOT_AFF_INIZIO_MAND  TOT_RATE_PAG  \\\n",
       "0                           12.00               277.24          0.00   \n",
       "1                           12.00               197.06          0.00   \n",
       "2                           15.97                73.05          0.00   \n",
       "3                           12.00               288.20          0.00   \n",
       "4                           32.40               199.57          0.00   \n",
       "...                           ...                  ...           ...   \n",
       "148458                       0.00               108.17          0.00   \n",
       "148459                       0.00               116.85          0.00   \n",
       "148460                       0.00               305.41          0.00   \n",
       "148461                       0.00                95.76          0.00   \n",
       "148462                       0.00               143.48          0.00   \n",
       "\n",
       "        TOT_MORA_PAG  TOT_ONERI_PAG  TOT_PAG  TOT_RATE_AFF  TOT_MORA_AFF  \\\n",
       "0               0.00           0.00     0.00        237.00         28.24   \n",
       "1               0.00           0.00     0.00        183.50          1.56   \n",
       "2               0.00           0.00     0.00         56.50          0.58   \n",
       "3               0.00           0.00     0.00        260.00         16.20   \n",
       "4               0.00           0.00     0.00        163.66          3.51   \n",
       "...              ...            ...      ...           ...           ...   \n",
       "148458          0.00           0.00     0.00        106.00          2.17   \n",
       "148459          0.00           0.00     0.00        116.00          0.85   \n",
       "148460          0.00           0.00     0.00        301.75          3.66   \n",
       "148461          0.00           0.00     0.00         95.76          0.00   \n",
       "148462          0.00           0.00     0.00        143.00          0.48   \n",
       "\n",
       "        TOT_ONERI_AFF  TOT_AFF Flag_Riciclo_SDD  TOT REC  TOT AFF  \\\n",
       "0               12.00   277.24             None     0.00   486.00   \n",
       "1               12.00   197.06                Y     0.00   379.00   \n",
       "2               15.97    73.05             None     0.00   144.94   \n",
       "3               12.00   288.20             None     0.00   532.00   \n",
       "4               32.40   199.57             None     2.34   396.80   \n",
       "...               ...      ...              ...      ...      ...   \n",
       "148458           0.00   108.17                Y     0.00   246.40   \n",
       "148459           0.00   116.85                Y   116.00   116.00   \n",
       "148460           0.00   305.41                Y     1.25   303.00   \n",
       "148461           0.00    95.76                Y    26.24   122.00   \n",
       "148462           0.00   143.48                Y     0.00   171.95   \n",
       "\n",
       "       Flag_Rifinanziamento Flag_Garante Flag_Cointestazione Metodo_pagamento  \\\n",
       "0                        NO           SI                  SI               BP   \n",
       "1                        NO           NO                  NO               BP   \n",
       "2                        NO           NO                  NO               BP   \n",
       "3                        NO           NO                  NO               BP   \n",
       "4                        NO           NO                  NO               BP   \n",
       "...                     ...          ...                 ...              ...   \n",
       "148458                   NO           NO                  SI               RI   \n",
       "148459                   NO           NO                  SI               RI   \n",
       "148460                   NO           NO                  SI               RI   \n",
       "148461                   NO           NO                  NO               RI   \n",
       "148462                   NO           NO                  NO               RI   \n",
       "\n",
       "        Ratio_Rate_Imp1  Ratio_Rate_Imp2  DistanzaAffidoUltimoPagamento  \\\n",
       "0                  0.44             0.45                          -1.00   \n",
       "1                  0.52             0.53                          -2.00   \n",
       "2                  0.17             0.20                          -2.00   \n",
       "3                  0.31             0.32                          -2.00   \n",
       "4                  0.12             0.15                          -0.00   \n",
       "...                 ...              ...                            ...   \n",
       "148458             0.29             0.31                          -1.00   \n",
       "148459             0.32             0.33                          -1.00   \n",
       "148460             0.45             0.47                         -14.00   \n",
       "148461             0.50             0.51                         -32.00   \n",
       "148462             0.18             0.20                          -2.00   \n",
       "\n",
       "        Sesso  Eta_Debitore NOSTART12M NOSTART6M  DurataFinanziamento  \\\n",
       "0        None           NaN         NO        NO                 7.00   \n",
       "1       Donna         59.00         NO        NO                10.00   \n",
       "2       Donna         57.00         SI        SI                 2.50   \n",
       "3        Uomo         76.00         NO        NO                 6.00   \n",
       "4        Uomo         57.00         SI        SI                 4.00   \n",
       "...       ...           ...        ...       ...                  ...   \n",
       "148458   None           NaN         NO        NO                 4.00   \n",
       "148459   None           NaN         NO        NO                 6.00   \n",
       "148460   None           NaN         NO        NO                 5.00   \n",
       "148461   Uomo         61.00         NO        NO                 5.83   \n",
       "148462  Donna         53.00         SI        NO                 5.00   \n",
       "\n",
       "       Flag_Galleggiamento_3M Flag_Galleggiamento_6M Flag_Gestione_Prec  \\\n",
       "0                          NO                     NO                 NO   \n",
       "1                          NO                     NO                 SI   \n",
       "2                          SI                     NO                 SI   \n",
       "3                          NO                     NO                 SI   \n",
       "4                          NO                     NO                 NO   \n",
       "...                       ...                    ...                ...   \n",
       "148458                     NO                     NO                 SI   \n",
       "148459                     NO                     NO                 NO   \n",
       "148460                     NO                     NO                 SI   \n",
       "148461                     NO                     NO                 NO   \n",
       "148462                     NO                     NO                 NO   \n",
       "\n",
       "       Flag_InsolvenzaGrave_3M Flag_InsolvenzaGrave_6M Flag_Miglioramento_3M  \\\n",
       "0                           SI                      SI                    NO   \n",
       "1                           NO                      NO                    NO   \n",
       "2                           NO                      NO                    NO   \n",
       "3                           NO                      NO                    NO   \n",
       "4                           NO                      NO                    NO   \n",
       "...                        ...                     ...                   ...   \n",
       "148458                      NO                      NO                    NO   \n",
       "148459                      NO                      NO                    NO   \n",
       "148460                      NO                      NO                    NO   \n",
       "148461                      NO                      NO                    NO   \n",
       "148462                      NO                      NO                    NO   \n",
       "\n",
       "       Flag_Rate_piu_uno Flag_rientrototale_6M Flag_Scivolamento_3M  \\\n",
       "0                     SI                    NO                   NO   \n",
       "1                     NO                    SI                   NO   \n",
       "2                     NO                    SI                   NO   \n",
       "3                     SI                    SI                   NO   \n",
       "4                     SI                    NO                   NO   \n",
       "...                  ...                   ...                  ...   \n",
       "148458                NO                    SI                   NO   \n",
       "148459                NO                    SI                   NO   \n",
       "148460                NO                    SI                   NO   \n",
       "148461                NO                    SI                   NO   \n",
       "148462                NO                    SI                   NO   \n",
       "\n",
       "       Flag_Scivolamento_6M  Numero_mesi_rec  Severity_12M  \\\n",
       "0                        NO            12.00          1.58   \n",
       "1                        NO             1.00          0.17   \n",
       "2                        NO             3.00          0.67   \n",
       "3                        NO             1.00          0.83   \n",
       "4                        NO             6.00          1.14   \n",
       "...                     ...              ...           ...   \n",
       "148458                   NO             2.00          0.58   \n",
       "148459                   NO             2.00          0.17   \n",
       "148460                   NO             1.00          0.17   \n",
       "148461                   NO             1.00          0.08   \n",
       "148462                   NO             1.00          0.09   \n",
       "\n",
       "        Severity_12M_pesata Flag_InsolvenzaGrave_12M FlagRecOver100  \\\n",
       "0                      0.95                       SI              0   \n",
       "1                      0.15                       NO              0   \n",
       "2                      0.57                       NO              0   \n",
       "3                      0.41                       NO              0   \n",
       "4                      0.92                       NO              0   \n",
       "...                     ...                      ...            ...   \n",
       "148458                 0.39                       NO              0   \n",
       "148459                 0.16                       NO              0   \n",
       "148460                 0.14                       NO              0   \n",
       "148461                 0.08                       NO              0   \n",
       "148462                 0.09                       NO              0   \n",
       "\n",
       "        AgeingGestioneGg  AgeingErogazioneGg       DESCRIZIONE PRODOTTO  \n",
       "0                  21.00            1,126.00         PRESTITO PERSONALE  \n",
       "1                  29.00            1,887.00      FINANZIAMENTI AZIENDE  \n",
       "2                  29.00              152.00  ELETTRONICA & ELETTRODOME  \n",
       "3                  29.00              669.00      FINANZIAMENTI AZIENDE  \n",
       "4                  28.00              180.00              SPESE MEDICHE  \n",
       "...                  ...                 ...                        ...  \n",
       "148458             15.00              426.00           AUTOMOTIVE NUOVO  \n",
       "148459             15.00              699.00         PRESTITO PERSONALE  \n",
       "148460             15.00              821.00    PRESTITI CON TRATTENUTA  \n",
       "148461             15.00            1,125.00              SPESE MEDICHE  \n",
       "148462             15.00              334.00           AUTOMOTIVE USATO  \n",
       "\n",
       "[148463 rows x 94 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ALTRI BENI E SERVIZI',\n",
       " 'ARREDAMENTO',\n",
       " 'AUTOMOTIVE NUOVO',\n",
       " 'AUTOMOTIVE USATO',\n",
       " 'CONSOLIDAMENTO DEL DEBITO',\n",
       " 'ELETTRONICA & ELETTRODOME',\n",
       " 'FINANZIAMENTI AZIENDE',\n",
       " 'INTERVENTI CASA',\n",
       " 'MOTO E CICLOMOTORI NUOVO',\n",
       " 'MOTO E CICLOMOTORI USATO',\n",
       " 'PRESTITI CON TRATTENUTA',\n",
       " 'PRESTITO PERSONALE',\n",
       " 'SPESE MEDICHE',\n",
       " 'TEMPO LIBERO']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(df_cluster['DESCRIZIONE PRODOTTO'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descrizioni statistiche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describex(data):\n",
    "    data = pd.DataFrame(data)\n",
    "    stats = data.describe(percentiles=[0.01,0.25,0.5,0.75,0.90,0.95,0.99])\n",
    "    skewness = data.skew()\n",
    "    kurtosis = data.kurtosis()\n",
    "    skewness_df = pd.DataFrame({'skewness' : skewness}).T\n",
    "    kurtosis_df = pd.DataFrame({'kurtosis' : kurtosis}).T\n",
    "    mediane = data.median()\n",
    "    mediane_df = pd.DataFrame({'mediane' : mediane}).T\n",
    "    valori_missing = pd.DataFrame({'valori missing': data.isna().sum()}).T\n",
    "    percentuale_missing = ((data.isna().sum() / len(data) * 100).round(2)).apply(lambda x: f'{x}%')\n",
    "    percentuale_missing_df = pd.DataFrame({'percentuale missing': percentuale_missing}).T\n",
    "    valori_unici = pd.DataFrame({'valori_unici':data.nunique()}).T\n",
    "    return pd.concat([stats, kurtosis_df, skewness_df, mediane_df, valori_missing, percentuale_missing_df, valori_unici])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target_95</th>\n",
       "      <th>IMP_FINANZIATO</th>\n",
       "      <th>IMP_MAXIRATA</th>\n",
       "      <th>CURRENT_BALANCE</th>\n",
       "      <th>RATE_TOTALI</th>\n",
       "      <th>NUM_RATE_RIFI</th>\n",
       "      <th>NUM_RAT_IMP1</th>\n",
       "      <th>NUM_RAT_IMP2</th>\n",
       "      <th>P_INST1_NUM</th>\n",
       "      <th>P_INST2_NUM</th>\n",
       "      <th>P_INST3_NUM</th>\n",
       "      <th>NUMERO RATE</th>\n",
       "      <th>P_INST1_AMOUNT</th>\n",
       "      <th>P_INST2_AMOUNT</th>\n",
       "      <th>P_INST3_AMOUNT</th>\n",
       "      <th>IMP_RAT_IMP1</th>\n",
       "      <th>IMP_RAT_IMP2</th>\n",
       "      <th>IMP_PAG_RAT_IMP1</th>\n",
       "      <th>IMP_PAG_RAT_IMP2</th>\n",
       "      <th>TOT_RATE_AFF_INIZIO_MAND</th>\n",
       "      <th>TOT_MORA_AFF_INIZIO_MAND</th>\n",
       "      <th>TOT_ONERI_AFF_INIZIO_MAND</th>\n",
       "      <th>TOT_AFF_INIZIO_MAND</th>\n",
       "      <th>TOT_RATE_PAG</th>\n",
       "      <th>TOT_MORA_PAG</th>\n",
       "      <th>TOT_ONERI_PAG</th>\n",
       "      <th>TOT_PAG</th>\n",
       "      <th>TOT_RATE_AFF</th>\n",
       "      <th>TOT_MORA_AFF</th>\n",
       "      <th>TOT_ONERI_AFF</th>\n",
       "      <th>TOT_AFF</th>\n",
       "      <th>TOT REC</th>\n",
       "      <th>TOT AFF</th>\n",
       "      <th>Ratio_Rate_Imp1</th>\n",
       "      <th>Ratio_Rate_Imp2</th>\n",
       "      <th>DistanzaAffidoUltimoPagamento</th>\n",
       "      <th>Eta_Debitore</th>\n",
       "      <th>DurataFinanziamento</th>\n",
       "      <th>Numero_mesi_rec</th>\n",
       "      <th>Severity_12M</th>\n",
       "      <th>Severity_12M_pesata</th>\n",
       "      <th>AgeingGestioneGg</th>\n",
       "      <th>AgeingErogazioneGg</th>\n",
       "      <th>Chiave</th>\n",
       "      <th>PRATICA</th>\n",
       "      <th>P_INST1_AMOUNT_PAID</th>\n",
       "      <th>P_INST2_AMOUNT_PAID</th>\n",
       "      <th>P_INST3_AMOUNT_PAID</th>\n",
       "      <th>FlagRecOver100</th>\n",
       "      <th>DELIN_HISTORY</th>\n",
       "      <th>MOD_PAGAM</th>\n",
       "      <th>MOD_PAG</th>\n",
       "      <th>TIPO_GARANTE</th>\n",
       "      <th>Denominazione Regione</th>\n",
       "      <th>CODICE_PHONIA</th>\n",
       "      <th>DES_PRODOTTO</th>\n",
       "      <th>DES_BENE</th>\n",
       "      <th>DATA_AFFIDO</th>\n",
       "      <th>DATA_FINE_AFFIDO</th>\n",
       "      <th>DATA_FINANZIAM</th>\n",
       "      <th>DT_VAL_ULT_PAGAM</th>\n",
       "      <th>DATA_ESTINZ</th>\n",
       "      <th>DATA_AGGIORNAMENTO</th>\n",
       "      <th>P_INST1_DUE_DATE</th>\n",
       "      <th>P_INST2_DUE_DATE</th>\n",
       "      <th>P_INST3_DUE_DATE</th>\n",
       "      <th>DT_VAL_RAT_IMP1</th>\n",
       "      <th>DT_VAL_RAT_IMP2</th>\n",
       "      <th>P_INST1_VAL_DATE</th>\n",
       "      <th>P_INST3_VAL_DATE</th>\n",
       "      <th>SCAD_MAXIRATA</th>\n",
       "      <th>SCAD_RAT_IMP2</th>\n",
       "      <th>SCAD_RAT_IMP1</th>\n",
       "      <th>PROVINCIA</th>\n",
       "      <th>Flag_Riciclo_SDD</th>\n",
       "      <th>Flag_Rifinanziamento</th>\n",
       "      <th>Flag_Garante</th>\n",
       "      <th>Flag_Cointestazione</th>\n",
       "      <th>Metodo_pagamento</th>\n",
       "      <th>Sesso</th>\n",
       "      <th>NOSTART12M</th>\n",
       "      <th>NOSTART6M</th>\n",
       "      <th>Flag_Galleggiamento_3M</th>\n",
       "      <th>Flag_Galleggiamento_6M</th>\n",
       "      <th>Flag_Gestione_Prec</th>\n",
       "      <th>Flag_InsolvenzaGrave_3M</th>\n",
       "      <th>Flag_InsolvenzaGrave_6M</th>\n",
       "      <th>Flag_Miglioramento_3M</th>\n",
       "      <th>Flag_Rate_piu_uno</th>\n",
       "      <th>Flag_rientrototale_6M</th>\n",
       "      <th>Flag_Scivolamento_3M</th>\n",
       "      <th>Flag_Scivolamento_6M</th>\n",
       "      <th>Flag_InsolvenzaGrave_12M</th>\n",
       "      <th>DESCRIZIONE PRODOTTO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>148,463.00</td>\n",
       "      <td>148,463.00</td>\n",
       "      <td>148,463.00</td>\n",
       "      <td>148,463.00</td>\n",
       "      <td>148,463.00</td>\n",
       "      <td>148,463.00</td>\n",
       "      <td>148,463.00</td>\n",
       "      <td>148,463.00</td>\n",
       "      <td>148,463.00</td>\n",
       "      <td>148,463.00</td>\n",
       "      <td>148,463.00</td>\n",
       "      <td>148,463.00</td>\n",
       "      <td>148,463.00</td>\n",
       "      <td>148,463.00</td>\n",
       "      <td>148,463.00</td>\n",
       "      <td>148,463.00</td>\n",
       "      <td>148,463.00</td>\n",
       "      <td>148,463.00</td>\n",
       "      <td>148,463.00</td>\n",
       "      <td>148,463.00</td>\n",
       "      <td>148,463.00</td>\n",
       "      <td>148,463.00</td>\n",
       "      <td>148,463.00</td>\n",
       "      <td>148,463.00</td>\n",
       "      <td>148,463.00</td>\n",
       "      <td>148,463.00</td>\n",
       "      <td>148,463.00</td>\n",
       "      <td>148,463.00</td>\n",
       "      <td>148,463.00</td>\n",
       "      <td>148,463.00</td>\n",
       "      <td>148,463.00</td>\n",
       "      <td>148,463.00</td>\n",
       "      <td>148,463.00</td>\n",
       "      <td>148,463.00</td>\n",
       "      <td>148,463.00</td>\n",
       "      <td>148,463.00</td>\n",
       "      <td>113,805.00</td>\n",
       "      <td>148,463.00</td>\n",
       "      <td>148,463.00</td>\n",
       "      <td>147,425.00</td>\n",
       "      <td>147,425.00</td>\n",
       "      <td>148,463.00</td>\n",
       "      <td>148,463.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.39</td>\n",
       "      <td>11,217.98</td>\n",
       "      <td>50.71</td>\n",
       "      <td>6,151.82</td>\n",
       "      <td>65.98</td>\n",
       "      <td>0.11</td>\n",
       "      <td>33.06</td>\n",
       "      <td>33.39</td>\n",
       "      <td>31.72</td>\n",
       "      <td>32.69</td>\n",
       "      <td>33.68</td>\n",
       "      <td>1.55</td>\n",
       "      <td>193.01</td>\n",
       "      <td>195.21</td>\n",
       "      <td>197.32</td>\n",
       "      <td>199.89</td>\n",
       "      <td>196.44</td>\n",
       "      <td>21.52</td>\n",
       "      <td>5.07</td>\n",
       "      <td>202.35</td>\n",
       "      <td>13.99</td>\n",
       "      <td>21.37</td>\n",
       "      <td>237.71</td>\n",
       "      <td>11.35</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.19</td>\n",
       "      <td>11.54</td>\n",
       "      <td>199.24</td>\n",
       "      <td>14.03</td>\n",
       "      <td>24.09</td>\n",
       "      <td>237.36</td>\n",
       "      <td>195.76</td>\n",
       "      <td>330.40</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.52</td>\n",
       "      <td>-396.78</td>\n",
       "      <td>50.96</td>\n",
       "      <td>5.50</td>\n",
       "      <td>7.68</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.59</td>\n",
       "      <td>-59.89</td>\n",
       "      <td>1,146.83</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.49</td>\n",
       "      <td>8,643.00</td>\n",
       "      <td>920.32</td>\n",
       "      <td>6,513.94</td>\n",
       "      <td>30.82</td>\n",
       "      <td>1.79</td>\n",
       "      <td>24.50</td>\n",
       "      <td>24.69</td>\n",
       "      <td>24.42</td>\n",
       "      <td>24.46</td>\n",
       "      <td>24.48</td>\n",
       "      <td>0.56</td>\n",
       "      <td>122.84</td>\n",
       "      <td>121.60</td>\n",
       "      <td>120.33</td>\n",
       "      <td>129.55</td>\n",
       "      <td>158.34</td>\n",
       "      <td>65.96</td>\n",
       "      <td>37.16</td>\n",
       "      <td>149.49</td>\n",
       "      <td>28.85</td>\n",
       "      <td>62.54</td>\n",
       "      <td>176.60</td>\n",
       "      <td>89.48</td>\n",
       "      <td>0.16</td>\n",
       "      <td>3.10</td>\n",
       "      <td>90.42</td>\n",
       "      <td>154.57</td>\n",
       "      <td>28.86</td>\n",
       "      <td>67.28</td>\n",
       "      <td>184.64</td>\n",
       "      <td>205.39</td>\n",
       "      <td>243.19</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.28</td>\n",
       "      <td>3,072.06</td>\n",
       "      <td>13.90</td>\n",
       "      <td>2.57</td>\n",
       "      <td>4.57</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.33</td>\n",
       "      <td>357.97</td>\n",
       "      <td>855.11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.00</td>\n",
       "      <td>190.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.29</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-24,270.00</td>\n",
       "      <td>18.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-3,364.00</td>\n",
       "      <td>29.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1%</th>\n",
       "      <td>0.00</td>\n",
       "      <td>598.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>74.85</td>\n",
       "      <td>12.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>29.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>15.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.34</td>\n",
       "      <td>0.00</td>\n",
       "      <td>32.26</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-24,254.00</td>\n",
       "      <td>23.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-1,995.00</td>\n",
       "      <td>30.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.00</td>\n",
       "      <td>4,050.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1,311.98</td>\n",
       "      <td>48.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>15.00</td>\n",
       "      <td>15.00</td>\n",
       "      <td>14.00</td>\n",
       "      <td>15.00</td>\n",
       "      <td>16.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>101.50</td>\n",
       "      <td>103.50</td>\n",
       "      <td>105.78</td>\n",
       "      <td>108.84</td>\n",
       "      <td>104.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.02</td>\n",
       "      <td>1.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>119.98</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>94.35</td>\n",
       "      <td>1.36</td>\n",
       "      <td>0.00</td>\n",
       "      <td>115.25</td>\n",
       "      <td>17.00</td>\n",
       "      <td>163.00</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.29</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>41.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.44</td>\n",
       "      <td>15.00</td>\n",
       "      <td>517.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.00</td>\n",
       "      <td>10,000.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3,988.73</td>\n",
       "      <td>60.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>29.00</td>\n",
       "      <td>30.00</td>\n",
       "      <td>28.00</td>\n",
       "      <td>29.00</td>\n",
       "      <td>30.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>180.00</td>\n",
       "      <td>181.05</td>\n",
       "      <td>182.50</td>\n",
       "      <td>184.00</td>\n",
       "      <td>181.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>181.00</td>\n",
       "      <td>5.31</td>\n",
       "      <td>12.00</td>\n",
       "      <td>209.23</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>178.00</td>\n",
       "      <td>5.35</td>\n",
       "      <td>12.00</td>\n",
       "      <td>207.71</td>\n",
       "      <td>160.28</td>\n",
       "      <td>271.30</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.52</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>51.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.54</td>\n",
       "      <td>16.00</td>\n",
       "      <td>1,003.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.00</td>\n",
       "      <td>16,208.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8,911.74</td>\n",
       "      <td>84.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>45.00</td>\n",
       "      <td>46.00</td>\n",
       "      <td>44.00</td>\n",
       "      <td>45.00</td>\n",
       "      <td>46.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>266.50</td>\n",
       "      <td>267.50</td>\n",
       "      <td>268.12</td>\n",
       "      <td>268.90</td>\n",
       "      <td>267.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>273.00</td>\n",
       "      <td>15.34</td>\n",
       "      <td>12.00</td>\n",
       "      <td>313.57</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>272.00</td>\n",
       "      <td>15.36</td>\n",
       "      <td>12.00</td>\n",
       "      <td>315.00</td>\n",
       "      <td>289.17</td>\n",
       "      <td>434.95</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>60.00</td>\n",
       "      <td>7.00</td>\n",
       "      <td>12.00</td>\n",
       "      <td>1.17</td>\n",
       "      <td>0.70</td>\n",
       "      <td>26.00</td>\n",
       "      <td>1,522.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90%</th>\n",
       "      <td>1.00</td>\n",
       "      <td>22,452.69</td>\n",
       "      <td>0.00</td>\n",
       "      <td>14,844.16</td>\n",
       "      <td>120.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>61.00</td>\n",
       "      <td>62.00</td>\n",
       "      <td>59.00</td>\n",
       "      <td>60.00</td>\n",
       "      <td>61.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>348.00</td>\n",
       "      <td>348.50</td>\n",
       "      <td>349.10</td>\n",
       "      <td>349.50</td>\n",
       "      <td>348.45</td>\n",
       "      <td>59.77</td>\n",
       "      <td>0.00</td>\n",
       "      <td>368.00</td>\n",
       "      <td>33.49</td>\n",
       "      <td>23.70</td>\n",
       "      <td>430.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>370.00</td>\n",
       "      <td>33.53</td>\n",
       "      <td>37.25</td>\n",
       "      <td>437.24</td>\n",
       "      <td>448.27</td>\n",
       "      <td>647.00</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.91</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>71.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>12.00</td>\n",
       "      <td>1.67</td>\n",
       "      <td>0.98</td>\n",
       "      <td>30.00</td>\n",
       "      <td>2,281.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95%</th>\n",
       "      <td>1.00</td>\n",
       "      <td>27,348.75</td>\n",
       "      <td>0.00</td>\n",
       "      <td>19,308.28</td>\n",
       "      <td>120.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>79.00</td>\n",
       "      <td>79.00</td>\n",
       "      <td>77.00</td>\n",
       "      <td>78.00</td>\n",
       "      <td>79.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>404.74</td>\n",
       "      <td>405.50</td>\n",
       "      <td>407.00</td>\n",
       "      <td>407.00</td>\n",
       "      <td>405.00</td>\n",
       "      <td>166.80</td>\n",
       "      <td>0.00</td>\n",
       "      <td>448.50</td>\n",
       "      <td>52.74</td>\n",
       "      <td>79.69</td>\n",
       "      <td>540.94</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>454.00</td>\n",
       "      <td>52.83</td>\n",
       "      <td>100.00</td>\n",
       "      <td>553.56</td>\n",
       "      <td>577.49</td>\n",
       "      <td>787.00</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.96</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>75.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>12.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.11</td>\n",
       "      <td>32.00</td>\n",
       "      <td>2,891.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99%</th>\n",
       "      <td>1.00</td>\n",
       "      <td>38,178.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>29,239.22</td>\n",
       "      <td>144.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>118.00</td>\n",
       "      <td>119.00</td>\n",
       "      <td>116.00</td>\n",
       "      <td>117.00</td>\n",
       "      <td>118.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>542.00</td>\n",
       "      <td>545.00</td>\n",
       "      <td>547.13</td>\n",
       "      <td>548.57</td>\n",
       "      <td>547.13</td>\n",
       "      <td>329.00</td>\n",
       "      <td>218.75</td>\n",
       "      <td>661.61</td>\n",
       "      <td>130.75</td>\n",
       "      <td>338.85</td>\n",
       "      <td>830.51</td>\n",
       "      <td>381.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>388.75</td>\n",
       "      <td>676.00</td>\n",
       "      <td>130.77</td>\n",
       "      <td>361.80</td>\n",
       "      <td>861.38</td>\n",
       "      <td>844.00</td>\n",
       "      <td>1,118.66</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>81.00</td>\n",
       "      <td>12.00</td>\n",
       "      <td>12.00</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1.64</td>\n",
       "      <td>43.00</td>\n",
       "      <td>3,862.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.00</td>\n",
       "      <td>79,260.80</td>\n",
       "      <td>45,100.00</td>\n",
       "      <td>67,376.72</td>\n",
       "      <td>149.00</td>\n",
       "      <td>48.00</td>\n",
       "      <td>144.00</td>\n",
       "      <td>144.00</td>\n",
       "      <td>146.00</td>\n",
       "      <td>147.00</td>\n",
       "      <td>148.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>2,482.50</td>\n",
       "      <td>2,482.50</td>\n",
       "      <td>2,482.50</td>\n",
       "      <td>15,609.50</td>\n",
       "      <td>15,483.00</td>\n",
       "      <td>1,640.50</td>\n",
       "      <td>1,640.50</td>\n",
       "      <td>15,609.50</td>\n",
       "      <td>694.12</td>\n",
       "      <td>1,325.27</td>\n",
       "      <td>15,660.02</td>\n",
       "      <td>13,212.23</td>\n",
       "      <td>29.76</td>\n",
       "      <td>267.87</td>\n",
       "      <td>13,213.13</td>\n",
       "      <td>15,609.50</td>\n",
       "      <td>694.12</td>\n",
       "      <td>1,325.27</td>\n",
       "      <td>15,660.02</td>\n",
       "      <td>15,621.50</td>\n",
       "      <td>15,621.50</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>93.00</td>\n",
       "      <td>12.42</td>\n",
       "      <td>12.00</td>\n",
       "      <td>8.92</td>\n",
       "      <td>4.87</td>\n",
       "      <td>1,090.00</td>\n",
       "      <td>4,717.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kurtosis</th>\n",
       "      <td>-1.79</td>\n",
       "      <td>1.87</td>\n",
       "      <td>556.35</td>\n",
       "      <td>4.71</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>323.02</td>\n",
       "      <td>2.56</td>\n",
       "      <td>2.47</td>\n",
       "      <td>2.46</td>\n",
       "      <td>2.44</td>\n",
       "      <td>2.44</td>\n",
       "      <td>-0.84</td>\n",
       "      <td>5.43</td>\n",
       "      <td>5.68</td>\n",
       "      <td>6.01</td>\n",
       "      <td>1,977.55</td>\n",
       "      <td>3,075.45</td>\n",
       "      <td>23.54</td>\n",
       "      <td>128.02</td>\n",
       "      <td>1,119.05</td>\n",
       "      <td>105.87</td>\n",
       "      <td>75.23</td>\n",
       "      <td>580.26</td>\n",
       "      <td>6,702.71</td>\n",
       "      <td>23,907.69</td>\n",
       "      <td>1,463.68</td>\n",
       "      <td>6,434.07</td>\n",
       "      <td>980.67</td>\n",
       "      <td>105.73</td>\n",
       "      <td>63.35</td>\n",
       "      <td>486.85</td>\n",
       "      <td>321.38</td>\n",
       "      <td>159.65</td>\n",
       "      <td>-1.12</td>\n",
       "      <td>-1.08</td>\n",
       "      <td>56.34</td>\n",
       "      <td>-0.56</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>-1.59</td>\n",
       "      <td>16.20</td>\n",
       "      <td>14.53</td>\n",
       "      <td>28.38</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.20</td>\n",
       "      <td>2.20</td>\n",
       "      <td>5.43</td>\n",
       "      <td>5.69</td>\n",
       "      <td>6.01</td>\n",
       "      <td>1,040.55</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>skewness</th>\n",
       "      <td>0.46</td>\n",
       "      <td>1.16</td>\n",
       "      <td>21.77</td>\n",
       "      <td>1.86</td>\n",
       "      <td>0.41</td>\n",
       "      <td>17.64</td>\n",
       "      <td>1.37</td>\n",
       "      <td>1.33</td>\n",
       "      <td>1.32</td>\n",
       "      <td>1.31</td>\n",
       "      <td>1.30</td>\n",
       "      <td>0.37</td>\n",
       "      <td>1.11</td>\n",
       "      <td>1.16</td>\n",
       "      <td>1.21</td>\n",
       "      <td>18.95</td>\n",
       "      <td>35.89</td>\n",
       "      <td>4.24</td>\n",
       "      <td>9.56</td>\n",
       "      <td>13.16</td>\n",
       "      <td>7.83</td>\n",
       "      <td>7.60</td>\n",
       "      <td>8.80</td>\n",
       "      <td>52.57</td>\n",
       "      <td>142.48</td>\n",
       "      <td>30.30</td>\n",
       "      <td>51.18</td>\n",
       "      <td>12.07</td>\n",
       "      <td>7.82</td>\n",
       "      <td>6.97</td>\n",
       "      <td>7.96</td>\n",
       "      <td>6.17</td>\n",
       "      <td>4.30</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-7.64</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.41</td>\n",
       "      <td>-0.37</td>\n",
       "      <td>2.43</td>\n",
       "      <td>2.20</td>\n",
       "      <td>-5.14</td>\n",
       "      <td>1.28</td>\n",
       "      <td>1.52</td>\n",
       "      <td>1.52</td>\n",
       "      <td>1.11</td>\n",
       "      <td>1.16</td>\n",
       "      <td>1.21</td>\n",
       "      <td>32.29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mediane</th>\n",
       "      <td>0.00</td>\n",
       "      <td>10,000.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3,988.73</td>\n",
       "      <td>60.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>29.00</td>\n",
       "      <td>30.00</td>\n",
       "      <td>28.00</td>\n",
       "      <td>29.00</td>\n",
       "      <td>30.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>180.00</td>\n",
       "      <td>181.05</td>\n",
       "      <td>182.50</td>\n",
       "      <td>184.00</td>\n",
       "      <td>181.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>181.00</td>\n",
       "      <td>5.31</td>\n",
       "      <td>12.00</td>\n",
       "      <td>209.23</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>178.00</td>\n",
       "      <td>5.35</td>\n",
       "      <td>12.00</td>\n",
       "      <td>207.71</td>\n",
       "      <td>160.28</td>\n",
       "      <td>271.30</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.52</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>51.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.54</td>\n",
       "      <td>16.00</td>\n",
       "      <td>1,003.00</td>\n",
       "      <td>1,720,981,301,082,021.00</td>\n",
       "      <td>1,720,981,301.00</td>\n",
       "      <td>18,000.00</td>\n",
       "      <td>18,105.00</td>\n",
       "      <td>18,250.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>valori missing</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>34658</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1038</td>\n",
       "      <td>1038</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>531</td>\n",
       "      <td>0</td>\n",
       "      <td>131487</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>72490</td>\n",
       "      <td>0</td>\n",
       "      <td>12566</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>148343</td>\n",
       "      <td>0</td>\n",
       "      <td>6583</td>\n",
       "      <td>4636</td>\n",
       "      <td>2683</td>\n",
       "      <td>104739</td>\n",
       "      <td>144061</td>\n",
       "      <td>6585</td>\n",
       "      <td>2688</td>\n",
       "      <td>147917</td>\n",
       "      <td>3629</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38916</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>34658</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>percentuale missing</th>\n",
       "      <td>0.0%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>23.34%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>0.7%</td>\n",
       "      <td>0.7%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>0.36%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>88.57%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>48.83%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>8.46%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>99.92%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>4.43%</td>\n",
       "      <td>3.12%</td>\n",
       "      <td>1.81%</td>\n",
       "      <td>70.55%</td>\n",
       "      <td>97.03%</td>\n",
       "      <td>4.44%</td>\n",
       "      <td>1.81%</td>\n",
       "      <td>99.63%</td>\n",
       "      <td>2.44%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>26.21%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>23.34%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>0.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>valori_unici</th>\n",
       "      <td>2</td>\n",
       "      <td>14874</td>\n",
       "      <td>229</td>\n",
       "      <td>83416</td>\n",
       "      <td>132</td>\n",
       "      <td>6</td>\n",
       "      <td>144</td>\n",
       "      <td>144</td>\n",
       "      <td>146</td>\n",
       "      <td>147</td>\n",
       "      <td>148</td>\n",
       "      <td>4</td>\n",
       "      <td>8437</td>\n",
       "      <td>8494</td>\n",
       "      <td>8598</td>\n",
       "      <td>7504</td>\n",
       "      <td>7313</td>\n",
       "      <td>7159</td>\n",
       "      <td>1847</td>\n",
       "      <td>17708</td>\n",
       "      <td>8424</td>\n",
       "      <td>5049</td>\n",
       "      <td>42089</td>\n",
       "      <td>2290</td>\n",
       "      <td>72</td>\n",
       "      <td>407</td>\n",
       "      <td>2536</td>\n",
       "      <td>17655</td>\n",
       "      <td>8435</td>\n",
       "      <td>5842</td>\n",
       "      <td>42966</td>\n",
       "      <td>16186</td>\n",
       "      <td>19416</td>\n",
       "      <td>2185</td>\n",
       "      <td>2269</td>\n",
       "      <td>78</td>\n",
       "      <td>75</td>\n",
       "      <td>132</td>\n",
       "      <td>13</td>\n",
       "      <td>190</td>\n",
       "      <td>1853</td>\n",
       "      <td>1754</td>\n",
       "      <td>2628</td>\n",
       "      <td>88652</td>\n",
       "      <td>46907</td>\n",
       "      <td>8437</td>\n",
       "      <td>8498</td>\n",
       "      <td>8598</td>\n",
       "      <td>2</td>\n",
       "      <td>63110</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>26</td>\n",
       "      <td>10021</td>\n",
       "      <td>473</td>\n",
       "      <td>716</td>\n",
       "      <td>2824</td>\n",
       "      <td>1032</td>\n",
       "      <td>93</td>\n",
       "      <td>23</td>\n",
       "      <td>817</td>\n",
       "      <td>805</td>\n",
       "      <td>795</td>\n",
       "      <td>1077</td>\n",
       "      <td>426</td>\n",
       "      <td>925</td>\n",
       "      <td>886</td>\n",
       "      <td>213</td>\n",
       "      <td>1655</td>\n",
       "      <td>1858</td>\n",
       "      <td>108</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Target_95 IMP_FINANZIATO IMP_MAXIRATA CURRENT_BALANCE  \\\n",
       "count               148,463.00     148,463.00   148,463.00      148,463.00   \n",
       "mean                      0.39      11,217.98        50.71        6,151.82   \n",
       "std                       0.49       8,643.00       920.32        6,513.94   \n",
       "min                       0.00         190.00         0.00            0.00   \n",
       "1%                        0.00         598.00         0.00           74.85   \n",
       "25%                       0.00       4,050.00         0.00        1,311.98   \n",
       "50%                       0.00      10,000.00         0.00        3,988.73   \n",
       "75%                       1.00      16,208.00         0.00        8,911.74   \n",
       "90%                       1.00      22,452.69         0.00       14,844.16   \n",
       "95%                       1.00      27,348.75         0.00       19,308.28   \n",
       "99%                       1.00      38,178.00         0.00       29,239.22   \n",
       "max                       1.00      79,260.80    45,100.00       67,376.72   \n",
       "kurtosis                 -1.79           1.87       556.35            4.71   \n",
       "skewness                  0.46           1.16        21.77            1.86   \n",
       "mediane                   0.00      10,000.00         0.00        3,988.73   \n",
       "valori missing               0              0            0               0   \n",
       "percentuale missing       0.0%           0.0%         0.0%            0.0%   \n",
       "valori_unici                 2          14874          229           83416   \n",
       "\n",
       "                    RATE_TOTALI NUM_RATE_RIFI NUM_RAT_IMP1 NUM_RAT_IMP2  \\\n",
       "count                148,463.00    148,463.00   148,463.00   148,463.00   \n",
       "mean                      65.98          0.11        33.06        33.39   \n",
       "std                       30.82          1.79        24.50        24.69   \n",
       "min                        3.00          0.00         1.00         0.00   \n",
       "1%                        12.00          0.00         1.00         0.00   \n",
       "25%                       48.00          0.00        15.00        15.00   \n",
       "50%                       60.00          0.00        29.00        30.00   \n",
       "75%                       84.00          0.00        45.00        46.00   \n",
       "90%                      120.00          0.00        61.00        62.00   \n",
       "95%                      120.00          0.00        79.00        79.00   \n",
       "99%                      144.00          0.00       118.00       119.00   \n",
       "max                      149.00         48.00       144.00       144.00   \n",
       "kurtosis                  -0.20        323.02         2.56         2.47   \n",
       "skewness                   0.41         17.64         1.37         1.33   \n",
       "mediane                   60.00          0.00        29.00        30.00   \n",
       "valori missing                0             0            0            0   \n",
       "percentuale missing        0.0%          0.0%         0.0%         0.0%   \n",
       "valori_unici                132             6          144          144   \n",
       "\n",
       "                    P_INST1_NUM P_INST2_NUM P_INST3_NUM NUMERO RATE  \\\n",
       "count                148,463.00  148,463.00  148,463.00  148,463.00   \n",
       "mean                      31.72       32.69       33.68        1.55   \n",
       "std                       24.42       24.46       24.48        0.56   \n",
       "min                        0.00        0.00        0.00        1.00   \n",
       "1%                         0.00        0.00        0.00        1.00   \n",
       "25%                       14.00       15.00       16.00        1.00   \n",
       "50%                       28.00       29.00       30.00        2.00   \n",
       "75%                       44.00       45.00       46.00        2.00   \n",
       "90%                       59.00       60.00       61.00        2.00   \n",
       "95%                       77.00       78.00       79.00        2.00   \n",
       "99%                      116.00      117.00      118.00        3.00   \n",
       "max                      146.00      147.00      148.00        4.00   \n",
       "kurtosis                   2.46        2.44        2.44       -0.84   \n",
       "skewness                   1.32        1.31        1.30        0.37   \n",
       "mediane                   28.00       29.00       30.00        2.00   \n",
       "valori missing                0           0           0           0   \n",
       "percentuale missing        0.0%        0.0%        0.0%        0.0%   \n",
       "valori_unici                146         147         148           4   \n",
       "\n",
       "                    P_INST1_AMOUNT P_INST2_AMOUNT P_INST3_AMOUNT IMP_RAT_IMP1  \\\n",
       "count                   148,463.00     148,463.00     148,463.00   148,463.00   \n",
       "mean                        193.01         195.21         197.32       199.89   \n",
       "std                         122.84         121.60         120.33       129.55   \n",
       "min                           0.00           0.00           0.00         6.29   \n",
       "1%                            0.00           0.00           0.00        29.50   \n",
       "25%                         101.50         103.50         105.78       108.84   \n",
       "50%                         180.00         181.05         182.50       184.00   \n",
       "75%                         266.50         267.50         268.12       268.90   \n",
       "90%                         348.00         348.50         349.10       349.50   \n",
       "95%                         404.74         405.50         407.00       407.00   \n",
       "99%                         542.00         545.00         547.13       548.57   \n",
       "max                       2,482.50       2,482.50       2,482.50    15,609.50   \n",
       "kurtosis                      5.43           5.68           6.01     1,977.55   \n",
       "skewness                      1.11           1.16           1.21        18.95   \n",
       "mediane                     180.00         181.05         182.50       184.00   \n",
       "valori missing                   0              0              0            0   \n",
       "percentuale missing           0.0%           0.0%           0.0%         0.0%   \n",
       "valori_unici                  8437           8494           8598         7504   \n",
       "\n",
       "                    IMP_RAT_IMP2 IMP_PAG_RAT_IMP1 IMP_PAG_RAT_IMP2  \\\n",
       "count                 148,463.00       148,463.00       148,463.00   \n",
       "mean                      196.44            21.52             5.07   \n",
       "std                       158.34            65.96            37.16   \n",
       "min                         0.00             0.00             0.00   \n",
       "1%                          0.00             0.00             0.00   \n",
       "25%                       104.00             0.00             0.00   \n",
       "50%                       181.50             0.00             0.00   \n",
       "75%                       267.00             1.00             0.00   \n",
       "90%                       348.45            59.77             0.00   \n",
       "95%                       405.00           166.80             0.00   \n",
       "99%                       547.13           329.00           218.75   \n",
       "max                    15,483.00         1,640.50         1,640.50   \n",
       "kurtosis                3,075.45            23.54           128.02   \n",
       "skewness                   35.89             4.24             9.56   \n",
       "mediane                   181.50             0.00             0.00   \n",
       "valori missing                 0                0                0   \n",
       "percentuale missing         0.0%             0.0%             0.0%   \n",
       "valori_unici                7313             7159             1847   \n",
       "\n",
       "                    TOT_RATE_AFF_INIZIO_MAND TOT_MORA_AFF_INIZIO_MAND  \\\n",
       "count                             148,463.00               148,463.00   \n",
       "mean                                  202.35                    13.99   \n",
       "std                                   149.49                    28.85   \n",
       "min                                    10.00                     0.00   \n",
       "1%                                     12.00                     0.00   \n",
       "25%                                   100.02                     1.31   \n",
       "50%                                   181.00                     5.31   \n",
       "75%                                   273.00                    15.34   \n",
       "90%                                   368.00                    33.49   \n",
       "95%                                   448.50                    52.74   \n",
       "99%                                   661.61                   130.75   \n",
       "max                                15,609.50                   694.12   \n",
       "kurtosis                            1,119.05                   105.87   \n",
       "skewness                               13.16                     7.83   \n",
       "mediane                               181.00                     5.31   \n",
       "valori missing                             0                        0   \n",
       "percentuale missing                     0.0%                     0.0%   \n",
       "valori_unici                           17708                     8424   \n",
       "\n",
       "                    TOT_ONERI_AFF_INIZIO_MAND TOT_AFF_INIZIO_MAND  \\\n",
       "count                              148,463.00          148,463.00   \n",
       "mean                                    21.37              237.71   \n",
       "std                                     62.54              176.60   \n",
       "min                                      0.00               10.00   \n",
       "1%                                       0.00               15.00   \n",
       "25%                                      0.00              119.98   \n",
       "50%                                     12.00              209.23   \n",
       "75%                                     12.00              313.57   \n",
       "90%                                     23.70              430.38   \n",
       "95%                                     79.69              540.94   \n",
       "99%                                    338.85              830.51   \n",
       "max                                  1,325.27           15,660.02   \n",
       "kurtosis                                75.23              580.26   \n",
       "skewness                                 7.60                8.80   \n",
       "mediane                                 12.00              209.23   \n",
       "valori missing                              0                   0   \n",
       "percentuale missing                      0.0%                0.0%   \n",
       "valori_unici                             5049               42089   \n",
       "\n",
       "                    TOT_RATE_PAG TOT_MORA_PAG TOT_ONERI_PAG    TOT_PAG  \\\n",
       "count                 148,463.00   148,463.00    148,463.00 148,463.00   \n",
       "mean                       11.35         0.00          0.19      11.54   \n",
       "std                        89.48         0.16          3.10      90.42   \n",
       "min                         0.00         0.00          0.00       0.00   \n",
       "1%                          0.00         0.00          0.00       0.00   \n",
       "25%                         0.00         0.00          0.00       0.00   \n",
       "50%                         0.00         0.00          0.00       0.00   \n",
       "75%                         0.00         0.00          0.00       0.00   \n",
       "90%                         0.00         0.00          0.00       0.00   \n",
       "95%                         0.00         0.00          0.00       0.00   \n",
       "99%                       381.38         0.00          0.00     388.75   \n",
       "max                    13,212.23        29.76        267.87  13,213.13   \n",
       "kurtosis                6,702.71    23,907.69      1,463.68   6,434.07   \n",
       "skewness                   52.57       142.48         30.30      51.18   \n",
       "mediane                     0.00         0.00          0.00       0.00   \n",
       "valori missing                 0            0             0          0   \n",
       "percentuale missing         0.0%         0.0%          0.0%       0.0%   \n",
       "valori_unici                2290           72           407       2536   \n",
       "\n",
       "                    TOT_RATE_AFF TOT_MORA_AFF TOT_ONERI_AFF    TOT_AFF  \\\n",
       "count                 148,463.00   148,463.00    148,463.00 148,463.00   \n",
       "mean                      199.24        14.03         24.09     237.36   \n",
       "std                       154.57        28.86         67.28     184.64   \n",
       "min                         0.00         0.00          0.00       0.00   \n",
       "1%                          0.00         0.00          0.00       3.34   \n",
       "25%                        94.35         1.36          0.00     115.25   \n",
       "50%                       178.00         5.35         12.00     207.71   \n",
       "75%                       272.00        15.36         12.00     315.00   \n",
       "90%                       370.00        33.53         37.25     437.24   \n",
       "95%                       454.00        52.83        100.00     553.56   \n",
       "99%                       676.00       130.77        361.80     861.38   \n",
       "max                    15,609.50       694.12      1,325.27  15,660.02   \n",
       "kurtosis                  980.67       105.73         63.35     486.85   \n",
       "skewness                   12.07         7.82          6.97       7.96   \n",
       "mediane                   178.00         5.35         12.00     207.71   \n",
       "valori missing                 0            0             0          0   \n",
       "percentuale missing         0.0%         0.0%          0.0%       0.0%   \n",
       "valori_unici               17655         8435          5842      42966   \n",
       "\n",
       "                       TOT REC    TOT AFF Ratio_Rate_Imp1 Ratio_Rate_Imp2  \\\n",
       "count               148,463.00 148,463.00      148,463.00      148,463.00   \n",
       "mean                    195.76     330.40            0.52            0.52   \n",
       "std                     205.39     243.19            0.28            0.28   \n",
       "min                       0.00       0.00            0.01            0.00   \n",
       "1%                        0.00      32.26            0.03            0.00   \n",
       "25%                      17.00     163.00            0.29            0.29   \n",
       "50%                     160.28     271.30            0.52            0.52   \n",
       "75%                     289.17     434.95            0.75            0.75   \n",
       "90%                     448.27     647.00            0.91            0.91   \n",
       "95%                     577.49     787.00            0.96            0.96   \n",
       "99%                     844.00   1,118.66            1.00            1.00   \n",
       "max                  15,621.50  15,621.50            1.00            1.00   \n",
       "kurtosis                321.38     159.65           -1.12           -1.08   \n",
       "skewness                  6.17       4.30           -0.01           -0.04   \n",
       "mediane                 160.28     271.30            0.52            0.52   \n",
       "valori missing               0          0               0               0   \n",
       "percentuale missing       0.0%       0.0%            0.0%            0.0%   \n",
       "valori_unici             16186      19416            2185            2269   \n",
       "\n",
       "                    DistanzaAffidoUltimoPagamento Eta_Debitore  \\\n",
       "count                                  148,463.00   113,805.00   \n",
       "mean                                      -396.78        50.96   \n",
       "std                                      3,072.06        13.90   \n",
       "min                                    -24,270.00        18.00   \n",
       "1%                                     -24,254.00        23.00   \n",
       "25%                                         -1.00        41.00   \n",
       "50%                                         -1.00        51.00   \n",
       "75%                                         -1.00        60.00   \n",
       "90%                                         -1.00        71.00   \n",
       "95%                                         -0.00        75.00   \n",
       "99%                                          0.00        81.00   \n",
       "max                                          1.00        93.00   \n",
       "kurtosis                                    56.34        -0.56   \n",
       "skewness                                    -7.64         0.12   \n",
       "mediane                                     -1.00        51.00   \n",
       "valori missing                                  0        34658   \n",
       "percentuale missing                          0.0%       23.34%   \n",
       "valori_unici                                   78           75   \n",
       "\n",
       "                    DurataFinanziamento Numero_mesi_rec Severity_12M  \\\n",
       "count                        148,463.00      148,463.00   147,425.00   \n",
       "mean                               5.50            7.68         1.00   \n",
       "std                                2.57            4.57         0.63   \n",
       "min                                0.25            0.00         0.00   \n",
       "1%                                 1.00            0.00         0.08   \n",
       "25%                                4.00            3.00         0.67   \n",
       "50%                                5.00           10.00         1.00   \n",
       "75%                                7.00           12.00         1.17   \n",
       "90%                               10.00           12.00         1.67   \n",
       "95%                               10.00           12.00         2.00   \n",
       "99%                               12.00           12.00         3.17   \n",
       "max                               12.42           12.00         8.92   \n",
       "kurtosis                          -0.20           -1.59        16.20   \n",
       "skewness                           0.41           -0.37         2.43   \n",
       "mediane                            5.00           10.00         1.00   \n",
       "valori missing                        0               0         1038   \n",
       "percentuale missing                0.0%            0.0%         0.7%   \n",
       "valori_unici                        132              13          190   \n",
       "\n",
       "                    Severity_12M_pesata AgeingGestioneGg AgeingErogazioneGg  \\\n",
       "count                        147,425.00       148,463.00         148,463.00   \n",
       "mean                               0.59           -59.89           1,146.83   \n",
       "std                                0.33           357.97             855.11   \n",
       "min                                0.00        -3,364.00              29.00   \n",
       "1%                                 0.01        -1,995.00              30.00   \n",
       "25%                                0.44            15.00             517.00   \n",
       "50%                                0.54            16.00           1,003.00   \n",
       "75%                                0.70            26.00           1,522.00   \n",
       "90%                                0.98            30.00           2,281.00   \n",
       "95%                                1.11            32.00           2,891.00   \n",
       "99%                                1.64            43.00           3,862.00   \n",
       "max                                4.87         1,090.00           4,717.00   \n",
       "kurtosis                          14.53            28.38               1.78   \n",
       "skewness                           2.20            -5.14               1.28   \n",
       "mediane                            0.54            16.00           1,003.00   \n",
       "valori missing                     1038                0                  0   \n",
       "percentuale missing                0.7%             0.0%               0.0%   \n",
       "valori_unici                       1853             1754               2628   \n",
       "\n",
       "                                      Chiave          PRATICA  \\\n",
       "count                                    NaN              NaN   \n",
       "mean                                     NaN              NaN   \n",
       "std                                      NaN              NaN   \n",
       "min                                      NaN              NaN   \n",
       "1%                                       NaN              NaN   \n",
       "25%                                      NaN              NaN   \n",
       "50%                                      NaN              NaN   \n",
       "75%                                      NaN              NaN   \n",
       "90%                                      NaN              NaN   \n",
       "95%                                      NaN              NaN   \n",
       "99%                                      NaN              NaN   \n",
       "max                                      NaN              NaN   \n",
       "kurtosis                                2.20             2.20   \n",
       "skewness                                1.52             1.52   \n",
       "mediane             1,720,981,301,082,021.00 1,720,981,301.00   \n",
       "valori missing                             0                0   \n",
       "percentuale missing                     0.0%             0.0%   \n",
       "valori_unici                           88652            46907   \n",
       "\n",
       "                    P_INST1_AMOUNT_PAID P_INST2_AMOUNT_PAID  \\\n",
       "count                               NaN                 NaN   \n",
       "mean                                NaN                 NaN   \n",
       "std                                 NaN                 NaN   \n",
       "min                                 NaN                 NaN   \n",
       "1%                                  NaN                 NaN   \n",
       "25%                                 NaN                 NaN   \n",
       "50%                                 NaN                 NaN   \n",
       "75%                                 NaN                 NaN   \n",
       "90%                                 NaN                 NaN   \n",
       "95%                                 NaN                 NaN   \n",
       "99%                                 NaN                 NaN   \n",
       "max                                 NaN                 NaN   \n",
       "kurtosis                           5.43                5.69   \n",
       "skewness                           1.11                1.16   \n",
       "mediane                       18,000.00           18,105.00   \n",
       "valori missing                        0                   0   \n",
       "percentuale missing                0.0%                0.0%   \n",
       "valori_unici                       8437                8498   \n",
       "\n",
       "                    P_INST3_AMOUNT_PAID FlagRecOver100 DELIN_HISTORY  \\\n",
       "count                               NaN            NaN           NaN   \n",
       "mean                                NaN            NaN           NaN   \n",
       "std                                 NaN            NaN           NaN   \n",
       "min                                 NaN            NaN           NaN   \n",
       "1%                                  NaN            NaN           NaN   \n",
       "25%                                 NaN            NaN           NaN   \n",
       "50%                                 NaN            NaN           NaN   \n",
       "75%                                 NaN            NaN           NaN   \n",
       "90%                                 NaN            NaN           NaN   \n",
       "95%                                 NaN            NaN           NaN   \n",
       "99%                                 NaN            NaN           NaN   \n",
       "max                                 NaN            NaN           NaN   \n",
       "kurtosis                           6.01       1,040.55           NaN   \n",
       "skewness                           1.21          32.29           NaN   \n",
       "mediane                       18,250.00           0.00           NaN   \n",
       "valori missing                        0              0             0   \n",
       "percentuale missing                0.0%           0.0%          0.0%   \n",
       "valori_unici                       8598              2         63110   \n",
       "\n",
       "                    MOD_PAGAM MOD_PAG TIPO_GARANTE Denominazione Regione  \\\n",
       "count                     NaN     NaN          NaN                   NaN   \n",
       "mean                      NaN     NaN          NaN                   NaN   \n",
       "std                       NaN     NaN          NaN                   NaN   \n",
       "min                       NaN     NaN          NaN                   NaN   \n",
       "1%                        NaN     NaN          NaN                   NaN   \n",
       "25%                       NaN     NaN          NaN                   NaN   \n",
       "50%                       NaN     NaN          NaN                   NaN   \n",
       "75%                       NaN     NaN          NaN                   NaN   \n",
       "90%                       NaN     NaN          NaN                   NaN   \n",
       "95%                       NaN     NaN          NaN                   NaN   \n",
       "99%                       NaN     NaN          NaN                   NaN   \n",
       "max                       NaN     NaN          NaN                   NaN   \n",
       "kurtosis                  NaN     NaN          NaN                   NaN   \n",
       "skewness                  NaN     NaN          NaN                   NaN   \n",
       "mediane                   NaN     NaN          NaN                   NaN   \n",
       "valori missing            531       0       131487                     1   \n",
       "percentuale missing     0.36%    0.0%       88.57%                  0.0%   \n",
       "valori_unici                6       4            1                    20   \n",
       "\n",
       "                    CODICE_PHONIA DES_PRODOTTO DES_BENE DATA_AFFIDO  \\\n",
       "count                         NaN          NaN      NaN         NaN   \n",
       "mean                          NaN          NaN      NaN         NaN   \n",
       "std                           NaN          NaN      NaN         NaN   \n",
       "min                           NaN          NaN      NaN         NaN   \n",
       "1%                            NaN          NaN      NaN         NaN   \n",
       "25%                           NaN          NaN      NaN         NaN   \n",
       "50%                           NaN          NaN      NaN         NaN   \n",
       "75%                           NaN          NaN      NaN         NaN   \n",
       "90%                           NaN          NaN      NaN         NaN   \n",
       "95%                           NaN          NaN      NaN         NaN   \n",
       "99%                           NaN          NaN      NaN         NaN   \n",
       "max                           NaN          NaN      NaN         NaN   \n",
       "kurtosis                      NaN          NaN      NaN         NaN   \n",
       "skewness                      NaN          NaN      NaN         NaN   \n",
       "mediane                       NaN          NaN      NaN         NaN   \n",
       "valori missing                  0            0    72490           0   \n",
       "percentuale missing          0.0%         0.0%   48.83%        0.0%   \n",
       "valori_unici                    3           26    10021         473   \n",
       "\n",
       "                    DATA_FINE_AFFIDO DATA_FINANZIAM DT_VAL_ULT_PAGAM  \\\n",
       "count                            NaN            NaN              NaN   \n",
       "mean                             NaN            NaN              NaN   \n",
       "std                              NaN            NaN              NaN   \n",
       "min                              NaN            NaN              NaN   \n",
       "1%                               NaN            NaN              NaN   \n",
       "25%                              NaN            NaN              NaN   \n",
       "50%                              NaN            NaN              NaN   \n",
       "75%                              NaN            NaN              NaN   \n",
       "90%                              NaN            NaN              NaN   \n",
       "95%                              NaN            NaN              NaN   \n",
       "99%                              NaN            NaN              NaN   \n",
       "max                              NaN            NaN              NaN   \n",
       "kurtosis                         NaN            NaN              NaN   \n",
       "skewness                         NaN            NaN              NaN   \n",
       "mediane                          NaN            NaN              NaN   \n",
       "valori missing                 12566              0                0   \n",
       "percentuale missing            8.46%           0.0%             0.0%   \n",
       "valori_unici                     716           2824             1032   \n",
       "\n",
       "                    DATA_ESTINZ DATA_AGGIORNAMENTO P_INST1_DUE_DATE  \\\n",
       "count                       NaN                NaN              NaN   \n",
       "mean                        NaN                NaN              NaN   \n",
       "std                         NaN                NaN              NaN   \n",
       "min                         NaN                NaN              NaN   \n",
       "1%                          NaN                NaN              NaN   \n",
       "25%                         NaN                NaN              NaN   \n",
       "50%                         NaN                NaN              NaN   \n",
       "75%                         NaN                NaN              NaN   \n",
       "90%                         NaN                NaN              NaN   \n",
       "95%                         NaN                NaN              NaN   \n",
       "99%                         NaN                NaN              NaN   \n",
       "max                         NaN                NaN              NaN   \n",
       "kurtosis                    NaN                NaN              NaN   \n",
       "skewness                    NaN                NaN              NaN   \n",
       "mediane                     NaN                NaN              NaN   \n",
       "valori missing           148343                  0             6583   \n",
       "percentuale missing      99.92%               0.0%            4.43%   \n",
       "valori_unici                 93                 23              817   \n",
       "\n",
       "                    P_INST2_DUE_DATE P_INST3_DUE_DATE DT_VAL_RAT_IMP1  \\\n",
       "count                            NaN              NaN             NaN   \n",
       "mean                             NaN              NaN             NaN   \n",
       "std                              NaN              NaN             NaN   \n",
       "min                              NaN              NaN             NaN   \n",
       "1%                               NaN              NaN             NaN   \n",
       "25%                              NaN              NaN             NaN   \n",
       "50%                              NaN              NaN             NaN   \n",
       "75%                              NaN              NaN             NaN   \n",
       "90%                              NaN              NaN             NaN   \n",
       "95%                              NaN              NaN             NaN   \n",
       "99%                              NaN              NaN             NaN   \n",
       "max                              NaN              NaN             NaN   \n",
       "kurtosis                         NaN              NaN             NaN   \n",
       "skewness                         NaN              NaN             NaN   \n",
       "mediane                          NaN              NaN             NaN   \n",
       "valori missing                  4636             2683          104739   \n",
       "percentuale missing            3.12%            1.81%          70.55%   \n",
       "valori_unici                     805              795            1077   \n",
       "\n",
       "                    DT_VAL_RAT_IMP2 P_INST1_VAL_DATE P_INST3_VAL_DATE  \\\n",
       "count                           NaN              NaN              NaN   \n",
       "mean                            NaN              NaN              NaN   \n",
       "std                             NaN              NaN              NaN   \n",
       "min                             NaN              NaN              NaN   \n",
       "1%                              NaN              NaN              NaN   \n",
       "25%                             NaN              NaN              NaN   \n",
       "50%                             NaN              NaN              NaN   \n",
       "75%                             NaN              NaN              NaN   \n",
       "90%                             NaN              NaN              NaN   \n",
       "95%                             NaN              NaN              NaN   \n",
       "99%                             NaN              NaN              NaN   \n",
       "max                             NaN              NaN              NaN   \n",
       "kurtosis                        NaN              NaN              NaN   \n",
       "skewness                        NaN              NaN              NaN   \n",
       "mediane                         NaN              NaN              NaN   \n",
       "valori missing               144061             6585             2688   \n",
       "percentuale missing          97.03%            4.44%            1.81%   \n",
       "valori_unici                    426              925              886   \n",
       "\n",
       "                    SCAD_MAXIRATA SCAD_RAT_IMP2 SCAD_RAT_IMP1 PROVINCIA  \\\n",
       "count                         NaN           NaN           NaN       NaN   \n",
       "mean                          NaN           NaN           NaN       NaN   \n",
       "std                           NaN           NaN           NaN       NaN   \n",
       "min                           NaN           NaN           NaN       NaN   \n",
       "1%                            NaN           NaN           NaN       NaN   \n",
       "25%                           NaN           NaN           NaN       NaN   \n",
       "50%                           NaN           NaN           NaN       NaN   \n",
       "75%                           NaN           NaN           NaN       NaN   \n",
       "90%                           NaN           NaN           NaN       NaN   \n",
       "95%                           NaN           NaN           NaN       NaN   \n",
       "99%                           NaN           NaN           NaN       NaN   \n",
       "max                           NaN           NaN           NaN       NaN   \n",
       "kurtosis                      NaN           NaN           NaN       NaN   \n",
       "skewness                      NaN           NaN           NaN       NaN   \n",
       "mediane                       NaN           NaN           NaN       NaN   \n",
       "valori missing             147917          3629             0         0   \n",
       "percentuale missing        99.63%         2.44%          0.0%      0.0%   \n",
       "valori_unici                  213          1655          1858       108   \n",
       "\n",
       "                    Flag_Riciclo_SDD Flag_Rifinanziamento Flag_Garante  \\\n",
       "count                            NaN                  NaN          NaN   \n",
       "mean                             NaN                  NaN          NaN   \n",
       "std                              NaN                  NaN          NaN   \n",
       "min                              NaN                  NaN          NaN   \n",
       "1%                               NaN                  NaN          NaN   \n",
       "25%                              NaN                  NaN          NaN   \n",
       "50%                              NaN                  NaN          NaN   \n",
       "75%                              NaN                  NaN          NaN   \n",
       "90%                              NaN                  NaN          NaN   \n",
       "95%                              NaN                  NaN          NaN   \n",
       "99%                              NaN                  NaN          NaN   \n",
       "max                              NaN                  NaN          NaN   \n",
       "kurtosis                         NaN                  NaN          NaN   \n",
       "skewness                         NaN                  NaN          NaN   \n",
       "mediane                          NaN                  NaN          NaN   \n",
       "valori missing                 38916                    0            0   \n",
       "percentuale missing           26.21%                 0.0%         0.0%   \n",
       "valori_unici                       1                    2            2   \n",
       "\n",
       "                    Flag_Cointestazione Metodo_pagamento   Sesso NOSTART12M  \\\n",
       "count                               NaN              NaN     NaN        NaN   \n",
       "mean                                NaN              NaN     NaN        NaN   \n",
       "std                                 NaN              NaN     NaN        NaN   \n",
       "min                                 NaN              NaN     NaN        NaN   \n",
       "1%                                  NaN              NaN     NaN        NaN   \n",
       "25%                                 NaN              NaN     NaN        NaN   \n",
       "50%                                 NaN              NaN     NaN        NaN   \n",
       "75%                                 NaN              NaN     NaN        NaN   \n",
       "90%                                 NaN              NaN     NaN        NaN   \n",
       "95%                                 NaN              NaN     NaN        NaN   \n",
       "99%                                 NaN              NaN     NaN        NaN   \n",
       "max                                 NaN              NaN     NaN        NaN   \n",
       "kurtosis                            NaN              NaN     NaN        NaN   \n",
       "skewness                            NaN              NaN     NaN        NaN   \n",
       "mediane                             NaN              NaN     NaN        NaN   \n",
       "valori missing                        0                0   34658          0   \n",
       "percentuale missing                0.0%             0.0%  23.34%       0.0%   \n",
       "valori_unici                          2                4       2          2   \n",
       "\n",
       "                    NOSTART6M Flag_Galleggiamento_3M Flag_Galleggiamento_6M  \\\n",
       "count                     NaN                    NaN                    NaN   \n",
       "mean                      NaN                    NaN                    NaN   \n",
       "std                       NaN                    NaN                    NaN   \n",
       "min                       NaN                    NaN                    NaN   \n",
       "1%                        NaN                    NaN                    NaN   \n",
       "25%                       NaN                    NaN                    NaN   \n",
       "50%                       NaN                    NaN                    NaN   \n",
       "75%                       NaN                    NaN                    NaN   \n",
       "90%                       NaN                    NaN                    NaN   \n",
       "95%                       NaN                    NaN                    NaN   \n",
       "99%                       NaN                    NaN                    NaN   \n",
       "max                       NaN                    NaN                    NaN   \n",
       "kurtosis                  NaN                    NaN                    NaN   \n",
       "skewness                  NaN                    NaN                    NaN   \n",
       "mediane                   NaN                    NaN                    NaN   \n",
       "valori missing              0                      0                      0   \n",
       "percentuale missing      0.0%                   0.0%                   0.0%   \n",
       "valori_unici                2                      2                      2   \n",
       "\n",
       "                    Flag_Gestione_Prec Flag_InsolvenzaGrave_3M  \\\n",
       "count                              NaN                     NaN   \n",
       "mean                               NaN                     NaN   \n",
       "std                                NaN                     NaN   \n",
       "min                                NaN                     NaN   \n",
       "1%                                 NaN                     NaN   \n",
       "25%                                NaN                     NaN   \n",
       "50%                                NaN                     NaN   \n",
       "75%                                NaN                     NaN   \n",
       "90%                                NaN                     NaN   \n",
       "95%                                NaN                     NaN   \n",
       "99%                                NaN                     NaN   \n",
       "max                                NaN                     NaN   \n",
       "kurtosis                           NaN                     NaN   \n",
       "skewness                           NaN                     NaN   \n",
       "mediane                            NaN                     NaN   \n",
       "valori missing                       0                       0   \n",
       "percentuale missing               0.0%                    0.0%   \n",
       "valori_unici                         2                       2   \n",
       "\n",
       "                    Flag_InsolvenzaGrave_6M Flag_Miglioramento_3M  \\\n",
       "count                                   NaN                   NaN   \n",
       "mean                                    NaN                   NaN   \n",
       "std                                     NaN                   NaN   \n",
       "min                                     NaN                   NaN   \n",
       "1%                                      NaN                   NaN   \n",
       "25%                                     NaN                   NaN   \n",
       "50%                                     NaN                   NaN   \n",
       "75%                                     NaN                   NaN   \n",
       "90%                                     NaN                   NaN   \n",
       "95%                                     NaN                   NaN   \n",
       "99%                                     NaN                   NaN   \n",
       "max                                     NaN                   NaN   \n",
       "kurtosis                                NaN                   NaN   \n",
       "skewness                                NaN                   NaN   \n",
       "mediane                                 NaN                   NaN   \n",
       "valori missing                            0                     0   \n",
       "percentuale missing                    0.0%                  0.0%   \n",
       "valori_unici                              2                     2   \n",
       "\n",
       "                    Flag_Rate_piu_uno Flag_rientrototale_6M  \\\n",
       "count                             NaN                   NaN   \n",
       "mean                              NaN                   NaN   \n",
       "std                               NaN                   NaN   \n",
       "min                               NaN                   NaN   \n",
       "1%                                NaN                   NaN   \n",
       "25%                               NaN                   NaN   \n",
       "50%                               NaN                   NaN   \n",
       "75%                               NaN                   NaN   \n",
       "90%                               NaN                   NaN   \n",
       "95%                               NaN                   NaN   \n",
       "99%                               NaN                   NaN   \n",
       "max                               NaN                   NaN   \n",
       "kurtosis                          NaN                   NaN   \n",
       "skewness                          NaN                   NaN   \n",
       "mediane                           NaN                   NaN   \n",
       "valori missing                      0                     0   \n",
       "percentuale missing              0.0%                  0.0%   \n",
       "valori_unici                        2                     2   \n",
       "\n",
       "                    Flag_Scivolamento_3M Flag_Scivolamento_6M  \\\n",
       "count                                NaN                  NaN   \n",
       "mean                                 NaN                  NaN   \n",
       "std                                  NaN                  NaN   \n",
       "min                                  NaN                  NaN   \n",
       "1%                                   NaN                  NaN   \n",
       "25%                                  NaN                  NaN   \n",
       "50%                                  NaN                  NaN   \n",
       "75%                                  NaN                  NaN   \n",
       "90%                                  NaN                  NaN   \n",
       "95%                                  NaN                  NaN   \n",
       "99%                                  NaN                  NaN   \n",
       "max                                  NaN                  NaN   \n",
       "kurtosis                             NaN                  NaN   \n",
       "skewness                             NaN                  NaN   \n",
       "mediane                              NaN                  NaN   \n",
       "valori missing                         0                    0   \n",
       "percentuale missing                 0.0%                 0.0%   \n",
       "valori_unici                           2                    2   \n",
       "\n",
       "                    Flag_InsolvenzaGrave_12M DESCRIZIONE PRODOTTO  \n",
       "count                                    NaN                  NaN  \n",
       "mean                                     NaN                  NaN  \n",
       "std                                      NaN                  NaN  \n",
       "min                                      NaN                  NaN  \n",
       "1%                                       NaN                  NaN  \n",
       "25%                                      NaN                  NaN  \n",
       "50%                                      NaN                  NaN  \n",
       "75%                                      NaN                  NaN  \n",
       "90%                                      NaN                  NaN  \n",
       "95%                                      NaN                  NaN  \n",
       "99%                                      NaN                  NaN  \n",
       "max                                      NaN                  NaN  \n",
       "kurtosis                                 NaN                  NaN  \n",
       "skewness                                 NaN                  NaN  \n",
       "mediane                                  NaN                  NaN  \n",
       "valori missing                             0                    0  \n",
       "percentuale missing                     0.0%                 0.0%  \n",
       "valori_unici                               2                   14  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "describex(df_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specificare la variabile target\n",
    "df_variabile_target = df_clienti.Target_95\n",
    "# Subset delle variabili numeriche e variabili categoriche\n",
    "df_variabili_numeriche = df_clienti.select_dtypes(exclude=['object', 'category', 'string', 'datetime64[ns, UTC]'])\n",
    "df_variabili_categoriche = df_clienti.select_dtypes(include=['object', 'category', 'string', 'datetime64[ns, UTC]'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "correlation_matrix = df_cluster.corr()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Matrice di Correlazione')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analisi grafica \n",
    "def func_analisi_grafica(df, numeric_var):\n",
    "    ''' Definisco la funzione per distribution plot, boxplot e Q-Q plot delle variabili numeriche '''\n",
    "    plt.figure(figsize=(16,8))\n",
    "    \n",
    "    # Distribution plot\n",
    "    plt.subplot(1,3,1)\n",
    "    sns.histplot(data=df, x=df[numeric_var], element=\"step\", stat=\"density\")\n",
    "    plt.title(\"Distribution plot of \"+numeric_var)\n",
    "       \n",
    "    # Box plot\n",
    "    plt.subplot(1,3,2)\n",
    "    sns.boxplot(x=df[numeric_var], data=df)\n",
    "    plt.title(\"Box plot of \"+numeric_var)\n",
    "    \n",
    "    # Box plot\n",
    "    plt.subplot(1,3,3)\n",
    "    df[numeric_var].fillna(0, inplace = True)\n",
    "    stats.probplot(df[numeric_var], dist=\"norm\", plot=plt)\n",
    "    plt.title(\"Q-Q plot of \"+numeric_var)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#numerical_vars\n",
    "numerical_vars = [f for f in df_variabili_numeriche.columns if f!='Target_95']\n",
    "\n",
    "#categorical_vars\n",
    "categorical_vars = [f for f in df_variabili_categoriche.columns if f!='Target_95']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> Analisi grafica </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for variable in numerical_vars:\n",
    "#     func_analisi_grafica(df_clienti, variable)\n",
    "#     print(\"*\"*110)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> Aggiungo una colonna RATIO_ONERI_AFF_TOT_AFF tra TOT_ONERI_AFF E TOT_AFF </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_arricchito = df_cluster.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_arricchito['RATIO_ONERI_AFF_TOT_AFF'] = df_arricchito['TOT_ONERI_AFF'] / df_arricchito['TOT_AFF']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target_95</th>\n",
       "      <th>Chiave</th>\n",
       "      <th>PRATICA</th>\n",
       "      <th>DELIN_HISTORY</th>\n",
       "      <th>MOD_PAGAM</th>\n",
       "      <th>MOD_PAG</th>\n",
       "      <th>TIPO_GARANTE</th>\n",
       "      <th>Denominazione Regione</th>\n",
       "      <th>CODICE_PHONIA</th>\n",
       "      <th>DES_PRODOTTO</th>\n",
       "      <th>DES_BENE</th>\n",
       "      <th>DATA_AFFIDO</th>\n",
       "      <th>DATA_FINE_AFFIDO</th>\n",
       "      <th>DATA_FINANZIAM</th>\n",
       "      <th>DT_VAL_ULT_PAGAM</th>\n",
       "      <th>DATA_ESTINZ</th>\n",
       "      <th>DATA_AGGIORNAMENTO</th>\n",
       "      <th>P_INST1_DUE_DATE</th>\n",
       "      <th>P_INST2_DUE_DATE</th>\n",
       "      <th>P_INST3_DUE_DATE</th>\n",
       "      <th>DT_VAL_RAT_IMP1</th>\n",
       "      <th>DT_VAL_RAT_IMP2</th>\n",
       "      <th>P_INST1_VAL_DATE</th>\n",
       "      <th>P_INST3_VAL_DATE</th>\n",
       "      <th>SCAD_MAXIRATA</th>\n",
       "      <th>SCAD_RAT_IMP2</th>\n",
       "      <th>SCAD_RAT_IMP1</th>\n",
       "      <th>IMP_FINANZIATO</th>\n",
       "      <th>IMP_MAXIRATA</th>\n",
       "      <th>CURRENT_BALANCE</th>\n",
       "      <th>P_INST1_AMOUNT_PAID</th>\n",
       "      <th>P_INST2_AMOUNT_PAID</th>\n",
       "      <th>P_INST3_AMOUNT_PAID</th>\n",
       "      <th>RATE_TOTALI</th>\n",
       "      <th>NUM_RATE_RIFI</th>\n",
       "      <th>NUM_RAT_IMP1</th>\n",
       "      <th>NUM_RAT_IMP2</th>\n",
       "      <th>P_INST1_NUM</th>\n",
       "      <th>P_INST2_NUM</th>\n",
       "      <th>P_INST3_NUM</th>\n",
       "      <th>NUMERO RATE</th>\n",
       "      <th>PROVINCIA</th>\n",
       "      <th>P_INST1_AMOUNT</th>\n",
       "      <th>P_INST2_AMOUNT</th>\n",
       "      <th>P_INST3_AMOUNT</th>\n",
       "      <th>IMP_RAT_IMP1</th>\n",
       "      <th>IMP_RAT_IMP2</th>\n",
       "      <th>IMP_PAG_RAT_IMP1</th>\n",
       "      <th>IMP_PAG_RAT_IMP2</th>\n",
       "      <th>TOT_RATE_AFF_INIZIO_MAND</th>\n",
       "      <th>TOT_MORA_AFF_INIZIO_MAND</th>\n",
       "      <th>TOT_ONERI_AFF_INIZIO_MAND</th>\n",
       "      <th>TOT_AFF_INIZIO_MAND</th>\n",
       "      <th>TOT_RATE_PAG</th>\n",
       "      <th>TOT_MORA_PAG</th>\n",
       "      <th>TOT_ONERI_PAG</th>\n",
       "      <th>TOT_PAG</th>\n",
       "      <th>TOT_RATE_AFF</th>\n",
       "      <th>TOT_MORA_AFF</th>\n",
       "      <th>TOT_ONERI_AFF</th>\n",
       "      <th>TOT_AFF</th>\n",
       "      <th>Flag_Riciclo_SDD</th>\n",
       "      <th>TOT REC</th>\n",
       "      <th>TOT AFF</th>\n",
       "      <th>Flag_Rifinanziamento</th>\n",
       "      <th>Flag_Garante</th>\n",
       "      <th>Flag_Cointestazione</th>\n",
       "      <th>Metodo_pagamento</th>\n",
       "      <th>Ratio_Rate_Imp1</th>\n",
       "      <th>Ratio_Rate_Imp2</th>\n",
       "      <th>DistanzaAffidoUltimoPagamento</th>\n",
       "      <th>Sesso</th>\n",
       "      <th>Eta_Debitore</th>\n",
       "      <th>NOSTART12M</th>\n",
       "      <th>NOSTART6M</th>\n",
       "      <th>DurataFinanziamento</th>\n",
       "      <th>Flag_Galleggiamento_3M</th>\n",
       "      <th>Flag_Galleggiamento_6M</th>\n",
       "      <th>Flag_Gestione_Prec</th>\n",
       "      <th>Flag_InsolvenzaGrave_3M</th>\n",
       "      <th>Flag_InsolvenzaGrave_6M</th>\n",
       "      <th>Flag_Miglioramento_3M</th>\n",
       "      <th>Flag_Rate_piu_uno</th>\n",
       "      <th>Flag_rientrototale_6M</th>\n",
       "      <th>Flag_Scivolamento_3M</th>\n",
       "      <th>Flag_Scivolamento_6M</th>\n",
       "      <th>Numero_mesi_rec</th>\n",
       "      <th>Severity_12M</th>\n",
       "      <th>Severity_12M_pesata</th>\n",
       "      <th>Flag_InsolvenzaGrave_12M</th>\n",
       "      <th>FlagRecOver100</th>\n",
       "      <th>AgeingGestioneGg</th>\n",
       "      <th>AgeingErogazioneGg</th>\n",
       "      <th>DESCRIZIONE PRODOTTO</th>\n",
       "      <th>RATIO_ONERI_AFF_TOT_AFF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Target_95, Chiave, PRATICA, DELIN_HISTORY, MOD_PAGAM, MOD_PAG, TIPO_GARANTE, Denominazione Regione, CODICE_PHONIA, DES_PRODOTTO, DES_BENE, DATA_AFFIDO, DATA_FINE_AFFIDO, DATA_FINANZIAM, DT_VAL_ULT_PAGAM, DATA_ESTINZ, DATA_AGGIORNAMENTO, P_INST1_DUE_DATE, P_INST2_DUE_DATE, P_INST3_DUE_DATE, DT_VAL_RAT_IMP1, DT_VAL_RAT_IMP2, P_INST1_VAL_DATE, P_INST3_VAL_DATE, SCAD_MAXIRATA, SCAD_RAT_IMP2, SCAD_RAT_IMP1, IMP_FINANZIATO, IMP_MAXIRATA, CURRENT_BALANCE, P_INST1_AMOUNT_PAID, P_INST2_AMOUNT_PAID, P_INST3_AMOUNT_PAID, RATE_TOTALI, NUM_RATE_RIFI, NUM_RAT_IMP1, NUM_RAT_IMP2, P_INST1_NUM, P_INST2_NUM, P_INST3_NUM, NUMERO RATE, PROVINCIA, P_INST1_AMOUNT, P_INST2_AMOUNT, P_INST3_AMOUNT, IMP_RAT_IMP1, IMP_RAT_IMP2, IMP_PAG_RAT_IMP1, IMP_PAG_RAT_IMP2, TOT_RATE_AFF_INIZIO_MAND, TOT_MORA_AFF_INIZIO_MAND, TOT_ONERI_AFF_INIZIO_MAND, TOT_AFF_INIZIO_MAND, TOT_RATE_PAG, TOT_MORA_PAG, TOT_ONERI_PAG, TOT_PAG, TOT_RATE_AFF, TOT_MORA_AFF, TOT_ONERI_AFF, TOT_AFF, Flag_Riciclo_SDD, TOT REC, TOT AFF, Flag_Rifinanziamento, Flag_Garante, Flag_Cointestazione, Metodo_pagamento, Ratio_Rate_Imp1, Ratio_Rate_Imp2, DistanzaAffidoUltimoPagamento, Sesso, Eta_Debitore, NOSTART12M, NOSTART6M, DurataFinanziamento, Flag_Galleggiamento_3M, Flag_Galleggiamento_6M, Flag_Gestione_Prec, Flag_InsolvenzaGrave_3M, Flag_InsolvenzaGrave_6M, Flag_Miglioramento_3M, Flag_Rate_piu_uno, Flag_rientrototale_6M, Flag_Scivolamento_3M, Flag_Scivolamento_6M, Numero_mesi_rec, Severity_12M, Severity_12M_pesata, Flag_InsolvenzaGrave_12M, FlagRecOver100, AgeingGestioneGg, AgeingErogazioneGg, DESCRIZIONE PRODOTTO, RATIO_ONERI_AFF_TOT_AFF]\n",
       "Index: []"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_arricchito[df_arricchito['RATIO_ONERI_AFF_TOT_AFF'] > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target_95</th>\n",
       "      <th>Chiave</th>\n",
       "      <th>PRATICA</th>\n",
       "      <th>DELIN_HISTORY</th>\n",
       "      <th>MOD_PAGAM</th>\n",
       "      <th>MOD_PAG</th>\n",
       "      <th>TIPO_GARANTE</th>\n",
       "      <th>Denominazione Regione</th>\n",
       "      <th>CODICE_PHONIA</th>\n",
       "      <th>DES_PRODOTTO</th>\n",
       "      <th>DES_BENE</th>\n",
       "      <th>DATA_AFFIDO</th>\n",
       "      <th>DATA_FINE_AFFIDO</th>\n",
       "      <th>DATA_FINANZIAM</th>\n",
       "      <th>DT_VAL_ULT_PAGAM</th>\n",
       "      <th>DATA_ESTINZ</th>\n",
       "      <th>DATA_AGGIORNAMENTO</th>\n",
       "      <th>P_INST1_DUE_DATE</th>\n",
       "      <th>P_INST2_DUE_DATE</th>\n",
       "      <th>P_INST3_DUE_DATE</th>\n",
       "      <th>DT_VAL_RAT_IMP1</th>\n",
       "      <th>DT_VAL_RAT_IMP2</th>\n",
       "      <th>P_INST1_VAL_DATE</th>\n",
       "      <th>P_INST3_VAL_DATE</th>\n",
       "      <th>SCAD_MAXIRATA</th>\n",
       "      <th>SCAD_RAT_IMP2</th>\n",
       "      <th>SCAD_RAT_IMP1</th>\n",
       "      <th>IMP_FINANZIATO</th>\n",
       "      <th>IMP_MAXIRATA</th>\n",
       "      <th>CURRENT_BALANCE</th>\n",
       "      <th>P_INST1_AMOUNT_PAID</th>\n",
       "      <th>P_INST2_AMOUNT_PAID</th>\n",
       "      <th>P_INST3_AMOUNT_PAID</th>\n",
       "      <th>RATE_TOTALI</th>\n",
       "      <th>NUM_RATE_RIFI</th>\n",
       "      <th>NUM_RAT_IMP1</th>\n",
       "      <th>NUM_RAT_IMP2</th>\n",
       "      <th>P_INST1_NUM</th>\n",
       "      <th>P_INST2_NUM</th>\n",
       "      <th>P_INST3_NUM</th>\n",
       "      <th>NUMERO RATE</th>\n",
       "      <th>PROVINCIA</th>\n",
       "      <th>P_INST1_AMOUNT</th>\n",
       "      <th>P_INST2_AMOUNT</th>\n",
       "      <th>P_INST3_AMOUNT</th>\n",
       "      <th>IMP_RAT_IMP1</th>\n",
       "      <th>IMP_RAT_IMP2</th>\n",
       "      <th>IMP_PAG_RAT_IMP1</th>\n",
       "      <th>IMP_PAG_RAT_IMP2</th>\n",
       "      <th>TOT_RATE_AFF_INIZIO_MAND</th>\n",
       "      <th>TOT_MORA_AFF_INIZIO_MAND</th>\n",
       "      <th>TOT_ONERI_AFF_INIZIO_MAND</th>\n",
       "      <th>TOT_AFF_INIZIO_MAND</th>\n",
       "      <th>TOT_RATE_PAG</th>\n",
       "      <th>TOT_MORA_PAG</th>\n",
       "      <th>TOT_ONERI_PAG</th>\n",
       "      <th>TOT_PAG</th>\n",
       "      <th>TOT_RATE_AFF</th>\n",
       "      <th>TOT_MORA_AFF</th>\n",
       "      <th>TOT_ONERI_AFF</th>\n",
       "      <th>TOT_AFF</th>\n",
       "      <th>Flag_Riciclo_SDD</th>\n",
       "      <th>TOT REC</th>\n",
       "      <th>TOT AFF</th>\n",
       "      <th>Flag_Rifinanziamento</th>\n",
       "      <th>Flag_Garante</th>\n",
       "      <th>Flag_Cointestazione</th>\n",
       "      <th>Metodo_pagamento</th>\n",
       "      <th>Ratio_Rate_Imp1</th>\n",
       "      <th>Ratio_Rate_Imp2</th>\n",
       "      <th>DistanzaAffidoUltimoPagamento</th>\n",
       "      <th>Sesso</th>\n",
       "      <th>Eta_Debitore</th>\n",
       "      <th>NOSTART12M</th>\n",
       "      <th>NOSTART6M</th>\n",
       "      <th>DurataFinanziamento</th>\n",
       "      <th>Flag_Galleggiamento_3M</th>\n",
       "      <th>Flag_Galleggiamento_6M</th>\n",
       "      <th>Flag_Gestione_Prec</th>\n",
       "      <th>Flag_InsolvenzaGrave_3M</th>\n",
       "      <th>Flag_InsolvenzaGrave_6M</th>\n",
       "      <th>Flag_Miglioramento_3M</th>\n",
       "      <th>Flag_Rate_piu_uno</th>\n",
       "      <th>Flag_rientrototale_6M</th>\n",
       "      <th>Flag_Scivolamento_3M</th>\n",
       "      <th>Flag_Scivolamento_6M</th>\n",
       "      <th>Numero_mesi_rec</th>\n",
       "      <th>Severity_12M</th>\n",
       "      <th>Severity_12M_pesata</th>\n",
       "      <th>Flag_InsolvenzaGrave_12M</th>\n",
       "      <th>FlagRecOver100</th>\n",
       "      <th>AgeingGestioneGg</th>\n",
       "      <th>AgeingErogazioneGg</th>\n",
       "      <th>DESCRIZIONE PRODOTTO</th>\n",
       "      <th>RATIO_ONERI_AFF_TOT_AFF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>00000854016301_08_2021</td>\n",
       "      <td>00000854016301</td>\n",
       "      <td>13221222111111143332432332111100000000</td>\n",
       "      <td>P</td>\n",
       "      <td>BP</td>\n",
       "      <td>C</td>\n",
       "      <td>Lazio</td>\n",
       "      <td>ALPTEL</td>\n",
       "      <td>PRESTITO PERSONALE</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-08-02</td>\n",
       "      <td>2021-09-05</td>\n",
       "      <td>2018-06-12</td>\n",
       "      <td>2021-07-05</td>\n",
       "      <td>None</td>\n",
       "      <td>01/01/0001</td>\n",
       "      <td>2021-04-12</td>\n",
       "      <td>2021-05-12</td>\n",
       "      <td>2021-06-12</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-06-16</td>\n",
       "      <td>2021-07-05</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-08-12</td>\n",
       "      <td>2021-07-12</td>\n",
       "      <td>15,200.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9,770.58</td>\n",
       "      <td>0000000023700</td>\n",
       "      <td>0000000023700</td>\n",
       "      <td>0000000023700</td>\n",
       "      <td>84.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>37.00</td>\n",
       "      <td>38.00</td>\n",
       "      <td>34.00</td>\n",
       "      <td>35.00</td>\n",
       "      <td>36.00</td>\n",
       "      <td>2</td>\n",
       "      <td>LT</td>\n",
       "      <td>237.00</td>\n",
       "      <td>237.00</td>\n",
       "      <td>237.00</td>\n",
       "      <td>237.00</td>\n",
       "      <td>237.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>237.00</td>\n",
       "      <td>28.24</td>\n",
       "      <td>12.00</td>\n",
       "      <td>277.24</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>237.00</td>\n",
       "      <td>28.24</td>\n",
       "      <td>12.00</td>\n",
       "      <td>277.24</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00</td>\n",
       "      <td>486.00</td>\n",
       "      <td>NO</td>\n",
       "      <td>SI</td>\n",
       "      <td>SI</td>\n",
       "      <td>BP</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.45</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>7.00</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>SI</td>\n",
       "      <td>SI</td>\n",
       "      <td>NO</td>\n",
       "      <td>SI</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>12.00</td>\n",
       "      <td>1.58</td>\n",
       "      <td>0.95</td>\n",
       "      <td>SI</td>\n",
       "      <td>0</td>\n",
       "      <td>21.00</td>\n",
       "      <td>1,126.00</td>\n",
       "      <td>PRESTITO PERSONALE</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>00006003826200_08_2021</td>\n",
       "      <td>00006003826200</td>\n",
       "      <td>1001000000000000000000000000000000001100100110...</td>\n",
       "      <td>P</td>\n",
       "      <td>BP</td>\n",
       "      <td>None</td>\n",
       "      <td>Liguria</td>\n",
       "      <td>ALPTEL</td>\n",
       "      <td>REPEAT BUSINESS</td>\n",
       "      <td>FIDELIZ.OTTIMI</td>\n",
       "      <td>2021-08-10</td>\n",
       "      <td>2021-09-05</td>\n",
       "      <td>2016-05-12</td>\n",
       "      <td>2021-06-09</td>\n",
       "      <td>None</td>\n",
       "      <td>01/01/0001</td>\n",
       "      <td>2021-04-12</td>\n",
       "      <td>2021-05-12</td>\n",
       "      <td>2021-06-12</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-04-16</td>\n",
       "      <td>2021-06-09</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-08-12</td>\n",
       "      <td>2021-07-12</td>\n",
       "      <td>14,234.40</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8,636.46</td>\n",
       "      <td>0000000018350</td>\n",
       "      <td>0000000018350</td>\n",
       "      <td>0000000018350</td>\n",
       "      <td>120.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>62.00</td>\n",
       "      <td>63.00</td>\n",
       "      <td>59.00</td>\n",
       "      <td>60.00</td>\n",
       "      <td>61.00</td>\n",
       "      <td>2</td>\n",
       "      <td>IM</td>\n",
       "      <td>183.50</td>\n",
       "      <td>183.50</td>\n",
       "      <td>183.50</td>\n",
       "      <td>183.50</td>\n",
       "      <td>183.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>183.50</td>\n",
       "      <td>1.56</td>\n",
       "      <td>12.00</td>\n",
       "      <td>197.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>183.50</td>\n",
       "      <td>1.56</td>\n",
       "      <td>12.00</td>\n",
       "      <td>197.06</td>\n",
       "      <td>Y</td>\n",
       "      <td>0.00</td>\n",
       "      <td>379.00</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>BP</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.53</td>\n",
       "      <td>-2.00</td>\n",
       "      <td>Donna</td>\n",
       "      <td>59.00</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>10.00</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>SI</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>SI</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.15</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>29.00</td>\n",
       "      <td>1,887.00</td>\n",
       "      <td>FINANZIAMENTI AZIENDE</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>00002286163301_08_2021</td>\n",
       "      <td>00002286163301</td>\n",
       "      <td>111010</td>\n",
       "      <td>P</td>\n",
       "      <td>BP</td>\n",
       "      <td>None</td>\n",
       "      <td>Calabria</td>\n",
       "      <td>ALPTEL</td>\n",
       "      <td>ELETTRONICA &amp; ELETTRODOME</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-08-10</td>\n",
       "      <td>2021-09-05</td>\n",
       "      <td>2021-02-10</td>\n",
       "      <td>2021-06-14</td>\n",
       "      <td>None</td>\n",
       "      <td>01/01/0001</td>\n",
       "      <td>2021-04-12</td>\n",
       "      <td>2021-05-12</td>\n",
       "      <td>2021-06-12</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-04-12</td>\n",
       "      <td>2021-06-14</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-08-12</td>\n",
       "      <td>2021-07-12</td>\n",
       "      <td>1,500.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1,329.19</td>\n",
       "      <td>0000000005650</td>\n",
       "      <td>0000000005650</td>\n",
       "      <td>0000000005650</td>\n",
       "      <td>30.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>2</td>\n",
       "      <td>KR</td>\n",
       "      <td>56.50</td>\n",
       "      <td>56.50</td>\n",
       "      <td>56.50</td>\n",
       "      <td>56.50</td>\n",
       "      <td>56.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>56.50</td>\n",
       "      <td>0.58</td>\n",
       "      <td>15.97</td>\n",
       "      <td>73.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>56.50</td>\n",
       "      <td>0.58</td>\n",
       "      <td>15.97</td>\n",
       "      <td>73.05</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00</td>\n",
       "      <td>144.94</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>BP</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-2.00</td>\n",
       "      <td>Donna</td>\n",
       "      <td>57.00</td>\n",
       "      <td>SI</td>\n",
       "      <td>SI</td>\n",
       "      <td>2.50</td>\n",
       "      <td>SI</td>\n",
       "      <td>NO</td>\n",
       "      <td>SI</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>SI</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.57</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>29.00</td>\n",
       "      <td>152.00</td>\n",
       "      <td>ELETTRONICA &amp; ELETTRODOME</td>\n",
       "      <td>0.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>00001576642301_08_2021</td>\n",
       "      <td>00001576642301</td>\n",
       "      <td>10012101111101110000000</td>\n",
       "      <td>P</td>\n",
       "      <td>BP</td>\n",
       "      <td>None</td>\n",
       "      <td>Sicilia</td>\n",
       "      <td>ALPTEL</td>\n",
       "      <td>REPEAT BUSINESS</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-08-10</td>\n",
       "      <td>2021-09-05</td>\n",
       "      <td>2019-09-12</td>\n",
       "      <td>2021-05-27</td>\n",
       "      <td>None</td>\n",
       "      <td>01/01/0001</td>\n",
       "      <td>2021-04-12</td>\n",
       "      <td>2021-05-12</td>\n",
       "      <td>2021-06-12</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-04-26</td>\n",
       "      <td>2021-05-27</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-08-12</td>\n",
       "      <td>2021-07-12</td>\n",
       "      <td>14,280.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10,954.69</td>\n",
       "      <td>0000000026000</td>\n",
       "      <td>0000000026000</td>\n",
       "      <td>0000000026000</td>\n",
       "      <td>72.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>22.00</td>\n",
       "      <td>23.00</td>\n",
       "      <td>19.00</td>\n",
       "      <td>20.00</td>\n",
       "      <td>21.00</td>\n",
       "      <td>2</td>\n",
       "      <td>EN</td>\n",
       "      <td>260.00</td>\n",
       "      <td>260.00</td>\n",
       "      <td>260.00</td>\n",
       "      <td>260.00</td>\n",
       "      <td>260.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>260.00</td>\n",
       "      <td>16.20</td>\n",
       "      <td>12.00</td>\n",
       "      <td>288.20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>260.00</td>\n",
       "      <td>16.20</td>\n",
       "      <td>12.00</td>\n",
       "      <td>288.20</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00</td>\n",
       "      <td>532.00</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>BP</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.32</td>\n",
       "      <td>-2.00</td>\n",
       "      <td>Uomo</td>\n",
       "      <td>76.00</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>6.00</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>SI</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>SI</td>\n",
       "      <td>SI</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.41</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>29.00</td>\n",
       "      <td>669.00</td>\n",
       "      <td>FINANZIAMENTI AZIENDE</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>00002230493301_08_2021</td>\n",
       "      <td>00002230493301</td>\n",
       "      <td>2111210</td>\n",
       "      <td>P</td>\n",
       "      <td>BP</td>\n",
       "      <td>None</td>\n",
       "      <td>Lazio</td>\n",
       "      <td>ALPTEL</td>\n",
       "      <td>SPESE MEDICHE</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-08-09</td>\n",
       "      <td>2021-09-05</td>\n",
       "      <td>2021-01-13</td>\n",
       "      <td>2021-07-27</td>\n",
       "      <td>None</td>\n",
       "      <td>01/01/0001</td>\n",
       "      <td>2021-04-12</td>\n",
       "      <td>2021-05-12</td>\n",
       "      <td>2021-06-12</td>\n",
       "      <td>2021-07-27</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-04-14</td>\n",
       "      <td>2021-07-27</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-08-12</td>\n",
       "      <td>2021-07-12</td>\n",
       "      <td>6,800.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6,214.51</td>\n",
       "      <td>0000000016600</td>\n",
       "      <td>0000000016600</td>\n",
       "      <td>0000000016600</td>\n",
       "      <td>48.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>7.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>2</td>\n",
       "      <td>RM</td>\n",
       "      <td>166.00</td>\n",
       "      <td>166.00</td>\n",
       "      <td>166.00</td>\n",
       "      <td>166.00</td>\n",
       "      <td>166.00</td>\n",
       "      <td>2.34</td>\n",
       "      <td>0.00</td>\n",
       "      <td>163.66</td>\n",
       "      <td>3.51</td>\n",
       "      <td>32.40</td>\n",
       "      <td>199.57</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>163.66</td>\n",
       "      <td>3.51</td>\n",
       "      <td>32.40</td>\n",
       "      <td>199.57</td>\n",
       "      <td>None</td>\n",
       "      <td>2.34</td>\n",
       "      <td>396.80</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>BP</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.15</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>Uomo</td>\n",
       "      <td>57.00</td>\n",
       "      <td>SI</td>\n",
       "      <td>SI</td>\n",
       "      <td>4.00</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>SI</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>6.00</td>\n",
       "      <td>1.14</td>\n",
       "      <td>0.92</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>28.00</td>\n",
       "      <td>180.00</td>\n",
       "      <td>SPESE MEDICHE</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148458</th>\n",
       "      <td>0</td>\n",
       "      <td>00002785945301_06_2023</td>\n",
       "      <td>00002785945301</td>\n",
       "      <td>11011101100000</td>\n",
       "      <td>R</td>\n",
       "      <td>RI</td>\n",
       "      <td>None</td>\n",
       "      <td>Lazio</td>\n",
       "      <td>ALPTEX</td>\n",
       "      <td>DISINTERMEDIATO AUTOMOTIV</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-06-29</td>\n",
       "      <td>None</td>\n",
       "      <td>2022-04-14</td>\n",
       "      <td>2023-05-24</td>\n",
       "      <td>None</td>\n",
       "      <td>01/01/0001</td>\n",
       "      <td>2023-03-14</td>\n",
       "      <td>2023-04-14</td>\n",
       "      <td>2023-05-14</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-03-24</td>\n",
       "      <td>2023-05-24</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-07-14</td>\n",
       "      <td>2023-06-14</td>\n",
       "      <td>4,120.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3,169.02</td>\n",
       "      <td>0000000010600</td>\n",
       "      <td>0000000010600</td>\n",
       "      <td>0000000010600</td>\n",
       "      <td>48.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>14.00</td>\n",
       "      <td>15.00</td>\n",
       "      <td>11.00</td>\n",
       "      <td>12.00</td>\n",
       "      <td>13.00</td>\n",
       "      <td>2</td>\n",
       "      <td>RM</td>\n",
       "      <td>106.00</td>\n",
       "      <td>106.00</td>\n",
       "      <td>106.00</td>\n",
       "      <td>106.00</td>\n",
       "      <td>106.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>106.00</td>\n",
       "      <td>2.17</td>\n",
       "      <td>0.00</td>\n",
       "      <td>108.17</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>106.00</td>\n",
       "      <td>2.17</td>\n",
       "      <td>0.00</td>\n",
       "      <td>108.17</td>\n",
       "      <td>Y</td>\n",
       "      <td>0.00</td>\n",
       "      <td>246.40</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>SI</td>\n",
       "      <td>RI</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.31</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>4.00</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>SI</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>SI</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.39</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>15.00</td>\n",
       "      <td>426.00</td>\n",
       "      <td>AUTOMOTIVE NUOVO</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148459</th>\n",
       "      <td>1</td>\n",
       "      <td>00002471469301_06_2023</td>\n",
       "      <td>00002471469301</td>\n",
       "      <td>110000000000000000000000</td>\n",
       "      <td>R</td>\n",
       "      <td>RI</td>\n",
       "      <td>None</td>\n",
       "      <td>Umbria</td>\n",
       "      <td>ALPTEX</td>\n",
       "      <td>PRESTITO PERSONALE</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-06-29</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-07-15</td>\n",
       "      <td>2023-05-24</td>\n",
       "      <td>None</td>\n",
       "      <td>01/01/0001</td>\n",
       "      <td>2023-03-14</td>\n",
       "      <td>2023-04-14</td>\n",
       "      <td>2023-05-14</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-03-14</td>\n",
       "      <td>2023-05-24</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-07-14</td>\n",
       "      <td>2023-06-14</td>\n",
       "      <td>6,250.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4,725.40</td>\n",
       "      <td>0000000011600</td>\n",
       "      <td>0000000011600</td>\n",
       "      <td>0000000011600</td>\n",
       "      <td>72.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>23.00</td>\n",
       "      <td>24.00</td>\n",
       "      <td>20.00</td>\n",
       "      <td>21.00</td>\n",
       "      <td>22.00</td>\n",
       "      <td>2</td>\n",
       "      <td>TR</td>\n",
       "      <td>116.00</td>\n",
       "      <td>116.00</td>\n",
       "      <td>116.00</td>\n",
       "      <td>116.00</td>\n",
       "      <td>116.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>116.00</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>116.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>116.00</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>116.85</td>\n",
       "      <td>Y</td>\n",
       "      <td>116.00</td>\n",
       "      <td>116.00</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>SI</td>\n",
       "      <td>RI</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.33</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>6.00</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>SI</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.16</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>15.00</td>\n",
       "      <td>699.00</td>\n",
       "      <td>PRESTITO PERSONALE</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148460</th>\n",
       "      <td>0</td>\n",
       "      <td>00002324126301_06_2023</td>\n",
       "      <td>00002324126301</td>\n",
       "      <td>1000100000000110000000000000</td>\n",
       "      <td>R</td>\n",
       "      <td>RI</td>\n",
       "      <td>None</td>\n",
       "      <td>Lombardia</td>\n",
       "      <td>ALPTEX</td>\n",
       "      <td>DIPENDENTI AZIENDE DOC -</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-06-29</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-03-15</td>\n",
       "      <td>2022-05-05</td>\n",
       "      <td>None</td>\n",
       "      <td>01/01/0001</td>\n",
       "      <td>2023-03-14</td>\n",
       "      <td>2023-04-14</td>\n",
       "      <td>2023-05-14</td>\n",
       "      <td>2022-05-05</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-03-14</td>\n",
       "      <td>2023-05-14</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-07-14</td>\n",
       "      <td>2023-06-14</td>\n",
       "      <td>14,492.80</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9,057.10</td>\n",
       "      <td>0000000030300</td>\n",
       "      <td>0000000030300</td>\n",
       "      <td>0000000030300</td>\n",
       "      <td>60.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>27.00</td>\n",
       "      <td>28.00</td>\n",
       "      <td>24.00</td>\n",
       "      <td>25.00</td>\n",
       "      <td>26.00</td>\n",
       "      <td>2</td>\n",
       "      <td>VA</td>\n",
       "      <td>303.00</td>\n",
       "      <td>303.00</td>\n",
       "      <td>303.00</td>\n",
       "      <td>303.00</td>\n",
       "      <td>303.00</td>\n",
       "      <td>1.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>301.75</td>\n",
       "      <td>3.66</td>\n",
       "      <td>0.00</td>\n",
       "      <td>305.41</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>301.75</td>\n",
       "      <td>3.66</td>\n",
       "      <td>0.00</td>\n",
       "      <td>305.41</td>\n",
       "      <td>Y</td>\n",
       "      <td>1.25</td>\n",
       "      <td>303.00</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>SI</td>\n",
       "      <td>RI</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.47</td>\n",
       "      <td>-14.00</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>5.00</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>SI</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>SI</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.14</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>15.00</td>\n",
       "      <td>821.00</td>\n",
       "      <td>PRESTITI CON TRATTENUTA</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148461</th>\n",
       "      <td>0</td>\n",
       "      <td>00001908346301_06_2023</td>\n",
       "      <td>00001908346301</td>\n",
       "      <td>10000000000000010000000000000000111000</td>\n",
       "      <td>R</td>\n",
       "      <td>RI</td>\n",
       "      <td>None</td>\n",
       "      <td>Lombardia</td>\n",
       "      <td>ALPTEX</td>\n",
       "      <td>SPESE MEDICHE</td>\n",
       "      <td>CURE ODONTOIATRICHE</td>\n",
       "      <td>2023-06-29</td>\n",
       "      <td>None</td>\n",
       "      <td>2020-05-15</td>\n",
       "      <td>2020-10-16</td>\n",
       "      <td>None</td>\n",
       "      <td>01/01/0001</td>\n",
       "      <td>2023-03-14</td>\n",
       "      <td>2023-04-14</td>\n",
       "      <td>2023-05-14</td>\n",
       "      <td>2020-10-16</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-03-14</td>\n",
       "      <td>2023-05-14</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-07-14</td>\n",
       "      <td>2023-06-14</td>\n",
       "      <td>6,654.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3,828.21</td>\n",
       "      <td>0000000012200</td>\n",
       "      <td>0000000012200</td>\n",
       "      <td>0000000012200</td>\n",
       "      <td>70.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>35.00</td>\n",
       "      <td>36.00</td>\n",
       "      <td>32.00</td>\n",
       "      <td>33.00</td>\n",
       "      <td>34.00</td>\n",
       "      <td>2</td>\n",
       "      <td>MI</td>\n",
       "      <td>122.00</td>\n",
       "      <td>122.00</td>\n",
       "      <td>122.00</td>\n",
       "      <td>122.00</td>\n",
       "      <td>122.00</td>\n",
       "      <td>26.24</td>\n",
       "      <td>0.00</td>\n",
       "      <td>95.76</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>95.76</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>95.76</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>95.76</td>\n",
       "      <td>Y</td>\n",
       "      <td>26.24</td>\n",
       "      <td>122.00</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>RI</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.51</td>\n",
       "      <td>-32.00</td>\n",
       "      <td>Uomo</td>\n",
       "      <td>61.00</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>5.83</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>SI</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.08</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>15.00</td>\n",
       "      <td>1,125.00</td>\n",
       "      <td>SPESE MEDICHE</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148462</th>\n",
       "      <td>0</td>\n",
       "      <td>00002905333301_06_2023</td>\n",
       "      <td>00002905333301</td>\n",
       "      <td>10000000000</td>\n",
       "      <td>R</td>\n",
       "      <td>RI</td>\n",
       "      <td>None</td>\n",
       "      <td>Lazio</td>\n",
       "      <td>ALPTEX</td>\n",
       "      <td>AUTOMOTIVE USATO</td>\n",
       "      <td>forfour 2as. (W453) FW098</td>\n",
       "      <td>2023-06-29</td>\n",
       "      <td>None</td>\n",
       "      <td>2022-07-15</td>\n",
       "      <td>2023-05-14</td>\n",
       "      <td>None</td>\n",
       "      <td>01/01/0001</td>\n",
       "      <td>2023-03-14</td>\n",
       "      <td>2023-04-14</td>\n",
       "      <td>2023-05-14</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-03-14</td>\n",
       "      <td>2023-05-14</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-07-14</td>\n",
       "      <td>2023-06-14</td>\n",
       "      <td>7,009.40</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6,037.66</td>\n",
       "      <td>0000000014300</td>\n",
       "      <td>0000000014300</td>\n",
       "      <td>0000000014300</td>\n",
       "      <td>60.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11.00</td>\n",
       "      <td>12.00</td>\n",
       "      <td>8.00</td>\n",
       "      <td>9.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>2</td>\n",
       "      <td>RM</td>\n",
       "      <td>143.00</td>\n",
       "      <td>143.00</td>\n",
       "      <td>143.00</td>\n",
       "      <td>143.00</td>\n",
       "      <td>143.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>143.00</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.00</td>\n",
       "      <td>143.48</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>143.00</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.00</td>\n",
       "      <td>143.48</td>\n",
       "      <td>Y</td>\n",
       "      <td>0.00</td>\n",
       "      <td>171.95</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>RI</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-2.00</td>\n",
       "      <td>Donna</td>\n",
       "      <td>53.00</td>\n",
       "      <td>SI</td>\n",
       "      <td>NO</td>\n",
       "      <td>5.00</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>SI</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.09</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>15.00</td>\n",
       "      <td>334.00</td>\n",
       "      <td>AUTOMOTIVE USATO</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>148463 rows × 95 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Target_95                  Chiave         PRATICA  \\\n",
       "0               0  00000854016301_08_2021  00000854016301   \n",
       "1               0  00006003826200_08_2021  00006003826200   \n",
       "2               0  00002286163301_08_2021  00002286163301   \n",
       "3               0  00001576642301_08_2021  00001576642301   \n",
       "4               0  00002230493301_08_2021  00002230493301   \n",
       "...           ...                     ...             ...   \n",
       "148458          0  00002785945301_06_2023  00002785945301   \n",
       "148459          1  00002471469301_06_2023  00002471469301   \n",
       "148460          0  00002324126301_06_2023  00002324126301   \n",
       "148461          0  00001908346301_06_2023  00001908346301   \n",
       "148462          0  00002905333301_06_2023  00002905333301   \n",
       "\n",
       "                                            DELIN_HISTORY MOD_PAGAM MOD_PAG  \\\n",
       "0                  13221222111111143332432332111100000000         P      BP   \n",
       "1       1001000000000000000000000000000000001100100110...         P      BP   \n",
       "2                                                  111010         P      BP   \n",
       "3                                 10012101111101110000000         P      BP   \n",
       "4                                                 2111210         P      BP   \n",
       "...                                                   ...       ...     ...   \n",
       "148458                                     11011101100000         R      RI   \n",
       "148459                           110000000000000000000000         R      RI   \n",
       "148460                       1000100000000110000000000000         R      RI   \n",
       "148461             10000000000000010000000000000000111000         R      RI   \n",
       "148462                                        10000000000         R      RI   \n",
       "\n",
       "       TIPO_GARANTE Denominazione Regione CODICE_PHONIA  \\\n",
       "0                 C                 Lazio        ALPTEL   \n",
       "1              None               Liguria        ALPTEL   \n",
       "2              None              Calabria        ALPTEL   \n",
       "3              None               Sicilia        ALPTEL   \n",
       "4              None                 Lazio        ALPTEL   \n",
       "...             ...                   ...           ...   \n",
       "148458         None                 Lazio        ALPTEX   \n",
       "148459         None                Umbria        ALPTEX   \n",
       "148460         None             Lombardia        ALPTEX   \n",
       "148461         None             Lombardia        ALPTEX   \n",
       "148462         None                 Lazio        ALPTEX   \n",
       "\n",
       "                     DES_PRODOTTO                   DES_BENE DATA_AFFIDO  \\\n",
       "0              PRESTITO PERSONALE                       None  2021-08-02   \n",
       "1                 REPEAT BUSINESS             FIDELIZ.OTTIMI  2021-08-10   \n",
       "2       ELETTRONICA & ELETTRODOME                       None  2021-08-10   \n",
       "3                 REPEAT BUSINESS                       None  2021-08-10   \n",
       "4                   SPESE MEDICHE                       None  2021-08-09   \n",
       "...                           ...                        ...         ...   \n",
       "148458  DISINTERMEDIATO AUTOMOTIV                       None  2023-06-29   \n",
       "148459         PRESTITO PERSONALE                       None  2023-06-29   \n",
       "148460   DIPENDENTI AZIENDE DOC -                       None  2023-06-29   \n",
       "148461              SPESE MEDICHE        CURE ODONTOIATRICHE  2023-06-29   \n",
       "148462           AUTOMOTIVE USATO  forfour 2as. (W453) FW098  2023-06-29   \n",
       "\n",
       "       DATA_FINE_AFFIDO DATA_FINANZIAM DT_VAL_ULT_PAGAM DATA_ESTINZ  \\\n",
       "0            2021-09-05     2018-06-12       2021-07-05        None   \n",
       "1            2021-09-05     2016-05-12       2021-06-09        None   \n",
       "2            2021-09-05     2021-02-10       2021-06-14        None   \n",
       "3            2021-09-05     2019-09-12       2021-05-27        None   \n",
       "4            2021-09-05     2021-01-13       2021-07-27        None   \n",
       "...                 ...            ...              ...         ...   \n",
       "148458             None     2022-04-14       2023-05-24        None   \n",
       "148459             None     2021-07-15       2023-05-24        None   \n",
       "148460             None     2021-03-15       2022-05-05        None   \n",
       "148461             None     2020-05-15       2020-10-16        None   \n",
       "148462             None     2022-07-15       2023-05-14        None   \n",
       "\n",
       "       DATA_AGGIORNAMENTO P_INST1_DUE_DATE P_INST2_DUE_DATE P_INST3_DUE_DATE  \\\n",
       "0              01/01/0001       2021-04-12       2021-05-12       2021-06-12   \n",
       "1              01/01/0001       2021-04-12       2021-05-12       2021-06-12   \n",
       "2              01/01/0001       2021-04-12       2021-05-12       2021-06-12   \n",
       "3              01/01/0001       2021-04-12       2021-05-12       2021-06-12   \n",
       "4              01/01/0001       2021-04-12       2021-05-12       2021-06-12   \n",
       "...                   ...              ...              ...              ...   \n",
       "148458         01/01/0001       2023-03-14       2023-04-14       2023-05-14   \n",
       "148459         01/01/0001       2023-03-14       2023-04-14       2023-05-14   \n",
       "148460         01/01/0001       2023-03-14       2023-04-14       2023-05-14   \n",
       "148461         01/01/0001       2023-03-14       2023-04-14       2023-05-14   \n",
       "148462         01/01/0001       2023-03-14       2023-04-14       2023-05-14   \n",
       "\n",
       "       DT_VAL_RAT_IMP1 DT_VAL_RAT_IMP2 P_INST1_VAL_DATE P_INST3_VAL_DATE  \\\n",
       "0                 None            None       2021-06-16       2021-07-05   \n",
       "1                 None            None       2021-04-16       2021-06-09   \n",
       "2                 None            None       2021-04-12       2021-06-14   \n",
       "3                 None            None       2021-04-26       2021-05-27   \n",
       "4           2021-07-27            None       2021-04-14       2021-07-27   \n",
       "...                ...             ...              ...              ...   \n",
       "148458            None            None       2023-03-24       2023-05-24   \n",
       "148459            None            None       2023-03-14       2023-05-24   \n",
       "148460      2022-05-05            None       2023-03-14       2023-05-14   \n",
       "148461      2020-10-16            None       2023-03-14       2023-05-14   \n",
       "148462            None            None       2023-03-14       2023-05-14   \n",
       "\n",
       "       SCAD_MAXIRATA SCAD_RAT_IMP2 SCAD_RAT_IMP1  IMP_FINANZIATO  \\\n",
       "0               None    2021-08-12    2021-07-12       15,200.00   \n",
       "1               None    2021-08-12    2021-07-12       14,234.40   \n",
       "2               None    2021-08-12    2021-07-12        1,500.00   \n",
       "3               None    2021-08-12    2021-07-12       14,280.00   \n",
       "4               None    2021-08-12    2021-07-12        6,800.00   \n",
       "...              ...           ...           ...             ...   \n",
       "148458          None    2023-07-14    2023-06-14        4,120.88   \n",
       "148459          None    2023-07-14    2023-06-14        6,250.00   \n",
       "148460          None    2023-07-14    2023-06-14       14,492.80   \n",
       "148461          None    2023-07-14    2023-06-14        6,654.00   \n",
       "148462          None    2023-07-14    2023-06-14        7,009.40   \n",
       "\n",
       "        IMP_MAXIRATA  CURRENT_BALANCE P_INST1_AMOUNT_PAID P_INST2_AMOUNT_PAID  \\\n",
       "0               0.00         9,770.58       0000000023700       0000000023700   \n",
       "1               0.00         8,636.46       0000000018350       0000000018350   \n",
       "2               0.00         1,329.19       0000000005650       0000000005650   \n",
       "3               0.00        10,954.69       0000000026000       0000000026000   \n",
       "4               0.00         6,214.51       0000000016600       0000000016600   \n",
       "...              ...              ...                 ...                 ...   \n",
       "148458          0.00         3,169.02       0000000010600       0000000010600   \n",
       "148459          0.00         4,725.40       0000000011600       0000000011600   \n",
       "148460          0.00         9,057.10       0000000030300       0000000030300   \n",
       "148461          0.00         3,828.21       0000000012200       0000000012200   \n",
       "148462          0.00         6,037.66       0000000014300       0000000014300   \n",
       "\n",
       "       P_INST3_AMOUNT_PAID  RATE_TOTALI  NUM_RATE_RIFI  NUM_RAT_IMP1  \\\n",
       "0            0000000023700        84.00           0.00         37.00   \n",
       "1            0000000018350       120.00           0.00         62.00   \n",
       "2            0000000005650        30.00           0.00          5.00   \n",
       "3            0000000026000        72.00           0.00         22.00   \n",
       "4            0000000016600        48.00           0.00          6.00   \n",
       "...                    ...          ...            ...           ...   \n",
       "148458       0000000010600        48.00           0.00         14.00   \n",
       "148459       0000000011600        72.00           0.00         23.00   \n",
       "148460       0000000030300        60.00           0.00         27.00   \n",
       "148461       0000000012200        70.00           0.00         35.00   \n",
       "148462       0000000014300        60.00           0.00         11.00   \n",
       "\n",
       "        NUM_RAT_IMP2  P_INST1_NUM  P_INST2_NUM  P_INST3_NUM  NUMERO RATE  \\\n",
       "0              38.00        34.00        35.00        36.00            2   \n",
       "1              63.00        59.00        60.00        61.00            2   \n",
       "2               6.00         2.00         3.00         4.00            2   \n",
       "3              23.00        19.00        20.00        21.00            2   \n",
       "4               7.00         3.00         4.00         5.00            2   \n",
       "...              ...          ...          ...          ...          ...   \n",
       "148458         15.00        11.00        12.00        13.00            2   \n",
       "148459         24.00        20.00        21.00        22.00            2   \n",
       "148460         28.00        24.00        25.00        26.00            2   \n",
       "148461         36.00        32.00        33.00        34.00            2   \n",
       "148462         12.00         8.00         9.00        10.00            2   \n",
       "\n",
       "       PROVINCIA  P_INST1_AMOUNT  P_INST2_AMOUNT  P_INST3_AMOUNT  \\\n",
       "0             LT          237.00          237.00          237.00   \n",
       "1             IM          183.50          183.50          183.50   \n",
       "2             KR           56.50           56.50           56.50   \n",
       "3             EN          260.00          260.00          260.00   \n",
       "4             RM          166.00          166.00          166.00   \n",
       "...          ...             ...             ...             ...   \n",
       "148458        RM          106.00          106.00          106.00   \n",
       "148459        TR          116.00          116.00          116.00   \n",
       "148460        VA          303.00          303.00          303.00   \n",
       "148461        MI          122.00          122.00          122.00   \n",
       "148462        RM          143.00          143.00          143.00   \n",
       "\n",
       "        IMP_RAT_IMP1  IMP_RAT_IMP2  IMP_PAG_RAT_IMP1  IMP_PAG_RAT_IMP2  \\\n",
       "0             237.00        237.00              0.00              0.00   \n",
       "1             183.50        183.50              0.00              0.00   \n",
       "2              56.50         56.50              0.00              0.00   \n",
       "3             260.00        260.00              0.00              0.00   \n",
       "4             166.00        166.00              2.34              0.00   \n",
       "...              ...           ...               ...               ...   \n",
       "148458        106.00        106.00              0.00              0.00   \n",
       "148459        116.00        116.00              0.00              0.00   \n",
       "148460        303.00        303.00              1.25              0.00   \n",
       "148461        122.00        122.00             26.24              0.00   \n",
       "148462        143.00        143.00              0.00              0.00   \n",
       "\n",
       "        TOT_RATE_AFF_INIZIO_MAND  TOT_MORA_AFF_INIZIO_MAND  \\\n",
       "0                         237.00                     28.24   \n",
       "1                         183.50                      1.56   \n",
       "2                          56.50                      0.58   \n",
       "3                         260.00                     16.20   \n",
       "4                         163.66                      3.51   \n",
       "...                          ...                       ...   \n",
       "148458                    106.00                      2.17   \n",
       "148459                    116.00                      0.85   \n",
       "148460                    301.75                      3.66   \n",
       "148461                     95.76                      0.00   \n",
       "148462                    143.00                      0.48   \n",
       "\n",
       "        TOT_ONERI_AFF_INIZIO_MAND  TOT_AFF_INIZIO_MAND  TOT_RATE_PAG  \\\n",
       "0                           12.00               277.24          0.00   \n",
       "1                           12.00               197.06          0.00   \n",
       "2                           15.97                73.05          0.00   \n",
       "3                           12.00               288.20          0.00   \n",
       "4                           32.40               199.57          0.00   \n",
       "...                           ...                  ...           ...   \n",
       "148458                       0.00               108.17          0.00   \n",
       "148459                       0.00               116.85          0.00   \n",
       "148460                       0.00               305.41          0.00   \n",
       "148461                       0.00                95.76          0.00   \n",
       "148462                       0.00               143.48          0.00   \n",
       "\n",
       "        TOT_MORA_PAG  TOT_ONERI_PAG  TOT_PAG  TOT_RATE_AFF  TOT_MORA_AFF  \\\n",
       "0               0.00           0.00     0.00        237.00         28.24   \n",
       "1               0.00           0.00     0.00        183.50          1.56   \n",
       "2               0.00           0.00     0.00         56.50          0.58   \n",
       "3               0.00           0.00     0.00        260.00         16.20   \n",
       "4               0.00           0.00     0.00        163.66          3.51   \n",
       "...              ...            ...      ...           ...           ...   \n",
       "148458          0.00           0.00     0.00        106.00          2.17   \n",
       "148459          0.00           0.00     0.00        116.00          0.85   \n",
       "148460          0.00           0.00     0.00        301.75          3.66   \n",
       "148461          0.00           0.00     0.00         95.76          0.00   \n",
       "148462          0.00           0.00     0.00        143.00          0.48   \n",
       "\n",
       "        TOT_ONERI_AFF  TOT_AFF Flag_Riciclo_SDD  TOT REC  TOT AFF  \\\n",
       "0               12.00   277.24             None     0.00   486.00   \n",
       "1               12.00   197.06                Y     0.00   379.00   \n",
       "2               15.97    73.05             None     0.00   144.94   \n",
       "3               12.00   288.20             None     0.00   532.00   \n",
       "4               32.40   199.57             None     2.34   396.80   \n",
       "...               ...      ...              ...      ...      ...   \n",
       "148458           0.00   108.17                Y     0.00   246.40   \n",
       "148459           0.00   116.85                Y   116.00   116.00   \n",
       "148460           0.00   305.41                Y     1.25   303.00   \n",
       "148461           0.00    95.76                Y    26.24   122.00   \n",
       "148462           0.00   143.48                Y     0.00   171.95   \n",
       "\n",
       "       Flag_Rifinanziamento Flag_Garante Flag_Cointestazione Metodo_pagamento  \\\n",
       "0                        NO           SI                  SI               BP   \n",
       "1                        NO           NO                  NO               BP   \n",
       "2                        NO           NO                  NO               BP   \n",
       "3                        NO           NO                  NO               BP   \n",
       "4                        NO           NO                  NO               BP   \n",
       "...                     ...          ...                 ...              ...   \n",
       "148458                   NO           NO                  SI               RI   \n",
       "148459                   NO           NO                  SI               RI   \n",
       "148460                   NO           NO                  SI               RI   \n",
       "148461                   NO           NO                  NO               RI   \n",
       "148462                   NO           NO                  NO               RI   \n",
       "\n",
       "        Ratio_Rate_Imp1  Ratio_Rate_Imp2  DistanzaAffidoUltimoPagamento  \\\n",
       "0                  0.44             0.45                          -1.00   \n",
       "1                  0.52             0.53                          -2.00   \n",
       "2                  0.17             0.20                          -2.00   \n",
       "3                  0.31             0.32                          -2.00   \n",
       "4                  0.12             0.15                          -0.00   \n",
       "...                 ...              ...                            ...   \n",
       "148458             0.29             0.31                          -1.00   \n",
       "148459             0.32             0.33                          -1.00   \n",
       "148460             0.45             0.47                         -14.00   \n",
       "148461             0.50             0.51                         -32.00   \n",
       "148462             0.18             0.20                          -2.00   \n",
       "\n",
       "        Sesso  Eta_Debitore NOSTART12M NOSTART6M  DurataFinanziamento  \\\n",
       "0        None           NaN         NO        NO                 7.00   \n",
       "1       Donna         59.00         NO        NO                10.00   \n",
       "2       Donna         57.00         SI        SI                 2.50   \n",
       "3        Uomo         76.00         NO        NO                 6.00   \n",
       "4        Uomo         57.00         SI        SI                 4.00   \n",
       "...       ...           ...        ...       ...                  ...   \n",
       "148458   None           NaN         NO        NO                 4.00   \n",
       "148459   None           NaN         NO        NO                 6.00   \n",
       "148460   None           NaN         NO        NO                 5.00   \n",
       "148461   Uomo         61.00         NO        NO                 5.83   \n",
       "148462  Donna         53.00         SI        NO                 5.00   \n",
       "\n",
       "       Flag_Galleggiamento_3M Flag_Galleggiamento_6M Flag_Gestione_Prec  \\\n",
       "0                          NO                     NO                 NO   \n",
       "1                          NO                     NO                 SI   \n",
       "2                          SI                     NO                 SI   \n",
       "3                          NO                     NO                 SI   \n",
       "4                          NO                     NO                 NO   \n",
       "...                       ...                    ...                ...   \n",
       "148458                     NO                     NO                 SI   \n",
       "148459                     NO                     NO                 NO   \n",
       "148460                     NO                     NO                 SI   \n",
       "148461                     NO                     NO                 NO   \n",
       "148462                     NO                     NO                 NO   \n",
       "\n",
       "       Flag_InsolvenzaGrave_3M Flag_InsolvenzaGrave_6M Flag_Miglioramento_3M  \\\n",
       "0                           SI                      SI                    NO   \n",
       "1                           NO                      NO                    NO   \n",
       "2                           NO                      NO                    NO   \n",
       "3                           NO                      NO                    NO   \n",
       "4                           NO                      NO                    NO   \n",
       "...                        ...                     ...                   ...   \n",
       "148458                      NO                      NO                    NO   \n",
       "148459                      NO                      NO                    NO   \n",
       "148460                      NO                      NO                    NO   \n",
       "148461                      NO                      NO                    NO   \n",
       "148462                      NO                      NO                    NO   \n",
       "\n",
       "       Flag_Rate_piu_uno Flag_rientrototale_6M Flag_Scivolamento_3M  \\\n",
       "0                     SI                    NO                   NO   \n",
       "1                     NO                    SI                   NO   \n",
       "2                     NO                    SI                   NO   \n",
       "3                     SI                    SI                   NO   \n",
       "4                     SI                    NO                   NO   \n",
       "...                  ...                   ...                  ...   \n",
       "148458                NO                    SI                   NO   \n",
       "148459                NO                    SI                   NO   \n",
       "148460                NO                    SI                   NO   \n",
       "148461                NO                    SI                   NO   \n",
       "148462                NO                    SI                   NO   \n",
       "\n",
       "       Flag_Scivolamento_6M  Numero_mesi_rec  Severity_12M  \\\n",
       "0                        NO            12.00          1.58   \n",
       "1                        NO             1.00          0.17   \n",
       "2                        NO             3.00          0.67   \n",
       "3                        NO             1.00          0.83   \n",
       "4                        NO             6.00          1.14   \n",
       "...                     ...              ...           ...   \n",
       "148458                   NO             2.00          0.58   \n",
       "148459                   NO             2.00          0.17   \n",
       "148460                   NO             1.00          0.17   \n",
       "148461                   NO             1.00          0.08   \n",
       "148462                   NO             1.00          0.09   \n",
       "\n",
       "        Severity_12M_pesata Flag_InsolvenzaGrave_12M FlagRecOver100  \\\n",
       "0                      0.95                       SI              0   \n",
       "1                      0.15                       NO              0   \n",
       "2                      0.57                       NO              0   \n",
       "3                      0.41                       NO              0   \n",
       "4                      0.92                       NO              0   \n",
       "...                     ...                      ...            ...   \n",
       "148458                 0.39                       NO              0   \n",
       "148459                 0.16                       NO              0   \n",
       "148460                 0.14                       NO              0   \n",
       "148461                 0.08                       NO              0   \n",
       "148462                 0.09                       NO              0   \n",
       "\n",
       "        AgeingGestioneGg  AgeingErogazioneGg       DESCRIZIONE PRODOTTO  \\\n",
       "0                  21.00            1,126.00         PRESTITO PERSONALE   \n",
       "1                  29.00            1,887.00      FINANZIAMENTI AZIENDE   \n",
       "2                  29.00              152.00  ELETTRONICA & ELETTRODOME   \n",
       "3                  29.00              669.00      FINANZIAMENTI AZIENDE   \n",
       "4                  28.00              180.00              SPESE MEDICHE   \n",
       "...                  ...                 ...                        ...   \n",
       "148458             15.00              426.00           AUTOMOTIVE NUOVO   \n",
       "148459             15.00              699.00         PRESTITO PERSONALE   \n",
       "148460             15.00              821.00    PRESTITI CON TRATTENUTA   \n",
       "148461             15.00            1,125.00              SPESE MEDICHE   \n",
       "148462             15.00              334.00           AUTOMOTIVE USATO   \n",
       "\n",
       "        RATIO_ONERI_AFF_TOT_AFF  \n",
       "0                          0.04  \n",
       "1                          0.06  \n",
       "2                          0.22  \n",
       "3                          0.04  \n",
       "4                          0.16  \n",
       "...                         ...  \n",
       "148458                     0.00  \n",
       "148459                     0.00  \n",
       "148460                     0.00  \n",
       "148461                     0.00  \n",
       "148462                     0.00  \n",
       "\n",
       "[148463 rows x 95 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_arricchito"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> Rimuovo le colonne non necessarie </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clienti_con_meno_colonne = df_arricchito.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target_95</th>\n",
       "      <th>Chiave</th>\n",
       "      <th>PRATICA</th>\n",
       "      <th>DELIN_HISTORY</th>\n",
       "      <th>MOD_PAGAM</th>\n",
       "      <th>MOD_PAG</th>\n",
       "      <th>TIPO_GARANTE</th>\n",
       "      <th>Denominazione Regione</th>\n",
       "      <th>CODICE_PHONIA</th>\n",
       "      <th>DES_PRODOTTO</th>\n",
       "      <th>DES_BENE</th>\n",
       "      <th>DATA_AFFIDO</th>\n",
       "      <th>DATA_FINE_AFFIDO</th>\n",
       "      <th>DATA_FINANZIAM</th>\n",
       "      <th>DT_VAL_ULT_PAGAM</th>\n",
       "      <th>DATA_ESTINZ</th>\n",
       "      <th>DATA_AGGIORNAMENTO</th>\n",
       "      <th>P_INST1_DUE_DATE</th>\n",
       "      <th>P_INST2_DUE_DATE</th>\n",
       "      <th>P_INST3_DUE_DATE</th>\n",
       "      <th>DT_VAL_RAT_IMP1</th>\n",
       "      <th>DT_VAL_RAT_IMP2</th>\n",
       "      <th>P_INST1_VAL_DATE</th>\n",
       "      <th>P_INST3_VAL_DATE</th>\n",
       "      <th>SCAD_MAXIRATA</th>\n",
       "      <th>SCAD_RAT_IMP2</th>\n",
       "      <th>SCAD_RAT_IMP1</th>\n",
       "      <th>IMP_FINANZIATO</th>\n",
       "      <th>IMP_MAXIRATA</th>\n",
       "      <th>CURRENT_BALANCE</th>\n",
       "      <th>P_INST1_AMOUNT_PAID</th>\n",
       "      <th>P_INST2_AMOUNT_PAID</th>\n",
       "      <th>P_INST3_AMOUNT_PAID</th>\n",
       "      <th>RATE_TOTALI</th>\n",
       "      <th>NUM_RATE_RIFI</th>\n",
       "      <th>NUM_RAT_IMP1</th>\n",
       "      <th>NUM_RAT_IMP2</th>\n",
       "      <th>P_INST1_NUM</th>\n",
       "      <th>P_INST2_NUM</th>\n",
       "      <th>P_INST3_NUM</th>\n",
       "      <th>NUMERO RATE</th>\n",
       "      <th>PROVINCIA</th>\n",
       "      <th>P_INST1_AMOUNT</th>\n",
       "      <th>P_INST2_AMOUNT</th>\n",
       "      <th>P_INST3_AMOUNT</th>\n",
       "      <th>IMP_RAT_IMP1</th>\n",
       "      <th>IMP_RAT_IMP2</th>\n",
       "      <th>IMP_PAG_RAT_IMP1</th>\n",
       "      <th>IMP_PAG_RAT_IMP2</th>\n",
       "      <th>TOT_RATE_AFF_INIZIO_MAND</th>\n",
       "      <th>TOT_MORA_AFF_INIZIO_MAND</th>\n",
       "      <th>TOT_ONERI_AFF_INIZIO_MAND</th>\n",
       "      <th>TOT_AFF_INIZIO_MAND</th>\n",
       "      <th>TOT_RATE_PAG</th>\n",
       "      <th>TOT_MORA_PAG</th>\n",
       "      <th>TOT_ONERI_PAG</th>\n",
       "      <th>TOT_PAG</th>\n",
       "      <th>TOT_RATE_AFF</th>\n",
       "      <th>TOT_MORA_AFF</th>\n",
       "      <th>TOT_ONERI_AFF</th>\n",
       "      <th>TOT_AFF</th>\n",
       "      <th>Flag_Riciclo_SDD</th>\n",
       "      <th>TOT REC</th>\n",
       "      <th>TOT AFF</th>\n",
       "      <th>Flag_Rifinanziamento</th>\n",
       "      <th>Flag_Garante</th>\n",
       "      <th>Flag_Cointestazione</th>\n",
       "      <th>Metodo_pagamento</th>\n",
       "      <th>Ratio_Rate_Imp1</th>\n",
       "      <th>Ratio_Rate_Imp2</th>\n",
       "      <th>DistanzaAffidoUltimoPagamento</th>\n",
       "      <th>Sesso</th>\n",
       "      <th>Eta_Debitore</th>\n",
       "      <th>NOSTART12M</th>\n",
       "      <th>NOSTART6M</th>\n",
       "      <th>DurataFinanziamento</th>\n",
       "      <th>Flag_Galleggiamento_3M</th>\n",
       "      <th>Flag_Galleggiamento_6M</th>\n",
       "      <th>Flag_Gestione_Prec</th>\n",
       "      <th>Flag_InsolvenzaGrave_3M</th>\n",
       "      <th>Flag_InsolvenzaGrave_6M</th>\n",
       "      <th>Flag_Miglioramento_3M</th>\n",
       "      <th>Flag_Rate_piu_uno</th>\n",
       "      <th>Flag_rientrototale_6M</th>\n",
       "      <th>Flag_Scivolamento_3M</th>\n",
       "      <th>Flag_Scivolamento_6M</th>\n",
       "      <th>Numero_mesi_rec</th>\n",
       "      <th>Severity_12M</th>\n",
       "      <th>Severity_12M_pesata</th>\n",
       "      <th>Flag_InsolvenzaGrave_12M</th>\n",
       "      <th>FlagRecOver100</th>\n",
       "      <th>AgeingGestioneGg</th>\n",
       "      <th>AgeingErogazioneGg</th>\n",
       "      <th>DESCRIZIONE PRODOTTO</th>\n",
       "      <th>RATIO_ONERI_AFF_TOT_AFF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>00000854016301_08_2021</td>\n",
       "      <td>00000854016301</td>\n",
       "      <td>13221222111111143332432332111100000000</td>\n",
       "      <td>P</td>\n",
       "      <td>BP</td>\n",
       "      <td>C</td>\n",
       "      <td>Lazio</td>\n",
       "      <td>ALPTEL</td>\n",
       "      <td>PRESTITO PERSONALE</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-08-02</td>\n",
       "      <td>2021-09-05</td>\n",
       "      <td>2018-06-12</td>\n",
       "      <td>2021-07-05</td>\n",
       "      <td>None</td>\n",
       "      <td>01/01/0001</td>\n",
       "      <td>2021-04-12</td>\n",
       "      <td>2021-05-12</td>\n",
       "      <td>2021-06-12</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-06-16</td>\n",
       "      <td>2021-07-05</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-08-12</td>\n",
       "      <td>2021-07-12</td>\n",
       "      <td>15,200.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9,770.58</td>\n",
       "      <td>0000000023700</td>\n",
       "      <td>0000000023700</td>\n",
       "      <td>0000000023700</td>\n",
       "      <td>84.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>37.00</td>\n",
       "      <td>38.00</td>\n",
       "      <td>34.00</td>\n",
       "      <td>35.00</td>\n",
       "      <td>36.00</td>\n",
       "      <td>2</td>\n",
       "      <td>LT</td>\n",
       "      <td>237.00</td>\n",
       "      <td>237.00</td>\n",
       "      <td>237.00</td>\n",
       "      <td>237.00</td>\n",
       "      <td>237.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>237.00</td>\n",
       "      <td>28.24</td>\n",
       "      <td>12.00</td>\n",
       "      <td>277.24</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>237.00</td>\n",
       "      <td>28.24</td>\n",
       "      <td>12.00</td>\n",
       "      <td>277.24</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00</td>\n",
       "      <td>486.00</td>\n",
       "      <td>NO</td>\n",
       "      <td>SI</td>\n",
       "      <td>SI</td>\n",
       "      <td>BP</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.45</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>7.00</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>SI</td>\n",
       "      <td>SI</td>\n",
       "      <td>NO</td>\n",
       "      <td>SI</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>12.00</td>\n",
       "      <td>1.58</td>\n",
       "      <td>0.95</td>\n",
       "      <td>SI</td>\n",
       "      <td>0</td>\n",
       "      <td>21.00</td>\n",
       "      <td>1,126.00</td>\n",
       "      <td>PRESTITO PERSONALE</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>00006003826200_08_2021</td>\n",
       "      <td>00006003826200</td>\n",
       "      <td>1001000000000000000000000000000000001100100110...</td>\n",
       "      <td>P</td>\n",
       "      <td>BP</td>\n",
       "      <td>None</td>\n",
       "      <td>Liguria</td>\n",
       "      <td>ALPTEL</td>\n",
       "      <td>REPEAT BUSINESS</td>\n",
       "      <td>FIDELIZ.OTTIMI</td>\n",
       "      <td>2021-08-10</td>\n",
       "      <td>2021-09-05</td>\n",
       "      <td>2016-05-12</td>\n",
       "      <td>2021-06-09</td>\n",
       "      <td>None</td>\n",
       "      <td>01/01/0001</td>\n",
       "      <td>2021-04-12</td>\n",
       "      <td>2021-05-12</td>\n",
       "      <td>2021-06-12</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-04-16</td>\n",
       "      <td>2021-06-09</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-08-12</td>\n",
       "      <td>2021-07-12</td>\n",
       "      <td>14,234.40</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8,636.46</td>\n",
       "      <td>0000000018350</td>\n",
       "      <td>0000000018350</td>\n",
       "      <td>0000000018350</td>\n",
       "      <td>120.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>62.00</td>\n",
       "      <td>63.00</td>\n",
       "      <td>59.00</td>\n",
       "      <td>60.00</td>\n",
       "      <td>61.00</td>\n",
       "      <td>2</td>\n",
       "      <td>IM</td>\n",
       "      <td>183.50</td>\n",
       "      <td>183.50</td>\n",
       "      <td>183.50</td>\n",
       "      <td>183.50</td>\n",
       "      <td>183.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>183.50</td>\n",
       "      <td>1.56</td>\n",
       "      <td>12.00</td>\n",
       "      <td>197.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>183.50</td>\n",
       "      <td>1.56</td>\n",
       "      <td>12.00</td>\n",
       "      <td>197.06</td>\n",
       "      <td>Y</td>\n",
       "      <td>0.00</td>\n",
       "      <td>379.00</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>BP</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.53</td>\n",
       "      <td>-2.00</td>\n",
       "      <td>Donna</td>\n",
       "      <td>59.00</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>10.00</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>SI</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>SI</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.15</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>29.00</td>\n",
       "      <td>1,887.00</td>\n",
       "      <td>FINANZIAMENTI AZIENDE</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>00002286163301_08_2021</td>\n",
       "      <td>00002286163301</td>\n",
       "      <td>111010</td>\n",
       "      <td>P</td>\n",
       "      <td>BP</td>\n",
       "      <td>None</td>\n",
       "      <td>Calabria</td>\n",
       "      <td>ALPTEL</td>\n",
       "      <td>ELETTRONICA &amp; ELETTRODOME</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-08-10</td>\n",
       "      <td>2021-09-05</td>\n",
       "      <td>2021-02-10</td>\n",
       "      <td>2021-06-14</td>\n",
       "      <td>None</td>\n",
       "      <td>01/01/0001</td>\n",
       "      <td>2021-04-12</td>\n",
       "      <td>2021-05-12</td>\n",
       "      <td>2021-06-12</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-04-12</td>\n",
       "      <td>2021-06-14</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-08-12</td>\n",
       "      <td>2021-07-12</td>\n",
       "      <td>1,500.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1,329.19</td>\n",
       "      <td>0000000005650</td>\n",
       "      <td>0000000005650</td>\n",
       "      <td>0000000005650</td>\n",
       "      <td>30.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>2</td>\n",
       "      <td>KR</td>\n",
       "      <td>56.50</td>\n",
       "      <td>56.50</td>\n",
       "      <td>56.50</td>\n",
       "      <td>56.50</td>\n",
       "      <td>56.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>56.50</td>\n",
       "      <td>0.58</td>\n",
       "      <td>15.97</td>\n",
       "      <td>73.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>56.50</td>\n",
       "      <td>0.58</td>\n",
       "      <td>15.97</td>\n",
       "      <td>73.05</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00</td>\n",
       "      <td>144.94</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>BP</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-2.00</td>\n",
       "      <td>Donna</td>\n",
       "      <td>57.00</td>\n",
       "      <td>SI</td>\n",
       "      <td>SI</td>\n",
       "      <td>2.50</td>\n",
       "      <td>SI</td>\n",
       "      <td>NO</td>\n",
       "      <td>SI</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>SI</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.57</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>29.00</td>\n",
       "      <td>152.00</td>\n",
       "      <td>ELETTRONICA &amp; ELETTRODOME</td>\n",
       "      <td>0.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>00001576642301_08_2021</td>\n",
       "      <td>00001576642301</td>\n",
       "      <td>10012101111101110000000</td>\n",
       "      <td>P</td>\n",
       "      <td>BP</td>\n",
       "      <td>None</td>\n",
       "      <td>Sicilia</td>\n",
       "      <td>ALPTEL</td>\n",
       "      <td>REPEAT BUSINESS</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-08-10</td>\n",
       "      <td>2021-09-05</td>\n",
       "      <td>2019-09-12</td>\n",
       "      <td>2021-05-27</td>\n",
       "      <td>None</td>\n",
       "      <td>01/01/0001</td>\n",
       "      <td>2021-04-12</td>\n",
       "      <td>2021-05-12</td>\n",
       "      <td>2021-06-12</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-04-26</td>\n",
       "      <td>2021-05-27</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-08-12</td>\n",
       "      <td>2021-07-12</td>\n",
       "      <td>14,280.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10,954.69</td>\n",
       "      <td>0000000026000</td>\n",
       "      <td>0000000026000</td>\n",
       "      <td>0000000026000</td>\n",
       "      <td>72.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>22.00</td>\n",
       "      <td>23.00</td>\n",
       "      <td>19.00</td>\n",
       "      <td>20.00</td>\n",
       "      <td>21.00</td>\n",
       "      <td>2</td>\n",
       "      <td>EN</td>\n",
       "      <td>260.00</td>\n",
       "      <td>260.00</td>\n",
       "      <td>260.00</td>\n",
       "      <td>260.00</td>\n",
       "      <td>260.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>260.00</td>\n",
       "      <td>16.20</td>\n",
       "      <td>12.00</td>\n",
       "      <td>288.20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>260.00</td>\n",
       "      <td>16.20</td>\n",
       "      <td>12.00</td>\n",
       "      <td>288.20</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00</td>\n",
       "      <td>532.00</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>BP</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.32</td>\n",
       "      <td>-2.00</td>\n",
       "      <td>Uomo</td>\n",
       "      <td>76.00</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>6.00</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>SI</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>SI</td>\n",
       "      <td>SI</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.41</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>29.00</td>\n",
       "      <td>669.00</td>\n",
       "      <td>FINANZIAMENTI AZIENDE</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>00002230493301_08_2021</td>\n",
       "      <td>00002230493301</td>\n",
       "      <td>2111210</td>\n",
       "      <td>P</td>\n",
       "      <td>BP</td>\n",
       "      <td>None</td>\n",
       "      <td>Lazio</td>\n",
       "      <td>ALPTEL</td>\n",
       "      <td>SPESE MEDICHE</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-08-09</td>\n",
       "      <td>2021-09-05</td>\n",
       "      <td>2021-01-13</td>\n",
       "      <td>2021-07-27</td>\n",
       "      <td>None</td>\n",
       "      <td>01/01/0001</td>\n",
       "      <td>2021-04-12</td>\n",
       "      <td>2021-05-12</td>\n",
       "      <td>2021-06-12</td>\n",
       "      <td>2021-07-27</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-04-14</td>\n",
       "      <td>2021-07-27</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-08-12</td>\n",
       "      <td>2021-07-12</td>\n",
       "      <td>6,800.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6,214.51</td>\n",
       "      <td>0000000016600</td>\n",
       "      <td>0000000016600</td>\n",
       "      <td>0000000016600</td>\n",
       "      <td>48.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>7.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>2</td>\n",
       "      <td>RM</td>\n",
       "      <td>166.00</td>\n",
       "      <td>166.00</td>\n",
       "      <td>166.00</td>\n",
       "      <td>166.00</td>\n",
       "      <td>166.00</td>\n",
       "      <td>2.34</td>\n",
       "      <td>0.00</td>\n",
       "      <td>163.66</td>\n",
       "      <td>3.51</td>\n",
       "      <td>32.40</td>\n",
       "      <td>199.57</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>163.66</td>\n",
       "      <td>3.51</td>\n",
       "      <td>32.40</td>\n",
       "      <td>199.57</td>\n",
       "      <td>None</td>\n",
       "      <td>2.34</td>\n",
       "      <td>396.80</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>BP</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.15</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>Uomo</td>\n",
       "      <td>57.00</td>\n",
       "      <td>SI</td>\n",
       "      <td>SI</td>\n",
       "      <td>4.00</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>SI</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>6.00</td>\n",
       "      <td>1.14</td>\n",
       "      <td>0.92</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>28.00</td>\n",
       "      <td>180.00</td>\n",
       "      <td>SPESE MEDICHE</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148458</th>\n",
       "      <td>0</td>\n",
       "      <td>00002785945301_06_2023</td>\n",
       "      <td>00002785945301</td>\n",
       "      <td>11011101100000</td>\n",
       "      <td>R</td>\n",
       "      <td>RI</td>\n",
       "      <td>None</td>\n",
       "      <td>Lazio</td>\n",
       "      <td>ALPTEX</td>\n",
       "      <td>DISINTERMEDIATO AUTOMOTIV</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-06-29</td>\n",
       "      <td>None</td>\n",
       "      <td>2022-04-14</td>\n",
       "      <td>2023-05-24</td>\n",
       "      <td>None</td>\n",
       "      <td>01/01/0001</td>\n",
       "      <td>2023-03-14</td>\n",
       "      <td>2023-04-14</td>\n",
       "      <td>2023-05-14</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-03-24</td>\n",
       "      <td>2023-05-24</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-07-14</td>\n",
       "      <td>2023-06-14</td>\n",
       "      <td>4,120.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3,169.02</td>\n",
       "      <td>0000000010600</td>\n",
       "      <td>0000000010600</td>\n",
       "      <td>0000000010600</td>\n",
       "      <td>48.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>14.00</td>\n",
       "      <td>15.00</td>\n",
       "      <td>11.00</td>\n",
       "      <td>12.00</td>\n",
       "      <td>13.00</td>\n",
       "      <td>2</td>\n",
       "      <td>RM</td>\n",
       "      <td>106.00</td>\n",
       "      <td>106.00</td>\n",
       "      <td>106.00</td>\n",
       "      <td>106.00</td>\n",
       "      <td>106.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>106.00</td>\n",
       "      <td>2.17</td>\n",
       "      <td>0.00</td>\n",
       "      <td>108.17</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>106.00</td>\n",
       "      <td>2.17</td>\n",
       "      <td>0.00</td>\n",
       "      <td>108.17</td>\n",
       "      <td>Y</td>\n",
       "      <td>0.00</td>\n",
       "      <td>246.40</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>SI</td>\n",
       "      <td>RI</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.31</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>4.00</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>SI</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>SI</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.39</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>15.00</td>\n",
       "      <td>426.00</td>\n",
       "      <td>AUTOMOTIVE NUOVO</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148459</th>\n",
       "      <td>1</td>\n",
       "      <td>00002471469301_06_2023</td>\n",
       "      <td>00002471469301</td>\n",
       "      <td>110000000000000000000000</td>\n",
       "      <td>R</td>\n",
       "      <td>RI</td>\n",
       "      <td>None</td>\n",
       "      <td>Umbria</td>\n",
       "      <td>ALPTEX</td>\n",
       "      <td>PRESTITO PERSONALE</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-06-29</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-07-15</td>\n",
       "      <td>2023-05-24</td>\n",
       "      <td>None</td>\n",
       "      <td>01/01/0001</td>\n",
       "      <td>2023-03-14</td>\n",
       "      <td>2023-04-14</td>\n",
       "      <td>2023-05-14</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-03-14</td>\n",
       "      <td>2023-05-24</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-07-14</td>\n",
       "      <td>2023-06-14</td>\n",
       "      <td>6,250.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4,725.40</td>\n",
       "      <td>0000000011600</td>\n",
       "      <td>0000000011600</td>\n",
       "      <td>0000000011600</td>\n",
       "      <td>72.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>23.00</td>\n",
       "      <td>24.00</td>\n",
       "      <td>20.00</td>\n",
       "      <td>21.00</td>\n",
       "      <td>22.00</td>\n",
       "      <td>2</td>\n",
       "      <td>TR</td>\n",
       "      <td>116.00</td>\n",
       "      <td>116.00</td>\n",
       "      <td>116.00</td>\n",
       "      <td>116.00</td>\n",
       "      <td>116.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>116.00</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>116.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>116.00</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>116.85</td>\n",
       "      <td>Y</td>\n",
       "      <td>116.00</td>\n",
       "      <td>116.00</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>SI</td>\n",
       "      <td>RI</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.33</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>6.00</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>SI</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.16</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>15.00</td>\n",
       "      <td>699.00</td>\n",
       "      <td>PRESTITO PERSONALE</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148460</th>\n",
       "      <td>0</td>\n",
       "      <td>00002324126301_06_2023</td>\n",
       "      <td>00002324126301</td>\n",
       "      <td>1000100000000110000000000000</td>\n",
       "      <td>R</td>\n",
       "      <td>RI</td>\n",
       "      <td>None</td>\n",
       "      <td>Lombardia</td>\n",
       "      <td>ALPTEX</td>\n",
       "      <td>DIPENDENTI AZIENDE DOC -</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-06-29</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-03-15</td>\n",
       "      <td>2022-05-05</td>\n",
       "      <td>None</td>\n",
       "      <td>01/01/0001</td>\n",
       "      <td>2023-03-14</td>\n",
       "      <td>2023-04-14</td>\n",
       "      <td>2023-05-14</td>\n",
       "      <td>2022-05-05</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-03-14</td>\n",
       "      <td>2023-05-14</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-07-14</td>\n",
       "      <td>2023-06-14</td>\n",
       "      <td>14,492.80</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9,057.10</td>\n",
       "      <td>0000000030300</td>\n",
       "      <td>0000000030300</td>\n",
       "      <td>0000000030300</td>\n",
       "      <td>60.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>27.00</td>\n",
       "      <td>28.00</td>\n",
       "      <td>24.00</td>\n",
       "      <td>25.00</td>\n",
       "      <td>26.00</td>\n",
       "      <td>2</td>\n",
       "      <td>VA</td>\n",
       "      <td>303.00</td>\n",
       "      <td>303.00</td>\n",
       "      <td>303.00</td>\n",
       "      <td>303.00</td>\n",
       "      <td>303.00</td>\n",
       "      <td>1.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>301.75</td>\n",
       "      <td>3.66</td>\n",
       "      <td>0.00</td>\n",
       "      <td>305.41</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>301.75</td>\n",
       "      <td>3.66</td>\n",
       "      <td>0.00</td>\n",
       "      <td>305.41</td>\n",
       "      <td>Y</td>\n",
       "      <td>1.25</td>\n",
       "      <td>303.00</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>SI</td>\n",
       "      <td>RI</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.47</td>\n",
       "      <td>-14.00</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>5.00</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>SI</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>SI</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.14</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>15.00</td>\n",
       "      <td>821.00</td>\n",
       "      <td>PRESTITI CON TRATTENUTA</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148461</th>\n",
       "      <td>0</td>\n",
       "      <td>00001908346301_06_2023</td>\n",
       "      <td>00001908346301</td>\n",
       "      <td>10000000000000010000000000000000111000</td>\n",
       "      <td>R</td>\n",
       "      <td>RI</td>\n",
       "      <td>None</td>\n",
       "      <td>Lombardia</td>\n",
       "      <td>ALPTEX</td>\n",
       "      <td>SPESE MEDICHE</td>\n",
       "      <td>CURE ODONTOIATRICHE</td>\n",
       "      <td>2023-06-29</td>\n",
       "      <td>None</td>\n",
       "      <td>2020-05-15</td>\n",
       "      <td>2020-10-16</td>\n",
       "      <td>None</td>\n",
       "      <td>01/01/0001</td>\n",
       "      <td>2023-03-14</td>\n",
       "      <td>2023-04-14</td>\n",
       "      <td>2023-05-14</td>\n",
       "      <td>2020-10-16</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-03-14</td>\n",
       "      <td>2023-05-14</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-07-14</td>\n",
       "      <td>2023-06-14</td>\n",
       "      <td>6,654.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3,828.21</td>\n",
       "      <td>0000000012200</td>\n",
       "      <td>0000000012200</td>\n",
       "      <td>0000000012200</td>\n",
       "      <td>70.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>35.00</td>\n",
       "      <td>36.00</td>\n",
       "      <td>32.00</td>\n",
       "      <td>33.00</td>\n",
       "      <td>34.00</td>\n",
       "      <td>2</td>\n",
       "      <td>MI</td>\n",
       "      <td>122.00</td>\n",
       "      <td>122.00</td>\n",
       "      <td>122.00</td>\n",
       "      <td>122.00</td>\n",
       "      <td>122.00</td>\n",
       "      <td>26.24</td>\n",
       "      <td>0.00</td>\n",
       "      <td>95.76</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>95.76</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>95.76</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>95.76</td>\n",
       "      <td>Y</td>\n",
       "      <td>26.24</td>\n",
       "      <td>122.00</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>RI</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.51</td>\n",
       "      <td>-32.00</td>\n",
       "      <td>Uomo</td>\n",
       "      <td>61.00</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>5.83</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>SI</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.08</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>15.00</td>\n",
       "      <td>1,125.00</td>\n",
       "      <td>SPESE MEDICHE</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148462</th>\n",
       "      <td>0</td>\n",
       "      <td>00002905333301_06_2023</td>\n",
       "      <td>00002905333301</td>\n",
       "      <td>10000000000</td>\n",
       "      <td>R</td>\n",
       "      <td>RI</td>\n",
       "      <td>None</td>\n",
       "      <td>Lazio</td>\n",
       "      <td>ALPTEX</td>\n",
       "      <td>AUTOMOTIVE USATO</td>\n",
       "      <td>forfour 2as. (W453) FW098</td>\n",
       "      <td>2023-06-29</td>\n",
       "      <td>None</td>\n",
       "      <td>2022-07-15</td>\n",
       "      <td>2023-05-14</td>\n",
       "      <td>None</td>\n",
       "      <td>01/01/0001</td>\n",
       "      <td>2023-03-14</td>\n",
       "      <td>2023-04-14</td>\n",
       "      <td>2023-05-14</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-03-14</td>\n",
       "      <td>2023-05-14</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-07-14</td>\n",
       "      <td>2023-06-14</td>\n",
       "      <td>7,009.40</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6,037.66</td>\n",
       "      <td>0000000014300</td>\n",
       "      <td>0000000014300</td>\n",
       "      <td>0000000014300</td>\n",
       "      <td>60.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11.00</td>\n",
       "      <td>12.00</td>\n",
       "      <td>8.00</td>\n",
       "      <td>9.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>2</td>\n",
       "      <td>RM</td>\n",
       "      <td>143.00</td>\n",
       "      <td>143.00</td>\n",
       "      <td>143.00</td>\n",
       "      <td>143.00</td>\n",
       "      <td>143.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>143.00</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.00</td>\n",
       "      <td>143.48</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>143.00</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.00</td>\n",
       "      <td>143.48</td>\n",
       "      <td>Y</td>\n",
       "      <td>0.00</td>\n",
       "      <td>171.95</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>RI</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-2.00</td>\n",
       "      <td>Donna</td>\n",
       "      <td>53.00</td>\n",
       "      <td>SI</td>\n",
       "      <td>NO</td>\n",
       "      <td>5.00</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>SI</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.09</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>15.00</td>\n",
       "      <td>334.00</td>\n",
       "      <td>AUTOMOTIVE USATO</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>148463 rows × 95 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Target_95                  Chiave         PRATICA  \\\n",
       "0               0  00000854016301_08_2021  00000854016301   \n",
       "1               0  00006003826200_08_2021  00006003826200   \n",
       "2               0  00002286163301_08_2021  00002286163301   \n",
       "3               0  00001576642301_08_2021  00001576642301   \n",
       "4               0  00002230493301_08_2021  00002230493301   \n",
       "...           ...                     ...             ...   \n",
       "148458          0  00002785945301_06_2023  00002785945301   \n",
       "148459          1  00002471469301_06_2023  00002471469301   \n",
       "148460          0  00002324126301_06_2023  00002324126301   \n",
       "148461          0  00001908346301_06_2023  00001908346301   \n",
       "148462          0  00002905333301_06_2023  00002905333301   \n",
       "\n",
       "                                            DELIN_HISTORY MOD_PAGAM MOD_PAG  \\\n",
       "0                  13221222111111143332432332111100000000         P      BP   \n",
       "1       1001000000000000000000000000000000001100100110...         P      BP   \n",
       "2                                                  111010         P      BP   \n",
       "3                                 10012101111101110000000         P      BP   \n",
       "4                                                 2111210         P      BP   \n",
       "...                                                   ...       ...     ...   \n",
       "148458                                     11011101100000         R      RI   \n",
       "148459                           110000000000000000000000         R      RI   \n",
       "148460                       1000100000000110000000000000         R      RI   \n",
       "148461             10000000000000010000000000000000111000         R      RI   \n",
       "148462                                        10000000000         R      RI   \n",
       "\n",
       "       TIPO_GARANTE Denominazione Regione CODICE_PHONIA  \\\n",
       "0                 C                 Lazio        ALPTEL   \n",
       "1              None               Liguria        ALPTEL   \n",
       "2              None              Calabria        ALPTEL   \n",
       "3              None               Sicilia        ALPTEL   \n",
       "4              None                 Lazio        ALPTEL   \n",
       "...             ...                   ...           ...   \n",
       "148458         None                 Lazio        ALPTEX   \n",
       "148459         None                Umbria        ALPTEX   \n",
       "148460         None             Lombardia        ALPTEX   \n",
       "148461         None             Lombardia        ALPTEX   \n",
       "148462         None                 Lazio        ALPTEX   \n",
       "\n",
       "                     DES_PRODOTTO                   DES_BENE DATA_AFFIDO  \\\n",
       "0              PRESTITO PERSONALE                       None  2021-08-02   \n",
       "1                 REPEAT BUSINESS             FIDELIZ.OTTIMI  2021-08-10   \n",
       "2       ELETTRONICA & ELETTRODOME                       None  2021-08-10   \n",
       "3                 REPEAT BUSINESS                       None  2021-08-10   \n",
       "4                   SPESE MEDICHE                       None  2021-08-09   \n",
       "...                           ...                        ...         ...   \n",
       "148458  DISINTERMEDIATO AUTOMOTIV                       None  2023-06-29   \n",
       "148459         PRESTITO PERSONALE                       None  2023-06-29   \n",
       "148460   DIPENDENTI AZIENDE DOC -                       None  2023-06-29   \n",
       "148461              SPESE MEDICHE        CURE ODONTOIATRICHE  2023-06-29   \n",
       "148462           AUTOMOTIVE USATO  forfour 2as. (W453) FW098  2023-06-29   \n",
       "\n",
       "       DATA_FINE_AFFIDO DATA_FINANZIAM DT_VAL_ULT_PAGAM DATA_ESTINZ  \\\n",
       "0            2021-09-05     2018-06-12       2021-07-05        None   \n",
       "1            2021-09-05     2016-05-12       2021-06-09        None   \n",
       "2            2021-09-05     2021-02-10       2021-06-14        None   \n",
       "3            2021-09-05     2019-09-12       2021-05-27        None   \n",
       "4            2021-09-05     2021-01-13       2021-07-27        None   \n",
       "...                 ...            ...              ...         ...   \n",
       "148458             None     2022-04-14       2023-05-24        None   \n",
       "148459             None     2021-07-15       2023-05-24        None   \n",
       "148460             None     2021-03-15       2022-05-05        None   \n",
       "148461             None     2020-05-15       2020-10-16        None   \n",
       "148462             None     2022-07-15       2023-05-14        None   \n",
       "\n",
       "       DATA_AGGIORNAMENTO P_INST1_DUE_DATE P_INST2_DUE_DATE P_INST3_DUE_DATE  \\\n",
       "0              01/01/0001       2021-04-12       2021-05-12       2021-06-12   \n",
       "1              01/01/0001       2021-04-12       2021-05-12       2021-06-12   \n",
       "2              01/01/0001       2021-04-12       2021-05-12       2021-06-12   \n",
       "3              01/01/0001       2021-04-12       2021-05-12       2021-06-12   \n",
       "4              01/01/0001       2021-04-12       2021-05-12       2021-06-12   \n",
       "...                   ...              ...              ...              ...   \n",
       "148458         01/01/0001       2023-03-14       2023-04-14       2023-05-14   \n",
       "148459         01/01/0001       2023-03-14       2023-04-14       2023-05-14   \n",
       "148460         01/01/0001       2023-03-14       2023-04-14       2023-05-14   \n",
       "148461         01/01/0001       2023-03-14       2023-04-14       2023-05-14   \n",
       "148462         01/01/0001       2023-03-14       2023-04-14       2023-05-14   \n",
       "\n",
       "       DT_VAL_RAT_IMP1 DT_VAL_RAT_IMP2 P_INST1_VAL_DATE P_INST3_VAL_DATE  \\\n",
       "0                 None            None       2021-06-16       2021-07-05   \n",
       "1                 None            None       2021-04-16       2021-06-09   \n",
       "2                 None            None       2021-04-12       2021-06-14   \n",
       "3                 None            None       2021-04-26       2021-05-27   \n",
       "4           2021-07-27            None       2021-04-14       2021-07-27   \n",
       "...                ...             ...              ...              ...   \n",
       "148458            None            None       2023-03-24       2023-05-24   \n",
       "148459            None            None       2023-03-14       2023-05-24   \n",
       "148460      2022-05-05            None       2023-03-14       2023-05-14   \n",
       "148461      2020-10-16            None       2023-03-14       2023-05-14   \n",
       "148462            None            None       2023-03-14       2023-05-14   \n",
       "\n",
       "       SCAD_MAXIRATA SCAD_RAT_IMP2 SCAD_RAT_IMP1  IMP_FINANZIATO  \\\n",
       "0               None    2021-08-12    2021-07-12       15,200.00   \n",
       "1               None    2021-08-12    2021-07-12       14,234.40   \n",
       "2               None    2021-08-12    2021-07-12        1,500.00   \n",
       "3               None    2021-08-12    2021-07-12       14,280.00   \n",
       "4               None    2021-08-12    2021-07-12        6,800.00   \n",
       "...              ...           ...           ...             ...   \n",
       "148458          None    2023-07-14    2023-06-14        4,120.88   \n",
       "148459          None    2023-07-14    2023-06-14        6,250.00   \n",
       "148460          None    2023-07-14    2023-06-14       14,492.80   \n",
       "148461          None    2023-07-14    2023-06-14        6,654.00   \n",
       "148462          None    2023-07-14    2023-06-14        7,009.40   \n",
       "\n",
       "        IMP_MAXIRATA  CURRENT_BALANCE P_INST1_AMOUNT_PAID P_INST2_AMOUNT_PAID  \\\n",
       "0               0.00         9,770.58       0000000023700       0000000023700   \n",
       "1               0.00         8,636.46       0000000018350       0000000018350   \n",
       "2               0.00         1,329.19       0000000005650       0000000005650   \n",
       "3               0.00        10,954.69       0000000026000       0000000026000   \n",
       "4               0.00         6,214.51       0000000016600       0000000016600   \n",
       "...              ...              ...                 ...                 ...   \n",
       "148458          0.00         3,169.02       0000000010600       0000000010600   \n",
       "148459          0.00         4,725.40       0000000011600       0000000011600   \n",
       "148460          0.00         9,057.10       0000000030300       0000000030300   \n",
       "148461          0.00         3,828.21       0000000012200       0000000012200   \n",
       "148462          0.00         6,037.66       0000000014300       0000000014300   \n",
       "\n",
       "       P_INST3_AMOUNT_PAID  RATE_TOTALI  NUM_RATE_RIFI  NUM_RAT_IMP1  \\\n",
       "0            0000000023700        84.00           0.00         37.00   \n",
       "1            0000000018350       120.00           0.00         62.00   \n",
       "2            0000000005650        30.00           0.00          5.00   \n",
       "3            0000000026000        72.00           0.00         22.00   \n",
       "4            0000000016600        48.00           0.00          6.00   \n",
       "...                    ...          ...            ...           ...   \n",
       "148458       0000000010600        48.00           0.00         14.00   \n",
       "148459       0000000011600        72.00           0.00         23.00   \n",
       "148460       0000000030300        60.00           0.00         27.00   \n",
       "148461       0000000012200        70.00           0.00         35.00   \n",
       "148462       0000000014300        60.00           0.00         11.00   \n",
       "\n",
       "        NUM_RAT_IMP2  P_INST1_NUM  P_INST2_NUM  P_INST3_NUM  NUMERO RATE  \\\n",
       "0              38.00        34.00        35.00        36.00            2   \n",
       "1              63.00        59.00        60.00        61.00            2   \n",
       "2               6.00         2.00         3.00         4.00            2   \n",
       "3              23.00        19.00        20.00        21.00            2   \n",
       "4               7.00         3.00         4.00         5.00            2   \n",
       "...              ...          ...          ...          ...          ...   \n",
       "148458         15.00        11.00        12.00        13.00            2   \n",
       "148459         24.00        20.00        21.00        22.00            2   \n",
       "148460         28.00        24.00        25.00        26.00            2   \n",
       "148461         36.00        32.00        33.00        34.00            2   \n",
       "148462         12.00         8.00         9.00        10.00            2   \n",
       "\n",
       "       PROVINCIA  P_INST1_AMOUNT  P_INST2_AMOUNT  P_INST3_AMOUNT  \\\n",
       "0             LT          237.00          237.00          237.00   \n",
       "1             IM          183.50          183.50          183.50   \n",
       "2             KR           56.50           56.50           56.50   \n",
       "3             EN          260.00          260.00          260.00   \n",
       "4             RM          166.00          166.00          166.00   \n",
       "...          ...             ...             ...             ...   \n",
       "148458        RM          106.00          106.00          106.00   \n",
       "148459        TR          116.00          116.00          116.00   \n",
       "148460        VA          303.00          303.00          303.00   \n",
       "148461        MI          122.00          122.00          122.00   \n",
       "148462        RM          143.00          143.00          143.00   \n",
       "\n",
       "        IMP_RAT_IMP1  IMP_RAT_IMP2  IMP_PAG_RAT_IMP1  IMP_PAG_RAT_IMP2  \\\n",
       "0             237.00        237.00              0.00              0.00   \n",
       "1             183.50        183.50              0.00              0.00   \n",
       "2              56.50         56.50              0.00              0.00   \n",
       "3             260.00        260.00              0.00              0.00   \n",
       "4             166.00        166.00              2.34              0.00   \n",
       "...              ...           ...               ...               ...   \n",
       "148458        106.00        106.00              0.00              0.00   \n",
       "148459        116.00        116.00              0.00              0.00   \n",
       "148460        303.00        303.00              1.25              0.00   \n",
       "148461        122.00        122.00             26.24              0.00   \n",
       "148462        143.00        143.00              0.00              0.00   \n",
       "\n",
       "        TOT_RATE_AFF_INIZIO_MAND  TOT_MORA_AFF_INIZIO_MAND  \\\n",
       "0                         237.00                     28.24   \n",
       "1                         183.50                      1.56   \n",
       "2                          56.50                      0.58   \n",
       "3                         260.00                     16.20   \n",
       "4                         163.66                      3.51   \n",
       "...                          ...                       ...   \n",
       "148458                    106.00                      2.17   \n",
       "148459                    116.00                      0.85   \n",
       "148460                    301.75                      3.66   \n",
       "148461                     95.76                      0.00   \n",
       "148462                    143.00                      0.48   \n",
       "\n",
       "        TOT_ONERI_AFF_INIZIO_MAND  TOT_AFF_INIZIO_MAND  TOT_RATE_PAG  \\\n",
       "0                           12.00               277.24          0.00   \n",
       "1                           12.00               197.06          0.00   \n",
       "2                           15.97                73.05          0.00   \n",
       "3                           12.00               288.20          0.00   \n",
       "4                           32.40               199.57          0.00   \n",
       "...                           ...                  ...           ...   \n",
       "148458                       0.00               108.17          0.00   \n",
       "148459                       0.00               116.85          0.00   \n",
       "148460                       0.00               305.41          0.00   \n",
       "148461                       0.00                95.76          0.00   \n",
       "148462                       0.00               143.48          0.00   \n",
       "\n",
       "        TOT_MORA_PAG  TOT_ONERI_PAG  TOT_PAG  TOT_RATE_AFF  TOT_MORA_AFF  \\\n",
       "0               0.00           0.00     0.00        237.00         28.24   \n",
       "1               0.00           0.00     0.00        183.50          1.56   \n",
       "2               0.00           0.00     0.00         56.50          0.58   \n",
       "3               0.00           0.00     0.00        260.00         16.20   \n",
       "4               0.00           0.00     0.00        163.66          3.51   \n",
       "...              ...            ...      ...           ...           ...   \n",
       "148458          0.00           0.00     0.00        106.00          2.17   \n",
       "148459          0.00           0.00     0.00        116.00          0.85   \n",
       "148460          0.00           0.00     0.00        301.75          3.66   \n",
       "148461          0.00           0.00     0.00         95.76          0.00   \n",
       "148462          0.00           0.00     0.00        143.00          0.48   \n",
       "\n",
       "        TOT_ONERI_AFF  TOT_AFF Flag_Riciclo_SDD  TOT REC  TOT AFF  \\\n",
       "0               12.00   277.24             None     0.00   486.00   \n",
       "1               12.00   197.06                Y     0.00   379.00   \n",
       "2               15.97    73.05             None     0.00   144.94   \n",
       "3               12.00   288.20             None     0.00   532.00   \n",
       "4               32.40   199.57             None     2.34   396.80   \n",
       "...               ...      ...              ...      ...      ...   \n",
       "148458           0.00   108.17                Y     0.00   246.40   \n",
       "148459           0.00   116.85                Y   116.00   116.00   \n",
       "148460           0.00   305.41                Y     1.25   303.00   \n",
       "148461           0.00    95.76                Y    26.24   122.00   \n",
       "148462           0.00   143.48                Y     0.00   171.95   \n",
       "\n",
       "       Flag_Rifinanziamento Flag_Garante Flag_Cointestazione Metodo_pagamento  \\\n",
       "0                        NO           SI                  SI               BP   \n",
       "1                        NO           NO                  NO               BP   \n",
       "2                        NO           NO                  NO               BP   \n",
       "3                        NO           NO                  NO               BP   \n",
       "4                        NO           NO                  NO               BP   \n",
       "...                     ...          ...                 ...              ...   \n",
       "148458                   NO           NO                  SI               RI   \n",
       "148459                   NO           NO                  SI               RI   \n",
       "148460                   NO           NO                  SI               RI   \n",
       "148461                   NO           NO                  NO               RI   \n",
       "148462                   NO           NO                  NO               RI   \n",
       "\n",
       "        Ratio_Rate_Imp1  Ratio_Rate_Imp2  DistanzaAffidoUltimoPagamento  \\\n",
       "0                  0.44             0.45                          -1.00   \n",
       "1                  0.52             0.53                          -2.00   \n",
       "2                  0.17             0.20                          -2.00   \n",
       "3                  0.31             0.32                          -2.00   \n",
       "4                  0.12             0.15                          -0.00   \n",
       "...                 ...              ...                            ...   \n",
       "148458             0.29             0.31                          -1.00   \n",
       "148459             0.32             0.33                          -1.00   \n",
       "148460             0.45             0.47                         -14.00   \n",
       "148461             0.50             0.51                         -32.00   \n",
       "148462             0.18             0.20                          -2.00   \n",
       "\n",
       "        Sesso  Eta_Debitore NOSTART12M NOSTART6M  DurataFinanziamento  \\\n",
       "0        None           NaN         NO        NO                 7.00   \n",
       "1       Donna         59.00         NO        NO                10.00   \n",
       "2       Donna         57.00         SI        SI                 2.50   \n",
       "3        Uomo         76.00         NO        NO                 6.00   \n",
       "4        Uomo         57.00         SI        SI                 4.00   \n",
       "...       ...           ...        ...       ...                  ...   \n",
       "148458   None           NaN         NO        NO                 4.00   \n",
       "148459   None           NaN         NO        NO                 6.00   \n",
       "148460   None           NaN         NO        NO                 5.00   \n",
       "148461   Uomo         61.00         NO        NO                 5.83   \n",
       "148462  Donna         53.00         SI        NO                 5.00   \n",
       "\n",
       "       Flag_Galleggiamento_3M Flag_Galleggiamento_6M Flag_Gestione_Prec  \\\n",
       "0                          NO                     NO                 NO   \n",
       "1                          NO                     NO                 SI   \n",
       "2                          SI                     NO                 SI   \n",
       "3                          NO                     NO                 SI   \n",
       "4                          NO                     NO                 NO   \n",
       "...                       ...                    ...                ...   \n",
       "148458                     NO                     NO                 SI   \n",
       "148459                     NO                     NO                 NO   \n",
       "148460                     NO                     NO                 SI   \n",
       "148461                     NO                     NO                 NO   \n",
       "148462                     NO                     NO                 NO   \n",
       "\n",
       "       Flag_InsolvenzaGrave_3M Flag_InsolvenzaGrave_6M Flag_Miglioramento_3M  \\\n",
       "0                           SI                      SI                    NO   \n",
       "1                           NO                      NO                    NO   \n",
       "2                           NO                      NO                    NO   \n",
       "3                           NO                      NO                    NO   \n",
       "4                           NO                      NO                    NO   \n",
       "...                        ...                     ...                   ...   \n",
       "148458                      NO                      NO                    NO   \n",
       "148459                      NO                      NO                    NO   \n",
       "148460                      NO                      NO                    NO   \n",
       "148461                      NO                      NO                    NO   \n",
       "148462                      NO                      NO                    NO   \n",
       "\n",
       "       Flag_Rate_piu_uno Flag_rientrototale_6M Flag_Scivolamento_3M  \\\n",
       "0                     SI                    NO                   NO   \n",
       "1                     NO                    SI                   NO   \n",
       "2                     NO                    SI                   NO   \n",
       "3                     SI                    SI                   NO   \n",
       "4                     SI                    NO                   NO   \n",
       "...                  ...                   ...                  ...   \n",
       "148458                NO                    SI                   NO   \n",
       "148459                NO                    SI                   NO   \n",
       "148460                NO                    SI                   NO   \n",
       "148461                NO                    SI                   NO   \n",
       "148462                NO                    SI                   NO   \n",
       "\n",
       "       Flag_Scivolamento_6M  Numero_mesi_rec  Severity_12M  \\\n",
       "0                        NO            12.00          1.58   \n",
       "1                        NO             1.00          0.17   \n",
       "2                        NO             3.00          0.67   \n",
       "3                        NO             1.00          0.83   \n",
       "4                        NO             6.00          1.14   \n",
       "...                     ...              ...           ...   \n",
       "148458                   NO             2.00          0.58   \n",
       "148459                   NO             2.00          0.17   \n",
       "148460                   NO             1.00          0.17   \n",
       "148461                   NO             1.00          0.08   \n",
       "148462                   NO             1.00          0.09   \n",
       "\n",
       "        Severity_12M_pesata Flag_InsolvenzaGrave_12M FlagRecOver100  \\\n",
       "0                      0.95                       SI              0   \n",
       "1                      0.15                       NO              0   \n",
       "2                      0.57                       NO              0   \n",
       "3                      0.41                       NO              0   \n",
       "4                      0.92                       NO              0   \n",
       "...                     ...                      ...            ...   \n",
       "148458                 0.39                       NO              0   \n",
       "148459                 0.16                       NO              0   \n",
       "148460                 0.14                       NO              0   \n",
       "148461                 0.08                       NO              0   \n",
       "148462                 0.09                       NO              0   \n",
       "\n",
       "        AgeingGestioneGg  AgeingErogazioneGg       DESCRIZIONE PRODOTTO  \\\n",
       "0                  21.00            1,126.00         PRESTITO PERSONALE   \n",
       "1                  29.00            1,887.00      FINANZIAMENTI AZIENDE   \n",
       "2                  29.00              152.00  ELETTRONICA & ELETTRODOME   \n",
       "3                  29.00              669.00      FINANZIAMENTI AZIENDE   \n",
       "4                  28.00              180.00              SPESE MEDICHE   \n",
       "...                  ...                 ...                        ...   \n",
       "148458             15.00              426.00           AUTOMOTIVE NUOVO   \n",
       "148459             15.00              699.00         PRESTITO PERSONALE   \n",
       "148460             15.00              821.00    PRESTITI CON TRATTENUTA   \n",
       "148461             15.00            1,125.00              SPESE MEDICHE   \n",
       "148462             15.00              334.00           AUTOMOTIVE USATO   \n",
       "\n",
       "        RATIO_ONERI_AFF_TOT_AFF  \n",
       "0                          0.04  \n",
       "1                          0.06  \n",
       "2                          0.22  \n",
       "3                          0.04  \n",
       "4                          0.16  \n",
       "...                         ...  \n",
       "148458                     0.00  \n",
       "148459                     0.00  \n",
       "148460                     0.00  \n",
       "148461                     0.00  \n",
       "148462                     0.00  \n",
       "\n",
       "[148463 rows x 95 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clienti_con_meno_colonne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AgeingErogazioneGg',\n",
       " 'AgeingGestioneGg',\n",
       " 'CODICE_PHONIA',\n",
       " 'CURRENT_BALANCE',\n",
       " 'Chiave',\n",
       " 'DATA_AFFIDO',\n",
       " 'DATA_AGGIORNAMENTO',\n",
       " 'DATA_ESTINZ',\n",
       " 'DATA_FINANZIAM',\n",
       " 'DATA_FINE_AFFIDO',\n",
       " 'DELIN_HISTORY',\n",
       " 'DESCRIZIONE PRODOTTO',\n",
       " 'DES_BENE',\n",
       " 'DES_PRODOTTO',\n",
       " 'DT_VAL_RAT_IMP1',\n",
       " 'DT_VAL_RAT_IMP2',\n",
       " 'DT_VAL_ULT_PAGAM',\n",
       " 'Denominazione Regione',\n",
       " 'DistanzaAffidoUltimoPagamento',\n",
       " 'DurataFinanziamento',\n",
       " 'Eta_Debitore',\n",
       " 'FlagRecOver100',\n",
       " 'Flag_Cointestazione',\n",
       " 'Flag_Galleggiamento_3M',\n",
       " 'Flag_Galleggiamento_6M',\n",
       " 'Flag_Garante',\n",
       " 'Flag_Gestione_Prec',\n",
       " 'Flag_InsolvenzaGrave_12M',\n",
       " 'Flag_InsolvenzaGrave_3M',\n",
       " 'Flag_InsolvenzaGrave_6M',\n",
       " 'Flag_Miglioramento_3M',\n",
       " 'Flag_Rate_piu_uno',\n",
       " 'Flag_Riciclo_SDD',\n",
       " 'Flag_Rifinanziamento',\n",
       " 'Flag_Scivolamento_3M',\n",
       " 'Flag_Scivolamento_6M',\n",
       " 'Flag_rientrototale_6M',\n",
       " 'IMP_FINANZIATO',\n",
       " 'IMP_MAXIRATA',\n",
       " 'IMP_PAG_RAT_IMP1',\n",
       " 'IMP_PAG_RAT_IMP2',\n",
       " 'IMP_RAT_IMP1',\n",
       " 'IMP_RAT_IMP2',\n",
       " 'MOD_PAG',\n",
       " 'MOD_PAGAM',\n",
       " 'Metodo_pagamento',\n",
       " 'NOSTART12M',\n",
       " 'NOSTART6M',\n",
       " 'NUMERO RATE',\n",
       " 'NUM_RATE_RIFI',\n",
       " 'NUM_RAT_IMP1',\n",
       " 'NUM_RAT_IMP2',\n",
       " 'Numero_mesi_rec',\n",
       " 'PRATICA',\n",
       " 'PROVINCIA',\n",
       " 'P_INST1_AMOUNT',\n",
       " 'P_INST1_AMOUNT_PAID',\n",
       " 'P_INST1_DUE_DATE',\n",
       " 'P_INST1_NUM',\n",
       " 'P_INST1_VAL_DATE',\n",
       " 'P_INST2_AMOUNT',\n",
       " 'P_INST2_AMOUNT_PAID',\n",
       " 'P_INST2_DUE_DATE',\n",
       " 'P_INST2_NUM',\n",
       " 'P_INST3_AMOUNT',\n",
       " 'P_INST3_AMOUNT_PAID',\n",
       " 'P_INST3_DUE_DATE',\n",
       " 'P_INST3_NUM',\n",
       " 'P_INST3_VAL_DATE',\n",
       " 'RATE_TOTALI',\n",
       " 'RATIO_ONERI_AFF_TOT_AFF',\n",
       " 'Ratio_Rate_Imp1',\n",
       " 'Ratio_Rate_Imp2',\n",
       " 'SCAD_MAXIRATA',\n",
       " 'SCAD_RAT_IMP1',\n",
       " 'SCAD_RAT_IMP2',\n",
       " 'Sesso',\n",
       " 'Severity_12M',\n",
       " 'Severity_12M_pesata',\n",
       " 'TIPO_GARANTE',\n",
       " 'TOT AFF',\n",
       " 'TOT REC',\n",
       " 'TOT_AFF',\n",
       " 'TOT_AFF_INIZIO_MAND',\n",
       " 'TOT_MORA_AFF',\n",
       " 'TOT_MORA_AFF_INIZIO_MAND',\n",
       " 'TOT_MORA_PAG',\n",
       " 'TOT_ONERI_AFF',\n",
       " 'TOT_ONERI_AFF_INIZIO_MAND',\n",
       " 'TOT_ONERI_PAG',\n",
       " 'TOT_PAG',\n",
       " 'TOT_RATE_AFF',\n",
       " 'TOT_RATE_AFF_INIZIO_MAND',\n",
       " 'TOT_RATE_PAG',\n",
       " 'Target_95']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(df_clienti_con_meno_colonne.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "colonne_da_usare = ['Target_95','Denominazione Regione', 'AgeingGestioneGg', 'DESCRIZIONE PRODOTTO', 'NUMERO RATE', 'Ratio_Rate_Imp1', 'TOT_AFF', 'AgeingErogazioneGg', 'Eta_Debitore', 'IMP_FINANZIATO', 'RATIO_ONERI_AFF_TOT_AFF']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clienti_con_meno_colonne = df_clienti_con_meno_colonne[colonne_da_usare]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target_95</th>\n",
       "      <th>Denominazione Regione</th>\n",
       "      <th>AgeingGestioneGg</th>\n",
       "      <th>DESCRIZIONE PRODOTTO</th>\n",
       "      <th>NUMERO RATE</th>\n",
       "      <th>Ratio_Rate_Imp1</th>\n",
       "      <th>TOT_AFF</th>\n",
       "      <th>AgeingErogazioneGg</th>\n",
       "      <th>Eta_Debitore</th>\n",
       "      <th>IMP_FINANZIATO</th>\n",
       "      <th>RATIO_ONERI_AFF_TOT_AFF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Lazio</td>\n",
       "      <td>21.00</td>\n",
       "      <td>PRESTITO PERSONALE</td>\n",
       "      <td>2</td>\n",
       "      <td>0.44</td>\n",
       "      <td>277.24</td>\n",
       "      <td>1,126.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15,200.00</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Liguria</td>\n",
       "      <td>29.00</td>\n",
       "      <td>FINANZIAMENTI AZIENDE</td>\n",
       "      <td>2</td>\n",
       "      <td>0.52</td>\n",
       "      <td>197.06</td>\n",
       "      <td>1,887.00</td>\n",
       "      <td>59.00</td>\n",
       "      <td>14,234.40</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Calabria</td>\n",
       "      <td>29.00</td>\n",
       "      <td>ELETTRONICA &amp; ELETTRODOME</td>\n",
       "      <td>2</td>\n",
       "      <td>0.17</td>\n",
       "      <td>73.05</td>\n",
       "      <td>152.00</td>\n",
       "      <td>57.00</td>\n",
       "      <td>1,500.00</td>\n",
       "      <td>0.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Sicilia</td>\n",
       "      <td>29.00</td>\n",
       "      <td>FINANZIAMENTI AZIENDE</td>\n",
       "      <td>2</td>\n",
       "      <td>0.31</td>\n",
       "      <td>288.20</td>\n",
       "      <td>669.00</td>\n",
       "      <td>76.00</td>\n",
       "      <td>14,280.00</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Lazio</td>\n",
       "      <td>28.00</td>\n",
       "      <td>SPESE MEDICHE</td>\n",
       "      <td>2</td>\n",
       "      <td>0.12</td>\n",
       "      <td>199.57</td>\n",
       "      <td>180.00</td>\n",
       "      <td>57.00</td>\n",
       "      <td>6,800.00</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148458</th>\n",
       "      <td>0</td>\n",
       "      <td>Lazio</td>\n",
       "      <td>15.00</td>\n",
       "      <td>AUTOMOTIVE NUOVO</td>\n",
       "      <td>2</td>\n",
       "      <td>0.29</td>\n",
       "      <td>108.17</td>\n",
       "      <td>426.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4,120.88</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148459</th>\n",
       "      <td>1</td>\n",
       "      <td>Umbria</td>\n",
       "      <td>15.00</td>\n",
       "      <td>PRESTITO PERSONALE</td>\n",
       "      <td>2</td>\n",
       "      <td>0.32</td>\n",
       "      <td>116.85</td>\n",
       "      <td>699.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6,250.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148460</th>\n",
       "      <td>0</td>\n",
       "      <td>Lombardia</td>\n",
       "      <td>15.00</td>\n",
       "      <td>PRESTITI CON TRATTENUTA</td>\n",
       "      <td>2</td>\n",
       "      <td>0.45</td>\n",
       "      <td>305.41</td>\n",
       "      <td>821.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14,492.80</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148461</th>\n",
       "      <td>0</td>\n",
       "      <td>Lombardia</td>\n",
       "      <td>15.00</td>\n",
       "      <td>SPESE MEDICHE</td>\n",
       "      <td>2</td>\n",
       "      <td>0.50</td>\n",
       "      <td>95.76</td>\n",
       "      <td>1,125.00</td>\n",
       "      <td>61.00</td>\n",
       "      <td>6,654.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148462</th>\n",
       "      <td>0</td>\n",
       "      <td>Lazio</td>\n",
       "      <td>15.00</td>\n",
       "      <td>AUTOMOTIVE USATO</td>\n",
       "      <td>2</td>\n",
       "      <td>0.18</td>\n",
       "      <td>143.48</td>\n",
       "      <td>334.00</td>\n",
       "      <td>53.00</td>\n",
       "      <td>7,009.40</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>148463 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Target_95 Denominazione Regione  AgeingGestioneGg  \\\n",
       "0               0                 Lazio             21.00   \n",
       "1               0               Liguria             29.00   \n",
       "2               0              Calabria             29.00   \n",
       "3               0               Sicilia             29.00   \n",
       "4               0                 Lazio             28.00   \n",
       "...           ...                   ...               ...   \n",
       "148458          0                 Lazio             15.00   \n",
       "148459          1                Umbria             15.00   \n",
       "148460          0             Lombardia             15.00   \n",
       "148461          0             Lombardia             15.00   \n",
       "148462          0                 Lazio             15.00   \n",
       "\n",
       "             DESCRIZIONE PRODOTTO  NUMERO RATE  Ratio_Rate_Imp1  TOT_AFF  \\\n",
       "0              PRESTITO PERSONALE            2             0.44   277.24   \n",
       "1           FINANZIAMENTI AZIENDE            2             0.52   197.06   \n",
       "2       ELETTRONICA & ELETTRODOME            2             0.17    73.05   \n",
       "3           FINANZIAMENTI AZIENDE            2             0.31   288.20   \n",
       "4                   SPESE MEDICHE            2             0.12   199.57   \n",
       "...                           ...          ...              ...      ...   \n",
       "148458           AUTOMOTIVE NUOVO            2             0.29   108.17   \n",
       "148459         PRESTITO PERSONALE            2             0.32   116.85   \n",
       "148460    PRESTITI CON TRATTENUTA            2             0.45   305.41   \n",
       "148461              SPESE MEDICHE            2             0.50    95.76   \n",
       "148462           AUTOMOTIVE USATO            2             0.18   143.48   \n",
       "\n",
       "        AgeingErogazioneGg  Eta_Debitore  IMP_FINANZIATO  \\\n",
       "0                 1,126.00           NaN       15,200.00   \n",
       "1                 1,887.00         59.00       14,234.40   \n",
       "2                   152.00         57.00        1,500.00   \n",
       "3                   669.00         76.00       14,280.00   \n",
       "4                   180.00         57.00        6,800.00   \n",
       "...                    ...           ...             ...   \n",
       "148458              426.00           NaN        4,120.88   \n",
       "148459              699.00           NaN        6,250.00   \n",
       "148460              821.00           NaN       14,492.80   \n",
       "148461            1,125.00         61.00        6,654.00   \n",
       "148462              334.00         53.00        7,009.40   \n",
       "\n",
       "        RATIO_ONERI_AFF_TOT_AFF  \n",
       "0                          0.04  \n",
       "1                          0.06  \n",
       "2                          0.22  \n",
       "3                          0.04  \n",
       "4                          0.16  \n",
       "...                         ...  \n",
       "148458                     0.00  \n",
       "148459                     0.00  \n",
       "148460                     0.00  \n",
       "148461                     0.00  \n",
       "148462                     0.00  \n",
       "\n",
       "[148463 rows x 11 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clienti_con_meno_colonne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6> Rimuovo le colonne che hanno più del 50% di valori missing </h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentuale_mancanti = (df_clienti_con_meno_colonne.isna().sum() / len(df_clienti_con_meno_colonne)).sort_values(ascending=False)\n",
    "colonne_da_mantenere = percentuale_mancanti[percentuale_mancanti <= 0.5].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], dtype: float64)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percentuale_mancanti[percentuale_mancanti>0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rimuovi_colonne_con_valori_mancanti(df, soglia=0.5):\n",
    "    # Calcolo la percentuale di valori mancanti per ciascuna colonna\n",
    "    percentuale_mancanti = (df.isna().sum() / len(df)).sort_values(ascending=False)\n",
    "    \n",
    "    # Seleziono le colonne con al massimo il 50% di valori mancanti\n",
    "    colonne_da_mantenere = percentuale_mancanti[percentuale_mancanti <= soglia].index\n",
    "    \n",
    "    # Creo un nuovo DataFrame con solo le colonne da mantenere\n",
    "    df_senza_colonne_mancanti = df[colonne_da_mantenere]\n",
    "    \n",
    "    return df_senza_colonne_mancanti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clienti_con_meno_colonne = rimuovi_colonne_con_valori_mancanti(df_clienti_con_meno_colonne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(148463, 11)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clienti_con_meno_colonne.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6> Rimuovo le colonne con 1 solo valore unico </h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([], dtype='object')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check colonne con valori univoci\n",
    "df_unique = df_clienti_con_meno_colonne.loc[:,df_clienti_con_meno_colonne.nunique()<2]\n",
    "df_unique.columns\n",
    "# Queste colonne sono da eliminare dal dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elimino le colonne che hanno 1 solo valore unico\n",
    "\n",
    "df_clienti_con_meno_colonne = df_clienti_con_meno_colonne.drop(columns= df_unique.columns, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(148463, 11)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clienti_con_meno_colonne.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Eta_Debitore</th>\n",
       "      <th>RATIO_ONERI_AFF_TOT_AFF</th>\n",
       "      <th>Denominazione Regione</th>\n",
       "      <th>Target_95</th>\n",
       "      <th>AgeingGestioneGg</th>\n",
       "      <th>DESCRIZIONE PRODOTTO</th>\n",
       "      <th>NUMERO RATE</th>\n",
       "      <th>Ratio_Rate_Imp1</th>\n",
       "      <th>TOT_AFF</th>\n",
       "      <th>AgeingErogazioneGg</th>\n",
       "      <th>IMP_FINANZIATO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.04</td>\n",
       "      <td>Lazio</td>\n",
       "      <td>0</td>\n",
       "      <td>21.00</td>\n",
       "      <td>PRESTITO PERSONALE</td>\n",
       "      <td>2</td>\n",
       "      <td>0.44</td>\n",
       "      <td>277.24</td>\n",
       "      <td>1,126.00</td>\n",
       "      <td>15,200.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>59.00</td>\n",
       "      <td>0.06</td>\n",
       "      <td>Liguria</td>\n",
       "      <td>0</td>\n",
       "      <td>29.00</td>\n",
       "      <td>FINANZIAMENTI AZIENDE</td>\n",
       "      <td>2</td>\n",
       "      <td>0.52</td>\n",
       "      <td>197.06</td>\n",
       "      <td>1,887.00</td>\n",
       "      <td>14,234.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>57.00</td>\n",
       "      <td>0.22</td>\n",
       "      <td>Calabria</td>\n",
       "      <td>0</td>\n",
       "      <td>29.00</td>\n",
       "      <td>ELETTRONICA &amp; ELETTRODOME</td>\n",
       "      <td>2</td>\n",
       "      <td>0.17</td>\n",
       "      <td>73.05</td>\n",
       "      <td>152.00</td>\n",
       "      <td>1,500.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>76.00</td>\n",
       "      <td>0.04</td>\n",
       "      <td>Sicilia</td>\n",
       "      <td>0</td>\n",
       "      <td>29.00</td>\n",
       "      <td>FINANZIAMENTI AZIENDE</td>\n",
       "      <td>2</td>\n",
       "      <td>0.31</td>\n",
       "      <td>288.20</td>\n",
       "      <td>669.00</td>\n",
       "      <td>14,280.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57.00</td>\n",
       "      <td>0.16</td>\n",
       "      <td>Lazio</td>\n",
       "      <td>0</td>\n",
       "      <td>28.00</td>\n",
       "      <td>SPESE MEDICHE</td>\n",
       "      <td>2</td>\n",
       "      <td>0.12</td>\n",
       "      <td>199.57</td>\n",
       "      <td>180.00</td>\n",
       "      <td>6,800.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148458</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Lazio</td>\n",
       "      <td>0</td>\n",
       "      <td>15.00</td>\n",
       "      <td>AUTOMOTIVE NUOVO</td>\n",
       "      <td>2</td>\n",
       "      <td>0.29</td>\n",
       "      <td>108.17</td>\n",
       "      <td>426.00</td>\n",
       "      <td>4,120.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148459</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Umbria</td>\n",
       "      <td>1</td>\n",
       "      <td>15.00</td>\n",
       "      <td>PRESTITO PERSONALE</td>\n",
       "      <td>2</td>\n",
       "      <td>0.32</td>\n",
       "      <td>116.85</td>\n",
       "      <td>699.00</td>\n",
       "      <td>6,250.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148460</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Lombardia</td>\n",
       "      <td>0</td>\n",
       "      <td>15.00</td>\n",
       "      <td>PRESTITI CON TRATTENUTA</td>\n",
       "      <td>2</td>\n",
       "      <td>0.45</td>\n",
       "      <td>305.41</td>\n",
       "      <td>821.00</td>\n",
       "      <td>14,492.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148461</th>\n",
       "      <td>61.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Lombardia</td>\n",
       "      <td>0</td>\n",
       "      <td>15.00</td>\n",
       "      <td>SPESE MEDICHE</td>\n",
       "      <td>2</td>\n",
       "      <td>0.50</td>\n",
       "      <td>95.76</td>\n",
       "      <td>1,125.00</td>\n",
       "      <td>6,654.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148462</th>\n",
       "      <td>53.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Lazio</td>\n",
       "      <td>0</td>\n",
       "      <td>15.00</td>\n",
       "      <td>AUTOMOTIVE USATO</td>\n",
       "      <td>2</td>\n",
       "      <td>0.18</td>\n",
       "      <td>143.48</td>\n",
       "      <td>334.00</td>\n",
       "      <td>7,009.40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>148463 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Eta_Debitore  RATIO_ONERI_AFF_TOT_AFF Denominazione Regione  \\\n",
       "0                NaN                     0.04                 Lazio   \n",
       "1              59.00                     0.06               Liguria   \n",
       "2              57.00                     0.22              Calabria   \n",
       "3              76.00                     0.04               Sicilia   \n",
       "4              57.00                     0.16                 Lazio   \n",
       "...              ...                      ...                   ...   \n",
       "148458           NaN                     0.00                 Lazio   \n",
       "148459           NaN                     0.00                Umbria   \n",
       "148460           NaN                     0.00             Lombardia   \n",
       "148461         61.00                     0.00             Lombardia   \n",
       "148462         53.00                     0.00                 Lazio   \n",
       "\n",
       "        Target_95  AgeingGestioneGg       DESCRIZIONE PRODOTTO  NUMERO RATE  \\\n",
       "0               0             21.00         PRESTITO PERSONALE            2   \n",
       "1               0             29.00      FINANZIAMENTI AZIENDE            2   \n",
       "2               0             29.00  ELETTRONICA & ELETTRODOME            2   \n",
       "3               0             29.00      FINANZIAMENTI AZIENDE            2   \n",
       "4               0             28.00              SPESE MEDICHE            2   \n",
       "...           ...               ...                        ...          ...   \n",
       "148458          0             15.00           AUTOMOTIVE NUOVO            2   \n",
       "148459          1             15.00         PRESTITO PERSONALE            2   \n",
       "148460          0             15.00    PRESTITI CON TRATTENUTA            2   \n",
       "148461          0             15.00              SPESE MEDICHE            2   \n",
       "148462          0             15.00           AUTOMOTIVE USATO            2   \n",
       "\n",
       "        Ratio_Rate_Imp1  TOT_AFF  AgeingErogazioneGg  IMP_FINANZIATO  \n",
       "0                  0.44   277.24            1,126.00       15,200.00  \n",
       "1                  0.52   197.06            1,887.00       14,234.40  \n",
       "2                  0.17    73.05              152.00        1,500.00  \n",
       "3                  0.31   288.20              669.00       14,280.00  \n",
       "4                  0.12   199.57              180.00        6,800.00  \n",
       "...                 ...      ...                 ...             ...  \n",
       "148458             0.29   108.17              426.00        4,120.88  \n",
       "148459             0.32   116.85              699.00        6,250.00  \n",
       "148460             0.45   305.41              821.00       14,492.80  \n",
       "148461             0.50    95.76            1,125.00        6,654.00  \n",
       "148462             0.18   143.48              334.00        7,009.40  \n",
       "\n",
       "[148463 rows x 11 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clienti_con_meno_colonne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> Trattamento outliers </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clienti_senza_outliers = df_clienti_con_meno_colonne.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Eta_Debitore</th>\n",
       "      <th>RATIO_ONERI_AFF_TOT_AFF</th>\n",
       "      <th>Denominazione Regione</th>\n",
       "      <th>Target_95</th>\n",
       "      <th>AgeingGestioneGg</th>\n",
       "      <th>DESCRIZIONE PRODOTTO</th>\n",
       "      <th>NUMERO RATE</th>\n",
       "      <th>Ratio_Rate_Imp1</th>\n",
       "      <th>TOT_AFF</th>\n",
       "      <th>AgeingErogazioneGg</th>\n",
       "      <th>IMP_FINANZIATO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.04</td>\n",
       "      <td>Lazio</td>\n",
       "      <td>0</td>\n",
       "      <td>21.00</td>\n",
       "      <td>PRESTITO PERSONALE</td>\n",
       "      <td>2</td>\n",
       "      <td>0.44</td>\n",
       "      <td>277.24</td>\n",
       "      <td>1,126.00</td>\n",
       "      <td>15,200.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>59.00</td>\n",
       "      <td>0.06</td>\n",
       "      <td>Liguria</td>\n",
       "      <td>0</td>\n",
       "      <td>29.00</td>\n",
       "      <td>FINANZIAMENTI AZIENDE</td>\n",
       "      <td>2</td>\n",
       "      <td>0.52</td>\n",
       "      <td>197.06</td>\n",
       "      <td>1,887.00</td>\n",
       "      <td>14,234.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>57.00</td>\n",
       "      <td>0.22</td>\n",
       "      <td>Calabria</td>\n",
       "      <td>0</td>\n",
       "      <td>29.00</td>\n",
       "      <td>ELETTRONICA &amp; ELETTRODOME</td>\n",
       "      <td>2</td>\n",
       "      <td>0.17</td>\n",
       "      <td>73.05</td>\n",
       "      <td>152.00</td>\n",
       "      <td>1,500.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>76.00</td>\n",
       "      <td>0.04</td>\n",
       "      <td>Sicilia</td>\n",
       "      <td>0</td>\n",
       "      <td>29.00</td>\n",
       "      <td>FINANZIAMENTI AZIENDE</td>\n",
       "      <td>2</td>\n",
       "      <td>0.31</td>\n",
       "      <td>288.20</td>\n",
       "      <td>669.00</td>\n",
       "      <td>14,280.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57.00</td>\n",
       "      <td>0.16</td>\n",
       "      <td>Lazio</td>\n",
       "      <td>0</td>\n",
       "      <td>28.00</td>\n",
       "      <td>SPESE MEDICHE</td>\n",
       "      <td>2</td>\n",
       "      <td>0.12</td>\n",
       "      <td>199.57</td>\n",
       "      <td>180.00</td>\n",
       "      <td>6,800.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148458</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Lazio</td>\n",
       "      <td>0</td>\n",
       "      <td>15.00</td>\n",
       "      <td>AUTOMOTIVE NUOVO</td>\n",
       "      <td>2</td>\n",
       "      <td>0.29</td>\n",
       "      <td>108.17</td>\n",
       "      <td>426.00</td>\n",
       "      <td>4,120.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148459</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Umbria</td>\n",
       "      <td>1</td>\n",
       "      <td>15.00</td>\n",
       "      <td>PRESTITO PERSONALE</td>\n",
       "      <td>2</td>\n",
       "      <td>0.32</td>\n",
       "      <td>116.85</td>\n",
       "      <td>699.00</td>\n",
       "      <td>6,250.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148460</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Lombardia</td>\n",
       "      <td>0</td>\n",
       "      <td>15.00</td>\n",
       "      <td>PRESTITI CON TRATTENUTA</td>\n",
       "      <td>2</td>\n",
       "      <td>0.45</td>\n",
       "      <td>305.41</td>\n",
       "      <td>821.00</td>\n",
       "      <td>14,492.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148461</th>\n",
       "      <td>61.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Lombardia</td>\n",
       "      <td>0</td>\n",
       "      <td>15.00</td>\n",
       "      <td>SPESE MEDICHE</td>\n",
       "      <td>2</td>\n",
       "      <td>0.50</td>\n",
       "      <td>95.76</td>\n",
       "      <td>1,125.00</td>\n",
       "      <td>6,654.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148462</th>\n",
       "      <td>53.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Lazio</td>\n",
       "      <td>0</td>\n",
       "      <td>15.00</td>\n",
       "      <td>AUTOMOTIVE USATO</td>\n",
       "      <td>2</td>\n",
       "      <td>0.18</td>\n",
       "      <td>143.48</td>\n",
       "      <td>334.00</td>\n",
       "      <td>7,009.40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>148463 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Eta_Debitore  RATIO_ONERI_AFF_TOT_AFF Denominazione Regione  \\\n",
       "0                NaN                     0.04                 Lazio   \n",
       "1              59.00                     0.06               Liguria   \n",
       "2              57.00                     0.22              Calabria   \n",
       "3              76.00                     0.04               Sicilia   \n",
       "4              57.00                     0.16                 Lazio   \n",
       "...              ...                      ...                   ...   \n",
       "148458           NaN                     0.00                 Lazio   \n",
       "148459           NaN                     0.00                Umbria   \n",
       "148460           NaN                     0.00             Lombardia   \n",
       "148461         61.00                     0.00             Lombardia   \n",
       "148462         53.00                     0.00                 Lazio   \n",
       "\n",
       "        Target_95  AgeingGestioneGg       DESCRIZIONE PRODOTTO  NUMERO RATE  \\\n",
       "0               0             21.00         PRESTITO PERSONALE            2   \n",
       "1               0             29.00      FINANZIAMENTI AZIENDE            2   \n",
       "2               0             29.00  ELETTRONICA & ELETTRODOME            2   \n",
       "3               0             29.00      FINANZIAMENTI AZIENDE            2   \n",
       "4               0             28.00              SPESE MEDICHE            2   \n",
       "...           ...               ...                        ...          ...   \n",
       "148458          0             15.00           AUTOMOTIVE NUOVO            2   \n",
       "148459          1             15.00         PRESTITO PERSONALE            2   \n",
       "148460          0             15.00    PRESTITI CON TRATTENUTA            2   \n",
       "148461          0             15.00              SPESE MEDICHE            2   \n",
       "148462          0             15.00           AUTOMOTIVE USATO            2   \n",
       "\n",
       "        Ratio_Rate_Imp1  TOT_AFF  AgeingErogazioneGg  IMP_FINANZIATO  \n",
       "0                  0.44   277.24            1,126.00       15,200.00  \n",
       "1                  0.52   197.06            1,887.00       14,234.40  \n",
       "2                  0.17    73.05              152.00        1,500.00  \n",
       "3                  0.31   288.20              669.00       14,280.00  \n",
       "4                  0.12   199.57              180.00        6,800.00  \n",
       "...                 ...      ...                 ...             ...  \n",
       "148458             0.29   108.17              426.00        4,120.88  \n",
       "148459             0.32   116.85              699.00        6,250.00  \n",
       "148460             0.45   305.41              821.00       14,492.80  \n",
       "148461             0.50    95.76            1,125.00        6,654.00  \n",
       "148462             0.18   143.48              334.00        7,009.40  \n",
       "\n",
       "[148463 rows x 11 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clienti_senza_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mi assicuro che la colonna \"Target_95\" sia numerica perchè altrimenti il codice dopo mi dà errore\n",
    "df_clienti_senza_outliers['Target_95'] = pd.to_numeric(df_clienti_senza_outliers['Target_95'], errors = 'coerce') # Quando viene utilizzato il valore 'coerce', la funzione tenterà di convertire i dati in numeri, ma se la conversione non è possibile imposterà il valore sulla rappresentazione numerica NaN (Not-a-Number). Questo significa che se ci sono dati non numerici, la funzione li \"ignorerà\" assegnando loro NaN anziché generare un errore; l'altro valore possibile per \"errors\" è \"raise\", che dà errore nel caso non riesca a convertire qualcosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spostiamo il Target_95 sulla prima colonna\n",
    "move_column_inplace(df_clienti_senza_outliers,'Target_95',0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colonne con 'exception': ['IMP_MAXIRATA', 'P_INST1_AMOUNT_PAID', 'P_INST2_AMOUNT_PAID', 'P_INST3_AMOUNT_PAID', 'RATE_TOTALI', 'NUM_RATE_RIFI', 'NUM_RAT_IMP1', 'NUM_RAT_IMP2', 'P_INST1_NUM', 'P_INST2_NUM', 'P_INST3_NUM', 'P_INST1_AMOUNT', 'P_INST2_AMOUNT', 'P_INST3_AMOUNT', 'IMP_RAT_IMP1', 'IMP_RAT_IMP2', 'IMP_PAG_RAT_IMP1', 'IMP_PAG_RAT_IMP2', 'TOT_MORA_AFF', 'TOT REC', 'Ratio_Rate_Imp1', 'Ratio_Rate_Imp2', 'Ageing_Erogazione_mesi', 'DistanzaAffidoUltimoPagamento', 'Eta_Debitore', 'DurataFinanziamento', 'Numero_mesi_rec', 'Severity_12M', 'Severity_12M_pesata']\n"
     ]
    }
   ],
   "source": [
    "df_eccezioni = pd.read_excel('Eccezioni outliers.xlsx')\n",
    "\n",
    "# Cerco la riga che contiene \"eccezioni\" nella prima cella\n",
    "riga_eccezioni = None\n",
    "for index, row in df_eccezioni.iterrows():\n",
    "    if row.iloc[0] == 'eccezioni':\n",
    "        riga_eccezioni = row\n",
    "        break\n",
    "\n",
    "# Creo una lista vuota per memorizzare i nomi delle colonne con \"exception\"\n",
    "colonne_eccezione = []\n",
    "\n",
    "# Itero sulle colonne del DataFrame\n",
    "for colonna in df_eccezioni.columns:\n",
    "    # Verifica se il valore nella cella corrispondente alla riga \"eccezioni\" è \"exception\"\n",
    "    if riga_eccezioni[colonna] == 'exception':\n",
    "        colonne_eccezione.append(colonna)\n",
    "\n",
    "print(\"Colonne con 'exception':\", colonne_eccezione)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rimuovi_outliers_oltre_x_deviazioni_standard_dalla_media(df, x, eccezioni=[]):\n",
    "    # Copia il DataFrame originale\n",
    "    df_copia = df.copy()\n",
    "\n",
    "    # Escludi le colonne presenti in eccezioni\n",
    "    colonne_da_escludere = [col for col in df_copia.columns if col in eccezioni]\n",
    "    df_copia = df_copia.drop(columns=colonne_da_escludere)\n",
    "\n",
    "    # Seleziona solo le colonne numeriche (escludendo 'Target_95' e le colonne in eccezioni)\n",
    "    colonne_numeriche = df_copia.select_dtypes(include=[np.number])\n",
    "\n",
    "    # Calcola lo z-score per ciascuna colonna nel DataFrame\n",
    "    z_scores = zscore(colonne_numeriche)\n",
    "    print('z_scores:')\n",
    "    print(z_scores)\n",
    "    \n",
    "    outlier_indices = (np.abs(z_scores) > x).any(axis=1)\n",
    "    print('outlier_indices:')\n",
    "    print(outlier_indices)\n",
    "\n",
    "    df_senza_outliers = df[~outlier_indices] # ho scoperto che la tilde \"~\" serve a negare la maschera booleana in questione, quindi in df_senza_outliers finiscono tutte le righe che NON fanno parte di outlier_indices\n",
    "    return df_senza_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z_scores:\n",
      "        Target_95  RATIO_ONERI_AFF_TOT_AFF  AgeingGestioneGg  NUMERO RATE  \\\n",
      "0           -0.80                      NaN              0.23         0.80   \n",
      "1           -0.80                      NaN              0.25         0.80   \n",
      "2           -0.80                      NaN              0.25         0.80   \n",
      "3           -0.80                      NaN              0.25         0.80   \n",
      "4           -0.80                      NaN              0.25         0.80   \n",
      "...           ...                      ...               ...          ...   \n",
      "148458      -0.80                      NaN              0.21         0.80   \n",
      "148459       1.25                      NaN              0.21         0.80   \n",
      "148460      -0.80                      NaN              0.21         0.80   \n",
      "148461      -0.80                      NaN              0.21         0.80   \n",
      "148462      -0.80                      NaN              0.21         0.80   \n",
      "\n",
      "        TOT_AFF  AgeingErogazioneGg  IMP_FINANZIATO  \n",
      "0          0.22               -0.02            0.46  \n",
      "1         -0.22                0.87            0.35  \n",
      "2         -0.89               -1.16           -1.12  \n",
      "3          0.28               -0.56            0.35  \n",
      "4         -0.20               -1.13           -0.51  \n",
      "...         ...                 ...             ...  \n",
      "148458    -0.70               -0.84           -0.82  \n",
      "148459    -0.65               -0.52           -0.57  \n",
      "148460     0.37               -0.38            0.38  \n",
      "148461    -0.77               -0.03           -0.53  \n",
      "148462    -0.51               -0.95           -0.49  \n",
      "\n",
      "[148463 rows x 7 columns]\n",
      "outlier_indices:\n",
      "0         False\n",
      "1         False\n",
      "2         False\n",
      "3         False\n",
      "4         False\n",
      "          ...  \n",
      "148458    False\n",
      "148459    False\n",
      "148460    False\n",
      "148461    False\n",
      "148462    False\n",
      "Length: 148463, dtype: bool\n"
     ]
    }
   ],
   "source": [
    "# rimuovo gli outliers (solo per le colonne numeriche ovviamente)\n",
    "df_clienti_senza_outliers = rimuovi_outliers_oltre_x_deviazioni_standard_dalla_media(df_clienti_senza_outliers, 3, colonne_eccezione)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(139010, 11)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clienti_senza_outliers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target_95</th>\n",
       "      <th>Eta_Debitore</th>\n",
       "      <th>RATIO_ONERI_AFF_TOT_AFF</th>\n",
       "      <th>Denominazione Regione</th>\n",
       "      <th>AgeingGestioneGg</th>\n",
       "      <th>DESCRIZIONE PRODOTTO</th>\n",
       "      <th>NUMERO RATE</th>\n",
       "      <th>Ratio_Rate_Imp1</th>\n",
       "      <th>TOT_AFF</th>\n",
       "      <th>AgeingErogazioneGg</th>\n",
       "      <th>IMP_FINANZIATO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.04</td>\n",
       "      <td>Lazio</td>\n",
       "      <td>21.00</td>\n",
       "      <td>PRESTITO PERSONALE</td>\n",
       "      <td>2</td>\n",
       "      <td>0.44</td>\n",
       "      <td>277.24</td>\n",
       "      <td>1,126.00</td>\n",
       "      <td>15,200.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>59.00</td>\n",
       "      <td>0.06</td>\n",
       "      <td>Liguria</td>\n",
       "      <td>29.00</td>\n",
       "      <td>FINANZIAMENTI AZIENDE</td>\n",
       "      <td>2</td>\n",
       "      <td>0.52</td>\n",
       "      <td>197.06</td>\n",
       "      <td>1,887.00</td>\n",
       "      <td>14,234.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>57.00</td>\n",
       "      <td>0.22</td>\n",
       "      <td>Calabria</td>\n",
       "      <td>29.00</td>\n",
       "      <td>ELETTRONICA &amp; ELETTRODOME</td>\n",
       "      <td>2</td>\n",
       "      <td>0.17</td>\n",
       "      <td>73.05</td>\n",
       "      <td>152.00</td>\n",
       "      <td>1,500.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>76.00</td>\n",
       "      <td>0.04</td>\n",
       "      <td>Sicilia</td>\n",
       "      <td>29.00</td>\n",
       "      <td>FINANZIAMENTI AZIENDE</td>\n",
       "      <td>2</td>\n",
       "      <td>0.31</td>\n",
       "      <td>288.20</td>\n",
       "      <td>669.00</td>\n",
       "      <td>14,280.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>57.00</td>\n",
       "      <td>0.16</td>\n",
       "      <td>Lazio</td>\n",
       "      <td>28.00</td>\n",
       "      <td>SPESE MEDICHE</td>\n",
       "      <td>2</td>\n",
       "      <td>0.12</td>\n",
       "      <td>199.57</td>\n",
       "      <td>180.00</td>\n",
       "      <td>6,800.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148458</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Lazio</td>\n",
       "      <td>15.00</td>\n",
       "      <td>AUTOMOTIVE NUOVO</td>\n",
       "      <td>2</td>\n",
       "      <td>0.29</td>\n",
       "      <td>108.17</td>\n",
       "      <td>426.00</td>\n",
       "      <td>4,120.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148459</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Umbria</td>\n",
       "      <td>15.00</td>\n",
       "      <td>PRESTITO PERSONALE</td>\n",
       "      <td>2</td>\n",
       "      <td>0.32</td>\n",
       "      <td>116.85</td>\n",
       "      <td>699.00</td>\n",
       "      <td>6,250.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148460</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Lombardia</td>\n",
       "      <td>15.00</td>\n",
       "      <td>PRESTITI CON TRATTENUTA</td>\n",
       "      <td>2</td>\n",
       "      <td>0.45</td>\n",
       "      <td>305.41</td>\n",
       "      <td>821.00</td>\n",
       "      <td>14,492.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148461</th>\n",
       "      <td>0</td>\n",
       "      <td>61.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Lombardia</td>\n",
       "      <td>15.00</td>\n",
       "      <td>SPESE MEDICHE</td>\n",
       "      <td>2</td>\n",
       "      <td>0.50</td>\n",
       "      <td>95.76</td>\n",
       "      <td>1,125.00</td>\n",
       "      <td>6,654.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148462</th>\n",
       "      <td>0</td>\n",
       "      <td>53.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Lazio</td>\n",
       "      <td>15.00</td>\n",
       "      <td>AUTOMOTIVE USATO</td>\n",
       "      <td>2</td>\n",
       "      <td>0.18</td>\n",
       "      <td>143.48</td>\n",
       "      <td>334.00</td>\n",
       "      <td>7,009.40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>139010 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Target_95  Eta_Debitore  RATIO_ONERI_AFF_TOT_AFF  \\\n",
       "0               0           NaN                     0.04   \n",
       "1               0         59.00                     0.06   \n",
       "2               0         57.00                     0.22   \n",
       "3               0         76.00                     0.04   \n",
       "4               0         57.00                     0.16   \n",
       "...           ...           ...                      ...   \n",
       "148458          0           NaN                     0.00   \n",
       "148459          1           NaN                     0.00   \n",
       "148460          0           NaN                     0.00   \n",
       "148461          0         61.00                     0.00   \n",
       "148462          0         53.00                     0.00   \n",
       "\n",
       "       Denominazione Regione  AgeingGestioneGg       DESCRIZIONE PRODOTTO  \\\n",
       "0                      Lazio             21.00         PRESTITO PERSONALE   \n",
       "1                    Liguria             29.00      FINANZIAMENTI AZIENDE   \n",
       "2                   Calabria             29.00  ELETTRONICA & ELETTRODOME   \n",
       "3                    Sicilia             29.00      FINANZIAMENTI AZIENDE   \n",
       "4                      Lazio             28.00              SPESE MEDICHE   \n",
       "...                      ...               ...                        ...   \n",
       "148458                 Lazio             15.00           AUTOMOTIVE NUOVO   \n",
       "148459                Umbria             15.00         PRESTITO PERSONALE   \n",
       "148460             Lombardia             15.00    PRESTITI CON TRATTENUTA   \n",
       "148461             Lombardia             15.00              SPESE MEDICHE   \n",
       "148462                 Lazio             15.00           AUTOMOTIVE USATO   \n",
       "\n",
       "        NUMERO RATE  Ratio_Rate_Imp1  TOT_AFF  AgeingErogazioneGg  \\\n",
       "0                 2             0.44   277.24            1,126.00   \n",
       "1                 2             0.52   197.06            1,887.00   \n",
       "2                 2             0.17    73.05              152.00   \n",
       "3                 2             0.31   288.20              669.00   \n",
       "4                 2             0.12   199.57              180.00   \n",
       "...             ...              ...      ...                 ...   \n",
       "148458            2             0.29   108.17              426.00   \n",
       "148459            2             0.32   116.85              699.00   \n",
       "148460            2             0.45   305.41              821.00   \n",
       "148461            2             0.50    95.76            1,125.00   \n",
       "148462            2             0.18   143.48              334.00   \n",
       "\n",
       "        IMP_FINANZIATO  \n",
       "0            15,200.00  \n",
       "1            14,234.40  \n",
       "2             1,500.00  \n",
       "3            14,280.00  \n",
       "4             6,800.00  \n",
       "...                ...  \n",
       "148458        4,120.88  \n",
       "148459        6,250.00  \n",
       "148460       14,492.80  \n",
       "148461        6,654.00  \n",
       "148462        7,009.40  \n",
       "\n",
       "[139010 rows x 11 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clienti_senza_outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> Mi salvo in un file le righe contenenti gli outliers che ho rimosso </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_rows = df_clienti_con_meno_colonne[~df_clienti_con_meno_colonne.index.isin(df_clienti_senza_outliers.index)]\n",
    "\n",
    "# removed_rows.to_excel('df_con_righe_rimosse_custom_più_colonne.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> Rimuovo di nuovo le colonne con 1 solo valore unico perchè dopo la rimozione degli outliers alcune potrebbero avere di nuovo 1 solo valore unico </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([], dtype='object')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check colonne con valori univoci\n",
    "df_unique = df_clienti_senza_outliers.loc[:,df_clienti_senza_outliers.nunique()<2]\n",
    "df_unique.columns\n",
    "# Queste colonne sono da eliminare dal dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elimino le colonne che hanno 1 solo valore unico\n",
    "\n",
    "df_clienti_con_meno_colonne_2 = df_clienti_senza_outliers.drop(columns= df_unique.columns, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(139010, 11)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clienti_con_meno_colonne_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target_95</th>\n",
       "      <th>Eta_Debitore</th>\n",
       "      <th>RATIO_ONERI_AFF_TOT_AFF</th>\n",
       "      <th>Denominazione Regione</th>\n",
       "      <th>AgeingGestioneGg</th>\n",
       "      <th>DESCRIZIONE PRODOTTO</th>\n",
       "      <th>NUMERO RATE</th>\n",
       "      <th>Ratio_Rate_Imp1</th>\n",
       "      <th>TOT_AFF</th>\n",
       "      <th>AgeingErogazioneGg</th>\n",
       "      <th>IMP_FINANZIATO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.04</td>\n",
       "      <td>Lazio</td>\n",
       "      <td>21.00</td>\n",
       "      <td>PRESTITO PERSONALE</td>\n",
       "      <td>2</td>\n",
       "      <td>0.44</td>\n",
       "      <td>277.24</td>\n",
       "      <td>1,126.00</td>\n",
       "      <td>15,200.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>59.00</td>\n",
       "      <td>0.06</td>\n",
       "      <td>Liguria</td>\n",
       "      <td>29.00</td>\n",
       "      <td>FINANZIAMENTI AZIENDE</td>\n",
       "      <td>2</td>\n",
       "      <td>0.52</td>\n",
       "      <td>197.06</td>\n",
       "      <td>1,887.00</td>\n",
       "      <td>14,234.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>57.00</td>\n",
       "      <td>0.22</td>\n",
       "      <td>Calabria</td>\n",
       "      <td>29.00</td>\n",
       "      <td>ELETTRONICA &amp; ELETTRODOME</td>\n",
       "      <td>2</td>\n",
       "      <td>0.17</td>\n",
       "      <td>73.05</td>\n",
       "      <td>152.00</td>\n",
       "      <td>1,500.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>76.00</td>\n",
       "      <td>0.04</td>\n",
       "      <td>Sicilia</td>\n",
       "      <td>29.00</td>\n",
       "      <td>FINANZIAMENTI AZIENDE</td>\n",
       "      <td>2</td>\n",
       "      <td>0.31</td>\n",
       "      <td>288.20</td>\n",
       "      <td>669.00</td>\n",
       "      <td>14,280.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>57.00</td>\n",
       "      <td>0.16</td>\n",
       "      <td>Lazio</td>\n",
       "      <td>28.00</td>\n",
       "      <td>SPESE MEDICHE</td>\n",
       "      <td>2</td>\n",
       "      <td>0.12</td>\n",
       "      <td>199.57</td>\n",
       "      <td>180.00</td>\n",
       "      <td>6,800.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148458</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Lazio</td>\n",
       "      <td>15.00</td>\n",
       "      <td>AUTOMOTIVE NUOVO</td>\n",
       "      <td>2</td>\n",
       "      <td>0.29</td>\n",
       "      <td>108.17</td>\n",
       "      <td>426.00</td>\n",
       "      <td>4,120.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148459</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Umbria</td>\n",
       "      <td>15.00</td>\n",
       "      <td>PRESTITO PERSONALE</td>\n",
       "      <td>2</td>\n",
       "      <td>0.32</td>\n",
       "      <td>116.85</td>\n",
       "      <td>699.00</td>\n",
       "      <td>6,250.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148460</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Lombardia</td>\n",
       "      <td>15.00</td>\n",
       "      <td>PRESTITI CON TRATTENUTA</td>\n",
       "      <td>2</td>\n",
       "      <td>0.45</td>\n",
       "      <td>305.41</td>\n",
       "      <td>821.00</td>\n",
       "      <td>14,492.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148461</th>\n",
       "      <td>0</td>\n",
       "      <td>61.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Lombardia</td>\n",
       "      <td>15.00</td>\n",
       "      <td>SPESE MEDICHE</td>\n",
       "      <td>2</td>\n",
       "      <td>0.50</td>\n",
       "      <td>95.76</td>\n",
       "      <td>1,125.00</td>\n",
       "      <td>6,654.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148462</th>\n",
       "      <td>0</td>\n",
       "      <td>53.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Lazio</td>\n",
       "      <td>15.00</td>\n",
       "      <td>AUTOMOTIVE USATO</td>\n",
       "      <td>2</td>\n",
       "      <td>0.18</td>\n",
       "      <td>143.48</td>\n",
       "      <td>334.00</td>\n",
       "      <td>7,009.40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>139010 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Target_95  Eta_Debitore  RATIO_ONERI_AFF_TOT_AFF  \\\n",
       "0               0           NaN                     0.04   \n",
       "1               0         59.00                     0.06   \n",
       "2               0         57.00                     0.22   \n",
       "3               0         76.00                     0.04   \n",
       "4               0         57.00                     0.16   \n",
       "...           ...           ...                      ...   \n",
       "148458          0           NaN                     0.00   \n",
       "148459          1           NaN                     0.00   \n",
       "148460          0           NaN                     0.00   \n",
       "148461          0         61.00                     0.00   \n",
       "148462          0         53.00                     0.00   \n",
       "\n",
       "       Denominazione Regione  AgeingGestioneGg       DESCRIZIONE PRODOTTO  \\\n",
       "0                      Lazio             21.00         PRESTITO PERSONALE   \n",
       "1                    Liguria             29.00      FINANZIAMENTI AZIENDE   \n",
       "2                   Calabria             29.00  ELETTRONICA & ELETTRODOME   \n",
       "3                    Sicilia             29.00      FINANZIAMENTI AZIENDE   \n",
       "4                      Lazio             28.00              SPESE MEDICHE   \n",
       "...                      ...               ...                        ...   \n",
       "148458                 Lazio             15.00           AUTOMOTIVE NUOVO   \n",
       "148459                Umbria             15.00         PRESTITO PERSONALE   \n",
       "148460             Lombardia             15.00    PRESTITI CON TRATTENUTA   \n",
       "148461             Lombardia             15.00              SPESE MEDICHE   \n",
       "148462                 Lazio             15.00           AUTOMOTIVE USATO   \n",
       "\n",
       "        NUMERO RATE  Ratio_Rate_Imp1  TOT_AFF  AgeingErogazioneGg  \\\n",
       "0                 2             0.44   277.24            1,126.00   \n",
       "1                 2             0.52   197.06            1,887.00   \n",
       "2                 2             0.17    73.05              152.00   \n",
       "3                 2             0.31   288.20              669.00   \n",
       "4                 2             0.12   199.57              180.00   \n",
       "...             ...              ...      ...                 ...   \n",
       "148458            2             0.29   108.17              426.00   \n",
       "148459            2             0.32   116.85              699.00   \n",
       "148460            2             0.45   305.41              821.00   \n",
       "148461            2             0.50    95.76            1,125.00   \n",
       "148462            2             0.18   143.48              334.00   \n",
       "\n",
       "        IMP_FINANZIATO  \n",
       "0            15,200.00  \n",
       "1            14,234.40  \n",
       "2             1,500.00  \n",
       "3            14,280.00  \n",
       "4             6,800.00  \n",
       "...                ...  \n",
       "148458        4,120.88  \n",
       "148459        6,250.00  \n",
       "148460       14,492.80  \n",
       "148461        6,654.00  \n",
       "148462        7,009.40  \n",
       "\n",
       "[139010 rows x 11 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clienti_con_meno_colonne_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> Riempio eventuali colonne con valori missing </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 139010 entries, 0 to 148462\n",
      "Data columns (total 11 columns):\n",
      " #   Column                   Non-Null Count   Dtype  \n",
      "---  ------                   --------------   -----  \n",
      " 0   Target_95                139010 non-null  int32  \n",
      " 1   Eta_Debitore             107139 non-null  float64\n",
      " 2   RATIO_ONERI_AFF_TOT_AFF  138884 non-null  float64\n",
      " 3   Denominazione Regione    139009 non-null  object \n",
      " 4   AgeingGestioneGg         139010 non-null  float64\n",
      " 5   DESCRIZIONE PRODOTTO     139010 non-null  object \n",
      " 6   NUMERO RATE              139010 non-null  int32  \n",
      " 7   Ratio_Rate_Imp1          139010 non-null  float64\n",
      " 8   TOT_AFF                  139010 non-null  float64\n",
      " 9   AgeingErogazioneGg       139010 non-null  float64\n",
      " 10  IMP_FINANZIATO           139010 non-null  float64\n",
      "dtypes: float64(7), int32(2), object(2)\n",
      "memory usage: 11.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df_clienti_con_meno_colonne_2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Target_95                      0\n",
       "Eta_Debitore               31871\n",
       "RATIO_ONERI_AFF_TOT_AFF      126\n",
       "Denominazione Regione          1\n",
       "AgeingGestioneGg               0\n",
       "DESCRIZIONE PRODOTTO           0\n",
       "NUMERO RATE                    0\n",
       "Ratio_Rate_Imp1                0\n",
       "TOT_AFF                        0\n",
       "AgeingErogazioneGg             0\n",
       "IMP_FINANZIATO                 0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# conto quanti nan ci sono in ogni colonna\n",
    "df_clienti_con_meno_colonne_2.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31998"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# conto quanti nan ci sono in totale\n",
    "df_clienti_con_meno_colonne_2.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def riempi_nan_con_media_e_missing(df):\n",
    "    # Crea una copia del DataFrame originale perchè altrimenti mi modifica anche il dataframe originale mi pare\n",
    "    df_copia = df.copy()\n",
    "    \n",
    "    for colonna in df_copia.columns:\n",
    "        tipo_colonna = pd.api.types.infer_dtype(df_copia[colonna])\n",
    "        # print(f'Tipo colonna {colonna}: {tipo_colonna}')\n",
    "        if tipo_colonna in ['integer', 'floating']:\n",
    "            # Per le colonne numeriche (e non bool o complex), calcolo la mediana e riempio i NaN con la mediana\n",
    "            mediana_colonna = df_copia[colonna].median()\n",
    "            df_copia[colonna] = df_copia[colonna].fillna(mediana_colonna)\n",
    "        else:\n",
    "            # Per le colonne non numeriche (inclusi però bool e complex, che qui non ci sono), riempio i NaN con \"Missing\"\n",
    "            df_copia[colonna] = df_copia[colonna].fillna(\"Missing\")\n",
    "    return df_copia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_senza_valori_nan = riempi_nan_con_media_e_missing(df_clienti_con_meno_colonne_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 139010 entries, 0 to 148462\n",
      "Data columns (total 11 columns):\n",
      " #   Column                   Non-Null Count   Dtype  \n",
      "---  ------                   --------------   -----  \n",
      " 0   Target_95                139010 non-null  int32  \n",
      " 1   Eta_Debitore             139010 non-null  float64\n",
      " 2   RATIO_ONERI_AFF_TOT_AFF  139010 non-null  float64\n",
      " 3   Denominazione Regione    139010 non-null  object \n",
      " 4   AgeingGestioneGg         139010 non-null  float64\n",
      " 5   DESCRIZIONE PRODOTTO     139010 non-null  object \n",
      " 6   NUMERO RATE              139010 non-null  int32  \n",
      " 7   Ratio_Rate_Imp1          139010 non-null  float64\n",
      " 8   TOT_AFF                  139010 non-null  float64\n",
      " 9   AgeingErogazioneGg       139010 non-null  float64\n",
      " 10  IMP_FINANZIATO           139010 non-null  float64\n",
      "dtypes: float64(7), int32(2), object(2)\n",
      "memory usage: 11.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df_senza_valori_nan.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_senza_valori_nan.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target_95</th>\n",
       "      <th>Eta_Debitore</th>\n",
       "      <th>RATIO_ONERI_AFF_TOT_AFF</th>\n",
       "      <th>AgeingGestioneGg</th>\n",
       "      <th>NUMERO RATE</th>\n",
       "      <th>Ratio_Rate_Imp1</th>\n",
       "      <th>TOT_AFF</th>\n",
       "      <th>AgeingErogazioneGg</th>\n",
       "      <th>IMP_FINANZIATO</th>\n",
       "      <th>Denominazione Regione</th>\n",
       "      <th>DESCRIZIONE PRODOTTO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>139,010.00</td>\n",
       "      <td>139,010.00</td>\n",
       "      <td>139,010.00</td>\n",
       "      <td>139,010.00</td>\n",
       "      <td>139,010.00</td>\n",
       "      <td>139,010.00</td>\n",
       "      <td>139,010.00</td>\n",
       "      <td>139,010.00</td>\n",
       "      <td>139,010.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.39</td>\n",
       "      <td>50.89</td>\n",
       "      <td>0.09</td>\n",
       "      <td>-1.11</td>\n",
       "      <td>1.52</td>\n",
       "      <td>0.53</td>\n",
       "      <td>220.69</td>\n",
       "      <td>1,049.67</td>\n",
       "      <td>10,353.46</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.49</td>\n",
       "      <td>12.31</td>\n",
       "      <td>0.16</td>\n",
       "      <td>129.03</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.27</td>\n",
       "      <td>145.80</td>\n",
       "      <td>724.24</td>\n",
       "      <td>7,671.68</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.00</td>\n",
       "      <td>18.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-1,132.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>29.00</td>\n",
       "      <td>190.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1%</th>\n",
       "      <td>0.00</td>\n",
       "      <td>24.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-823.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>2.86</td>\n",
       "      <td>30.00</td>\n",
       "      <td>575.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.00</td>\n",
       "      <td>44.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>15.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>110.89</td>\n",
       "      <td>487.00</td>\n",
       "      <td>3,860.98</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.00</td>\n",
       "      <td>51.00</td>\n",
       "      <td>0.04</td>\n",
       "      <td>16.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.53</td>\n",
       "      <td>200.32</td>\n",
       "      <td>945.00</td>\n",
       "      <td>9,100.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.00</td>\n",
       "      <td>57.00</td>\n",
       "      <td>0.09</td>\n",
       "      <td>26.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.75</td>\n",
       "      <td>302.66</td>\n",
       "      <td>1,460.00</td>\n",
       "      <td>15,300.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90%</th>\n",
       "      <td>1.00</td>\n",
       "      <td>68.00</td>\n",
       "      <td>0.24</td>\n",
       "      <td>30.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.92</td>\n",
       "      <td>409.94</td>\n",
       "      <td>1,948.00</td>\n",
       "      <td>21,430.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95%</th>\n",
       "      <td>1.00</td>\n",
       "      <td>74.00</td>\n",
       "      <td>0.46</td>\n",
       "      <td>32.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.96</td>\n",
       "      <td>497.34</td>\n",
       "      <td>2,403.00</td>\n",
       "      <td>25,195.50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99%</th>\n",
       "      <td>1.00</td>\n",
       "      <td>80.00</td>\n",
       "      <td>0.78</td>\n",
       "      <td>56.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>680.60</td>\n",
       "      <td>3,467.00</td>\n",
       "      <td>32,566.32</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.00</td>\n",
       "      <td>93.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>765.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>791.25</td>\n",
       "      <td>3,712.00</td>\n",
       "      <td>37,127.60</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kurtosis</th>\n",
       "      <td>-1.79</td>\n",
       "      <td>0.14</td>\n",
       "      <td>10.14</td>\n",
       "      <td>40.52</td>\n",
       "      <td>-1.16</td>\n",
       "      <td>-1.07</td>\n",
       "      <td>0.97</td>\n",
       "      <td>1.24</td>\n",
       "      <td>0.13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>skewness</th>\n",
       "      <td>0.46</td>\n",
       "      <td>0.15</td>\n",
       "      <td>3.08</td>\n",
       "      <td>-6.15</td>\n",
       "      <td>0.29</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.94</td>\n",
       "      <td>1.02</td>\n",
       "      <td>0.83</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mediane</th>\n",
       "      <td>0.00</td>\n",
       "      <td>51.00</td>\n",
       "      <td>0.04</td>\n",
       "      <td>16.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.53</td>\n",
       "      <td>200.32</td>\n",
       "      <td>945.00</td>\n",
       "      <td>9,100.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>valori missing</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>percentuale missing</th>\n",
       "      <td>0.0%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>0.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>valori_unici</th>\n",
       "      <td>2</td>\n",
       "      <td>75</td>\n",
       "      <td>39986</td>\n",
       "      <td>865</td>\n",
       "      <td>3</td>\n",
       "      <td>1905</td>\n",
       "      <td>40169</td>\n",
       "      <td>2396</td>\n",
       "      <td>14056</td>\n",
       "      <td>21</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Target_95 Eta_Debitore RATIO_ONERI_AFF_TOT_AFF  \\\n",
       "count               139,010.00   139,010.00              139,010.00   \n",
       "mean                      0.39        50.89                    0.09   \n",
       "std                       0.49        12.31                    0.16   \n",
       "min                       0.00        18.00                    0.00   \n",
       "1%                        0.00        24.00                    0.00   \n",
       "25%                       0.00        44.00                    0.00   \n",
       "50%                       0.00        51.00                    0.04   \n",
       "75%                       1.00        57.00                    0.09   \n",
       "90%                       1.00        68.00                    0.24   \n",
       "95%                       1.00        74.00                    0.46   \n",
       "99%                       1.00        80.00                    0.78   \n",
       "max                       1.00        93.00                    1.00   \n",
       "kurtosis                 -1.79         0.14                   10.14   \n",
       "skewness                  0.46         0.15                    3.08   \n",
       "mediane                   0.00        51.00                    0.04   \n",
       "valori missing               0            0                       0   \n",
       "percentuale missing       0.0%         0.0%                    0.0%   \n",
       "valori_unici                 2           75                   39986   \n",
       "\n",
       "                    AgeingGestioneGg NUMERO RATE Ratio_Rate_Imp1    TOT_AFF  \\\n",
       "count                     139,010.00  139,010.00      139,010.00 139,010.00   \n",
       "mean                           -1.11        1.52            0.53     220.69   \n",
       "std                           129.03        0.54            0.27     145.80   \n",
       "min                        -1,132.00        1.00            0.01       0.00   \n",
       "1%                           -823.00        1.00            0.03       2.86   \n",
       "25%                            15.00        1.00            0.31     110.89   \n",
       "50%                            16.00        1.00            0.53     200.32   \n",
       "75%                            26.00        2.00            0.75     302.66   \n",
       "90%                            30.00        2.00            0.92     409.94   \n",
       "95%                            32.00        2.00            0.96     497.34   \n",
       "99%                            56.00        3.00            1.00     680.60   \n",
       "max                           765.00        3.00            1.00     791.25   \n",
       "kurtosis                       40.52       -1.16           -1.07       0.97   \n",
       "skewness                       -6.15        0.29           -0.05       0.94   \n",
       "mediane                        16.00        1.00            0.53     200.32   \n",
       "valori missing                     0           0               0          0   \n",
       "percentuale missing             0.0%        0.0%            0.0%       0.0%   \n",
       "valori_unici                     865           3            1905      40169   \n",
       "\n",
       "                    AgeingErogazioneGg IMP_FINANZIATO Denominazione Regione  \\\n",
       "count                       139,010.00     139,010.00                   NaN   \n",
       "mean                          1,049.67      10,353.46                   NaN   \n",
       "std                             724.24       7,671.68                   NaN   \n",
       "min                              29.00         190.00                   NaN   \n",
       "1%                               30.00         575.00                   NaN   \n",
       "25%                             487.00       3,860.98                   NaN   \n",
       "50%                             945.00       9,100.00                   NaN   \n",
       "75%                           1,460.00      15,300.00                   NaN   \n",
       "90%                           1,948.00      21,430.00                   NaN   \n",
       "95%                           2,403.00      25,195.50                   NaN   \n",
       "99%                           3,467.00      32,566.32                   NaN   \n",
       "max                           3,712.00      37,127.60                   NaN   \n",
       "kurtosis                          1.24           0.13                   NaN   \n",
       "skewness                          1.02           0.83                   NaN   \n",
       "mediane                         945.00       9,100.00                   NaN   \n",
       "valori missing                       0              0                     0   \n",
       "percentuale missing               0.0%           0.0%                  0.0%   \n",
       "valori_unici                      2396          14056                    21   \n",
       "\n",
       "                    DESCRIZIONE PRODOTTO  \n",
       "count                                NaN  \n",
       "mean                                 NaN  \n",
       "std                                  NaN  \n",
       "min                                  NaN  \n",
       "1%                                   NaN  \n",
       "25%                                  NaN  \n",
       "50%                                  NaN  \n",
       "75%                                  NaN  \n",
       "90%                                  NaN  \n",
       "95%                                  NaN  \n",
       "99%                                  NaN  \n",
       "max                                  NaN  \n",
       "kurtosis                             NaN  \n",
       "skewness                             NaN  \n",
       "mediane                              NaN  \n",
       "valori missing                         0  \n",
       "percentuale missing                 0.0%  \n",
       "valori_unici                          14  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "describex(df_senza_valori_nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target_95</th>\n",
       "      <th>Eta_Debitore</th>\n",
       "      <th>RATIO_ONERI_AFF_TOT_AFF</th>\n",
       "      <th>Denominazione Regione</th>\n",
       "      <th>AgeingGestioneGg</th>\n",
       "      <th>DESCRIZIONE PRODOTTO</th>\n",
       "      <th>NUMERO RATE</th>\n",
       "      <th>Ratio_Rate_Imp1</th>\n",
       "      <th>TOT_AFF</th>\n",
       "      <th>AgeingErogazioneGg</th>\n",
       "      <th>IMP_FINANZIATO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>51.00</td>\n",
       "      <td>0.04</td>\n",
       "      <td>Lazio</td>\n",
       "      <td>21.00</td>\n",
       "      <td>PRESTITO PERSONALE</td>\n",
       "      <td>2</td>\n",
       "      <td>0.44</td>\n",
       "      <td>277.24</td>\n",
       "      <td>1,126.00</td>\n",
       "      <td>15,200.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>59.00</td>\n",
       "      <td>0.06</td>\n",
       "      <td>Liguria</td>\n",
       "      <td>29.00</td>\n",
       "      <td>FINANZIAMENTI AZIENDE</td>\n",
       "      <td>2</td>\n",
       "      <td>0.52</td>\n",
       "      <td>197.06</td>\n",
       "      <td>1,887.00</td>\n",
       "      <td>14,234.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>57.00</td>\n",
       "      <td>0.22</td>\n",
       "      <td>Calabria</td>\n",
       "      <td>29.00</td>\n",
       "      <td>ELETTRONICA &amp; ELETTRODOME</td>\n",
       "      <td>2</td>\n",
       "      <td>0.17</td>\n",
       "      <td>73.05</td>\n",
       "      <td>152.00</td>\n",
       "      <td>1,500.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>76.00</td>\n",
       "      <td>0.04</td>\n",
       "      <td>Sicilia</td>\n",
       "      <td>29.00</td>\n",
       "      <td>FINANZIAMENTI AZIENDE</td>\n",
       "      <td>2</td>\n",
       "      <td>0.31</td>\n",
       "      <td>288.20</td>\n",
       "      <td>669.00</td>\n",
       "      <td>14,280.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>57.00</td>\n",
       "      <td>0.16</td>\n",
       "      <td>Lazio</td>\n",
       "      <td>28.00</td>\n",
       "      <td>SPESE MEDICHE</td>\n",
       "      <td>2</td>\n",
       "      <td>0.12</td>\n",
       "      <td>199.57</td>\n",
       "      <td>180.00</td>\n",
       "      <td>6,800.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148458</th>\n",
       "      <td>0</td>\n",
       "      <td>51.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Lazio</td>\n",
       "      <td>15.00</td>\n",
       "      <td>AUTOMOTIVE NUOVO</td>\n",
       "      <td>2</td>\n",
       "      <td>0.29</td>\n",
       "      <td>108.17</td>\n",
       "      <td>426.00</td>\n",
       "      <td>4,120.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148459</th>\n",
       "      <td>1</td>\n",
       "      <td>51.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Umbria</td>\n",
       "      <td>15.00</td>\n",
       "      <td>PRESTITO PERSONALE</td>\n",
       "      <td>2</td>\n",
       "      <td>0.32</td>\n",
       "      <td>116.85</td>\n",
       "      <td>699.00</td>\n",
       "      <td>6,250.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148460</th>\n",
       "      <td>0</td>\n",
       "      <td>51.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Lombardia</td>\n",
       "      <td>15.00</td>\n",
       "      <td>PRESTITI CON TRATTENUTA</td>\n",
       "      <td>2</td>\n",
       "      <td>0.45</td>\n",
       "      <td>305.41</td>\n",
       "      <td>821.00</td>\n",
       "      <td>14,492.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148461</th>\n",
       "      <td>0</td>\n",
       "      <td>61.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Lombardia</td>\n",
       "      <td>15.00</td>\n",
       "      <td>SPESE MEDICHE</td>\n",
       "      <td>2</td>\n",
       "      <td>0.50</td>\n",
       "      <td>95.76</td>\n",
       "      <td>1,125.00</td>\n",
       "      <td>6,654.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148462</th>\n",
       "      <td>0</td>\n",
       "      <td>53.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Lazio</td>\n",
       "      <td>15.00</td>\n",
       "      <td>AUTOMOTIVE USATO</td>\n",
       "      <td>2</td>\n",
       "      <td>0.18</td>\n",
       "      <td>143.48</td>\n",
       "      <td>334.00</td>\n",
       "      <td>7,009.40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>139010 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Target_95  Eta_Debitore  RATIO_ONERI_AFF_TOT_AFF  \\\n",
       "0               0         51.00                     0.04   \n",
       "1               0         59.00                     0.06   \n",
       "2               0         57.00                     0.22   \n",
       "3               0         76.00                     0.04   \n",
       "4               0         57.00                     0.16   \n",
       "...           ...           ...                      ...   \n",
       "148458          0         51.00                     0.00   \n",
       "148459          1         51.00                     0.00   \n",
       "148460          0         51.00                     0.00   \n",
       "148461          0         61.00                     0.00   \n",
       "148462          0         53.00                     0.00   \n",
       "\n",
       "       Denominazione Regione  AgeingGestioneGg       DESCRIZIONE PRODOTTO  \\\n",
       "0                      Lazio             21.00         PRESTITO PERSONALE   \n",
       "1                    Liguria             29.00      FINANZIAMENTI AZIENDE   \n",
       "2                   Calabria             29.00  ELETTRONICA & ELETTRODOME   \n",
       "3                    Sicilia             29.00      FINANZIAMENTI AZIENDE   \n",
       "4                      Lazio             28.00              SPESE MEDICHE   \n",
       "...                      ...               ...                        ...   \n",
       "148458                 Lazio             15.00           AUTOMOTIVE NUOVO   \n",
       "148459                Umbria             15.00         PRESTITO PERSONALE   \n",
       "148460             Lombardia             15.00    PRESTITI CON TRATTENUTA   \n",
       "148461             Lombardia             15.00              SPESE MEDICHE   \n",
       "148462                 Lazio             15.00           AUTOMOTIVE USATO   \n",
       "\n",
       "        NUMERO RATE  Ratio_Rate_Imp1  TOT_AFF  AgeingErogazioneGg  \\\n",
       "0                 2             0.44   277.24            1,126.00   \n",
       "1                 2             0.52   197.06            1,887.00   \n",
       "2                 2             0.17    73.05              152.00   \n",
       "3                 2             0.31   288.20              669.00   \n",
       "4                 2             0.12   199.57              180.00   \n",
       "...             ...              ...      ...                 ...   \n",
       "148458            2             0.29   108.17              426.00   \n",
       "148459            2             0.32   116.85              699.00   \n",
       "148460            2             0.45   305.41              821.00   \n",
       "148461            2             0.50    95.76            1,125.00   \n",
       "148462            2             0.18   143.48              334.00   \n",
       "\n",
       "        IMP_FINANZIATO  \n",
       "0            15,200.00  \n",
       "1            14,234.40  \n",
       "2             1,500.00  \n",
       "3            14,280.00  \n",
       "4             6,800.00  \n",
       "...                ...  \n",
       "148458        4,120.88  \n",
       "148459        6,250.00  \n",
       "148460       14,492.80  \n",
       "148461        6,654.00  \n",
       "148462        7,009.40  \n",
       "\n",
       "[139010 rows x 11 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_senza_valori_nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Già fatta prima in 'Rimuovo le colonne non necessarie'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reti neurali (features in short_list_finale_breve usando df_senza_valori_nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valori unici in Denominazione Regione: ['Lazio' 'Liguria' 'Calabria' 'Sicilia' 'Emilia-Romagna' 'Lombardia'\n",
      " 'Puglia' 'Campania' 'Marche' 'Toscana' 'Friuli-Venezia Giulia' 'Piemonte'\n",
      " 'Umbria' 'Veneto' 'Molise' 'Abruzzo' 'Basilicata'\n",
      " \"Valle d'Aosta/Vallée d'Aoste\" 'Sardegna' 'Trentino-Alto Adige/Südtirol'\n",
      " 'Missing']\n",
      "Valori unici in DESCRIZIONE PRODOTTO: ['PRESTITO PERSONALE' 'FINANZIAMENTI AZIENDE' 'ELETTRONICA & ELETTRODOME'\n",
      " 'SPESE MEDICHE' 'ARREDAMENTO' 'AUTOMOTIVE NUOVO' 'INTERVENTI CASA'\n",
      " 'PRESTITI CON TRATTENUTA' 'AUTOMOTIVE USATO' 'ALTRI BENI E SERVIZI'\n",
      " 'MOTO E CICLOMOTORI USATO' 'TEMPO LIBERO' 'CONSOLIDAMENTO DEL DEBITO'\n",
      " 'MOTO E CICLOMOTORI NUOVO']\n"
     ]
    }
   ],
   "source": [
    "# guardo quali sono i valori unici di ogni variabile categorica del dataset per controllare che non ce ne siano troppi per la RAM\n",
    "categorical_columns = df_senza_valori_nan.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "for col in categorical_columns:\n",
    "    unique_values = df_senza_valori_nan[col].unique()\n",
    "    print(f\"Valori unici in {col}: {unique_values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6> Converto i dati categorici in numerici con la codifica one-hot </h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gli alberi decisionali e le random forest possono lavorare con i dati categorici in generale, ma non è gestita da sklearn questa cosa, quindi devo farlo io\n",
    "dummies = pd.get_dummies(df_senza_valori_nan, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target_95</th>\n",
       "      <th>Eta_Debitore</th>\n",
       "      <th>RATIO_ONERI_AFF_TOT_AFF</th>\n",
       "      <th>AgeingGestioneGg</th>\n",
       "      <th>NUMERO RATE</th>\n",
       "      <th>Ratio_Rate_Imp1</th>\n",
       "      <th>TOT_AFF</th>\n",
       "      <th>AgeingErogazioneGg</th>\n",
       "      <th>IMP_FINANZIATO</th>\n",
       "      <th>Denominazione Regione_Basilicata</th>\n",
       "      <th>Denominazione Regione_Calabria</th>\n",
       "      <th>Denominazione Regione_Campania</th>\n",
       "      <th>Denominazione Regione_Emilia-Romagna</th>\n",
       "      <th>Denominazione Regione_Friuli-Venezia Giulia</th>\n",
       "      <th>Denominazione Regione_Lazio</th>\n",
       "      <th>Denominazione Regione_Liguria</th>\n",
       "      <th>Denominazione Regione_Lombardia</th>\n",
       "      <th>Denominazione Regione_Marche</th>\n",
       "      <th>Denominazione Regione_Missing</th>\n",
       "      <th>Denominazione Regione_Molise</th>\n",
       "      <th>Denominazione Regione_Piemonte</th>\n",
       "      <th>Denominazione Regione_Puglia</th>\n",
       "      <th>Denominazione Regione_Sardegna</th>\n",
       "      <th>Denominazione Regione_Sicilia</th>\n",
       "      <th>Denominazione Regione_Toscana</th>\n",
       "      <th>Denominazione Regione_Trentino-Alto Adige/Südtirol</th>\n",
       "      <th>Denominazione Regione_Umbria</th>\n",
       "      <th>Denominazione Regione_Valle d'Aosta/Vallée d'Aoste</th>\n",
       "      <th>Denominazione Regione_Veneto</th>\n",
       "      <th>DESCRIZIONE PRODOTTO_ARREDAMENTO</th>\n",
       "      <th>DESCRIZIONE PRODOTTO_AUTOMOTIVE NUOVO</th>\n",
       "      <th>DESCRIZIONE PRODOTTO_AUTOMOTIVE USATO</th>\n",
       "      <th>DESCRIZIONE PRODOTTO_CONSOLIDAMENTO DEL DEBITO</th>\n",
       "      <th>DESCRIZIONE PRODOTTO_ELETTRONICA &amp; ELETTRODOME</th>\n",
       "      <th>DESCRIZIONE PRODOTTO_FINANZIAMENTI AZIENDE</th>\n",
       "      <th>DESCRIZIONE PRODOTTO_INTERVENTI CASA</th>\n",
       "      <th>DESCRIZIONE PRODOTTO_MOTO E CICLOMOTORI NUOVO</th>\n",
       "      <th>DESCRIZIONE PRODOTTO_MOTO E CICLOMOTORI USATO</th>\n",
       "      <th>DESCRIZIONE PRODOTTO_PRESTITI CON TRATTENUTA</th>\n",
       "      <th>DESCRIZIONE PRODOTTO_PRESTITO PERSONALE</th>\n",
       "      <th>DESCRIZIONE PRODOTTO_SPESE MEDICHE</th>\n",
       "      <th>DESCRIZIONE PRODOTTO_TEMPO LIBERO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>51.00</td>\n",
       "      <td>0.04</td>\n",
       "      <td>21.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0.44</td>\n",
       "      <td>277.24</td>\n",
       "      <td>1,126.00</td>\n",
       "      <td>15,200.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>59.00</td>\n",
       "      <td>0.06</td>\n",
       "      <td>29.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0.52</td>\n",
       "      <td>197.06</td>\n",
       "      <td>1,887.00</td>\n",
       "      <td>14,234.40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>57.00</td>\n",
       "      <td>0.22</td>\n",
       "      <td>29.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0.17</td>\n",
       "      <td>73.05</td>\n",
       "      <td>152.00</td>\n",
       "      <td>1,500.00</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>76.00</td>\n",
       "      <td>0.04</td>\n",
       "      <td>29.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0.31</td>\n",
       "      <td>288.20</td>\n",
       "      <td>669.00</td>\n",
       "      <td>14,280.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>57.00</td>\n",
       "      <td>0.16</td>\n",
       "      <td>28.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0.12</td>\n",
       "      <td>199.57</td>\n",
       "      <td>180.00</td>\n",
       "      <td>6,800.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148458</th>\n",
       "      <td>0</td>\n",
       "      <td>51.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>15.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0.29</td>\n",
       "      <td>108.17</td>\n",
       "      <td>426.00</td>\n",
       "      <td>4,120.88</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148459</th>\n",
       "      <td>1</td>\n",
       "      <td>51.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>15.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0.32</td>\n",
       "      <td>116.85</td>\n",
       "      <td>699.00</td>\n",
       "      <td>6,250.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148460</th>\n",
       "      <td>0</td>\n",
       "      <td>51.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>15.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0.45</td>\n",
       "      <td>305.41</td>\n",
       "      <td>821.00</td>\n",
       "      <td>14,492.80</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148461</th>\n",
       "      <td>0</td>\n",
       "      <td>61.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>15.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0.50</td>\n",
       "      <td>95.76</td>\n",
       "      <td>1,125.00</td>\n",
       "      <td>6,654.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148462</th>\n",
       "      <td>0</td>\n",
       "      <td>53.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>15.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0.18</td>\n",
       "      <td>143.48</td>\n",
       "      <td>334.00</td>\n",
       "      <td>7,009.40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>139010 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Target_95  Eta_Debitore  RATIO_ONERI_AFF_TOT_AFF  AgeingGestioneGg  \\\n",
       "0               0         51.00                     0.04             21.00   \n",
       "1               0         59.00                     0.06             29.00   \n",
       "2               0         57.00                     0.22             29.00   \n",
       "3               0         76.00                     0.04             29.00   \n",
       "4               0         57.00                     0.16             28.00   \n",
       "...           ...           ...                      ...               ...   \n",
       "148458          0         51.00                     0.00             15.00   \n",
       "148459          1         51.00                     0.00             15.00   \n",
       "148460          0         51.00                     0.00             15.00   \n",
       "148461          0         61.00                     0.00             15.00   \n",
       "148462          0         53.00                     0.00             15.00   \n",
       "\n",
       "        NUMERO RATE  Ratio_Rate_Imp1  TOT_AFF  AgeingErogazioneGg  \\\n",
       "0                 2             0.44   277.24            1,126.00   \n",
       "1                 2             0.52   197.06            1,887.00   \n",
       "2                 2             0.17    73.05              152.00   \n",
       "3                 2             0.31   288.20              669.00   \n",
       "4                 2             0.12   199.57              180.00   \n",
       "...             ...              ...      ...                 ...   \n",
       "148458            2             0.29   108.17              426.00   \n",
       "148459            2             0.32   116.85              699.00   \n",
       "148460            2             0.45   305.41              821.00   \n",
       "148461            2             0.50    95.76            1,125.00   \n",
       "148462            2             0.18   143.48              334.00   \n",
       "\n",
       "        IMP_FINANZIATO  Denominazione Regione_Basilicata  \\\n",
       "0            15,200.00                                 0   \n",
       "1            14,234.40                                 0   \n",
       "2             1,500.00                                 0   \n",
       "3            14,280.00                                 0   \n",
       "4             6,800.00                                 0   \n",
       "...                ...                               ...   \n",
       "148458        4,120.88                                 0   \n",
       "148459        6,250.00                                 0   \n",
       "148460       14,492.80                                 0   \n",
       "148461        6,654.00                                 0   \n",
       "148462        7,009.40                                 0   \n",
       "\n",
       "        Denominazione Regione_Calabria  Denominazione Regione_Campania  \\\n",
       "0                                    0                               0   \n",
       "1                                    0                               0   \n",
       "2                                    1                               0   \n",
       "3                                    0                               0   \n",
       "4                                    0                               0   \n",
       "...                                ...                             ...   \n",
       "148458                               0                               0   \n",
       "148459                               0                               0   \n",
       "148460                               0                               0   \n",
       "148461                               0                               0   \n",
       "148462                               0                               0   \n",
       "\n",
       "        Denominazione Regione_Emilia-Romagna  \\\n",
       "0                                          0   \n",
       "1                                          0   \n",
       "2                                          0   \n",
       "3                                          0   \n",
       "4                                          0   \n",
       "...                                      ...   \n",
       "148458                                     0   \n",
       "148459                                     0   \n",
       "148460                                     0   \n",
       "148461                                     0   \n",
       "148462                                     0   \n",
       "\n",
       "        Denominazione Regione_Friuli-Venezia Giulia  \\\n",
       "0                                                 0   \n",
       "1                                                 0   \n",
       "2                                                 0   \n",
       "3                                                 0   \n",
       "4                                                 0   \n",
       "...                                             ...   \n",
       "148458                                            0   \n",
       "148459                                            0   \n",
       "148460                                            0   \n",
       "148461                                            0   \n",
       "148462                                            0   \n",
       "\n",
       "        Denominazione Regione_Lazio  Denominazione Regione_Liguria  \\\n",
       "0                                 1                              0   \n",
       "1                                 0                              1   \n",
       "2                                 0                              0   \n",
       "3                                 0                              0   \n",
       "4                                 1                              0   \n",
       "...                             ...                            ...   \n",
       "148458                            1                              0   \n",
       "148459                            0                              0   \n",
       "148460                            0                              0   \n",
       "148461                            0                              0   \n",
       "148462                            1                              0   \n",
       "\n",
       "        Denominazione Regione_Lombardia  Denominazione Regione_Marche  \\\n",
       "0                                     0                             0   \n",
       "1                                     0                             0   \n",
       "2                                     0                             0   \n",
       "3                                     0                             0   \n",
       "4                                     0                             0   \n",
       "...                                 ...                           ...   \n",
       "148458                                0                             0   \n",
       "148459                                0                             0   \n",
       "148460                                1                             0   \n",
       "148461                                1                             0   \n",
       "148462                                0                             0   \n",
       "\n",
       "        Denominazione Regione_Missing  Denominazione Regione_Molise  \\\n",
       "0                                   0                             0   \n",
       "1                                   0                             0   \n",
       "2                                   0                             0   \n",
       "3                                   0                             0   \n",
       "4                                   0                             0   \n",
       "...                               ...                           ...   \n",
       "148458                              0                             0   \n",
       "148459                              0                             0   \n",
       "148460                              0                             0   \n",
       "148461                              0                             0   \n",
       "148462                              0                             0   \n",
       "\n",
       "        Denominazione Regione_Piemonte  Denominazione Regione_Puglia  \\\n",
       "0                                    0                             0   \n",
       "1                                    0                             0   \n",
       "2                                    0                             0   \n",
       "3                                    0                             0   \n",
       "4                                    0                             0   \n",
       "...                                ...                           ...   \n",
       "148458                               0                             0   \n",
       "148459                               0                             0   \n",
       "148460                               0                             0   \n",
       "148461                               0                             0   \n",
       "148462                               0                             0   \n",
       "\n",
       "        Denominazione Regione_Sardegna  Denominazione Regione_Sicilia  \\\n",
       "0                                    0                              0   \n",
       "1                                    0                              0   \n",
       "2                                    0                              0   \n",
       "3                                    0                              1   \n",
       "4                                    0                              0   \n",
       "...                                ...                            ...   \n",
       "148458                               0                              0   \n",
       "148459                               0                              0   \n",
       "148460                               0                              0   \n",
       "148461                               0                              0   \n",
       "148462                               0                              0   \n",
       "\n",
       "        Denominazione Regione_Toscana  \\\n",
       "0                                   0   \n",
       "1                                   0   \n",
       "2                                   0   \n",
       "3                                   0   \n",
       "4                                   0   \n",
       "...                               ...   \n",
       "148458                              0   \n",
       "148459                              0   \n",
       "148460                              0   \n",
       "148461                              0   \n",
       "148462                              0   \n",
       "\n",
       "        Denominazione Regione_Trentino-Alto Adige/Südtirol  \\\n",
       "0                                                       0    \n",
       "1                                                       0    \n",
       "2                                                       0    \n",
       "3                                                       0    \n",
       "4                                                       0    \n",
       "...                                                   ...    \n",
       "148458                                                  0    \n",
       "148459                                                  0    \n",
       "148460                                                  0    \n",
       "148461                                                  0    \n",
       "148462                                                  0    \n",
       "\n",
       "        Denominazione Regione_Umbria  \\\n",
       "0                                  0   \n",
       "1                                  0   \n",
       "2                                  0   \n",
       "3                                  0   \n",
       "4                                  0   \n",
       "...                              ...   \n",
       "148458                             0   \n",
       "148459                             1   \n",
       "148460                             0   \n",
       "148461                             0   \n",
       "148462                             0   \n",
       "\n",
       "        Denominazione Regione_Valle d'Aosta/Vallée d'Aoste  \\\n",
       "0                                                       0    \n",
       "1                                                       0    \n",
       "2                                                       0    \n",
       "3                                                       0    \n",
       "4                                                       0    \n",
       "...                                                   ...    \n",
       "148458                                                  0    \n",
       "148459                                                  0    \n",
       "148460                                                  0    \n",
       "148461                                                  0    \n",
       "148462                                                  0    \n",
       "\n",
       "        Denominazione Regione_Veneto  DESCRIZIONE PRODOTTO_ARREDAMENTO  \\\n",
       "0                                  0                                 0   \n",
       "1                                  0                                 0   \n",
       "2                                  0                                 0   \n",
       "3                                  0                                 0   \n",
       "4                                  0                                 0   \n",
       "...                              ...                               ...   \n",
       "148458                             0                                 0   \n",
       "148459                             0                                 0   \n",
       "148460                             0                                 0   \n",
       "148461                             0                                 0   \n",
       "148462                             0                                 0   \n",
       "\n",
       "        DESCRIZIONE PRODOTTO_AUTOMOTIVE NUOVO  \\\n",
       "0                                           0   \n",
       "1                                           0   \n",
       "2                                           0   \n",
       "3                                           0   \n",
       "4                                           0   \n",
       "...                                       ...   \n",
       "148458                                      1   \n",
       "148459                                      0   \n",
       "148460                                      0   \n",
       "148461                                      0   \n",
       "148462                                      0   \n",
       "\n",
       "        DESCRIZIONE PRODOTTO_AUTOMOTIVE USATO  \\\n",
       "0                                           0   \n",
       "1                                           0   \n",
       "2                                           0   \n",
       "3                                           0   \n",
       "4                                           0   \n",
       "...                                       ...   \n",
       "148458                                      0   \n",
       "148459                                      0   \n",
       "148460                                      0   \n",
       "148461                                      0   \n",
       "148462                                      1   \n",
       "\n",
       "        DESCRIZIONE PRODOTTO_CONSOLIDAMENTO DEL DEBITO  \\\n",
       "0                                                    0   \n",
       "1                                                    0   \n",
       "2                                                    0   \n",
       "3                                                    0   \n",
       "4                                                    0   \n",
       "...                                                ...   \n",
       "148458                                               0   \n",
       "148459                                               0   \n",
       "148460                                               0   \n",
       "148461                                               0   \n",
       "148462                                               0   \n",
       "\n",
       "        DESCRIZIONE PRODOTTO_ELETTRONICA & ELETTRODOME  \\\n",
       "0                                                    0   \n",
       "1                                                    0   \n",
       "2                                                    1   \n",
       "3                                                    0   \n",
       "4                                                    0   \n",
       "...                                                ...   \n",
       "148458                                               0   \n",
       "148459                                               0   \n",
       "148460                                               0   \n",
       "148461                                               0   \n",
       "148462                                               0   \n",
       "\n",
       "        DESCRIZIONE PRODOTTO_FINANZIAMENTI AZIENDE  \\\n",
       "0                                                0   \n",
       "1                                                1   \n",
       "2                                                0   \n",
       "3                                                1   \n",
       "4                                                0   \n",
       "...                                            ...   \n",
       "148458                                           0   \n",
       "148459                                           0   \n",
       "148460                                           0   \n",
       "148461                                           0   \n",
       "148462                                           0   \n",
       "\n",
       "        DESCRIZIONE PRODOTTO_INTERVENTI CASA  \\\n",
       "0                                          0   \n",
       "1                                          0   \n",
       "2                                          0   \n",
       "3                                          0   \n",
       "4                                          0   \n",
       "...                                      ...   \n",
       "148458                                     0   \n",
       "148459                                     0   \n",
       "148460                                     0   \n",
       "148461                                     0   \n",
       "148462                                     0   \n",
       "\n",
       "        DESCRIZIONE PRODOTTO_MOTO E CICLOMOTORI NUOVO  \\\n",
       "0                                                   0   \n",
       "1                                                   0   \n",
       "2                                                   0   \n",
       "3                                                   0   \n",
       "4                                                   0   \n",
       "...                                               ...   \n",
       "148458                                              0   \n",
       "148459                                              0   \n",
       "148460                                              0   \n",
       "148461                                              0   \n",
       "148462                                              0   \n",
       "\n",
       "        DESCRIZIONE PRODOTTO_MOTO E CICLOMOTORI USATO  \\\n",
       "0                                                   0   \n",
       "1                                                   0   \n",
       "2                                                   0   \n",
       "3                                                   0   \n",
       "4                                                   0   \n",
       "...                                               ...   \n",
       "148458                                              0   \n",
       "148459                                              0   \n",
       "148460                                              0   \n",
       "148461                                              0   \n",
       "148462                                              0   \n",
       "\n",
       "        DESCRIZIONE PRODOTTO_PRESTITI CON TRATTENUTA  \\\n",
       "0                                                  0   \n",
       "1                                                  0   \n",
       "2                                                  0   \n",
       "3                                                  0   \n",
       "4                                                  0   \n",
       "...                                              ...   \n",
       "148458                                             0   \n",
       "148459                                             0   \n",
       "148460                                             1   \n",
       "148461                                             0   \n",
       "148462                                             0   \n",
       "\n",
       "        DESCRIZIONE PRODOTTO_PRESTITO PERSONALE  \\\n",
       "0                                             1   \n",
       "1                                             0   \n",
       "2                                             0   \n",
       "3                                             0   \n",
       "4                                             0   \n",
       "...                                         ...   \n",
       "148458                                        0   \n",
       "148459                                        1   \n",
       "148460                                        0   \n",
       "148461                                        0   \n",
       "148462                                        0   \n",
       "\n",
       "        DESCRIZIONE PRODOTTO_SPESE MEDICHE  DESCRIZIONE PRODOTTO_TEMPO LIBERO  \n",
       "0                                        0                                  0  \n",
       "1                                        0                                  0  \n",
       "2                                        0                                  0  \n",
       "3                                        0                                  0  \n",
       "4                                        1                                  0  \n",
       "...                                    ...                                ...  \n",
       "148458                                   0                                  0  \n",
       "148459                                   0                                  0  \n",
       "148460                                   0                                  0  \n",
       "148461                                   1                                  0  \n",
       "148462                                   0                                  0  \n",
       "\n",
       "[139010 rows x 42 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dummies.drop(\"Target_95\", axis=1)\n",
    "y = dummies.Target_95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Eta_Debitore</th>\n",
       "      <th>RATIO_ONERI_AFF_TOT_AFF</th>\n",
       "      <th>AgeingGestioneGg</th>\n",
       "      <th>NUMERO RATE</th>\n",
       "      <th>Ratio_Rate_Imp1</th>\n",
       "      <th>TOT_AFF</th>\n",
       "      <th>AgeingErogazioneGg</th>\n",
       "      <th>IMP_FINANZIATO</th>\n",
       "      <th>Denominazione Regione_Basilicata</th>\n",
       "      <th>Denominazione Regione_Calabria</th>\n",
       "      <th>Denominazione Regione_Campania</th>\n",
       "      <th>Denominazione Regione_Emilia-Romagna</th>\n",
       "      <th>Denominazione Regione_Friuli-Venezia Giulia</th>\n",
       "      <th>Denominazione Regione_Lazio</th>\n",
       "      <th>Denominazione Regione_Liguria</th>\n",
       "      <th>Denominazione Regione_Lombardia</th>\n",
       "      <th>Denominazione Regione_Marche</th>\n",
       "      <th>Denominazione Regione_Missing</th>\n",
       "      <th>Denominazione Regione_Molise</th>\n",
       "      <th>Denominazione Regione_Piemonte</th>\n",
       "      <th>Denominazione Regione_Puglia</th>\n",
       "      <th>Denominazione Regione_Sardegna</th>\n",
       "      <th>Denominazione Regione_Sicilia</th>\n",
       "      <th>Denominazione Regione_Toscana</th>\n",
       "      <th>Denominazione Regione_Trentino-Alto Adige/Südtirol</th>\n",
       "      <th>Denominazione Regione_Umbria</th>\n",
       "      <th>Denominazione Regione_Valle d'Aosta/Vallée d'Aoste</th>\n",
       "      <th>Denominazione Regione_Veneto</th>\n",
       "      <th>DESCRIZIONE PRODOTTO_ARREDAMENTO</th>\n",
       "      <th>DESCRIZIONE PRODOTTO_AUTOMOTIVE NUOVO</th>\n",
       "      <th>DESCRIZIONE PRODOTTO_AUTOMOTIVE USATO</th>\n",
       "      <th>DESCRIZIONE PRODOTTO_CONSOLIDAMENTO DEL DEBITO</th>\n",
       "      <th>DESCRIZIONE PRODOTTO_ELETTRONICA &amp; ELETTRODOME</th>\n",
       "      <th>DESCRIZIONE PRODOTTO_FINANZIAMENTI AZIENDE</th>\n",
       "      <th>DESCRIZIONE PRODOTTO_INTERVENTI CASA</th>\n",
       "      <th>DESCRIZIONE PRODOTTO_MOTO E CICLOMOTORI NUOVO</th>\n",
       "      <th>DESCRIZIONE PRODOTTO_MOTO E CICLOMOTORI USATO</th>\n",
       "      <th>DESCRIZIONE PRODOTTO_PRESTITI CON TRATTENUTA</th>\n",
       "      <th>DESCRIZIONE PRODOTTO_PRESTITO PERSONALE</th>\n",
       "      <th>DESCRIZIONE PRODOTTO_SPESE MEDICHE</th>\n",
       "      <th>DESCRIZIONE PRODOTTO_TEMPO LIBERO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>51.00</td>\n",
       "      <td>0.04</td>\n",
       "      <td>21.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0.44</td>\n",
       "      <td>277.24</td>\n",
       "      <td>1,126.00</td>\n",
       "      <td>15,200.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>59.00</td>\n",
       "      <td>0.06</td>\n",
       "      <td>29.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0.52</td>\n",
       "      <td>197.06</td>\n",
       "      <td>1,887.00</td>\n",
       "      <td>14,234.40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>57.00</td>\n",
       "      <td>0.22</td>\n",
       "      <td>29.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0.17</td>\n",
       "      <td>73.05</td>\n",
       "      <td>152.00</td>\n",
       "      <td>1,500.00</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>76.00</td>\n",
       "      <td>0.04</td>\n",
       "      <td>29.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0.31</td>\n",
       "      <td>288.20</td>\n",
       "      <td>669.00</td>\n",
       "      <td>14,280.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57.00</td>\n",
       "      <td>0.16</td>\n",
       "      <td>28.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0.12</td>\n",
       "      <td>199.57</td>\n",
       "      <td>180.00</td>\n",
       "      <td>6,800.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148458</th>\n",
       "      <td>51.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>15.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0.29</td>\n",
       "      <td>108.17</td>\n",
       "      <td>426.00</td>\n",
       "      <td>4,120.88</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148459</th>\n",
       "      <td>51.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>15.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0.32</td>\n",
       "      <td>116.85</td>\n",
       "      <td>699.00</td>\n",
       "      <td>6,250.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148460</th>\n",
       "      <td>51.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>15.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0.45</td>\n",
       "      <td>305.41</td>\n",
       "      <td>821.00</td>\n",
       "      <td>14,492.80</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148461</th>\n",
       "      <td>61.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>15.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0.50</td>\n",
       "      <td>95.76</td>\n",
       "      <td>1,125.00</td>\n",
       "      <td>6,654.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148462</th>\n",
       "      <td>53.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>15.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0.18</td>\n",
       "      <td>143.48</td>\n",
       "      <td>334.00</td>\n",
       "      <td>7,009.40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>139010 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Eta_Debitore  RATIO_ONERI_AFF_TOT_AFF  AgeingGestioneGg  NUMERO RATE  \\\n",
       "0              51.00                     0.04             21.00            2   \n",
       "1              59.00                     0.06             29.00            2   \n",
       "2              57.00                     0.22             29.00            2   \n",
       "3              76.00                     0.04             29.00            2   \n",
       "4              57.00                     0.16             28.00            2   \n",
       "...              ...                      ...               ...          ...   \n",
       "148458         51.00                     0.00             15.00            2   \n",
       "148459         51.00                     0.00             15.00            2   \n",
       "148460         51.00                     0.00             15.00            2   \n",
       "148461         61.00                     0.00             15.00            2   \n",
       "148462         53.00                     0.00             15.00            2   \n",
       "\n",
       "        Ratio_Rate_Imp1  TOT_AFF  AgeingErogazioneGg  IMP_FINANZIATO  \\\n",
       "0                  0.44   277.24            1,126.00       15,200.00   \n",
       "1                  0.52   197.06            1,887.00       14,234.40   \n",
       "2                  0.17    73.05              152.00        1,500.00   \n",
       "3                  0.31   288.20              669.00       14,280.00   \n",
       "4                  0.12   199.57              180.00        6,800.00   \n",
       "...                 ...      ...                 ...             ...   \n",
       "148458             0.29   108.17              426.00        4,120.88   \n",
       "148459             0.32   116.85              699.00        6,250.00   \n",
       "148460             0.45   305.41              821.00       14,492.80   \n",
       "148461             0.50    95.76            1,125.00        6,654.00   \n",
       "148462             0.18   143.48              334.00        7,009.40   \n",
       "\n",
       "        Denominazione Regione_Basilicata  Denominazione Regione_Calabria  \\\n",
       "0                                      0                               0   \n",
       "1                                      0                               0   \n",
       "2                                      0                               1   \n",
       "3                                      0                               0   \n",
       "4                                      0                               0   \n",
       "...                                  ...                             ...   \n",
       "148458                                 0                               0   \n",
       "148459                                 0                               0   \n",
       "148460                                 0                               0   \n",
       "148461                                 0                               0   \n",
       "148462                                 0                               0   \n",
       "\n",
       "        Denominazione Regione_Campania  Denominazione Regione_Emilia-Romagna  \\\n",
       "0                                    0                                     0   \n",
       "1                                    0                                     0   \n",
       "2                                    0                                     0   \n",
       "3                                    0                                     0   \n",
       "4                                    0                                     0   \n",
       "...                                ...                                   ...   \n",
       "148458                               0                                     0   \n",
       "148459                               0                                     0   \n",
       "148460                               0                                     0   \n",
       "148461                               0                                     0   \n",
       "148462                               0                                     0   \n",
       "\n",
       "        Denominazione Regione_Friuli-Venezia Giulia  \\\n",
       "0                                                 0   \n",
       "1                                                 0   \n",
       "2                                                 0   \n",
       "3                                                 0   \n",
       "4                                                 0   \n",
       "...                                             ...   \n",
       "148458                                            0   \n",
       "148459                                            0   \n",
       "148460                                            0   \n",
       "148461                                            0   \n",
       "148462                                            0   \n",
       "\n",
       "        Denominazione Regione_Lazio  Denominazione Regione_Liguria  \\\n",
       "0                                 1                              0   \n",
       "1                                 0                              1   \n",
       "2                                 0                              0   \n",
       "3                                 0                              0   \n",
       "4                                 1                              0   \n",
       "...                             ...                            ...   \n",
       "148458                            1                              0   \n",
       "148459                            0                              0   \n",
       "148460                            0                              0   \n",
       "148461                            0                              0   \n",
       "148462                            1                              0   \n",
       "\n",
       "        Denominazione Regione_Lombardia  Denominazione Regione_Marche  \\\n",
       "0                                     0                             0   \n",
       "1                                     0                             0   \n",
       "2                                     0                             0   \n",
       "3                                     0                             0   \n",
       "4                                     0                             0   \n",
       "...                                 ...                           ...   \n",
       "148458                                0                             0   \n",
       "148459                                0                             0   \n",
       "148460                                1                             0   \n",
       "148461                                1                             0   \n",
       "148462                                0                             0   \n",
       "\n",
       "        Denominazione Regione_Missing  Denominazione Regione_Molise  \\\n",
       "0                                   0                             0   \n",
       "1                                   0                             0   \n",
       "2                                   0                             0   \n",
       "3                                   0                             0   \n",
       "4                                   0                             0   \n",
       "...                               ...                           ...   \n",
       "148458                              0                             0   \n",
       "148459                              0                             0   \n",
       "148460                              0                             0   \n",
       "148461                              0                             0   \n",
       "148462                              0                             0   \n",
       "\n",
       "        Denominazione Regione_Piemonte  Denominazione Regione_Puglia  \\\n",
       "0                                    0                             0   \n",
       "1                                    0                             0   \n",
       "2                                    0                             0   \n",
       "3                                    0                             0   \n",
       "4                                    0                             0   \n",
       "...                                ...                           ...   \n",
       "148458                               0                             0   \n",
       "148459                               0                             0   \n",
       "148460                               0                             0   \n",
       "148461                               0                             0   \n",
       "148462                               0                             0   \n",
       "\n",
       "        Denominazione Regione_Sardegna  Denominazione Regione_Sicilia  \\\n",
       "0                                    0                              0   \n",
       "1                                    0                              0   \n",
       "2                                    0                              0   \n",
       "3                                    0                              1   \n",
       "4                                    0                              0   \n",
       "...                                ...                            ...   \n",
       "148458                               0                              0   \n",
       "148459                               0                              0   \n",
       "148460                               0                              0   \n",
       "148461                               0                              0   \n",
       "148462                               0                              0   \n",
       "\n",
       "        Denominazione Regione_Toscana  \\\n",
       "0                                   0   \n",
       "1                                   0   \n",
       "2                                   0   \n",
       "3                                   0   \n",
       "4                                   0   \n",
       "...                               ...   \n",
       "148458                              0   \n",
       "148459                              0   \n",
       "148460                              0   \n",
       "148461                              0   \n",
       "148462                              0   \n",
       "\n",
       "        Denominazione Regione_Trentino-Alto Adige/Südtirol  \\\n",
       "0                                                       0    \n",
       "1                                                       0    \n",
       "2                                                       0    \n",
       "3                                                       0    \n",
       "4                                                       0    \n",
       "...                                                   ...    \n",
       "148458                                                  0    \n",
       "148459                                                  0    \n",
       "148460                                                  0    \n",
       "148461                                                  0    \n",
       "148462                                                  0    \n",
       "\n",
       "        Denominazione Regione_Umbria  \\\n",
       "0                                  0   \n",
       "1                                  0   \n",
       "2                                  0   \n",
       "3                                  0   \n",
       "4                                  0   \n",
       "...                              ...   \n",
       "148458                             0   \n",
       "148459                             1   \n",
       "148460                             0   \n",
       "148461                             0   \n",
       "148462                             0   \n",
       "\n",
       "        Denominazione Regione_Valle d'Aosta/Vallée d'Aoste  \\\n",
       "0                                                       0    \n",
       "1                                                       0    \n",
       "2                                                       0    \n",
       "3                                                       0    \n",
       "4                                                       0    \n",
       "...                                                   ...    \n",
       "148458                                                  0    \n",
       "148459                                                  0    \n",
       "148460                                                  0    \n",
       "148461                                                  0    \n",
       "148462                                                  0    \n",
       "\n",
       "        Denominazione Regione_Veneto  DESCRIZIONE PRODOTTO_ARREDAMENTO  \\\n",
       "0                                  0                                 0   \n",
       "1                                  0                                 0   \n",
       "2                                  0                                 0   \n",
       "3                                  0                                 0   \n",
       "4                                  0                                 0   \n",
       "...                              ...                               ...   \n",
       "148458                             0                                 0   \n",
       "148459                             0                                 0   \n",
       "148460                             0                                 0   \n",
       "148461                             0                                 0   \n",
       "148462                             0                                 0   \n",
       "\n",
       "        DESCRIZIONE PRODOTTO_AUTOMOTIVE NUOVO  \\\n",
       "0                                           0   \n",
       "1                                           0   \n",
       "2                                           0   \n",
       "3                                           0   \n",
       "4                                           0   \n",
       "...                                       ...   \n",
       "148458                                      1   \n",
       "148459                                      0   \n",
       "148460                                      0   \n",
       "148461                                      0   \n",
       "148462                                      0   \n",
       "\n",
       "        DESCRIZIONE PRODOTTO_AUTOMOTIVE USATO  \\\n",
       "0                                           0   \n",
       "1                                           0   \n",
       "2                                           0   \n",
       "3                                           0   \n",
       "4                                           0   \n",
       "...                                       ...   \n",
       "148458                                      0   \n",
       "148459                                      0   \n",
       "148460                                      0   \n",
       "148461                                      0   \n",
       "148462                                      1   \n",
       "\n",
       "        DESCRIZIONE PRODOTTO_CONSOLIDAMENTO DEL DEBITO  \\\n",
       "0                                                    0   \n",
       "1                                                    0   \n",
       "2                                                    0   \n",
       "3                                                    0   \n",
       "4                                                    0   \n",
       "...                                                ...   \n",
       "148458                                               0   \n",
       "148459                                               0   \n",
       "148460                                               0   \n",
       "148461                                               0   \n",
       "148462                                               0   \n",
       "\n",
       "        DESCRIZIONE PRODOTTO_ELETTRONICA & ELETTRODOME  \\\n",
       "0                                                    0   \n",
       "1                                                    0   \n",
       "2                                                    1   \n",
       "3                                                    0   \n",
       "4                                                    0   \n",
       "...                                                ...   \n",
       "148458                                               0   \n",
       "148459                                               0   \n",
       "148460                                               0   \n",
       "148461                                               0   \n",
       "148462                                               0   \n",
       "\n",
       "        DESCRIZIONE PRODOTTO_FINANZIAMENTI AZIENDE  \\\n",
       "0                                                0   \n",
       "1                                                1   \n",
       "2                                                0   \n",
       "3                                                1   \n",
       "4                                                0   \n",
       "...                                            ...   \n",
       "148458                                           0   \n",
       "148459                                           0   \n",
       "148460                                           0   \n",
       "148461                                           0   \n",
       "148462                                           0   \n",
       "\n",
       "        DESCRIZIONE PRODOTTO_INTERVENTI CASA  \\\n",
       "0                                          0   \n",
       "1                                          0   \n",
       "2                                          0   \n",
       "3                                          0   \n",
       "4                                          0   \n",
       "...                                      ...   \n",
       "148458                                     0   \n",
       "148459                                     0   \n",
       "148460                                     0   \n",
       "148461                                     0   \n",
       "148462                                     0   \n",
       "\n",
       "        DESCRIZIONE PRODOTTO_MOTO E CICLOMOTORI NUOVO  \\\n",
       "0                                                   0   \n",
       "1                                                   0   \n",
       "2                                                   0   \n",
       "3                                                   0   \n",
       "4                                                   0   \n",
       "...                                               ...   \n",
       "148458                                              0   \n",
       "148459                                              0   \n",
       "148460                                              0   \n",
       "148461                                              0   \n",
       "148462                                              0   \n",
       "\n",
       "        DESCRIZIONE PRODOTTO_MOTO E CICLOMOTORI USATO  \\\n",
       "0                                                   0   \n",
       "1                                                   0   \n",
       "2                                                   0   \n",
       "3                                                   0   \n",
       "4                                                   0   \n",
       "...                                               ...   \n",
       "148458                                              0   \n",
       "148459                                              0   \n",
       "148460                                              0   \n",
       "148461                                              0   \n",
       "148462                                              0   \n",
       "\n",
       "        DESCRIZIONE PRODOTTO_PRESTITI CON TRATTENUTA  \\\n",
       "0                                                  0   \n",
       "1                                                  0   \n",
       "2                                                  0   \n",
       "3                                                  0   \n",
       "4                                                  0   \n",
       "...                                              ...   \n",
       "148458                                             0   \n",
       "148459                                             0   \n",
       "148460                                             1   \n",
       "148461                                             0   \n",
       "148462                                             0   \n",
       "\n",
       "        DESCRIZIONE PRODOTTO_PRESTITO PERSONALE  \\\n",
       "0                                             1   \n",
       "1                                             0   \n",
       "2                                             0   \n",
       "3                                             0   \n",
       "4                                             0   \n",
       "...                                         ...   \n",
       "148458                                        0   \n",
       "148459                                        1   \n",
       "148460                                        0   \n",
       "148461                                        0   \n",
       "148462                                        0   \n",
       "\n",
       "        DESCRIZIONE PRODOTTO_SPESE MEDICHE  DESCRIZIONE PRODOTTO_TEMPO LIBERO  \n",
       "0                                        0                                  0  \n",
       "1                                        0                                  0  \n",
       "2                                        0                                  0  \n",
       "3                                        0                                  0  \n",
       "4                                        1                                  0  \n",
       "...                                    ...                                ...  \n",
       "148458                                   0                                  0  \n",
       "148459                                   0                                  0  \n",
       "148460                                   0                                  0  \n",
       "148461                                   1                                  0  \n",
       "148462                                   0                                  0  \n",
       "\n",
       "[139010 rows x 41 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         0\n",
       "1         0\n",
       "2         0\n",
       "3         0\n",
       "4         0\n",
       "         ..\n",
       "148458    0\n",
       "148459    1\n",
       "148460    0\n",
       "148461    0\n",
       "148462    0\n",
       "Name: Target_95, Length: 139010, dtype: int32"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_value = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=seed_value) \n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=seed_value)\n",
    "# quindi alla fine il train è l'81%, il validation 9% e il test 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Eta_Debitore</th>\n",
       "      <th>RATIO_ONERI_AFF_TOT_AFF</th>\n",
       "      <th>AgeingGestioneGg</th>\n",
       "      <th>NUMERO RATE</th>\n",
       "      <th>Ratio_Rate_Imp1</th>\n",
       "      <th>TOT_AFF</th>\n",
       "      <th>AgeingErogazioneGg</th>\n",
       "      <th>IMP_FINANZIATO</th>\n",
       "      <th>Denominazione Regione_Basilicata</th>\n",
       "      <th>Denominazione Regione_Calabria</th>\n",
       "      <th>Denominazione Regione_Campania</th>\n",
       "      <th>Denominazione Regione_Emilia-Romagna</th>\n",
       "      <th>Denominazione Regione_Friuli-Venezia Giulia</th>\n",
       "      <th>Denominazione Regione_Lazio</th>\n",
       "      <th>Denominazione Regione_Liguria</th>\n",
       "      <th>Denominazione Regione_Lombardia</th>\n",
       "      <th>Denominazione Regione_Marche</th>\n",
       "      <th>Denominazione Regione_Missing</th>\n",
       "      <th>Denominazione Regione_Molise</th>\n",
       "      <th>Denominazione Regione_Piemonte</th>\n",
       "      <th>Denominazione Regione_Puglia</th>\n",
       "      <th>Denominazione Regione_Sardegna</th>\n",
       "      <th>Denominazione Regione_Sicilia</th>\n",
       "      <th>Denominazione Regione_Toscana</th>\n",
       "      <th>Denominazione Regione_Trentino-Alto Adige/Südtirol</th>\n",
       "      <th>Denominazione Regione_Umbria</th>\n",
       "      <th>Denominazione Regione_Valle d'Aosta/Vallée d'Aoste</th>\n",
       "      <th>Denominazione Regione_Veneto</th>\n",
       "      <th>DESCRIZIONE PRODOTTO_ARREDAMENTO</th>\n",
       "      <th>DESCRIZIONE PRODOTTO_AUTOMOTIVE NUOVO</th>\n",
       "      <th>DESCRIZIONE PRODOTTO_AUTOMOTIVE USATO</th>\n",
       "      <th>DESCRIZIONE PRODOTTO_CONSOLIDAMENTO DEL DEBITO</th>\n",
       "      <th>DESCRIZIONE PRODOTTO_ELETTRONICA &amp; ELETTRODOME</th>\n",
       "      <th>DESCRIZIONE PRODOTTO_FINANZIAMENTI AZIENDE</th>\n",
       "      <th>DESCRIZIONE PRODOTTO_INTERVENTI CASA</th>\n",
       "      <th>DESCRIZIONE PRODOTTO_MOTO E CICLOMOTORI NUOVO</th>\n",
       "      <th>DESCRIZIONE PRODOTTO_MOTO E CICLOMOTORI USATO</th>\n",
       "      <th>DESCRIZIONE PRODOTTO_PRESTITI CON TRATTENUTA</th>\n",
       "      <th>DESCRIZIONE PRODOTTO_PRESTITO PERSONALE</th>\n",
       "      <th>DESCRIZIONE PRODOTTO_SPESE MEDICHE</th>\n",
       "      <th>DESCRIZIONE PRODOTTO_TEMPO LIBERO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>86628</th>\n",
       "      <td>46.00</td>\n",
       "      <td>0.28</td>\n",
       "      <td>15.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.08</td>\n",
       "      <td>278.37</td>\n",
       "      <td>122.00</td>\n",
       "      <td>8,400.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68428</th>\n",
       "      <td>60.00</td>\n",
       "      <td>0.06</td>\n",
       "      <td>15.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.78</td>\n",
       "      <td>197.77</td>\n",
       "      <td>3,558.00</td>\n",
       "      <td>18,922.94</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15652</th>\n",
       "      <td>73.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>31.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0.39</td>\n",
       "      <td>103.05</td>\n",
       "      <td>485.00</td>\n",
       "      <td>1,500.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94489</th>\n",
       "      <td>51.00</td>\n",
       "      <td>0.04</td>\n",
       "      <td>15.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.39</td>\n",
       "      <td>325.01</td>\n",
       "      <td>1,006.00</td>\n",
       "      <td>27,584.70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43072</th>\n",
       "      <td>42.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>26.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0.80</td>\n",
       "      <td>347.52</td>\n",
       "      <td>2,039.00</td>\n",
       "      <td>16,645.20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89336</th>\n",
       "      <td>66.00</td>\n",
       "      <td>0.04</td>\n",
       "      <td>15.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.77</td>\n",
       "      <td>281.32</td>\n",
       "      <td>1,979.00</td>\n",
       "      <td>17,640.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48305</th>\n",
       "      <td>51.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>15.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0.85</td>\n",
       "      <td>140.89</td>\n",
       "      <td>1,552.00</td>\n",
       "      <td>6,649.50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15203</th>\n",
       "      <td>40.00</td>\n",
       "      <td>0.05</td>\n",
       "      <td>33.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0.75</td>\n",
       "      <td>242.78</td>\n",
       "      <td>1,369.00</td>\n",
       "      <td>8,258.25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36915</th>\n",
       "      <td>52.00</td>\n",
       "      <td>0.12</td>\n",
       "      <td>31.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0.47</td>\n",
       "      <td>97.22</td>\n",
       "      <td>912.00</td>\n",
       "      <td>1,900.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111462</th>\n",
       "      <td>45.00</td>\n",
       "      <td>0.07</td>\n",
       "      <td>15.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.90</td>\n",
       "      <td>166.85</td>\n",
       "      <td>1,734.00</td>\n",
       "      <td>6,789.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>112598 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Eta_Debitore  RATIO_ONERI_AFF_TOT_AFF  AgeingGestioneGg  NUMERO RATE  \\\n",
       "86628          46.00                     0.28             15.00            1   \n",
       "68428          60.00                     0.06             15.00            1   \n",
       "15652          73.00                     0.00             31.00            2   \n",
       "94489          51.00                     0.04             15.00            1   \n",
       "43072          42.00                     0.03             26.00            2   \n",
       "...              ...                      ...               ...          ...   \n",
       "89336          66.00                     0.04             15.00            1   \n",
       "48305          51.00                     0.00             15.00            2   \n",
       "15203          40.00                     0.05             33.00            2   \n",
       "36915          52.00                     0.12             31.00            2   \n",
       "111462         45.00                     0.07             15.00            1   \n",
       "\n",
       "        Ratio_Rate_Imp1  TOT_AFF  AgeingErogazioneGg  IMP_FINANZIATO  \\\n",
       "86628              0.08   278.37              122.00        8,400.00   \n",
       "68428              0.78   197.77            3,558.00       18,922.94   \n",
       "15652              0.39   103.05              485.00        1,500.00   \n",
       "94489              0.39   325.01            1,006.00       27,584.70   \n",
       "43072              0.80   347.52            2,039.00       16,645.20   \n",
       "...                 ...      ...                 ...             ...   \n",
       "89336              0.77   281.32            1,979.00       17,640.00   \n",
       "48305              0.85   140.89            1,552.00        6,649.50   \n",
       "15203              0.75   242.78            1,369.00        8,258.25   \n",
       "36915              0.47    97.22              912.00        1,900.00   \n",
       "111462             0.90   166.85            1,734.00        6,789.00   \n",
       "\n",
       "        Denominazione Regione_Basilicata  Denominazione Regione_Calabria  \\\n",
       "86628                                  0                               0   \n",
       "68428                                  0                               0   \n",
       "15652                                  0                               0   \n",
       "94489                                  0                               0   \n",
       "43072                                  0                               0   \n",
       "...                                  ...                             ...   \n",
       "89336                                  0                               0   \n",
       "48305                                  0                               0   \n",
       "15203                                  0                               0   \n",
       "36915                                  0                               0   \n",
       "111462                                 0                               0   \n",
       "\n",
       "        Denominazione Regione_Campania  Denominazione Regione_Emilia-Romagna  \\\n",
       "86628                                0                                     0   \n",
       "68428                                1                                     0   \n",
       "15652                                0                                     0   \n",
       "94489                                1                                     0   \n",
       "43072                                1                                     0   \n",
       "...                                ...                                   ...   \n",
       "89336                                0                                     0   \n",
       "48305                                0                                     0   \n",
       "15203                                0                                     0   \n",
       "36915                                0                                     0   \n",
       "111462                               0                                     0   \n",
       "\n",
       "        Denominazione Regione_Friuli-Venezia Giulia  \\\n",
       "86628                                             0   \n",
       "68428                                             0   \n",
       "15652                                             0   \n",
       "94489                                             0   \n",
       "43072                                             0   \n",
       "...                                             ...   \n",
       "89336                                             0   \n",
       "48305                                             0   \n",
       "15203                                             0   \n",
       "36915                                             0   \n",
       "111462                                            0   \n",
       "\n",
       "        Denominazione Regione_Lazio  Denominazione Regione_Liguria  \\\n",
       "86628                             0                              0   \n",
       "68428                             0                              0   \n",
       "15652                             0                              0   \n",
       "94489                             0                              0   \n",
       "43072                             0                              0   \n",
       "...                             ...                            ...   \n",
       "89336                             0                              0   \n",
       "48305                             0                              0   \n",
       "15203                             0                              0   \n",
       "36915                             0                              0   \n",
       "111462                            0                              0   \n",
       "\n",
       "        Denominazione Regione_Lombardia  Denominazione Regione_Marche  \\\n",
       "86628                                 0                             0   \n",
       "68428                                 0                             0   \n",
       "15652                                 0                             0   \n",
       "94489                                 0                             0   \n",
       "43072                                 0                             0   \n",
       "...                                 ...                           ...   \n",
       "89336                                 0                             0   \n",
       "48305                                 1                             0   \n",
       "15203                                 1                             0   \n",
       "36915                                 1                             0   \n",
       "111462                                0                             0   \n",
       "\n",
       "        Denominazione Regione_Missing  Denominazione Regione_Molise  \\\n",
       "86628                               0                             0   \n",
       "68428                               0                             0   \n",
       "15652                               0                             0   \n",
       "94489                               0                             0   \n",
       "43072                               0                             0   \n",
       "...                               ...                           ...   \n",
       "89336                               0                             0   \n",
       "48305                               0                             0   \n",
       "15203                               0                             0   \n",
       "36915                               0                             0   \n",
       "111462                              0                             0   \n",
       "\n",
       "        Denominazione Regione_Piemonte  Denominazione Regione_Puglia  \\\n",
       "86628                                0                             0   \n",
       "68428                                0                             0   \n",
       "15652                                0                             1   \n",
       "94489                                0                             0   \n",
       "43072                                0                             0   \n",
       "...                                ...                           ...   \n",
       "89336                                0                             1   \n",
       "48305                                0                             0   \n",
       "15203                                0                             0   \n",
       "36915                                0                             0   \n",
       "111462                               0                             1   \n",
       "\n",
       "        Denominazione Regione_Sardegna  Denominazione Regione_Sicilia  \\\n",
       "86628                                0                              0   \n",
       "68428                                0                              0   \n",
       "15652                                0                              0   \n",
       "94489                                0                              0   \n",
       "43072                                0                              0   \n",
       "...                                ...                            ...   \n",
       "89336                                0                              0   \n",
       "48305                                0                              0   \n",
       "15203                                0                              0   \n",
       "36915                                0                              0   \n",
       "111462                               0                              0   \n",
       "\n",
       "        Denominazione Regione_Toscana  \\\n",
       "86628                               1   \n",
       "68428                               0   \n",
       "15652                               0   \n",
       "94489                               0   \n",
       "43072                               0   \n",
       "...                               ...   \n",
       "89336                               0   \n",
       "48305                               0   \n",
       "15203                               0   \n",
       "36915                               0   \n",
       "111462                              0   \n",
       "\n",
       "        Denominazione Regione_Trentino-Alto Adige/Südtirol  \\\n",
       "86628                                                   0    \n",
       "68428                                                   0    \n",
       "15652                                                   0    \n",
       "94489                                                   0    \n",
       "43072                                                   0    \n",
       "...                                                   ...    \n",
       "89336                                                   0    \n",
       "48305                                                   0    \n",
       "15203                                                   0    \n",
       "36915                                                   0    \n",
       "111462                                                  0    \n",
       "\n",
       "        Denominazione Regione_Umbria  \\\n",
       "86628                              0   \n",
       "68428                              0   \n",
       "15652                              0   \n",
       "94489                              0   \n",
       "43072                              0   \n",
       "...                              ...   \n",
       "89336                              0   \n",
       "48305                              0   \n",
       "15203                              0   \n",
       "36915                              0   \n",
       "111462                             0   \n",
       "\n",
       "        Denominazione Regione_Valle d'Aosta/Vallée d'Aoste  \\\n",
       "86628                                                   0    \n",
       "68428                                                   0    \n",
       "15652                                                   0    \n",
       "94489                                                   0    \n",
       "43072                                                   0    \n",
       "...                                                   ...    \n",
       "89336                                                   0    \n",
       "48305                                                   0    \n",
       "15203                                                   0    \n",
       "36915                                                   0    \n",
       "111462                                                  0    \n",
       "\n",
       "        Denominazione Regione_Veneto  DESCRIZIONE PRODOTTO_ARREDAMENTO  \\\n",
       "86628                              0                                 0   \n",
       "68428                              0                                 0   \n",
       "15652                              0                                 1   \n",
       "94489                              0                                 0   \n",
       "43072                              0                                 0   \n",
       "...                              ...                               ...   \n",
       "89336                              0                                 0   \n",
       "48305                              0                                 0   \n",
       "15203                              0                                 0   \n",
       "36915                              0                                 1   \n",
       "111462                             0                                 0   \n",
       "\n",
       "        DESCRIZIONE PRODOTTO_AUTOMOTIVE NUOVO  \\\n",
       "86628                                       0   \n",
       "68428                                       0   \n",
       "15652                                       0   \n",
       "94489                                       0   \n",
       "43072                                       0   \n",
       "...                                       ...   \n",
       "89336                                       1   \n",
       "48305                                       0   \n",
       "15203                                       0   \n",
       "36915                                       0   \n",
       "111462                                      0   \n",
       "\n",
       "        DESCRIZIONE PRODOTTO_AUTOMOTIVE USATO  \\\n",
       "86628                                       0   \n",
       "68428                                       0   \n",
       "15652                                       0   \n",
       "94489                                       0   \n",
       "43072                                       0   \n",
       "...                                       ...   \n",
       "89336                                       0   \n",
       "48305                                       0   \n",
       "15203                                       1   \n",
       "36915                                       0   \n",
       "111462                                      0   \n",
       "\n",
       "        DESCRIZIONE PRODOTTO_CONSOLIDAMENTO DEL DEBITO  \\\n",
       "86628                                                0   \n",
       "68428                                                0   \n",
       "15652                                                0   \n",
       "94489                                                0   \n",
       "43072                                                0   \n",
       "...                                                ...   \n",
       "89336                                                0   \n",
       "48305                                                0   \n",
       "15203                                                0   \n",
       "36915                                                0   \n",
       "111462                                               0   \n",
       "\n",
       "        DESCRIZIONE PRODOTTO_ELETTRONICA & ELETTRODOME  \\\n",
       "86628                                                0   \n",
       "68428                                                0   \n",
       "15652                                                0   \n",
       "94489                                                0   \n",
       "43072                                                0   \n",
       "...                                                ...   \n",
       "89336                                                0   \n",
       "48305                                                0   \n",
       "15203                                                0   \n",
       "36915                                                0   \n",
       "111462                                               0   \n",
       "\n",
       "        DESCRIZIONE PRODOTTO_FINANZIAMENTI AZIENDE  \\\n",
       "86628                                            0   \n",
       "68428                                            0   \n",
       "15652                                            0   \n",
       "94489                                            1   \n",
       "43072                                            1   \n",
       "...                                            ...   \n",
       "89336                                            0   \n",
       "48305                                            0   \n",
       "15203                                            0   \n",
       "36915                                            0   \n",
       "111462                                           0   \n",
       "\n",
       "        DESCRIZIONE PRODOTTO_INTERVENTI CASA  \\\n",
       "86628                                      0   \n",
       "68428                                      1   \n",
       "15652                                      0   \n",
       "94489                                      0   \n",
       "43072                                      0   \n",
       "...                                      ...   \n",
       "89336                                      0   \n",
       "48305                                      0   \n",
       "15203                                      0   \n",
       "36915                                      0   \n",
       "111462                                     1   \n",
       "\n",
       "        DESCRIZIONE PRODOTTO_MOTO E CICLOMOTORI NUOVO  \\\n",
       "86628                                               0   \n",
       "68428                                               0   \n",
       "15652                                               0   \n",
       "94489                                               0   \n",
       "43072                                               0   \n",
       "...                                               ...   \n",
       "89336                                               0   \n",
       "48305                                               0   \n",
       "15203                                               0   \n",
       "36915                                               0   \n",
       "111462                                              0   \n",
       "\n",
       "        DESCRIZIONE PRODOTTO_MOTO E CICLOMOTORI USATO  \\\n",
       "86628                                               0   \n",
       "68428                                               0   \n",
       "15652                                               0   \n",
       "94489                                               0   \n",
       "43072                                               0   \n",
       "...                                               ...   \n",
       "89336                                               0   \n",
       "48305                                               0   \n",
       "15203                                               0   \n",
       "36915                                               0   \n",
       "111462                                              0   \n",
       "\n",
       "        DESCRIZIONE PRODOTTO_PRESTITI CON TRATTENUTA  \\\n",
       "86628                                              0   \n",
       "68428                                              0   \n",
       "15652                                              0   \n",
       "94489                                              0   \n",
       "43072                                              0   \n",
       "...                                              ...   \n",
       "89336                                              0   \n",
       "48305                                              0   \n",
       "15203                                              0   \n",
       "36915                                              0   \n",
       "111462                                             0   \n",
       "\n",
       "        DESCRIZIONE PRODOTTO_PRESTITO PERSONALE  \\\n",
       "86628                                         0   \n",
       "68428                                         0   \n",
       "15652                                         0   \n",
       "94489                                         0   \n",
       "43072                                         0   \n",
       "...                                         ...   \n",
       "89336                                         0   \n",
       "48305                                         1   \n",
       "15203                                         0   \n",
       "36915                                         0   \n",
       "111462                                        0   \n",
       "\n",
       "        DESCRIZIONE PRODOTTO_SPESE MEDICHE  DESCRIZIONE PRODOTTO_TEMPO LIBERO  \n",
       "86628                                    1                                  0  \n",
       "68428                                    0                                  0  \n",
       "15652                                    0                                  0  \n",
       "94489                                    0                                  0  \n",
       "43072                                    0                                  0  \n",
       "...                                    ...                                ...  \n",
       "89336                                    0                                  0  \n",
       "48305                                    0                                  0  \n",
       "15203                                    0                                  0  \n",
       "36915                                    0                                  0  \n",
       "111462                                   0                                  0  \n",
       "\n",
       "[112598 rows x 41 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86628     0\n",
       "68428     0\n",
       "15652     0\n",
       "94489     0\n",
       "43072     0\n",
       "         ..\n",
       "89336     0\n",
       "48305     0\n",
       "15203     1\n",
       "36915     0\n",
       "111462    0\n",
       "Name: Target_95, Length: 112598, dtype: int32"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Eta_Debitore</th>\n",
       "      <th>RATIO_ONERI_AFF_TOT_AFF</th>\n",
       "      <th>AgeingGestioneGg</th>\n",
       "      <th>NUMERO RATE</th>\n",
       "      <th>Ratio_Rate_Imp1</th>\n",
       "      <th>TOT_AFF</th>\n",
       "      <th>AgeingErogazioneGg</th>\n",
       "      <th>IMP_FINANZIATO</th>\n",
       "      <th>Denominazione Regione_Basilicata</th>\n",
       "      <th>Denominazione Regione_Calabria</th>\n",
       "      <th>Denominazione Regione_Campania</th>\n",
       "      <th>Denominazione Regione_Emilia-Romagna</th>\n",
       "      <th>Denominazione Regione_Friuli-Venezia Giulia</th>\n",
       "      <th>Denominazione Regione_Lazio</th>\n",
       "      <th>Denominazione Regione_Liguria</th>\n",
       "      <th>Denominazione Regione_Lombardia</th>\n",
       "      <th>Denominazione Regione_Marche</th>\n",
       "      <th>Denominazione Regione_Missing</th>\n",
       "      <th>Denominazione Regione_Molise</th>\n",
       "      <th>Denominazione Regione_Piemonte</th>\n",
       "      <th>Denominazione Regione_Puglia</th>\n",
       "      <th>Denominazione Regione_Sardegna</th>\n",
       "      <th>Denominazione Regione_Sicilia</th>\n",
       "      <th>Denominazione Regione_Toscana</th>\n",
       "      <th>Denominazione Regione_Trentino-Alto Adige/Südtirol</th>\n",
       "      <th>Denominazione Regione_Umbria</th>\n",
       "      <th>Denominazione Regione_Valle d'Aosta/Vallée d'Aoste</th>\n",
       "      <th>Denominazione Regione_Veneto</th>\n",
       "      <th>DESCRIZIONE PRODOTTO_ARREDAMENTO</th>\n",
       "      <th>DESCRIZIONE PRODOTTO_AUTOMOTIVE NUOVO</th>\n",
       "      <th>DESCRIZIONE PRODOTTO_AUTOMOTIVE USATO</th>\n",
       "      <th>DESCRIZIONE PRODOTTO_CONSOLIDAMENTO DEL DEBITO</th>\n",
       "      <th>DESCRIZIONE PRODOTTO_ELETTRONICA &amp; ELETTRODOME</th>\n",
       "      <th>DESCRIZIONE PRODOTTO_FINANZIAMENTI AZIENDE</th>\n",
       "      <th>DESCRIZIONE PRODOTTO_INTERVENTI CASA</th>\n",
       "      <th>DESCRIZIONE PRODOTTO_MOTO E CICLOMOTORI NUOVO</th>\n",
       "      <th>DESCRIZIONE PRODOTTO_MOTO E CICLOMOTORI USATO</th>\n",
       "      <th>DESCRIZIONE PRODOTTO_PRESTITI CON TRATTENUTA</th>\n",
       "      <th>DESCRIZIONE PRODOTTO_PRESTITO PERSONALE</th>\n",
       "      <th>DESCRIZIONE PRODOTTO_SPESE MEDICHE</th>\n",
       "      <th>DESCRIZIONE PRODOTTO_TEMPO LIBERO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>82759</th>\n",
       "      <td>35.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>15.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.47</td>\n",
       "      <td>54.86</td>\n",
       "      <td>1,034.00</td>\n",
       "      <td>3,030.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58997</th>\n",
       "      <td>58.00</td>\n",
       "      <td>0.08</td>\n",
       "      <td>15.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.69</td>\n",
       "      <td>154.43</td>\n",
       "      <td>3,163.00</td>\n",
       "      <td>12,400.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23854</th>\n",
       "      <td>36.00</td>\n",
       "      <td>0.04</td>\n",
       "      <td>29.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0.32</td>\n",
       "      <td>290.34</td>\n",
       "      <td>1,156.00</td>\n",
       "      <td>22,248.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24632</th>\n",
       "      <td>51.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>31.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0.08</td>\n",
       "      <td>590.92</td>\n",
       "      <td>213.00</td>\n",
       "      <td>17,751.92</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25894</th>\n",
       "      <td>32.00</td>\n",
       "      <td>0.04</td>\n",
       "      <td>27.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0.32</td>\n",
       "      <td>287.25</td>\n",
       "      <td>823.00</td>\n",
       "      <td>18,850.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65390</th>\n",
       "      <td>57.00</td>\n",
       "      <td>0.05</td>\n",
       "      <td>15.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.75</td>\n",
       "      <td>234.01</td>\n",
       "      <td>1,642.00</td>\n",
       "      <td>11,312.73</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65798</th>\n",
       "      <td>51.00</td>\n",
       "      <td>0.05</td>\n",
       "      <td>14.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.90</td>\n",
       "      <td>244.60</td>\n",
       "      <td>1,642.00</td>\n",
       "      <td>10,630.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11636</th>\n",
       "      <td>71.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>30.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0.68</td>\n",
       "      <td>760.93</td>\n",
       "      <td>1,248.00</td>\n",
       "      <td>31,256.39</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79021</th>\n",
       "      <td>56.00</td>\n",
       "      <td>0.34</td>\n",
       "      <td>15.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.17</td>\n",
       "      <td>116.98</td>\n",
       "      <td>61.00</td>\n",
       "      <td>900.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21712</th>\n",
       "      <td>38.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>31.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0.58</td>\n",
       "      <td>488.21</td>\n",
       "      <td>1,491.00</td>\n",
       "      <td>13,914.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12511 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Eta_Debitore  RATIO_ONERI_AFF_TOT_AFF  AgeingGestioneGg  NUMERO RATE  \\\n",
       "82759         35.00                     0.00             15.00            1   \n",
       "58997         58.00                     0.08             15.00            1   \n",
       "23854         36.00                     0.04             29.00            2   \n",
       "24632         51.00                     0.00             31.00            2   \n",
       "25894         32.00                     0.04             27.00            2   \n",
       "...             ...                      ...               ...          ...   \n",
       "65390         57.00                     0.05             15.00            1   \n",
       "65798         51.00                     0.05             14.00            1   \n",
       "11636         71.00                     0.02             30.00            2   \n",
       "79021         56.00                     0.34             15.00            1   \n",
       "21712         38.00                     0.02             31.00            2   \n",
       "\n",
       "       Ratio_Rate_Imp1  TOT_AFF  AgeingErogazioneGg  IMP_FINANZIATO  \\\n",
       "82759             0.47    54.86            1,034.00        3,030.00   \n",
       "58997             0.69   154.43            3,163.00       12,400.00   \n",
       "23854             0.32   290.34            1,156.00       22,248.00   \n",
       "24632             0.08   590.92              213.00       17,751.92   \n",
       "25894             0.32   287.25              823.00       18,850.00   \n",
       "...                ...      ...                 ...             ...   \n",
       "65390             0.75   234.01            1,642.00       11,312.73   \n",
       "65798             0.90   244.60            1,642.00       10,630.00   \n",
       "11636             0.68   760.93            1,248.00       31,256.39   \n",
       "79021             0.17   116.98               61.00          900.00   \n",
       "21712             0.58   488.21            1,491.00       13,914.00   \n",
       "\n",
       "       Denominazione Regione_Basilicata  Denominazione Regione_Calabria  \\\n",
       "82759                                 0                               0   \n",
       "58997                                 0                               0   \n",
       "23854                                 0                               0   \n",
       "24632                                 0                               0   \n",
       "25894                                 0                               0   \n",
       "...                                 ...                             ...   \n",
       "65390                                 0                               0   \n",
       "65798                                 0                               0   \n",
       "11636                                 0                               0   \n",
       "79021                                 0                               0   \n",
       "21712                                 0                               0   \n",
       "\n",
       "       Denominazione Regione_Campania  Denominazione Regione_Emilia-Romagna  \\\n",
       "82759                               0                                     0   \n",
       "58997                               0                                     0   \n",
       "23854                               1                                     0   \n",
       "24632                               0                                     0   \n",
       "25894                               0                                     0   \n",
       "...                               ...                                   ...   \n",
       "65390                               0                                     1   \n",
       "65798                               0                                     0   \n",
       "11636                               1                                     0   \n",
       "79021                               0                                     0   \n",
       "21712                               0                                     0   \n",
       "\n",
       "       Denominazione Regione_Friuli-Venezia Giulia  \\\n",
       "82759                                            0   \n",
       "58997                                            0   \n",
       "23854                                            0   \n",
       "24632                                            0   \n",
       "25894                                            0   \n",
       "...                                            ...   \n",
       "65390                                            0   \n",
       "65798                                            0   \n",
       "11636                                            0   \n",
       "79021                                            0   \n",
       "21712                                            0   \n",
       "\n",
       "       Denominazione Regione_Lazio  Denominazione Regione_Liguria  \\\n",
       "82759                            0                              0   \n",
       "58997                            0                              0   \n",
       "23854                            0                              0   \n",
       "24632                            1                              0   \n",
       "25894                            0                              0   \n",
       "...                            ...                            ...   \n",
       "65390                            0                              0   \n",
       "65798                            0                              0   \n",
       "11636                            0                              0   \n",
       "79021                            0                              1   \n",
       "21712                            1                              0   \n",
       "\n",
       "       Denominazione Regione_Lombardia  Denominazione Regione_Marche  \\\n",
       "82759                                1                             0   \n",
       "58997                                0                             0   \n",
       "23854                                0                             0   \n",
       "24632                                0                             0   \n",
       "25894                                1                             0   \n",
       "...                                ...                           ...   \n",
       "65390                                0                             0   \n",
       "65798                                1                             0   \n",
       "11636                                0                             0   \n",
       "79021                                0                             0   \n",
       "21712                                0                             0   \n",
       "\n",
       "       Denominazione Regione_Missing  Denominazione Regione_Molise  \\\n",
       "82759                              0                             0   \n",
       "58997                              0                             0   \n",
       "23854                              0                             0   \n",
       "24632                              0                             0   \n",
       "25894                              0                             0   \n",
       "...                              ...                           ...   \n",
       "65390                              0                             0   \n",
       "65798                              0                             0   \n",
       "11636                              0                             0   \n",
       "79021                              0                             0   \n",
       "21712                              0                             0   \n",
       "\n",
       "       Denominazione Regione_Piemonte  Denominazione Regione_Puglia  \\\n",
       "82759                               0                             0   \n",
       "58997                               0                             0   \n",
       "23854                               0                             0   \n",
       "24632                               0                             0   \n",
       "25894                               0                             0   \n",
       "...                               ...                           ...   \n",
       "65390                               0                             0   \n",
       "65798                               0                             0   \n",
       "11636                               0                             0   \n",
       "79021                               0                             0   \n",
       "21712                               0                             0   \n",
       "\n",
       "       Denominazione Regione_Sardegna  Denominazione Regione_Sicilia  \\\n",
       "82759                               0                              0   \n",
       "58997                               0                              0   \n",
       "23854                               0                              0   \n",
       "24632                               0                              0   \n",
       "25894                               0                              0   \n",
       "...                               ...                            ...   \n",
       "65390                               0                              0   \n",
       "65798                               0                              0   \n",
       "11636                               0                              0   \n",
       "79021                               0                              0   \n",
       "21712                               0                              0   \n",
       "\n",
       "       Denominazione Regione_Toscana  \\\n",
       "82759                              0   \n",
       "58997                              0   \n",
       "23854                              0   \n",
       "24632                              0   \n",
       "25894                              0   \n",
       "...                              ...   \n",
       "65390                              0   \n",
       "65798                              0   \n",
       "11636                              0   \n",
       "79021                              0   \n",
       "21712                              0   \n",
       "\n",
       "       Denominazione Regione_Trentino-Alto Adige/Südtirol  \\\n",
       "82759                                                  0    \n",
       "58997                                                  0    \n",
       "23854                                                  0    \n",
       "24632                                                  0    \n",
       "25894                                                  0    \n",
       "...                                                  ...    \n",
       "65390                                                  0    \n",
       "65798                                                  0    \n",
       "11636                                                  0    \n",
       "79021                                                  0    \n",
       "21712                                                  0    \n",
       "\n",
       "       Denominazione Regione_Umbria  \\\n",
       "82759                             0   \n",
       "58997                             1   \n",
       "23854                             0   \n",
       "24632                             0   \n",
       "25894                             0   \n",
       "...                             ...   \n",
       "65390                             0   \n",
       "65798                             0   \n",
       "11636                             0   \n",
       "79021                             0   \n",
       "21712                             0   \n",
       "\n",
       "       Denominazione Regione_Valle d'Aosta/Vallée d'Aoste  \\\n",
       "82759                                                  0    \n",
       "58997                                                  0    \n",
       "23854                                                  0    \n",
       "24632                                                  0    \n",
       "25894                                                  0    \n",
       "...                                                  ...    \n",
       "65390                                                  0    \n",
       "65798                                                  0    \n",
       "11636                                                  0    \n",
       "79021                                                  0    \n",
       "21712                                                  0    \n",
       "\n",
       "       Denominazione Regione_Veneto  DESCRIZIONE PRODOTTO_ARREDAMENTO  \\\n",
       "82759                             0                                 0   \n",
       "58997                             0                                 0   \n",
       "23854                             0                                 0   \n",
       "24632                             0                                 0   \n",
       "25894                             0                                 0   \n",
       "...                             ...                               ...   \n",
       "65390                             0                                 0   \n",
       "65798                             0                                 0   \n",
       "11636                             0                                 0   \n",
       "79021                             0                                 1   \n",
       "21712                             0                                 0   \n",
       "\n",
       "       DESCRIZIONE PRODOTTO_AUTOMOTIVE NUOVO  \\\n",
       "82759                                      0   \n",
       "58997                                      0   \n",
       "23854                                      0   \n",
       "24632                                      0   \n",
       "25894                                      0   \n",
       "...                                      ...   \n",
       "65390                                      0   \n",
       "65798                                      0   \n",
       "11636                                      1   \n",
       "79021                                      0   \n",
       "21712                                      1   \n",
       "\n",
       "       DESCRIZIONE PRODOTTO_AUTOMOTIVE USATO  \\\n",
       "82759                                      0   \n",
       "58997                                      0   \n",
       "23854                                      0   \n",
       "24632                                      0   \n",
       "25894                                      1   \n",
       "...                                      ...   \n",
       "65390                                      0   \n",
       "65798                                      1   \n",
       "11636                                      0   \n",
       "79021                                      0   \n",
       "21712                                      0   \n",
       "\n",
       "       DESCRIZIONE PRODOTTO_CONSOLIDAMENTO DEL DEBITO  \\\n",
       "82759                                               0   \n",
       "58997                                               0   \n",
       "23854                                               0   \n",
       "24632                                               0   \n",
       "25894                                               0   \n",
       "...                                               ...   \n",
       "65390                                               0   \n",
       "65798                                               0   \n",
       "11636                                               0   \n",
       "79021                                               0   \n",
       "21712                                               0   \n",
       "\n",
       "       DESCRIZIONE PRODOTTO_ELETTRONICA & ELETTRODOME  \\\n",
       "82759                                               0   \n",
       "58997                                               0   \n",
       "23854                                               0   \n",
       "24632                                               0   \n",
       "25894                                               0   \n",
       "...                                               ...   \n",
       "65390                                               0   \n",
       "65798                                               0   \n",
       "11636                                               0   \n",
       "79021                                               0   \n",
       "21712                                               0   \n",
       "\n",
       "       DESCRIZIONE PRODOTTO_FINANZIAMENTI AZIENDE  \\\n",
       "82759                                           0   \n",
       "58997                                           0   \n",
       "23854                                           1   \n",
       "24632                                           1   \n",
       "25894                                           0   \n",
       "...                                           ...   \n",
       "65390                                           0   \n",
       "65798                                           0   \n",
       "11636                                           0   \n",
       "79021                                           0   \n",
       "21712                                           0   \n",
       "\n",
       "       DESCRIZIONE PRODOTTO_INTERVENTI CASA  \\\n",
       "82759                                     0   \n",
       "58997                                     1   \n",
       "23854                                     0   \n",
       "24632                                     0   \n",
       "25894                                     0   \n",
       "...                                     ...   \n",
       "65390                                     0   \n",
       "65798                                     0   \n",
       "11636                                     0   \n",
       "79021                                     0   \n",
       "21712                                     0   \n",
       "\n",
       "       DESCRIZIONE PRODOTTO_MOTO E CICLOMOTORI NUOVO  \\\n",
       "82759                                              0   \n",
       "58997                                              0   \n",
       "23854                                              0   \n",
       "24632                                              0   \n",
       "25894                                              0   \n",
       "...                                              ...   \n",
       "65390                                              0   \n",
       "65798                                              0   \n",
       "11636                                              0   \n",
       "79021                                              0   \n",
       "21712                                              0   \n",
       "\n",
       "       DESCRIZIONE PRODOTTO_MOTO E CICLOMOTORI USATO  \\\n",
       "82759                                              0   \n",
       "58997                                              0   \n",
       "23854                                              0   \n",
       "24632                                              0   \n",
       "25894                                              0   \n",
       "...                                              ...   \n",
       "65390                                              0   \n",
       "65798                                              0   \n",
       "11636                                              0   \n",
       "79021                                              0   \n",
       "21712                                              0   \n",
       "\n",
       "       DESCRIZIONE PRODOTTO_PRESTITI CON TRATTENUTA  \\\n",
       "82759                                             0   \n",
       "58997                                             0   \n",
       "23854                                             0   \n",
       "24632                                             0   \n",
       "25894                                             0   \n",
       "...                                             ...   \n",
       "65390                                             0   \n",
       "65798                                             0   \n",
       "11636                                             0   \n",
       "79021                                             0   \n",
       "21712                                             0   \n",
       "\n",
       "       DESCRIZIONE PRODOTTO_PRESTITO PERSONALE  \\\n",
       "82759                                        0   \n",
       "58997                                        0   \n",
       "23854                                        0   \n",
       "24632                                        0   \n",
       "25894                                        0   \n",
       "...                                        ...   \n",
       "65390                                        1   \n",
       "65798                                        0   \n",
       "11636                                        0   \n",
       "79021                                        0   \n",
       "21712                                        0   \n",
       "\n",
       "       DESCRIZIONE PRODOTTO_SPESE MEDICHE  DESCRIZIONE PRODOTTO_TEMPO LIBERO  \n",
       "82759                                   1                                  0  \n",
       "58997                                   0                                  0  \n",
       "23854                                   0                                  0  \n",
       "24632                                   0                                  0  \n",
       "25894                                   0                                  0  \n",
       "...                                   ...                                ...  \n",
       "65390                                   0                                  0  \n",
       "65798                                   0                                  0  \n",
       "11636                                   0                                  0  \n",
       "79021                                   0                                  0  \n",
       "21712                                   0                                  0  \n",
       "\n",
       "[12511 rows x 41 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82759    1\n",
       "58997    0\n",
       "23854    0\n",
       "24632    0\n",
       "25894    0\n",
       "        ..\n",
       "65390    0\n",
       "65798    1\n",
       "11636    1\n",
       "79021    0\n",
       "21712    0\n",
       "Name: Target_95, Length: 12511, dtype: int32"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Eta_Debitore</th>\n",
       "      <th>RATIO_ONERI_AFF_TOT_AFF</th>\n",
       "      <th>AgeingGestioneGg</th>\n",
       "      <th>NUMERO RATE</th>\n",
       "      <th>Ratio_Rate_Imp1</th>\n",
       "      <th>TOT_AFF</th>\n",
       "      <th>AgeingErogazioneGg</th>\n",
       "      <th>IMP_FINANZIATO</th>\n",
       "      <th>Denominazione Regione_Basilicata</th>\n",
       "      <th>Denominazione Regione_Calabria</th>\n",
       "      <th>Denominazione Regione_Campania</th>\n",
       "      <th>Denominazione Regione_Emilia-Romagna</th>\n",
       "      <th>Denominazione Regione_Friuli-Venezia Giulia</th>\n",
       "      <th>Denominazione Regione_Lazio</th>\n",
       "      <th>Denominazione Regione_Liguria</th>\n",
       "      <th>Denominazione Regione_Lombardia</th>\n",
       "      <th>Denominazione Regione_Marche</th>\n",
       "      <th>Denominazione Regione_Missing</th>\n",
       "      <th>Denominazione Regione_Molise</th>\n",
       "      <th>Denominazione Regione_Piemonte</th>\n",
       "      <th>Denominazione Regione_Puglia</th>\n",
       "      <th>Denominazione Regione_Sardegna</th>\n",
       "      <th>Denominazione Regione_Sicilia</th>\n",
       "      <th>Denominazione Regione_Toscana</th>\n",
       "      <th>Denominazione Regione_Trentino-Alto Adige/Südtirol</th>\n",
       "      <th>Denominazione Regione_Umbria</th>\n",
       "      <th>Denominazione Regione_Valle d'Aosta/Vallée d'Aoste</th>\n",
       "      <th>Denominazione Regione_Veneto</th>\n",
       "      <th>DESCRIZIONE PRODOTTO_ARREDAMENTO</th>\n",
       "      <th>DESCRIZIONE PRODOTTO_AUTOMOTIVE NUOVO</th>\n",
       "      <th>DESCRIZIONE PRODOTTO_AUTOMOTIVE USATO</th>\n",
       "      <th>DESCRIZIONE PRODOTTO_CONSOLIDAMENTO DEL DEBITO</th>\n",
       "      <th>DESCRIZIONE PRODOTTO_ELETTRONICA &amp; ELETTRODOME</th>\n",
       "      <th>DESCRIZIONE PRODOTTO_FINANZIAMENTI AZIENDE</th>\n",
       "      <th>DESCRIZIONE PRODOTTO_INTERVENTI CASA</th>\n",
       "      <th>DESCRIZIONE PRODOTTO_MOTO E CICLOMOTORI NUOVO</th>\n",
       "      <th>DESCRIZIONE PRODOTTO_MOTO E CICLOMOTORI USATO</th>\n",
       "      <th>DESCRIZIONE PRODOTTO_PRESTITI CON TRATTENUTA</th>\n",
       "      <th>DESCRIZIONE PRODOTTO_PRESTITO PERSONALE</th>\n",
       "      <th>DESCRIZIONE PRODOTTO_SPESE MEDICHE</th>\n",
       "      <th>DESCRIZIONE PRODOTTO_TEMPO LIBERO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>78176</th>\n",
       "      <td>55.00</td>\n",
       "      <td>0.06</td>\n",
       "      <td>17.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.92</td>\n",
       "      <td>201.13</td>\n",
       "      <td>1,338.00</td>\n",
       "      <td>6,936.93</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118480</th>\n",
       "      <td>51.00</td>\n",
       "      <td>0.04</td>\n",
       "      <td>15.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.44</td>\n",
       "      <td>290.37</td>\n",
       "      <td>1,126.00</td>\n",
       "      <td>18,300.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56500</th>\n",
       "      <td>46.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>17.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0.15</td>\n",
       "      <td>179.68</td>\n",
       "      <td>334.00</td>\n",
       "      <td>10,540.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139621</th>\n",
       "      <td>54.00</td>\n",
       "      <td>0.18</td>\n",
       "      <td>17.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.08</td>\n",
       "      <td>134.39</td>\n",
       "      <td>30.00</td>\n",
       "      <td>1,067.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7678</th>\n",
       "      <td>24.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>29.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0.93</td>\n",
       "      <td>25.35</td>\n",
       "      <td>425.00</td>\n",
       "      <td>1,725.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32260</th>\n",
       "      <td>72.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>15.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0.25</td>\n",
       "      <td>54.33</td>\n",
       "      <td>554.00</td>\n",
       "      <td>3,000.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110596</th>\n",
       "      <td>77.00</td>\n",
       "      <td>0.11</td>\n",
       "      <td>16.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.61</td>\n",
       "      <td>110.95</td>\n",
       "      <td>1,369.00</td>\n",
       "      <td>5,900.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42062</th>\n",
       "      <td>51.00</td>\n",
       "      <td>0.19</td>\n",
       "      <td>29.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0.03</td>\n",
       "      <td>111.41</td>\n",
       "      <td>61.00</td>\n",
       "      <td>4,632.24</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35981</th>\n",
       "      <td>38.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-580.00</td>\n",
       "      <td>3</td>\n",
       "      <td>0.16</td>\n",
       "      <td>370.02</td>\n",
       "      <td>1,552.00</td>\n",
       "      <td>16,670.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23823</th>\n",
       "      <td>45.00</td>\n",
       "      <td>0.11</td>\n",
       "      <td>26.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0.40</td>\n",
       "      <td>104.88</td>\n",
       "      <td>577.00</td>\n",
       "      <td>3,525.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13901 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Eta_Debitore  RATIO_ONERI_AFF_TOT_AFF  AgeingGestioneGg  NUMERO RATE  \\\n",
       "78176          55.00                     0.06             17.00            1   \n",
       "118480         51.00                     0.04             15.00            1   \n",
       "56500          46.00                     0.00             17.00            2   \n",
       "139621         54.00                     0.18             17.00            1   \n",
       "7678           24.00                     0.00             29.00            2   \n",
       "...              ...                      ...               ...          ...   \n",
       "32260          72.00                     0.00             15.00            2   \n",
       "110596         77.00                     0.11             16.00            1   \n",
       "42062          51.00                     0.19             29.00            2   \n",
       "35981          38.00                     0.03           -580.00            3   \n",
       "23823          45.00                     0.11             26.00            2   \n",
       "\n",
       "        Ratio_Rate_Imp1  TOT_AFF  AgeingErogazioneGg  IMP_FINANZIATO  \\\n",
       "78176              0.92   201.13            1,338.00        6,936.93   \n",
       "118480             0.44   290.37            1,126.00       18,300.00   \n",
       "56500              0.15   179.68              334.00       10,540.00   \n",
       "139621             0.08   134.39               30.00        1,067.00   \n",
       "7678               0.93    25.35              425.00        1,725.00   \n",
       "...                 ...      ...                 ...             ...   \n",
       "32260              0.25    54.33              554.00        3,000.00   \n",
       "110596             0.61   110.95            1,369.00        5,900.00   \n",
       "42062              0.03   111.41               61.00        4,632.24   \n",
       "35981              0.16   370.02            1,552.00       16,670.00   \n",
       "23823              0.40   104.88              577.00        3,525.00   \n",
       "\n",
       "        Denominazione Regione_Basilicata  Denominazione Regione_Calabria  \\\n",
       "78176                                  0                               1   \n",
       "118480                                 0                               0   \n",
       "56500                                  0                               0   \n",
       "139621                                 0                               0   \n",
       "7678                                   0                               0   \n",
       "...                                  ...                             ...   \n",
       "32260                                  0                               0   \n",
       "110596                                 0                               0   \n",
       "42062                                  0                               0   \n",
       "35981                                  0                               0   \n",
       "23823                                  0                               0   \n",
       "\n",
       "        Denominazione Regione_Campania  Denominazione Regione_Emilia-Romagna  \\\n",
       "78176                                0                                     0   \n",
       "118480                               0                                     0   \n",
       "56500                                0                                     0   \n",
       "139621                               0                                     0   \n",
       "7678                                 0                                     0   \n",
       "...                                ...                                   ...   \n",
       "32260                                1                                     0   \n",
       "110596                               0                                     0   \n",
       "42062                                0                                     0   \n",
       "35981                                0                                     0   \n",
       "23823                                0                                     0   \n",
       "\n",
       "        Denominazione Regione_Friuli-Venezia Giulia  \\\n",
       "78176                                             0   \n",
       "118480                                            0   \n",
       "56500                                             0   \n",
       "139621                                            0   \n",
       "7678                                              0   \n",
       "...                                             ...   \n",
       "32260                                             0   \n",
       "110596                                            0   \n",
       "42062                                             0   \n",
       "35981                                             0   \n",
       "23823                                             0   \n",
       "\n",
       "        Denominazione Regione_Lazio  Denominazione Regione_Liguria  \\\n",
       "78176                             0                              0   \n",
       "118480                            0                              0   \n",
       "56500                             1                              0   \n",
       "139621                            0                              0   \n",
       "7678                              0                              1   \n",
       "...                             ...                            ...   \n",
       "32260                             0                              0   \n",
       "110596                            0                              0   \n",
       "42062                             1                              0   \n",
       "35981                             0                              0   \n",
       "23823                             0                              0   \n",
       "\n",
       "        Denominazione Regione_Lombardia  Denominazione Regione_Marche  \\\n",
       "78176                                 0                             0   \n",
       "118480                                1                             0   \n",
       "56500                                 0                             0   \n",
       "139621                                0                             0   \n",
       "7678                                  0                             0   \n",
       "...                                 ...                           ...   \n",
       "32260                                 0                             0   \n",
       "110596                                0                             0   \n",
       "42062                                 0                             0   \n",
       "35981                                 1                             0   \n",
       "23823                                 0                             0   \n",
       "\n",
       "        Denominazione Regione_Missing  Denominazione Regione_Molise  \\\n",
       "78176                               0                             0   \n",
       "118480                              0                             0   \n",
       "56500                               0                             0   \n",
       "139621                              0                             0   \n",
       "7678                                0                             0   \n",
       "...                               ...                           ...   \n",
       "32260                               0                             0   \n",
       "110596                              0                             0   \n",
       "42062                               0                             0   \n",
       "35981                               0                             0   \n",
       "23823                               0                             0   \n",
       "\n",
       "        Denominazione Regione_Piemonte  Denominazione Regione_Puglia  \\\n",
       "78176                                0                             0   \n",
       "118480                               0                             0   \n",
       "56500                                0                             0   \n",
       "139621                               0                             0   \n",
       "7678                                 0                             0   \n",
       "...                                ...                           ...   \n",
       "32260                                0                             0   \n",
       "110596                               1                             0   \n",
       "42062                                0                             0   \n",
       "35981                                0                             0   \n",
       "23823                                0                             0   \n",
       "\n",
       "        Denominazione Regione_Sardegna  Denominazione Regione_Sicilia  \\\n",
       "78176                                0                              0   \n",
       "118480                               0                              0   \n",
       "56500                                0                              0   \n",
       "139621                               0                              0   \n",
       "7678                                 0                              0   \n",
       "...                                ...                            ...   \n",
       "32260                                0                              0   \n",
       "110596                               0                              0   \n",
       "42062                                0                              0   \n",
       "35981                                0                              0   \n",
       "23823                                0                              1   \n",
       "\n",
       "        Denominazione Regione_Toscana  \\\n",
       "78176                               0   \n",
       "118480                              0   \n",
       "56500                               0   \n",
       "139621                              1   \n",
       "7678                                0   \n",
       "...                               ...   \n",
       "32260                               0   \n",
       "110596                              0   \n",
       "42062                               0   \n",
       "35981                               0   \n",
       "23823                               0   \n",
       "\n",
       "        Denominazione Regione_Trentino-Alto Adige/Südtirol  \\\n",
       "78176                                                   0    \n",
       "118480                                                  0    \n",
       "56500                                                   0    \n",
       "139621                                                  0    \n",
       "7678                                                    0    \n",
       "...                                                   ...    \n",
       "32260                                                   0    \n",
       "110596                                                  0    \n",
       "42062                                                   0    \n",
       "35981                                                   0    \n",
       "23823                                                   0    \n",
       "\n",
       "        Denominazione Regione_Umbria  \\\n",
       "78176                              0   \n",
       "118480                             0   \n",
       "56500                              0   \n",
       "139621                             0   \n",
       "7678                               0   \n",
       "...                              ...   \n",
       "32260                              0   \n",
       "110596                             0   \n",
       "42062                              0   \n",
       "35981                              0   \n",
       "23823                              0   \n",
       "\n",
       "        Denominazione Regione_Valle d'Aosta/Vallée d'Aoste  \\\n",
       "78176                                                   0    \n",
       "118480                                                  0    \n",
       "56500                                                   0    \n",
       "139621                                                  0    \n",
       "7678                                                    0    \n",
       "...                                                   ...    \n",
       "32260                                                   0    \n",
       "110596                                                  0    \n",
       "42062                                                   0    \n",
       "35981                                                   0    \n",
       "23823                                                   0    \n",
       "\n",
       "        Denominazione Regione_Veneto  DESCRIZIONE PRODOTTO_ARREDAMENTO  \\\n",
       "78176                              0                                 0   \n",
       "118480                             0                                 0   \n",
       "56500                              0                                 0   \n",
       "139621                             0                                 0   \n",
       "7678                               0                                 0   \n",
       "...                              ...                               ...   \n",
       "32260                              0                                 0   \n",
       "110596                             0                                 0   \n",
       "42062                              0                                 0   \n",
       "35981                              0                                 0   \n",
       "23823                              0                                 1   \n",
       "\n",
       "        DESCRIZIONE PRODOTTO_AUTOMOTIVE NUOVO  \\\n",
       "78176                                       0   \n",
       "118480                                      1   \n",
       "56500                                       0   \n",
       "139621                                      0   \n",
       "7678                                        0   \n",
       "...                                       ...   \n",
       "32260                                       0   \n",
       "110596                                      0   \n",
       "42062                                       0   \n",
       "35981                                       0   \n",
       "23823                                       0   \n",
       "\n",
       "        DESCRIZIONE PRODOTTO_AUTOMOTIVE USATO  \\\n",
       "78176                                       0   \n",
       "118480                                      0   \n",
       "56500                                       1   \n",
       "139621                                      0   \n",
       "7678                                        0   \n",
       "...                                       ...   \n",
       "32260                                       0   \n",
       "110596                                      0   \n",
       "42062                                       0   \n",
       "35981                                       0   \n",
       "23823                                       0   \n",
       "\n",
       "        DESCRIZIONE PRODOTTO_CONSOLIDAMENTO DEL DEBITO  \\\n",
       "78176                                                0   \n",
       "118480                                               0   \n",
       "56500                                                0   \n",
       "139621                                               0   \n",
       "7678                                                 0   \n",
       "...                                                ...   \n",
       "32260                                                0   \n",
       "110596                                               0   \n",
       "42062                                                0   \n",
       "35981                                                0   \n",
       "23823                                                0   \n",
       "\n",
       "        DESCRIZIONE PRODOTTO_ELETTRONICA & ELETTRODOME  \\\n",
       "78176                                                0   \n",
       "118480                                               0   \n",
       "56500                                                0   \n",
       "139621                                               0   \n",
       "7678                                                 0   \n",
       "...                                                ...   \n",
       "32260                                                0   \n",
       "110596                                               0   \n",
       "42062                                                0   \n",
       "35981                                                0   \n",
       "23823                                                0   \n",
       "\n",
       "        DESCRIZIONE PRODOTTO_FINANZIAMENTI AZIENDE  \\\n",
       "78176                                            1   \n",
       "118480                                           0   \n",
       "56500                                            0   \n",
       "139621                                           0   \n",
       "7678                                             0   \n",
       "...                                            ...   \n",
       "32260                                            0   \n",
       "110596                                           0   \n",
       "42062                                            0   \n",
       "35981                                            0   \n",
       "23823                                            0   \n",
       "\n",
       "        DESCRIZIONE PRODOTTO_INTERVENTI CASA  \\\n",
       "78176                                      0   \n",
       "118480                                     0   \n",
       "56500                                      0   \n",
       "139621                                     0   \n",
       "7678                                       0   \n",
       "...                                      ...   \n",
       "32260                                      0   \n",
       "110596                                     0   \n",
       "42062                                      0   \n",
       "35981                                      0   \n",
       "23823                                      0   \n",
       "\n",
       "        DESCRIZIONE PRODOTTO_MOTO E CICLOMOTORI NUOVO  \\\n",
       "78176                                               0   \n",
       "118480                                              0   \n",
       "56500                                               0   \n",
       "139621                                              0   \n",
       "7678                                                0   \n",
       "...                                               ...   \n",
       "32260                                               0   \n",
       "110596                                              0   \n",
       "42062                                               1   \n",
       "35981                                               0   \n",
       "23823                                               0   \n",
       "\n",
       "        DESCRIZIONE PRODOTTO_MOTO E CICLOMOTORI USATO  \\\n",
       "78176                                               0   \n",
       "118480                                              0   \n",
       "56500                                               0   \n",
       "139621                                              0   \n",
       "7678                                                0   \n",
       "...                                               ...   \n",
       "32260                                               0   \n",
       "110596                                              0   \n",
       "42062                                               0   \n",
       "35981                                               0   \n",
       "23823                                               0   \n",
       "\n",
       "        DESCRIZIONE PRODOTTO_PRESTITI CON TRATTENUTA  \\\n",
       "78176                                              0   \n",
       "118480                                             0   \n",
       "56500                                              0   \n",
       "139621                                             0   \n",
       "7678                                               0   \n",
       "...                                              ...   \n",
       "32260                                              0   \n",
       "110596                                             0   \n",
       "42062                                              0   \n",
       "35981                                              0   \n",
       "23823                                              0   \n",
       "\n",
       "        DESCRIZIONE PRODOTTO_PRESTITO PERSONALE  \\\n",
       "78176                                         0   \n",
       "118480                                        0   \n",
       "56500                                         0   \n",
       "139621                                        0   \n",
       "7678                                          0   \n",
       "...                                         ...   \n",
       "32260                                         0   \n",
       "110596                                        0   \n",
       "42062                                         0   \n",
       "35981                                         1   \n",
       "23823                                         0   \n",
       "\n",
       "        DESCRIZIONE PRODOTTO_SPESE MEDICHE  DESCRIZIONE PRODOTTO_TEMPO LIBERO  \n",
       "78176                                    0                                  0  \n",
       "118480                                   0                                  0  \n",
       "56500                                    0                                  0  \n",
       "139621                                   0                                  1  \n",
       "7678                                     0                                  0  \n",
       "...                                    ...                                ...  \n",
       "32260                                    0                                  0  \n",
       "110596                                   1                                  0  \n",
       "42062                                    0                                  0  \n",
       "35981                                    0                                  0  \n",
       "23823                                    0                                  0  \n",
       "\n",
       "[13901 rows x 41 columns]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78176     1\n",
       "118480    1\n",
       "56500     1\n",
       "139621    1\n",
       "7678      1\n",
       "         ..\n",
       "32260     0\n",
       "110596    0\n",
       "42062     0\n",
       "35981     1\n",
       "23823     0\n",
       "Name: Target_95, Length: 13901, dtype: int32"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6> Grid search semplice </h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10000\n",
      "11260/11260 - 21s - loss: 34390.6836 - accuracy: 0.5270 - val_loss: 237.8782 - val_accuracy: 0.6059 - 21s/epoch - 2ms/step\n",
      "Epoch 2/10000\n",
      "11260/11260 - 18s - loss: 252.5667 - accuracy: 0.5708 - val_loss: 378.7124 - val_accuracy: 0.4554 - 18s/epoch - 2ms/step\n",
      "Epoch 3/10000\n",
      "11260/11260 - 18s - loss: 198.6163 - accuracy: 0.5737 - val_loss: 51.4697 - val_accuracy: 0.6478 - 18s/epoch - 2ms/step\n",
      "Epoch 4/10000\n",
      "11260/11260 - 17s - loss: 179.2727 - accuracy: 0.5763 - val_loss: 220.4094 - val_accuracy: 0.6159 - 17s/epoch - 2ms/step\n",
      "Epoch 5/10000\n",
      "11260/11260 - 18s - loss: 164.6054 - accuracy: 0.5797 - val_loss: 149.9419 - val_accuracy: 0.5296 - 18s/epoch - 2ms/step\n",
      "Epoch 6/10000\n",
      "11260/11260 - 20s - loss: 159.9972 - accuracy: 0.5790 - val_loss: 47.7317 - val_accuracy: 0.5794 - 20s/epoch - 2ms/step\n",
      "Epoch 7/10000\n",
      "11260/11260 - 19s - loss: 158.7990 - accuracy: 0.5799 - val_loss: 69.8702 - val_accuracy: 0.6347 - 19s/epoch - 2ms/step\n",
      "Epoch 8/10000\n",
      "11260/11260 - 15s - loss: 148.1020 - accuracy: 0.5803 - val_loss: 154.7886 - val_accuracy: 0.6252 - 15s/epoch - 1ms/step\n",
      "Epoch 9/10000\n",
      "11260/11260 - 16s - loss: 144.2188 - accuracy: 0.5798 - val_loss: 287.7018 - val_accuracy: 0.6183 - 16s/epoch - 1ms/step\n",
      "Epoch 10/10000\n",
      "11260/11260 - 17s - loss: 143.3672 - accuracy: 0.5785 - val_loss: 119.2865 - val_accuracy: 0.6190 - 17s/epoch - 1ms/step\n",
      "Epoch 11/10000\n",
      "11260/11260 - 17s - loss: 158.0755 - accuracy: 0.5805 - val_loss: 803.4454 - val_accuracy: 0.6131 - 17s/epoch - 2ms/step\n",
      "Epoch 12/10000\n",
      "11260/11260 - 17s - loss: 154.3991 - accuracy: 0.5822 - val_loss: 60.0875 - val_accuracy: 0.5390 - 17s/epoch - 2ms/step\n",
      "Epoch 13/10000\n",
      "11260/11260 - 17s - loss: 142.9785 - accuracy: 0.5801 - val_loss: 143.5534 - val_accuracy: 0.5262 - 17s/epoch - 1ms/step\n",
      "435/435 [==============================] - 1s 2ms/step - loss: 49.7958 - accuracy: 0.6429\n",
      "Larghezza: 50 \n",
      "Profondità: 1 \n",
      "Dimensione del batch: 10\n",
      "Test loss (binary_crossentropy): 49.7957649230957\n",
      "Test metrics (accuracy): 0.6429033875465393\n",
      "Epoch 1/10000\n",
      "1126/1126 - 2s - loss: 30380.5234 - accuracy: 0.5834 - val_loss: 6215.8354 - val_accuracy: 0.5906 - 2s/epoch - 2ms/step\n",
      "Epoch 2/10000\n",
      "1126/1126 - 2s - loss: 4549.3862 - accuracy: 0.5761 - val_loss: 3191.1689 - val_accuracy: 0.5774 - 2s/epoch - 2ms/step\n",
      "Epoch 3/10000\n",
      "1126/1126 - 2s - loss: 2254.0498 - accuracy: 0.5663 - val_loss: 1266.4301 - val_accuracy: 0.5638 - 2s/epoch - 2ms/step\n",
      "Epoch 4/10000\n",
      "1126/1126 - 2s - loss: 716.1651 - accuracy: 0.5618 - val_loss: 589.0158 - val_accuracy: 0.5056 - 2s/epoch - 2ms/step\n",
      "Epoch 5/10000\n",
      "1126/1126 - 2s - loss: 284.0710 - accuracy: 0.5862 - val_loss: 268.1823 - val_accuracy: 0.5512 - 2s/epoch - 2ms/step\n",
      "Epoch 6/10000\n",
      "1126/1126 - 2s - loss: 234.8551 - accuracy: 0.5864 - val_loss: 233.4872 - val_accuracy: 0.6128 - 2s/epoch - 2ms/step\n",
      "Epoch 7/10000\n",
      "1126/1126 - 2s - loss: 219.1349 - accuracy: 0.5860 - val_loss: 143.0214 - val_accuracy: 0.6049 - 2s/epoch - 2ms/step\n",
      "Epoch 8/10000\n",
      "1126/1126 - 2s - loss: 177.5577 - accuracy: 0.5887 - val_loss: 460.4684 - val_accuracy: 0.6151 - 2s/epoch - 2ms/step\n",
      "Epoch 9/10000\n",
      "1126/1126 - 2s - loss: 161.3240 - accuracy: 0.5863 - val_loss: 105.8961 - val_accuracy: 0.6251 - 2s/epoch - 2ms/step\n",
      "Epoch 10/10000\n",
      "1126/1126 - 2s - loss: 141.9520 - accuracy: 0.5882 - val_loss: 73.8641 - val_accuracy: 0.6288 - 2s/epoch - 2ms/step\n",
      "Epoch 11/10000\n",
      "1126/1126 - 2s - loss: 156.3038 - accuracy: 0.5851 - val_loss: 138.9179 - val_accuracy: 0.6141 - 2s/epoch - 2ms/step\n",
      "Epoch 12/10000\n",
      "1126/1126 - 2s - loss: 135.4382 - accuracy: 0.5875 - val_loss: 174.2393 - val_accuracy: 0.4924 - 2s/epoch - 2ms/step\n",
      "Epoch 13/10000\n",
      "1126/1126 - 2s - loss: 136.9803 - accuracy: 0.5882 - val_loss: 85.2941 - val_accuracy: 0.6221 - 2s/epoch - 2ms/step\n",
      "Epoch 14/10000\n",
      "1126/1126 - 2s - loss: 137.4675 - accuracy: 0.5844 - val_loss: 74.9231 - val_accuracy: 0.6295 - 2s/epoch - 2ms/step\n",
      "Epoch 15/10000\n",
      "1126/1126 - 3s - loss: 132.8021 - accuracy: 0.5859 - val_loss: 85.4427 - val_accuracy: 0.6247 - 3s/epoch - 2ms/step\n",
      "Epoch 16/10000\n",
      "1126/1126 - 2s - loss: 130.7608 - accuracy: 0.5840 - val_loss: 81.6980 - val_accuracy: 0.5896 - 2s/epoch - 2ms/step\n",
      "Epoch 17/10000\n",
      "1126/1126 - 2s - loss: 141.5181 - accuracy: 0.5859 - val_loss: 326.9955 - val_accuracy: 0.6167 - 2s/epoch - 2ms/step\n",
      "Epoch 18/10000\n",
      "1126/1126 - 2s - loss: 129.7768 - accuracy: 0.5841 - val_loss: 122.5024 - val_accuracy: 0.5694 - 2s/epoch - 2ms/step\n",
      "Epoch 19/10000\n",
      "1126/1126 - 2s - loss: 136.0697 - accuracy: 0.5859 - val_loss: 107.7762 - val_accuracy: 0.5211 - 2s/epoch - 2ms/step\n",
      "Epoch 20/10000\n",
      "1126/1126 - 2s - loss: 126.8376 - accuracy: 0.5869 - val_loss: 42.7144 - val_accuracy: 0.6151 - 2s/epoch - 2ms/step\n",
      "Epoch 21/10000\n",
      "1126/1126 - 2s - loss: 123.3034 - accuracy: 0.5845 - val_loss: 87.7487 - val_accuracy: 0.5302 - 2s/epoch - 2ms/step\n",
      "Epoch 22/10000\n",
      "1126/1126 - 2s - loss: 126.0235 - accuracy: 0.5841 - val_loss: 203.0831 - val_accuracy: 0.5064 - 2s/epoch - 2ms/step\n",
      "Epoch 23/10000\n",
      "1126/1126 - 2s - loss: 115.0361 - accuracy: 0.5867 - val_loss: 262.8881 - val_accuracy: 0.4648 - 2s/epoch - 2ms/step\n",
      "Epoch 24/10000\n",
      "1126/1126 - 2s - loss: 129.9853 - accuracy: 0.5855 - val_loss: 174.1147 - val_accuracy: 0.6204 - 2s/epoch - 2ms/step\n",
      "435/435 [==============================] - 1s 1ms/step - loss: 74.9029 - accuracy: 0.6282\n",
      "Larghezza: 50 \n",
      "Profondità: 1 \n",
      "Dimensione del batch: 100\n",
      "Test loss (binary_crossentropy): 74.90288543701172\n",
      "Test metrics (accuracy): 0.6282281875610352\n",
      "Epoch 1/10000\n",
      "113/113 - 1s - loss: 95957.3906 - accuracy: 0.6106 - val_loss: 32580.9961 - val_accuracy: 0.6043 - 845ms/epoch - 7ms/step\n",
      "Epoch 2/10000\n",
      "113/113 - 0s - loss: 19411.0645 - accuracy: 0.5672 - val_loss: 17097.5684 - val_accuracy: 0.5645 - 265ms/epoch - 2ms/step\n",
      "Epoch 3/10000\n",
      "113/113 - 0s - loss: 15169.1172 - accuracy: 0.5639 - val_loss: 13476.5771 - val_accuracy: 0.5648 - 270ms/epoch - 2ms/step\n",
      "Epoch 4/10000\n",
      "113/113 - 0s - loss: 11287.3145 - accuracy: 0.5638 - val_loss: 8913.4082 - val_accuracy: 0.5663 - 282ms/epoch - 2ms/step\n",
      "Epoch 5/10000\n",
      "113/113 - 0s - loss: 6862.3081 - accuracy: 0.5823 - val_loss: 5611.5107 - val_accuracy: 0.6016 - 257ms/epoch - 2ms/step\n",
      "Epoch 6/10000\n",
      "113/113 - 0s - loss: 4754.9941 - accuracy: 0.6041 - val_loss: 4151.2261 - val_accuracy: 0.6063 - 264ms/epoch - 2ms/step\n",
      "Epoch 7/10000\n",
      "113/113 - 0s - loss: 3546.1641 - accuracy: 0.6076 - val_loss: 3146.3633 - val_accuracy: 0.6065 - 267ms/epoch - 2ms/step\n",
      "Epoch 8/10000\n",
      "113/113 - 0s - loss: 2708.4031 - accuracy: 0.6051 - val_loss: 2417.1543 - val_accuracy: 0.6032 - 257ms/epoch - 2ms/step\n",
      "Epoch 9/10000\n",
      "113/113 - 0s - loss: 2079.1643 - accuracy: 0.6026 - val_loss: 1841.2101 - val_accuracy: 0.6004 - 269ms/epoch - 2ms/step\n",
      "Epoch 10/10000\n",
      "113/113 - 0s - loss: 1574.6176 - accuracy: 0.5941 - val_loss: 1382.8423 - val_accuracy: 0.5840 - 272ms/epoch - 2ms/step\n",
      "Epoch 11/10000\n",
      "113/113 - 0s - loss: 1194.7489 - accuracy: 0.5800 - val_loss: 1065.9663 - val_accuracy: 0.5734 - 263ms/epoch - 2ms/step\n",
      "Epoch 12/10000\n",
      "113/113 - 0s - loss: 935.0845 - accuracy: 0.5703 - val_loss: 823.6624 - val_accuracy: 0.5730 - 269ms/epoch - 2ms/step\n",
      "Epoch 13/10000\n",
      "113/113 - 0s - loss: 737.7010 - accuracy: 0.5661 - val_loss: 690.2699 - val_accuracy: 0.5406 - 268ms/epoch - 2ms/step\n",
      "Epoch 14/10000\n",
      "113/113 - 0s - loss: 635.9042 - accuracy: 0.5616 - val_loss: 587.3771 - val_accuracy: 0.5590 - 268ms/epoch - 2ms/step\n",
      "Epoch 15/10000\n",
      "113/113 - 0s - loss: 577.4209 - accuracy: 0.5579 - val_loss: 560.6775 - val_accuracy: 0.5713 - 278ms/epoch - 2ms/step\n",
      "Epoch 16/10000\n",
      "113/113 - 0s - loss: 528.3970 - accuracy: 0.5570 - val_loss: 483.6128 - val_accuracy: 0.5544 - 273ms/epoch - 2ms/step\n",
      "Epoch 17/10000\n",
      "113/113 - 0s - loss: 487.7640 - accuracy: 0.5565 - val_loss: 441.2602 - val_accuracy: 0.5542 - 275ms/epoch - 2ms/step\n",
      "435/435 [==============================] - 1s 1ms/step - loss: 3023.6377 - accuracy: 0.6111\n",
      "Larghezza: 50 \n",
      "Profondità: 1 \n",
      "Dimensione del batch: 1000\n",
      "Test loss (binary_crossentropy): 3023.6376953125\n",
      "Test metrics (accuracy): 0.6111071109771729\n",
      "Epoch 1/10000\n",
      "11260/11260 - 16s - loss: 1.6561 - accuracy: 0.5568 - val_loss: 0.9365 - val_accuracy: 0.5955 - 16s/epoch - 1ms/step\n",
      "Epoch 2/10000\n",
      "11260/11260 - 15s - loss: 0.9193 - accuracy: 0.5939 - val_loss: 0.8874 - val_accuracy: 0.6003 - 15s/epoch - 1ms/step\n",
      "Epoch 3/10000\n",
      "11260/11260 - 18s - loss: 0.8337 - accuracy: 0.5942 - val_loss: 0.7952 - val_accuracy: 0.6051 - 18s/epoch - 2ms/step\n",
      "Epoch 4/10000\n",
      "11260/11260 - 16s - loss: 0.7825 - accuracy: 0.5953 - val_loss: 0.7427 - val_accuracy: 0.6000 - 16s/epoch - 1ms/step\n",
      "Epoch 5/10000\n",
      "11260/11260 - 15s - loss: 0.7423 - accuracy: 0.5946 - val_loss: 0.7068 - val_accuracy: 0.5978 - 15s/epoch - 1ms/step\n",
      "Epoch 6/10000\n",
      "11260/11260 - 17s - loss: 0.7072 - accuracy: 0.6016 - val_loss: 0.6916 - val_accuracy: 0.6121 - 17s/epoch - 2ms/step\n",
      "Epoch 7/10000\n",
      "11260/11260 - 17s - loss: 0.6920 - accuracy: 0.6076 - val_loss: 0.6761 - val_accuracy: 0.6132 - 17s/epoch - 2ms/step\n",
      "Epoch 8/10000\n",
      "11260/11260 - 18s - loss: 0.6817 - accuracy: 0.6061 - val_loss: 0.6705 - val_accuracy: 0.6031 - 18s/epoch - 2ms/step\n",
      "Epoch 9/10000\n",
      "11260/11260 - 17s - loss: 0.6765 - accuracy: 0.6058 - val_loss: 0.6650 - val_accuracy: 0.6163 - 17s/epoch - 2ms/step\n",
      "Epoch 10/10000\n",
      "11260/11260 - 19s - loss: 0.6707 - accuracy: 0.6124 - val_loss: 0.6707 - val_accuracy: 0.6155 - 19s/epoch - 2ms/step\n",
      "Epoch 11/10000\n",
      "11260/11260 - 20s - loss: 0.6680 - accuracy: 0.6135 - val_loss: 0.6609 - val_accuracy: 0.6187 - 20s/epoch - 2ms/step\n",
      "Epoch 12/10000\n",
      "11260/11260 - 21s - loss: 0.6648 - accuracy: 0.6129 - val_loss: 0.6581 - val_accuracy: 0.6193 - 21s/epoch - 2ms/step\n",
      "Epoch 13/10000\n",
      "11260/11260 - 20s - loss: 0.6654 - accuracy: 0.6141 - val_loss: 0.6568 - val_accuracy: 0.6195 - 20s/epoch - 2ms/step\n",
      "Epoch 14/10000\n",
      "11260/11260 - 14s - loss: 0.6590 - accuracy: 0.6218 - val_loss: 0.6578 - val_accuracy: 0.6321 - 14s/epoch - 1ms/step\n",
      "Epoch 15/10000\n",
      "11260/11260 - 19s - loss: 0.6583 - accuracy: 0.6290 - val_loss: 0.6486 - val_accuracy: 0.6393 - 19s/epoch - 2ms/step\n",
      "Epoch 16/10000\n",
      "11260/11260 - 21s - loss: 0.6606 - accuracy: 0.6268 - val_loss: 0.6523 - val_accuracy: 0.6347 - 21s/epoch - 2ms/step\n",
      "Epoch 17/10000\n",
      "11260/11260 - 17s - loss: 0.6572 - accuracy: 0.6281 - val_loss: 0.6553 - val_accuracy: 0.6309 - 17s/epoch - 1ms/step\n",
      "Epoch 18/10000\n",
      "11260/11260 - 20s - loss: 0.6571 - accuracy: 0.6307 - val_loss: 0.6537 - val_accuracy: 0.6191 - 20s/epoch - 2ms/step\n",
      "Epoch 19/10000\n",
      "11260/11260 - 17s - loss: 0.6586 - accuracy: 0.6304 - val_loss: 0.6600 - val_accuracy: 0.6326 - 17s/epoch - 2ms/step\n",
      "Epoch 20/10000\n",
      "11260/11260 - 21s - loss: 0.6607 - accuracy: 0.6268 - val_loss: 0.6540 - val_accuracy: 0.6331 - 21s/epoch - 2ms/step\n",
      "Epoch 21/10000\n",
      "11260/11260 - 18s - loss: 0.6568 - accuracy: 0.6283 - val_loss: 0.6548 - val_accuracy: 0.6312 - 18s/epoch - 2ms/step\n",
      "Epoch 22/10000\n",
      "11260/11260 - 18s - loss: 0.6585 - accuracy: 0.6280 - val_loss: 0.6617 - val_accuracy: 0.6364 - 18s/epoch - 2ms/step\n",
      "Epoch 23/10000\n",
      "11260/11260 - 15s - loss: 0.6594 - accuracy: 0.6288 - val_loss: 0.6583 - val_accuracy: 0.6265 - 15s/epoch - 1ms/step\n",
      "Epoch 24/10000\n",
      "11260/11260 - 14s - loss: 0.6583 - accuracy: 0.6289 - val_loss: 0.6513 - val_accuracy: 0.6333 - 14s/epoch - 1ms/step\n",
      "Epoch 25/10000\n",
      "11260/11260 - 14s - loss: 0.6583 - accuracy: 0.6287 - val_loss: 0.6526 - val_accuracy: 0.6281 - 14s/epoch - 1ms/step\n",
      "435/435 [==============================] - 1s 1ms/step - loss: 0.6598 - accuracy: 0.6397\n",
      "Larghezza: 50 \n",
      "Profondità: 1 \n",
      "Dimensione del batch: 10\n",
      "Test loss (binary_crossentropy): 0.659808874130249\n",
      "Test metrics (accuracy): 0.639738142490387\n",
      "Epoch 1/10000\n",
      "1126/1126 - 2s - loss: 4.5548 - accuracy: 0.5788 - val_loss: 1.6023 - val_accuracy: 0.5055 - 2s/epoch - 2ms/step\n",
      "Epoch 2/10000\n",
      "1126/1126 - 2s - loss: 1.3246 - accuracy: 0.5307 - val_loss: 1.1799 - val_accuracy: 0.5650 - 2s/epoch - 1ms/step\n",
      "Epoch 3/10000\n",
      "1126/1126 - 2s - loss: 1.0652 - accuracy: 0.5631 - val_loss: 1.0084 - val_accuracy: 0.5569 - 2s/epoch - 2ms/step\n",
      "Epoch 4/10000\n",
      "1126/1126 - 2s - loss: 0.9550 - accuracy: 0.5593 - val_loss: 0.9278 - val_accuracy: 0.5582 - 2s/epoch - 2ms/step\n",
      "Epoch 5/10000\n",
      "1126/1126 - 2s - loss: 0.8811 - accuracy: 0.5784 - val_loss: 0.8524 - val_accuracy: 0.6020 - 2s/epoch - 2ms/step\n",
      "Epoch 6/10000\n",
      "1126/1126 - 2s - loss: 0.8243 - accuracy: 0.6003 - val_loss: 0.8035 - val_accuracy: 0.6036 - 2s/epoch - 2ms/step\n",
      "Epoch 7/10000\n",
      "1126/1126 - 2s - loss: 0.7879 - accuracy: 0.6013 - val_loss: 0.7792 - val_accuracy: 0.6043 - 2s/epoch - 2ms/step\n",
      "Epoch 8/10000\n",
      "1126/1126 - 2s - loss: 0.7743 - accuracy: 0.6038 - val_loss: 0.7702 - val_accuracy: 0.6082 - 2s/epoch - 2ms/step\n",
      "Epoch 9/10000\n",
      "1126/1126 - 2s - loss: 0.7572 - accuracy: 0.6047 - val_loss: 0.7474 - val_accuracy: 0.6082 - 2s/epoch - 2ms/step\n",
      "Epoch 10/10000\n",
      "1126/1126 - 2s - loss: 0.7462 - accuracy: 0.6072 - val_loss: 0.7406 - val_accuracy: 0.6139 - 2s/epoch - 2ms/step\n",
      "Epoch 11/10000\n",
      "1126/1126 - 2s - loss: 0.7411 - accuracy: 0.6132 - val_loss: 0.7351 - val_accuracy: 0.6174 - 2s/epoch - 2ms/step\n",
      "Epoch 12/10000\n",
      "1126/1126 - 2s - loss: 0.7360 - accuracy: 0.6163 - val_loss: 0.7333 - val_accuracy: 0.6187 - 2s/epoch - 2ms/step\n",
      "Epoch 13/10000\n",
      "1126/1126 - 2s - loss: 0.7303 - accuracy: 0.6168 - val_loss: 0.7252 - val_accuracy: 0.6192 - 2s/epoch - 2ms/step\n",
      "Epoch 14/10000\n",
      "1126/1126 - 3s - loss: 0.7263 - accuracy: 0.6186 - val_loss: 0.7261 - val_accuracy: 0.6200 - 3s/epoch - 2ms/step\n",
      "Epoch 15/10000\n",
      "1126/1126 - 2s - loss: 0.7226 - accuracy: 0.6191 - val_loss: 0.7188 - val_accuracy: 0.6225 - 2s/epoch - 2ms/step\n",
      "Epoch 16/10000\n",
      "1126/1126 - 2s - loss: 0.7179 - accuracy: 0.6188 - val_loss: 0.7166 - val_accuracy: 0.6205 - 2s/epoch - 2ms/step\n",
      "Epoch 17/10000\n",
      "1126/1126 - 3s - loss: 0.7154 - accuracy: 0.6192 - val_loss: 0.7165 - val_accuracy: 0.6233 - 3s/epoch - 2ms/step\n",
      "Epoch 18/10000\n",
      "1126/1126 - 2s - loss: 0.7147 - accuracy: 0.6203 - val_loss: 0.7275 - val_accuracy: 0.6207 - 2s/epoch - 2ms/step\n",
      "Epoch 19/10000\n",
      "1126/1126 - 3s - loss: 0.7158 - accuracy: 0.6181 - val_loss: 0.7117 - val_accuracy: 0.6242 - 3s/epoch - 2ms/step\n",
      "Epoch 20/10000\n",
      "1126/1126 - 3s - loss: 0.7064 - accuracy: 0.6207 - val_loss: 0.7106 - val_accuracy: 0.6195 - 3s/epoch - 2ms/step\n",
      "Epoch 21/10000\n",
      "1126/1126 - 3s - loss: 0.7062 - accuracy: 0.6189 - val_loss: 0.7044 - val_accuracy: 0.6235 - 3s/epoch - 3ms/step\n",
      "Epoch 22/10000\n",
      "1126/1126 - 3s - loss: 0.7019 - accuracy: 0.6206 - val_loss: 0.7021 - val_accuracy: 0.6244 - 3s/epoch - 3ms/step\n",
      "Epoch 23/10000\n",
      "1126/1126 - 3s - loss: 0.6986 - accuracy: 0.6218 - val_loss: 0.6999 - val_accuracy: 0.6243 - 3s/epoch - 2ms/step\n",
      "Epoch 24/10000\n",
      "1126/1126 - 2s - loss: 0.6969 - accuracy: 0.6221 - val_loss: 0.6961 - val_accuracy: 0.6256 - 2s/epoch - 2ms/step\n",
      "Epoch 25/10000\n",
      "1126/1126 - 2s - loss: 0.6941 - accuracy: 0.6225 - val_loss: 0.6962 - val_accuracy: 0.6241 - 2s/epoch - 2ms/step\n",
      "Epoch 26/10000\n",
      "1126/1126 - 2s - loss: 0.6925 - accuracy: 0.6231 - val_loss: 0.6902 - val_accuracy: 0.6269 - 2s/epoch - 2ms/step\n",
      "Epoch 27/10000\n",
      "1126/1126 - 2s - loss: 0.6909 - accuracy: 0.6221 - val_loss: 0.6912 - val_accuracy: 0.6261 - 2s/epoch - 2ms/step\n",
      "Epoch 28/10000\n",
      "1126/1126 - 2s - loss: 0.6908 - accuracy: 0.6229 - val_loss: 0.6968 - val_accuracy: 0.6236 - 2s/epoch - 2ms/step\n",
      "Epoch 29/10000\n",
      "1126/1126 - 2s - loss: 0.6885 - accuracy: 0.6237 - val_loss: 0.6934 - val_accuracy: 0.6262 - 2s/epoch - 2ms/step\n",
      "Epoch 30/10000\n",
      "1126/1126 - 2s - loss: 0.6833 - accuracy: 0.6296 - val_loss: 0.6843 - val_accuracy: 0.6328 - 2s/epoch - 2ms/step\n",
      "Epoch 31/10000\n",
      "1126/1126 - 2s - loss: 0.6802 - accuracy: 0.6300 - val_loss: 0.6846 - val_accuracy: 0.6306 - 2s/epoch - 2ms/step\n",
      "Epoch 32/10000\n",
      "1126/1126 - 2s - loss: 0.6800 - accuracy: 0.6291 - val_loss: 0.6818 - val_accuracy: 0.6316 - 2s/epoch - 2ms/step\n",
      "Epoch 33/10000\n",
      "1126/1126 - 2s - loss: 0.6796 - accuracy: 0.6307 - val_loss: 0.6831 - val_accuracy: 0.6313 - 2s/epoch - 1ms/step\n",
      "Epoch 34/10000\n",
      "1126/1126 - 2s - loss: 0.6792 - accuracy: 0.6305 - val_loss: 0.6849 - val_accuracy: 0.6294 - 2s/epoch - 1ms/step\n",
      "Epoch 35/10000\n",
      "1126/1126 - 2s - loss: 0.6780 - accuracy: 0.6307 - val_loss: 0.6791 - val_accuracy: 0.6313 - 2s/epoch - 2ms/step\n",
      "Epoch 36/10000\n",
      "1126/1126 - 2s - loss: 0.6768 - accuracy: 0.6306 - val_loss: 0.6800 - val_accuracy: 0.6275 - 2s/epoch - 1ms/step\n",
      "Epoch 37/10000\n",
      "1126/1126 - 2s - loss: 0.6779 - accuracy: 0.6289 - val_loss: 0.6820 - val_accuracy: 0.6287 - 2s/epoch - 2ms/step\n",
      "Epoch 38/10000\n",
      "1126/1126 - 2s - loss: 0.6788 - accuracy: 0.6301 - val_loss: 0.6742 - val_accuracy: 0.6302 - 2s/epoch - 1ms/step\n",
      "Epoch 39/10000\n",
      "1126/1126 - 2s - loss: 0.6785 - accuracy: 0.6292 - val_loss: 0.6775 - val_accuracy: 0.6264 - 2s/epoch - 1ms/step\n",
      "Epoch 40/10000\n",
      "1126/1126 - 2s - loss: 0.6787 - accuracy: 0.6245 - val_loss: 0.6797 - val_accuracy: 0.6268 - 2s/epoch - 1ms/step\n",
      "435/435 [==============================] - 1s 1ms/step - loss: 0.6811 - accuracy: 0.6289\n",
      "Larghezza: 50 \n",
      "Profondità: 1 \n",
      "Dimensione del batch: 100\n",
      "Test loss (binary_crossentropy): 0.6811442971229553\n",
      "Test metrics (accuracy): 0.6288756132125854\n",
      "Epoch 1/10000\n",
      "113/113 - 1s - loss: 8.6502 - accuracy: 0.3895 - val_loss: 7.6777 - val_accuracy: 0.3911 - 833ms/epoch - 7ms/step\n",
      "Epoch 2/10000\n",
      "113/113 - 0s - loss: 6.7013 - accuracy: 0.3947 - val_loss: 5.7725 - val_accuracy: 0.3940 - 275ms/epoch - 2ms/step\n",
      "Epoch 3/10000\n",
      "113/113 - 0s - loss: 4.8985 - accuracy: 0.3948 - val_loss: 4.0673 - val_accuracy: 0.3931 - 269ms/epoch - 2ms/step\n",
      "Epoch 4/10000\n",
      "113/113 - 0s - loss: 3.3640 - accuracy: 0.3960 - val_loss: 2.8182 - val_accuracy: 0.3967 - 257ms/epoch - 2ms/step\n",
      "Epoch 5/10000\n",
      "113/113 - 0s - loss: 2.5102 - accuracy: 0.4417 - val_loss: 2.3613 - val_accuracy: 0.4624 - 257ms/epoch - 2ms/step\n",
      "Epoch 6/10000\n",
      "113/113 - 0s - loss: 2.2303 - accuracy: 0.4690 - val_loss: 2.1771 - val_accuracy: 0.4713 - 280ms/epoch - 2ms/step\n",
      "Epoch 7/10000\n",
      "113/113 - 0s - loss: 2.1121 - accuracy: 0.4734 - val_loss: 2.0948 - val_accuracy: 0.4741 - 265ms/epoch - 2ms/step\n",
      "Epoch 8/10000\n",
      "113/113 - 0s - loss: 2.0290 - accuracy: 0.4777 - val_loss: 2.0053 - val_accuracy: 0.4804 - 251ms/epoch - 2ms/step\n",
      "Epoch 9/10000\n",
      "113/113 - 0s - loss: 1.9472 - accuracy: 0.4829 - val_loss: 1.9072 - val_accuracy: 0.4871 - 306ms/epoch - 3ms/step\n",
      "Epoch 10/10000\n",
      "113/113 - 0s - loss: 1.8558 - accuracy: 0.4888 - val_loss: 1.8209 - val_accuracy: 0.4921 - 268ms/epoch - 2ms/step\n",
      "Epoch 11/10000\n",
      "113/113 - 0s - loss: 1.7655 - accuracy: 0.4958 - val_loss: 1.7112 - val_accuracy: 0.5025 - 262ms/epoch - 2ms/step\n",
      "Epoch 12/10000\n",
      "113/113 - 0s - loss: 1.6634 - accuracy: 0.5052 - val_loss: 1.6018 - val_accuracy: 0.5126 - 249ms/epoch - 2ms/step\n",
      "Epoch 13/10000\n",
      "113/113 - 0s - loss: 1.5642 - accuracy: 0.5134 - val_loss: 1.5127 - val_accuracy: 0.5207 - 250ms/epoch - 2ms/step\n",
      "Epoch 14/10000\n",
      "113/113 - 0s - loss: 1.4831 - accuracy: 0.5213 - val_loss: 1.4316 - val_accuracy: 0.5284 - 308ms/epoch - 3ms/step\n",
      "Epoch 15/10000\n",
      "113/113 - 0s - loss: 1.4091 - accuracy: 0.5281 - val_loss: 1.3636 - val_accuracy: 0.5345 - 287ms/epoch - 3ms/step\n",
      "Epoch 16/10000\n",
      "113/113 - 0s - loss: 1.3540 - accuracy: 0.5331 - val_loss: 1.3162 - val_accuracy: 0.5387 - 312ms/epoch - 3ms/step\n",
      "Epoch 17/10000\n",
      "113/113 - 0s - loss: 1.3115 - accuracy: 0.5371 - val_loss: 1.2879 - val_accuracy: 0.5402 - 289ms/epoch - 3ms/step\n",
      "Epoch 18/10000\n",
      "113/113 - 0s - loss: 1.2750 - accuracy: 0.5408 - val_loss: 1.2469 - val_accuracy: 0.5445 - 274ms/epoch - 2ms/step\n",
      "Epoch 19/10000\n",
      "113/113 - 0s - loss: 1.2409 - accuracy: 0.5438 - val_loss: 1.2183 - val_accuracy: 0.5470 - 266ms/epoch - 2ms/step\n",
      "Epoch 20/10000\n",
      "113/113 - 0s - loss: 1.2137 - accuracy: 0.5460 - val_loss: 1.1835 - val_accuracy: 0.5515 - 280ms/epoch - 2ms/step\n",
      "Epoch 21/10000\n",
      "113/113 - 0s - loss: 1.1801 - accuracy: 0.5496 - val_loss: 1.1542 - val_accuracy: 0.5533 - 258ms/epoch - 2ms/step\n",
      "Epoch 22/10000\n",
      "113/113 - 0s - loss: 1.1432 - accuracy: 0.5532 - val_loss: 1.1248 - val_accuracy: 0.5557 - 274ms/epoch - 2ms/step\n",
      "Epoch 23/10000\n",
      "113/113 - 0s - loss: 1.1147 - accuracy: 0.5555 - val_loss: 1.1012 - val_accuracy: 0.5580 - 276ms/epoch - 2ms/step\n",
      "Epoch 24/10000\n",
      "113/113 - 0s - loss: 1.0886 - accuracy: 0.5581 - val_loss: 1.0731 - val_accuracy: 0.5606 - 253ms/epoch - 2ms/step\n",
      "Epoch 25/10000\n",
      "113/113 - 0s - loss: 1.0664 - accuracy: 0.5601 - val_loss: 1.0503 - val_accuracy: 0.5625 - 252ms/epoch - 2ms/step\n",
      "Epoch 26/10000\n",
      "113/113 - 0s - loss: 1.0454 - accuracy: 0.5621 - val_loss: 1.0262 - val_accuracy: 0.5654 - 256ms/epoch - 2ms/step\n",
      "Epoch 27/10000\n",
      "113/113 - 0s - loss: 1.0172 - accuracy: 0.5658 - val_loss: 1.0010 - val_accuracy: 0.5677 - 265ms/epoch - 2ms/step\n",
      "Epoch 28/10000\n",
      "113/113 - 0s - loss: 0.9954 - accuracy: 0.5678 - val_loss: 0.9819 - val_accuracy: 0.5696 - 250ms/epoch - 2ms/step\n",
      "Epoch 29/10000\n",
      "113/113 - 0s - loss: 0.9686 - accuracy: 0.5706 - val_loss: 0.9538 - val_accuracy: 0.5728 - 248ms/epoch - 2ms/step\n",
      "Epoch 30/10000\n",
      "113/113 - 0s - loss: 0.9474 - accuracy: 0.5735 - val_loss: 0.9331 - val_accuracy: 0.5758 - 267ms/epoch - 2ms/step\n",
      "Epoch 31/10000\n",
      "113/113 - 0s - loss: 0.9237 - accuracy: 0.5774 - val_loss: 0.9086 - val_accuracy: 0.5805 - 262ms/epoch - 2ms/step\n",
      "Epoch 32/10000\n",
      "113/113 - 0s - loss: 0.9011 - accuracy: 0.5803 - val_loss: 0.8846 - val_accuracy: 0.5831 - 250ms/epoch - 2ms/step\n",
      "Epoch 33/10000\n",
      "113/113 - 0s - loss: 0.8805 - accuracy: 0.5830 - val_loss: 0.8666 - val_accuracy: 0.5862 - 244ms/epoch - 2ms/step\n",
      "Epoch 34/10000\n",
      "113/113 - 0s - loss: 0.8628 - accuracy: 0.5855 - val_loss: 0.8500 - val_accuracy: 0.5889 - 267ms/epoch - 2ms/step\n",
      "Epoch 35/10000\n",
      "113/113 - 0s - loss: 0.8479 - accuracy: 0.5875 - val_loss: 0.8364 - val_accuracy: 0.5908 - 256ms/epoch - 2ms/step\n",
      "Epoch 36/10000\n",
      "113/113 - 0s - loss: 0.8338 - accuracy: 0.5894 - val_loss: 0.8182 - val_accuracy: 0.5931 - 270ms/epoch - 2ms/step\n",
      "Epoch 37/10000\n",
      "113/113 - 0s - loss: 0.8187 - accuracy: 0.5914 - val_loss: 0.8036 - val_accuracy: 0.5956 - 270ms/epoch - 2ms/step\n",
      "Epoch 38/10000\n",
      "113/113 - 0s - loss: 0.8056 - accuracy: 0.5931 - val_loss: 0.7980 - val_accuracy: 0.5965 - 258ms/epoch - 2ms/step\n",
      "Epoch 39/10000\n",
      "113/113 - 0s - loss: 0.7967 - accuracy: 0.5945 - val_loss: 0.7873 - val_accuracy: 0.5975 - 264ms/epoch - 2ms/step\n",
      "Epoch 40/10000\n",
      "113/113 - 0s - loss: 0.7839 - accuracy: 0.5963 - val_loss: 0.7809 - val_accuracy: 0.5992 - 255ms/epoch - 2ms/step\n",
      "Epoch 41/10000\n",
      "113/113 - 0s - loss: 0.7787 - accuracy: 0.5976 - val_loss: 0.7796 - val_accuracy: 0.5988 - 265ms/epoch - 2ms/step\n",
      "Epoch 42/10000\n",
      "113/113 - 0s - loss: 0.7762 - accuracy: 0.5979 - val_loss: 0.7776 - val_accuracy: 0.5990 - 270ms/epoch - 2ms/step\n",
      "Epoch 43/10000\n",
      "113/113 - 0s - loss: 0.7745 - accuracy: 0.5979 - val_loss: 0.7757 - val_accuracy: 0.5994 - 412ms/epoch - 4ms/step\n",
      "Epoch 44/10000\n",
      "113/113 - 0s - loss: 0.7741 - accuracy: 0.5981 - val_loss: 0.7751 - val_accuracy: 0.5992 - 336ms/epoch - 3ms/step\n",
      "Epoch 45/10000\n",
      "113/113 - 0s - loss: 0.7725 - accuracy: 0.5981 - val_loss: 0.7721 - val_accuracy: 0.5996 - 461ms/epoch - 4ms/step\n",
      "Epoch 46/10000\n",
      "113/113 - 0s - loss: 0.7699 - accuracy: 0.5991 - val_loss: 0.7705 - val_accuracy: 0.6012 - 474ms/epoch - 4ms/step\n",
      "Epoch 47/10000\n",
      "113/113 - 1s - loss: 0.7689 - accuracy: 0.5990 - val_loss: 0.7709 - val_accuracy: 0.6015 - 549ms/epoch - 5ms/step\n",
      "Epoch 48/10000\n",
      "113/113 - 0s - loss: 0.7677 - accuracy: 0.5996 - val_loss: 0.7680 - val_accuracy: 0.6020 - 442ms/epoch - 4ms/step\n",
      "Epoch 49/10000\n",
      "113/113 - 1s - loss: 0.7662 - accuracy: 0.5999 - val_loss: 0.7669 - val_accuracy: 0.6018 - 548ms/epoch - 5ms/step\n",
      "Epoch 50/10000\n",
      "113/113 - 1s - loss: 0.7653 - accuracy: 0.5999 - val_loss: 0.7658 - val_accuracy: 0.6019 - 535ms/epoch - 5ms/step\n",
      "Epoch 51/10000\n",
      "113/113 - 1s - loss: 0.7641 - accuracy: 0.5999 - val_loss: 0.7643 - val_accuracy: 0.6021 - 531ms/epoch - 5ms/step\n",
      "Epoch 52/10000\n",
      "113/113 - 0s - loss: 0.7624 - accuracy: 0.6002 - val_loss: 0.7652 - val_accuracy: 0.6021 - 424ms/epoch - 4ms/step\n",
      "Epoch 53/10000\n",
      "113/113 - 1s - loss: 0.7609 - accuracy: 0.6005 - val_loss: 0.7635 - val_accuracy: 0.6023 - 551ms/epoch - 5ms/step\n",
      "Epoch 54/10000\n",
      "113/113 - 1s - loss: 0.7568 - accuracy: 0.6007 - val_loss: 0.7533 - val_accuracy: 0.6033 - 537ms/epoch - 5ms/step\n",
      "Epoch 55/10000\n",
      "113/113 - 1s - loss: 0.7535 - accuracy: 0.6009 - val_loss: 0.7525 - val_accuracy: 0.6032 - 555ms/epoch - 5ms/step\n",
      "Epoch 56/10000\n",
      "113/113 - 0s - loss: 0.7518 - accuracy: 0.6010 - val_loss: 0.7482 - val_accuracy: 0.6035 - 393ms/epoch - 3ms/step\n",
      "Epoch 57/10000\n",
      "113/113 - 0s - loss: 0.7498 - accuracy: 0.6012 - val_loss: 0.7447 - val_accuracy: 0.6039 - 307ms/epoch - 3ms/step\n",
      "Epoch 58/10000\n",
      "113/113 - 0s - loss: 0.7462 - accuracy: 0.6016 - val_loss: 0.7400 - val_accuracy: 0.6047 - 337ms/epoch - 3ms/step\n",
      "Epoch 59/10000\n",
      "113/113 - 0s - loss: 0.7433 - accuracy: 0.6021 - val_loss: 0.7378 - val_accuracy: 0.6042 - 345ms/epoch - 3ms/step\n",
      "Epoch 60/10000\n",
      "113/113 - 0s - loss: 0.7391 - accuracy: 0.6028 - val_loss: 0.7354 - val_accuracy: 0.6045 - 380ms/epoch - 3ms/step\n",
      "Epoch 61/10000\n",
      "113/113 - 0s - loss: 0.7344 - accuracy: 0.6036 - val_loss: 0.7299 - val_accuracy: 0.6060 - 319ms/epoch - 3ms/step\n",
      "Epoch 62/10000\n",
      "113/113 - 0s - loss: 0.7321 - accuracy: 0.6038 - val_loss: 0.7282 - val_accuracy: 0.6060 - 314ms/epoch - 3ms/step\n",
      "Epoch 63/10000\n",
      "113/113 - 0s - loss: 0.7307 - accuracy: 0.6044 - val_loss: 0.7259 - val_accuracy: 0.6067 - 317ms/epoch - 3ms/step\n",
      "Epoch 64/10000\n",
      "113/113 - 0s - loss: 0.7292 - accuracy: 0.6046 - val_loss: 0.7256 - val_accuracy: 0.6063 - 402ms/epoch - 4ms/step\n",
      "Epoch 65/10000\n",
      "113/113 - 1s - loss: 0.7279 - accuracy: 0.6048 - val_loss: 0.7246 - val_accuracy: 0.6063 - 536ms/epoch - 5ms/step\n",
      "Epoch 66/10000\n",
      "113/113 - 1s - loss: 0.7259 - accuracy: 0.6048 - val_loss: 0.7224 - val_accuracy: 0.6069 - 557ms/epoch - 5ms/step\n",
      "Epoch 67/10000\n",
      "113/113 - 1s - loss: 0.7231 - accuracy: 0.6050 - val_loss: 0.7184 - val_accuracy: 0.6073 - 525ms/epoch - 5ms/step\n",
      "Epoch 68/10000\n",
      "113/113 - 1s - loss: 0.7199 - accuracy: 0.6053 - val_loss: 0.7166 - val_accuracy: 0.6076 - 549ms/epoch - 5ms/step\n",
      "Epoch 69/10000\n",
      "113/113 - 1s - loss: 0.7188 - accuracy: 0.6055 - val_loss: 0.7161 - val_accuracy: 0.6075 - 549ms/epoch - 5ms/step\n",
      "Epoch 70/10000\n",
      "113/113 - 0s - loss: 0.7170 - accuracy: 0.6056 - val_loss: 0.7155 - val_accuracy: 0.6075 - 297ms/epoch - 3ms/step\n",
      "Epoch 71/10000\n",
      "113/113 - 0s - loss: 0.7161 - accuracy: 0.6057 - val_loss: 0.7143 - val_accuracy: 0.6073 - 344ms/epoch - 3ms/step\n",
      "Epoch 72/10000\n",
      "113/113 - 0s - loss: 0.7164 - accuracy: 0.6057 - val_loss: 0.7155 - val_accuracy: 0.6066 - 354ms/epoch - 3ms/step\n",
      "Epoch 73/10000\n",
      "113/113 - 0s - loss: 0.7150 - accuracy: 0.6058 - val_loss: 0.7142 - val_accuracy: 0.6066 - 355ms/epoch - 3ms/step\n",
      "Epoch 74/10000\n",
      "113/113 - 0s - loss: 0.7147 - accuracy: 0.6056 - val_loss: 0.7141 - val_accuracy: 0.6066 - 326ms/epoch - 3ms/step\n",
      "Epoch 75/10000\n",
      "113/113 - 0s - loss: 0.7139 - accuracy: 0.6054 - val_loss: 0.7136 - val_accuracy: 0.6060 - 332ms/epoch - 3ms/step\n",
      "Epoch 76/10000\n",
      "113/113 - 0s - loss: 0.7140 - accuracy: 0.6054 - val_loss: 0.7122 - val_accuracy: 0.6062 - 263ms/epoch - 2ms/step\n",
      "Epoch 77/10000\n",
      "113/113 - 0s - loss: 0.7130 - accuracy: 0.6056 - val_loss: 0.7122 - val_accuracy: 0.6061 - 291ms/epoch - 3ms/step\n",
      "Epoch 78/10000\n",
      "113/113 - 0s - loss: 0.7123 - accuracy: 0.6056 - val_loss: 0.7114 - val_accuracy: 0.6064 - 299ms/epoch - 3ms/step\n",
      "435/435 [==============================] - 1s 2ms/step - loss: 0.7228 - accuracy: 0.6077\n",
      "Larghezza: 50 \n",
      "Profondità: 1 \n",
      "Dimensione del batch: 1000\n",
      "Test loss (binary_crossentropy): 0.7228109240531921\n",
      "Test metrics (accuracy): 0.6076541543006897\n",
      "Epoch 1/10000\n",
      "11260/11260 - 20s - loss: 415310.5625 - accuracy: 0.5657 - val_loss: 68699.7734 - val_accuracy: 0.4595 - 20s/epoch - 2ms/step\n",
      "Epoch 2/10000\n",
      "11260/11260 - 28s - loss: 27268.7812 - accuracy: 0.5579 - val_loss: 30069.0547 - val_accuracy: 0.6036 - 28s/epoch - 2ms/step\n",
      "Epoch 3/10000\n",
      "11260/11260 - 26s - loss: 24731.7344 - accuracy: 0.5663 - val_loss: 8620.3887 - val_accuracy: 0.6168 - 26s/epoch - 2ms/step\n",
      "Epoch 4/10000\n",
      "11260/11260 - 23s - loss: 22745.6133 - accuracy: 0.5709 - val_loss: 21476.0098 - val_accuracy: 0.6123 - 23s/epoch - 2ms/step\n",
      "Epoch 5/10000\n",
      "11260/11260 - 25s - loss: 22158.7539 - accuracy: 0.5729 - val_loss: 6526.4653 - val_accuracy: 0.6012 - 25s/epoch - 2ms/step\n",
      "Epoch 6/10000\n",
      "11260/11260 - 26s - loss: 20268.8652 - accuracy: 0.5753 - val_loss: 32791.0977 - val_accuracy: 0.6144 - 26s/epoch - 2ms/step\n",
      "Epoch 7/10000\n",
      "11260/11260 - 28s - loss: 19601.7871 - accuracy: 0.5770 - val_loss: 4734.0586 - val_accuracy: 0.6424 - 28s/epoch - 2ms/step\n",
      "Epoch 8/10000\n",
      "11260/11260 - 30s - loss: 19436.1816 - accuracy: 0.5768 - val_loss: 33492.1211 - val_accuracy: 0.6190 - 30s/epoch - 3ms/step\n",
      "Epoch 9/10000\n",
      "11260/11260 - 28s - loss: 19774.3223 - accuracy: 0.5778 - val_loss: 5816.5674 - val_accuracy: 0.5728 - 28s/epoch - 2ms/step\n",
      "Epoch 10/10000\n",
      "11260/11260 - 21s - loss: 20244.4551 - accuracy: 0.5778 - val_loss: 56015.7617 - val_accuracy: 0.6120 - 21s/epoch - 2ms/step\n",
      "Epoch 11/10000\n",
      "11260/11260 - 19s - loss: 20454.5430 - accuracy: 0.5751 - val_loss: 26106.4238 - val_accuracy: 0.6137 - 19s/epoch - 2ms/step\n",
      "Epoch 12/10000\n",
      "11260/11260 - 19s - loss: 19292.3457 - accuracy: 0.5798 - val_loss: 4599.9160 - val_accuracy: 0.6263 - 19s/epoch - 2ms/step\n",
      "Epoch 13/10000\n",
      "11260/11260 - 21s - loss: 19292.4375 - accuracy: 0.5804 - val_loss: 10621.0557 - val_accuracy: 0.6326 - 21s/epoch - 2ms/step\n",
      "Epoch 14/10000\n",
      "11260/11260 - 21s - loss: 19993.0566 - accuracy: 0.5794 - val_loss: 74242.5859 - val_accuracy: 0.4295 - 21s/epoch - 2ms/step\n",
      "Epoch 15/10000\n",
      "11260/11260 - 24s - loss: 20576.6738 - accuracy: 0.5804 - val_loss: 5894.9531 - val_accuracy: 0.6489 - 24s/epoch - 2ms/step\n",
      "Epoch 16/10000\n",
      "11260/11260 - 18s - loss: 20499.5430 - accuracy: 0.5805 - val_loss: 5876.4331 - val_accuracy: 0.6470 - 18s/epoch - 2ms/step\n",
      "Epoch 17/10000\n",
      "11260/11260 - 18s - loss: 20681.0859 - accuracy: 0.5790 - val_loss: 21194.8125 - val_accuracy: 0.6186 - 18s/epoch - 2ms/step\n",
      "Epoch 18/10000\n",
      "11260/11260 - 22s - loss: 18681.4727 - accuracy: 0.5803 - val_loss: 3784.3914 - val_accuracy: 0.6509 - 22s/epoch - 2ms/step\n",
      "Epoch 19/10000\n",
      "11260/11260 - 20s - loss: 19523.5820 - accuracy: 0.5791 - val_loss: 30535.0078 - val_accuracy: 0.6151 - 20s/epoch - 2ms/step\n",
      "Epoch 20/10000\n",
      "11260/11260 - 18s - loss: 19637.3594 - accuracy: 0.5799 - val_loss: 4660.8179 - val_accuracy: 0.6508 - 18s/epoch - 2ms/step\n",
      "Epoch 21/10000\n",
      "11260/11260 - 19s - loss: 18932.2246 - accuracy: 0.5835 - val_loss: 10004.5098 - val_accuracy: 0.6346 - 19s/epoch - 2ms/step\n",
      "Epoch 22/10000\n",
      "11260/11260 - 25s - loss: 19281.5215 - accuracy: 0.5817 - val_loss: 20340.6289 - val_accuracy: 0.6214 - 25s/epoch - 2ms/step\n",
      "Epoch 23/10000\n",
      "11260/11260 - 27s - loss: 18795.2949 - accuracy: 0.5798 - val_loss: 18798.8789 - val_accuracy: 0.6167 - 27s/epoch - 2ms/step\n",
      "Epoch 24/10000\n",
      "11260/11260 - 20s - loss: 20572.7695 - accuracy: 0.5825 - val_loss: 6800.1914 - val_accuracy: 0.6308 - 20s/epoch - 2ms/step\n",
      "Epoch 25/10000\n",
      "11260/11260 - 17s - loss: 18873.4453 - accuracy: 0.5804 - val_loss: 9634.4062 - val_accuracy: 0.5392 - 17s/epoch - 2ms/step\n",
      "Epoch 26/10000\n",
      "11260/11260 - 22s - loss: 18732.3633 - accuracy: 0.5844 - val_loss: 6037.3677 - val_accuracy: 0.6426 - 22s/epoch - 2ms/step\n",
      "Epoch 27/10000\n",
      "11260/11260 - 16s - loss: 18951.6289 - accuracy: 0.5830 - val_loss: 13734.9189 - val_accuracy: 0.6338 - 16s/epoch - 1ms/step\n",
      "Epoch 28/10000\n",
      "11260/11260 - 27s - loss: 19965.4863 - accuracy: 0.5831 - val_loss: 49889.8203 - val_accuracy: 0.4502 - 27s/epoch - 2ms/step\n",
      "435/435 [==============================] - 1s 1ms/step - loss: 3828.4346 - accuracy: 0.6512\n",
      "Larghezza: 50 \n",
      "Profondità: 2 \n",
      "Dimensione del batch: 10\n",
      "Test loss (binary_crossentropy): 3828.4345703125\n",
      "Test metrics (accuracy): 0.6511761546134949\n",
      "Epoch 1/10000\n",
      "1126/1126 - 3s - loss: 2213438.0000 - accuracy: 0.5739 - val_loss: 88553.1719 - val_accuracy: 0.6247 - 3s/epoch - 2ms/step\n",
      "Epoch 2/10000\n",
      "1126/1126 - 3s - loss: 55452.1406 - accuracy: 0.6134 - val_loss: 27095.1035 - val_accuracy: 0.6131 - 3s/epoch - 2ms/step\n",
      "Epoch 3/10000\n",
      "1126/1126 - 3s - loss: 19246.8750 - accuracy: 0.5722 - val_loss: 25803.0957 - val_accuracy: 0.4797 - 3s/epoch - 3ms/step\n",
      "Epoch 4/10000\n",
      "1126/1126 - 2s - loss: 14252.3652 - accuracy: 0.5634 - val_loss: 8251.3057 - val_accuracy: 0.5996 - 2s/epoch - 2ms/step\n",
      "Epoch 5/10000\n",
      "1126/1126 - 3s - loss: 11595.9570 - accuracy: 0.5648 - val_loss: 6323.7344 - val_accuracy: 0.5728 - 3s/epoch - 3ms/step\n",
      "Epoch 6/10000\n",
      "1126/1126 - 4s - loss: 11578.6924 - accuracy: 0.5632 - val_loss: 5424.3145 - val_accuracy: 0.5785 - 4s/epoch - 3ms/step\n",
      "Epoch 7/10000\n",
      "1126/1126 - 2s - loss: 11070.1729 - accuracy: 0.5657 - val_loss: 11201.2627 - val_accuracy: 0.5997 - 2s/epoch - 2ms/step\n",
      "Epoch 8/10000\n",
      "1126/1126 - 2s - loss: 10498.0078 - accuracy: 0.5671 - val_loss: 8596.5322 - val_accuracy: 0.5063 - 2s/epoch - 2ms/step\n",
      "Epoch 9/10000\n",
      "1126/1126 - 2s - loss: 9493.8730 - accuracy: 0.5682 - val_loss: 10958.4453 - val_accuracy: 0.6059 - 2s/epoch - 2ms/step\n",
      "Epoch 10/10000\n",
      "1126/1126 - 3s - loss: 9825.8037 - accuracy: 0.5714 - val_loss: 9558.6523 - val_accuracy: 0.4901 - 3s/epoch - 2ms/step\n",
      "Epoch 11/10000\n",
      "1126/1126 - 2s - loss: 9615.9775 - accuracy: 0.5700 - val_loss: 12399.1357 - val_accuracy: 0.6067 - 2s/epoch - 2ms/step\n",
      "435/435 [==============================] - 1s 2ms/step - loss: 86573.6875 - accuracy: 0.6234\n",
      "Larghezza: 50 \n",
      "Profondità: 2 \n",
      "Dimensione del batch: 100\n",
      "Test loss (binary_crossentropy): 86573.6875\n",
      "Test metrics (accuracy): 0.6234083771705627\n",
      "Epoch 1/10000\n",
      "113/113 - 2s - loss: 6727129.0000 - accuracy: 0.3929 - val_loss: 2063508.7500 - val_accuracy: 0.4112 - 2s/epoch - 13ms/step\n",
      "Epoch 2/10000\n",
      "113/113 - 0s - loss: 1738600.2500 - accuracy: 0.4111 - val_loss: 1180605.5000 - val_accuracy: 0.4104 - 441ms/epoch - 4ms/step\n",
      "Epoch 3/10000\n",
      "113/113 - 0s - loss: 694586.0000 - accuracy: 0.4094 - val_loss: 362910.0625 - val_accuracy: 0.4081 - 401ms/epoch - 4ms/step\n",
      "Epoch 4/10000\n",
      "113/113 - 0s - loss: 230133.4062 - accuracy: 0.4464 - val_loss: 175085.0312 - val_accuracy: 0.4466 - 400ms/epoch - 4ms/step\n",
      "Epoch 5/10000\n",
      "113/113 - 0s - loss: 146042.6250 - accuracy: 0.4559 - val_loss: 123412.1562 - val_accuracy: 0.4564 - 497ms/epoch - 4ms/step\n",
      "Epoch 6/10000\n",
      "113/113 - 0s - loss: 105004.2734 - accuracy: 0.4595 - val_loss: 89845.0859 - val_accuracy: 0.4591 - 459ms/epoch - 4ms/step\n",
      "Epoch 7/10000\n",
      "113/113 - 0s - loss: 77244.1641 - accuracy: 0.4726 - val_loss: 65907.1172 - val_accuracy: 0.4859 - 418ms/epoch - 4ms/step\n",
      "Epoch 8/10000\n",
      "113/113 - 1s - loss: 59478.4688 - accuracy: 0.5004 - val_loss: 52894.4414 - val_accuracy: 0.5191 - 652ms/epoch - 6ms/step\n",
      "Epoch 9/10000\n",
      "113/113 - 1s - loss: 50367.0312 - accuracy: 0.5238 - val_loss: 47332.0781 - val_accuracy: 0.5381 - 599ms/epoch - 5ms/step\n",
      "Epoch 10/10000\n",
      "113/113 - 1s - loss: 45352.1758 - accuracy: 0.5385 - val_loss: 42600.6680 - val_accuracy: 0.5442 - 592ms/epoch - 5ms/step\n",
      "Epoch 11/10000\n",
      "113/113 - 1s - loss: 41161.9570 - accuracy: 0.5481 - val_loss: 38458.9102 - val_accuracy: 0.5531 - 596ms/epoch - 5ms/step\n",
      "Epoch 12/10000\n",
      "113/113 - 1s - loss: 38082.0625 - accuracy: 0.5511 - val_loss: 35521.5742 - val_accuracy: 0.5609 - 594ms/epoch - 5ms/step\n",
      "Epoch 13/10000\n",
      "113/113 - 1s - loss: 34853.3086 - accuracy: 0.5549 - val_loss: 32955.3438 - val_accuracy: 0.5648 - 699ms/epoch - 6ms/step\n",
      "Epoch 14/10000\n",
      "113/113 - 1s - loss: 32426.1953 - accuracy: 0.5559 - val_loss: 31255.5918 - val_accuracy: 0.5644 - 541ms/epoch - 5ms/step\n",
      "Epoch 15/10000\n",
      "113/113 - 1s - loss: 31027.4902 - accuracy: 0.5580 - val_loss: 29148.1484 - val_accuracy: 0.5649 - 550ms/epoch - 5ms/step\n",
      "Epoch 16/10000\n",
      "113/113 - 0s - loss: 27905.9355 - accuracy: 0.5578 - val_loss: 25719.0000 - val_accuracy: 0.5633 - 328ms/epoch - 3ms/step\n",
      "Epoch 17/10000\n",
      "113/113 - 0s - loss: 25359.5918 - accuracy: 0.5578 - val_loss: 23718.0078 - val_accuracy: 0.5623 - 340ms/epoch - 3ms/step\n",
      "Epoch 18/10000\n",
      "113/113 - 0s - loss: 23906.8145 - accuracy: 0.5606 - val_loss: 25760.0840 - val_accuracy: 0.5808 - 361ms/epoch - 3ms/step\n",
      "Epoch 19/10000\n",
      "113/113 - 1s - loss: 22189.1934 - accuracy: 0.5617 - val_loss: 18766.2871 - val_accuracy: 0.5633 - 577ms/epoch - 5ms/step\n",
      "Epoch 20/10000\n",
      "113/113 - 0s - loss: 20194.5117 - accuracy: 0.5635 - val_loss: 16618.6621 - val_accuracy: 0.5672 - 399ms/epoch - 4ms/step\n",
      "Epoch 21/10000\n",
      "113/113 - 0s - loss: 20249.1348 - accuracy: 0.5652 - val_loss: 16829.9746 - val_accuracy: 0.5650 - 331ms/epoch - 3ms/step\n",
      "Epoch 22/10000\n",
      "113/113 - 0s - loss: 15667.6445 - accuracy: 0.5651 - val_loss: 21720.4785 - val_accuracy: 0.5273 - 443ms/epoch - 4ms/step\n",
      "Epoch 23/10000\n",
      "113/113 - 1s - loss: 18590.4629 - accuracy: 0.5665 - val_loss: 15827.1855 - val_accuracy: 0.5472 - 613ms/epoch - 5ms/step\n",
      "Epoch 24/10000\n",
      "113/113 - 1s - loss: 13956.1104 - accuracy: 0.5720 - val_loss: 13400.8203 - val_accuracy: 0.5486 - 601ms/epoch - 5ms/step\n",
      "Epoch 25/10000\n",
      "113/113 - 1s - loss: 12952.6631 - accuracy: 0.5692 - val_loss: 13409.1943 - val_accuracy: 0.5984 - 591ms/epoch - 5ms/step\n",
      "Epoch 26/10000\n",
      "113/113 - 1s - loss: 12195.6387 - accuracy: 0.5713 - val_loss: 11355.0068 - val_accuracy: 0.5464 - 563ms/epoch - 5ms/step\n",
      "Epoch 27/10000\n",
      "113/113 - 0s - loss: 11022.1855 - accuracy: 0.5743 - val_loss: 7943.5083 - val_accuracy: 0.5822 - 411ms/epoch - 4ms/step\n",
      "Epoch 28/10000\n",
      "113/113 - 0s - loss: 11721.6709 - accuracy: 0.5725 - val_loss: 10066.1758 - val_accuracy: 0.5550 - 485ms/epoch - 4ms/step\n",
      "Epoch 29/10000\n",
      "113/113 - 0s - loss: 11265.4600 - accuracy: 0.5757 - val_loss: 16233.8828 - val_accuracy: 0.5129 - 408ms/epoch - 4ms/step\n",
      "Epoch 30/10000\n",
      "113/113 - 0s - loss: 9967.1904 - accuracy: 0.5820 - val_loss: 7852.4121 - val_accuracy: 0.5961 - 426ms/epoch - 4ms/step\n",
      "Epoch 31/10000\n",
      "113/113 - 1s - loss: 7911.2256 - accuracy: 0.5822 - val_loss: 9641.9053 - val_accuracy: 0.6002 - 560ms/epoch - 5ms/step\n",
      "Epoch 32/10000\n",
      "113/113 - 1s - loss: 11080.2822 - accuracy: 0.5767 - val_loss: 6291.2944 - val_accuracy: 0.5908 - 650ms/epoch - 6ms/step\n",
      "Epoch 33/10000\n",
      "113/113 - 0s - loss: 9254.1504 - accuracy: 0.5797 - val_loss: 9152.6660 - val_accuracy: 0.6038 - 459ms/epoch - 4ms/step\n",
      "Epoch 34/10000\n",
      "113/113 - 0s - loss: 10606.9707 - accuracy: 0.5762 - val_loss: 10993.5713 - val_accuracy: 0.5329 - 410ms/epoch - 4ms/step\n",
      "Epoch 35/10000\n",
      "113/113 - 0s - loss: 10773.9766 - accuracy: 0.5758 - val_loss: 10306.6553 - val_accuracy: 0.6079 - 456ms/epoch - 4ms/step\n",
      "Epoch 36/10000\n",
      "113/113 - 0s - loss: 10167.2412 - accuracy: 0.5758 - val_loss: 12074.4766 - val_accuracy: 0.5294 - 347ms/epoch - 3ms/step\n",
      "Epoch 37/10000\n",
      "113/113 - 0s - loss: 8051.9399 - accuracy: 0.5880 - val_loss: 5751.2642 - val_accuracy: 0.6027 - 312ms/epoch - 3ms/step\n",
      "Epoch 38/10000\n",
      "113/113 - 0s - loss: 9787.2422 - accuracy: 0.5759 - val_loss: 11227.7148 - val_accuracy: 0.5376 - 323ms/epoch - 3ms/step\n",
      "Epoch 39/10000\n",
      "113/113 - 0s - loss: 9987.6875 - accuracy: 0.5760 - val_loss: 9067.0391 - val_accuracy: 0.6034 - 341ms/epoch - 3ms/step\n",
      "Epoch 40/10000\n",
      "113/113 - 0s - loss: 9716.0576 - accuracy: 0.5775 - val_loss: 6240.9248 - val_accuracy: 0.5916 - 311ms/epoch - 3ms/step\n",
      "Epoch 41/10000\n",
      "113/113 - 0s - loss: 8379.9297 - accuracy: 0.5847 - val_loss: 7209.7983 - val_accuracy: 0.5663 - 326ms/epoch - 3ms/step\n",
      "Epoch 42/10000\n",
      "113/113 - 0s - loss: 8217.8799 - accuracy: 0.5857 - val_loss: 29042.5430 - val_accuracy: 0.6106 - 328ms/epoch - 3ms/step\n",
      "Epoch 43/10000\n",
      "113/113 - 0s - loss: 12719.0508 - accuracy: 0.5755 - val_loss: 9154.0088 - val_accuracy: 0.5366 - 337ms/epoch - 3ms/step\n",
      "Epoch 44/10000\n",
      "113/113 - 0s - loss: 6746.2002 - accuracy: 0.5885 - val_loss: 5938.5327 - val_accuracy: 0.6020 - 384ms/epoch - 3ms/step\n",
      "Epoch 45/10000\n",
      "113/113 - 1s - loss: 8585.5264 - accuracy: 0.5779 - val_loss: 5230.8120 - val_accuracy: 0.6090 - 541ms/epoch - 5ms/step\n",
      "Epoch 46/10000\n",
      "113/113 - 1s - loss: 9290.2275 - accuracy: 0.5767 - val_loss: 6956.9165 - val_accuracy: 0.5684 - 554ms/epoch - 5ms/step\n",
      "Epoch 47/10000\n",
      "113/113 - 1s - loss: 8268.3936 - accuracy: 0.5832 - val_loss: 27456.9688 - val_accuracy: 0.6096 - 578ms/epoch - 5ms/step\n",
      "Epoch 48/10000\n",
      "113/113 - 0s - loss: 11923.4336 - accuracy: 0.5716 - val_loss: 7547.3726 - val_accuracy: 0.5666 - 490ms/epoch - 4ms/step\n",
      "Epoch 49/10000\n",
      "113/113 - 1s - loss: 7554.7866 - accuracy: 0.5901 - val_loss: 10312.5371 - val_accuracy: 0.6046 - 545ms/epoch - 5ms/step\n",
      "Epoch 50/10000\n",
      "113/113 - 1s - loss: 8521.8711 - accuracy: 0.5787 - val_loss: 6615.2539 - val_accuracy: 0.6011 - 504ms/epoch - 4ms/step\n",
      "Epoch 51/10000\n",
      "113/113 - 0s - loss: 9113.9004 - accuracy: 0.5791 - val_loss: 7395.0884 - val_accuracy: 0.5610 - 446ms/epoch - 4ms/step\n",
      "Epoch 52/10000\n",
      "113/113 - 0s - loss: 9547.1562 - accuracy: 0.5840 - val_loss: 4971.0620 - val_accuracy: 0.6082 - 457ms/epoch - 4ms/step\n",
      "435/435 [==============================] - 1s 2ms/step - loss: 28921.3125 - accuracy: 0.6119\n",
      "Larghezza: 50 \n",
      "Profondità: 2 \n",
      "Dimensione del batch: 1000\n",
      "Test loss (binary_crossentropy): 28921.3125\n",
      "Test metrics (accuracy): 0.6118984222412109\n",
      "Epoch 1/10000\n",
      "11260/11260 - 19s - loss: 1.0592 - accuracy: 0.6054 - val_loss: 0.6584 - val_accuracy: 0.6156 - 19s/epoch - 2ms/step\n",
      "Epoch 2/10000\n",
      "11260/11260 - 21s - loss: 0.6596 - accuracy: 0.6076 - val_loss: 0.6550 - val_accuracy: 0.6163 - 21s/epoch - 2ms/step\n",
      "Epoch 3/10000\n",
      "11260/11260 - 21s - loss: 0.6566 - accuracy: 0.6175 - val_loss: 0.6511 - val_accuracy: 0.6346 - 21s/epoch - 2ms/step\n",
      "Epoch 4/10000\n",
      "11260/11260 - 22s - loss: 0.6570 - accuracy: 0.6254 - val_loss: 0.6501 - val_accuracy: 0.6298 - 22s/epoch - 2ms/step\n",
      "Epoch 5/10000\n",
      "11260/11260 - 20s - loss: 0.6572 - accuracy: 0.6195 - val_loss: 0.6535 - val_accuracy: 0.6159 - 20s/epoch - 2ms/step\n",
      "Epoch 6/10000\n",
      "11260/11260 - 24s - loss: 0.6573 - accuracy: 0.6137 - val_loss: 0.6617 - val_accuracy: 0.6169 - 24s/epoch - 2ms/step\n",
      "Epoch 7/10000\n",
      "11260/11260 - 20s - loss: 0.6575 - accuracy: 0.6134 - val_loss: 0.6563 - val_accuracy: 0.6172 - 20s/epoch - 2ms/step\n",
      "Epoch 8/10000\n",
      "11260/11260 - 19s - loss: 0.6610 - accuracy: 0.6109 - val_loss: 0.6574 - val_accuracy: 0.6150 - 19s/epoch - 2ms/step\n",
      "Epoch 9/10000\n",
      "11260/11260 - 19s - loss: 0.6620 - accuracy: 0.6112 - val_loss: 0.6609 - val_accuracy: 0.6155 - 19s/epoch - 2ms/step\n",
      "Epoch 10/10000\n",
      "11260/11260 - 22s - loss: 0.6623 - accuracy: 0.6114 - val_loss: 0.6628 - val_accuracy: 0.6172 - 22s/epoch - 2ms/step\n",
      "Epoch 11/10000\n",
      "11260/11260 - 18s - loss: 0.6615 - accuracy: 0.6114 - val_loss: 0.6583 - val_accuracy: 0.6163 - 18s/epoch - 2ms/step\n",
      "Epoch 12/10000\n",
      "11260/11260 - 24s - loss: 0.6605 - accuracy: 0.6099 - val_loss: 0.6571 - val_accuracy: 0.6140 - 24s/epoch - 2ms/step\n",
      "Epoch 13/10000\n",
      "11260/11260 - 20s - loss: 0.6586 - accuracy: 0.6096 - val_loss: 0.6533 - val_accuracy: 0.6161 - 20s/epoch - 2ms/step\n",
      "435/435 [==============================] - 1s 1ms/step - loss: 0.6517 - accuracy: 0.6372\n",
      "Larghezza: 50 \n",
      "Profondità: 2 \n",
      "Dimensione del batch: 10\n",
      "Test loss (binary_crossentropy): 0.651658296585083\n",
      "Test metrics (accuracy): 0.6372203230857849\n",
      "Epoch 1/10000\n",
      "1126/1126 - 4s - loss: 2.3872 - accuracy: 0.5964 - val_loss: 0.8053 - val_accuracy: 0.6035 - 4s/epoch - 3ms/step\n",
      "Epoch 2/10000\n",
      "1126/1126 - 2s - loss: 0.7619 - accuracy: 0.6046 - val_loss: 0.7160 - val_accuracy: 0.6101 - 2s/epoch - 2ms/step\n",
      "Epoch 3/10000\n",
      "1126/1126 - 2s - loss: 0.7023 - accuracy: 0.6070 - val_loss: 0.6806 - val_accuracy: 0.6147 - 2s/epoch - 2ms/step\n",
      "Epoch 4/10000\n",
      "1126/1126 - 3s - loss: 0.6834 - accuracy: 0.6101 - val_loss: 0.6716 - val_accuracy: 0.6163 - 3s/epoch - 2ms/step\n",
      "Epoch 5/10000\n",
      "1126/1126 - 2s - loss: 0.6765 - accuracy: 0.6099 - val_loss: 0.6674 - val_accuracy: 0.6151 - 2s/epoch - 2ms/step\n",
      "Epoch 6/10000\n",
      "1126/1126 - 3s - loss: 0.6707 - accuracy: 0.6074 - val_loss: 0.6633 - val_accuracy: 0.6167 - 3s/epoch - 3ms/step\n",
      "Epoch 7/10000\n",
      "1126/1126 - 3s - loss: 0.6671 - accuracy: 0.6071 - val_loss: 0.6603 - val_accuracy: 0.6163 - 3s/epoch - 2ms/step\n",
      "Epoch 8/10000\n",
      "1126/1126 - 3s - loss: 0.6660 - accuracy: 0.6078 - val_loss: 0.6615 - val_accuracy: 0.6166 - 3s/epoch - 2ms/step\n",
      "Epoch 9/10000\n",
      "1126/1126 - 3s - loss: 0.6637 - accuracy: 0.6110 - val_loss: 0.6614 - val_accuracy: 0.6163 - 3s/epoch - 3ms/step\n",
      "Epoch 10/10000\n",
      "1126/1126 - 2s - loss: 0.6620 - accuracy: 0.6117 - val_loss: 0.6622 - val_accuracy: 0.6156 - 2s/epoch - 2ms/step\n",
      "Epoch 11/10000\n",
      "1126/1126 - 2s - loss: 0.6626 - accuracy: 0.6100 - val_loss: 0.6613 - val_accuracy: 0.6149 - 2s/epoch - 2ms/step\n",
      "Epoch 12/10000\n",
      "1126/1126 - 2s - loss: 0.6617 - accuracy: 0.6122 - val_loss: 0.6582 - val_accuracy: 0.6155 - 2s/epoch - 2ms/step\n",
      "Epoch 13/10000\n",
      "1126/1126 - 2s - loss: 0.6609 - accuracy: 0.6120 - val_loss: 0.6585 - val_accuracy: 0.6157 - 2s/epoch - 2ms/step\n",
      "Epoch 14/10000\n",
      "1126/1126 - 2s - loss: 0.6608 - accuracy: 0.6133 - val_loss: 0.6596 - val_accuracy: 0.6167 - 2s/epoch - 2ms/step\n",
      "Epoch 15/10000\n",
      "1126/1126 - 2s - loss: 0.6606 - accuracy: 0.6112 - val_loss: 0.6590 - val_accuracy: 0.6167 - 2s/epoch - 2ms/step\n",
      "Epoch 16/10000\n",
      "1126/1126 - 3s - loss: 0.6601 - accuracy: 0.6125 - val_loss: 0.6563 - val_accuracy: 0.6161 - 3s/epoch - 3ms/step\n",
      "Epoch 17/10000\n",
      "1126/1126 - 3s - loss: 0.6576 - accuracy: 0.6129 - val_loss: 0.6556 - val_accuracy: 0.6176 - 3s/epoch - 2ms/step\n",
      "Epoch 18/10000\n",
      "1126/1126 - 2s - loss: 0.6582 - accuracy: 0.6128 - val_loss: 0.6619 - val_accuracy: 0.6169 - 2s/epoch - 2ms/step\n",
      "Epoch 19/10000\n",
      "1126/1126 - 3s - loss: 0.6568 - accuracy: 0.6140 - val_loss: 0.6600 - val_accuracy: 0.6172 - 3s/epoch - 2ms/step\n",
      "Epoch 20/10000\n",
      "1126/1126 - 2s - loss: 0.6574 - accuracy: 0.6127 - val_loss: 0.6582 - val_accuracy: 0.6179 - 2s/epoch - 2ms/step\n",
      "Epoch 21/10000\n",
      "1126/1126 - 2s - loss: 0.6571 - accuracy: 0.6129 - val_loss: 0.6582 - val_accuracy: 0.6179 - 2s/epoch - 2ms/step\n",
      "Epoch 22/10000\n",
      "1126/1126 - 2s - loss: 0.6569 - accuracy: 0.6143 - val_loss: 0.6619 - val_accuracy: 0.5460 - 2s/epoch - 1ms/step\n",
      "Epoch 23/10000\n",
      "1126/1126 - 2s - loss: 0.6569 - accuracy: 0.6131 - val_loss: 0.6541 - val_accuracy: 0.6196 - 2s/epoch - 2ms/step\n",
      "Epoch 24/10000\n",
      "1126/1126 - 3s - loss: 0.6565 - accuracy: 0.6138 - val_loss: 0.6560 - val_accuracy: 0.6195 - 3s/epoch - 3ms/step\n",
      "Epoch 25/10000\n",
      "1126/1126 - 3s - loss: 0.6560 - accuracy: 0.6145 - val_loss: 0.6544 - val_accuracy: 0.6197 - 3s/epoch - 2ms/step\n",
      "Epoch 26/10000\n",
      "1126/1126 - 3s - loss: 0.6552 - accuracy: 0.6145 - val_loss: 0.6537 - val_accuracy: 0.6196 - 3s/epoch - 3ms/step\n",
      "Epoch 27/10000\n",
      "1126/1126 - 3s - loss: 0.6549 - accuracy: 0.6164 - val_loss: 0.6558 - val_accuracy: 0.6178 - 3s/epoch - 2ms/step\n",
      "Epoch 28/10000\n",
      "1126/1126 - 3s - loss: 0.6565 - accuracy: 0.6141 - val_loss: 0.6538 - val_accuracy: 0.6170 - 3s/epoch - 3ms/step\n",
      "Epoch 29/10000\n",
      "1126/1126 - 2s - loss: 0.6564 - accuracy: 0.6133 - val_loss: 0.6606 - val_accuracy: 0.5439 - 2s/epoch - 2ms/step\n",
      "Epoch 30/10000\n",
      "1126/1126 - 3s - loss: 0.6560 - accuracy: 0.6122 - val_loss: 0.6562 - val_accuracy: 0.6180 - 3s/epoch - 3ms/step\n",
      "Epoch 31/10000\n",
      "1126/1126 - 2s - loss: 0.6558 - accuracy: 0.6139 - val_loss: 0.6550 - val_accuracy: 0.6143 - 2s/epoch - 2ms/step\n",
      "Epoch 32/10000\n",
      "1126/1126 - 2s - loss: 0.6560 - accuracy: 0.6142 - val_loss: 0.6549 - val_accuracy: 0.6176 - 2s/epoch - 2ms/step\n",
      "Epoch 33/10000\n",
      "1126/1126 - 2s - loss: 0.6561 - accuracy: 0.6141 - val_loss: 0.6539 - val_accuracy: 0.6127 - 2s/epoch - 2ms/step\n",
      "Epoch 34/10000\n",
      "1126/1126 - 3s - loss: 0.6570 - accuracy: 0.6148 - val_loss: 0.6589 - val_accuracy: 0.6142 - 3s/epoch - 2ms/step\n",
      "Epoch 35/10000\n",
      "1126/1126 - 3s - loss: 0.6569 - accuracy: 0.6134 - val_loss: 0.6569 - val_accuracy: 0.6172 - 3s/epoch - 3ms/step\n",
      "435/435 [==============================] - 1s 2ms/step - loss: 0.6519 - accuracy: 0.6226\n",
      "Larghezza: 50 \n",
      "Profondità: 2 \n",
      "Dimensione del batch: 100\n",
      "Test loss (binary_crossentropy): 0.6518840193748474\n",
      "Test metrics (accuracy): 0.6226170659065247\n",
      "Epoch 1/10000\n",
      "113/113 - 1s - loss: 27.9473 - accuracy: 0.3887 - val_loss: 22.8754 - val_accuracy: 0.3869 - 1s/epoch - 13ms/step\n",
      "Epoch 2/10000\n",
      "113/113 - 1s - loss: 13.9513 - accuracy: 0.3887 - val_loss: 3.1723 - val_accuracy: 0.3874 - 607ms/epoch - 5ms/step\n",
      "Epoch 3/10000\n",
      "113/113 - 1s - loss: 1.9953 - accuracy: 0.5289 - val_loss: 1.6912 - val_accuracy: 0.5446 - 535ms/epoch - 5ms/step\n",
      "Epoch 4/10000\n",
      "113/113 - 1s - loss: 1.1820 - accuracy: 0.5736 - val_loss: 0.7813 - val_accuracy: 0.6169 - 894ms/epoch - 8ms/step\n",
      "Epoch 5/10000\n",
      "113/113 - 1s - loss: 0.7962 - accuracy: 0.6132 - val_loss: 0.7699 - val_accuracy: 0.6161 - 616ms/epoch - 5ms/step\n",
      "Epoch 6/10000\n",
      "113/113 - 1s - loss: 0.7786 - accuracy: 0.6132 - val_loss: 0.7591 - val_accuracy: 0.6172 - 529ms/epoch - 5ms/step\n",
      "Epoch 7/10000\n",
      "113/113 - 0s - loss: 0.7697 - accuracy: 0.6130 - val_loss: 0.7547 - val_accuracy: 0.6165 - 455ms/epoch - 4ms/step\n",
      "Epoch 8/10000\n",
      "113/113 - 0s - loss: 0.7587 - accuracy: 0.6130 - val_loss: 0.7272 - val_accuracy: 0.6172 - 433ms/epoch - 4ms/step\n",
      "Epoch 9/10000\n",
      "113/113 - 0s - loss: 0.7327 - accuracy: 0.6133 - val_loss: 0.7100 - val_accuracy: 0.6170 - 338ms/epoch - 3ms/step\n",
      "Epoch 10/10000\n",
      "113/113 - 0s - loss: 0.7180 - accuracy: 0.6136 - val_loss: 0.7016 - val_accuracy: 0.6171 - 325ms/epoch - 3ms/step\n",
      "Epoch 11/10000\n",
      "113/113 - 0s - loss: 0.7118 - accuracy: 0.6140 - val_loss: 0.6992 - val_accuracy: 0.6131 - 321ms/epoch - 3ms/step\n",
      "Epoch 12/10000\n",
      "113/113 - 0s - loss: 0.7073 - accuracy: 0.6145 - val_loss: 0.6939 - val_accuracy: 0.6171 - 322ms/epoch - 3ms/step\n",
      "Epoch 13/10000\n",
      "113/113 - 0s - loss: 0.7023 - accuracy: 0.6144 - val_loss: 0.6911 - val_accuracy: 0.6167 - 389ms/epoch - 3ms/step\n",
      "Epoch 14/10000\n",
      "113/113 - 0s - loss: 0.6998 - accuracy: 0.6142 - val_loss: 0.6880 - val_accuracy: 0.6167 - 399ms/epoch - 4ms/step\n",
      "Epoch 15/10000\n",
      "113/113 - 0s - loss: 0.6961 - accuracy: 0.6138 - val_loss: 0.6863 - val_accuracy: 0.6163 - 338ms/epoch - 3ms/step\n",
      "Epoch 16/10000\n",
      "113/113 - 0s - loss: 0.6945 - accuracy: 0.6140 - val_loss: 0.6853 - val_accuracy: 0.6177 - 362ms/epoch - 3ms/step\n",
      "Epoch 17/10000\n",
      "113/113 - 0s - loss: 0.6927 - accuracy: 0.6140 - val_loss: 0.6836 - val_accuracy: 0.6167 - 377ms/epoch - 3ms/step\n",
      "Epoch 18/10000\n",
      "113/113 - 0s - loss: 0.6911 - accuracy: 0.6140 - val_loss: 0.6808 - val_accuracy: 0.6175 - 337ms/epoch - 3ms/step\n",
      "Epoch 19/10000\n",
      "113/113 - 0s - loss: 0.6894 - accuracy: 0.6143 - val_loss: 0.6835 - val_accuracy: 0.6137 - 353ms/epoch - 3ms/step\n",
      "Epoch 20/10000\n",
      "113/113 - 0s - loss: 0.6883 - accuracy: 0.6142 - val_loss: 0.6795 - val_accuracy: 0.6167 - 371ms/epoch - 3ms/step\n",
      "Epoch 21/10000\n",
      "113/113 - 0s - loss: 0.6855 - accuracy: 0.6144 - val_loss: 0.6771 - val_accuracy: 0.6175 - 466ms/epoch - 4ms/step\n",
      "Epoch 22/10000\n",
      "113/113 - 0s - loss: 0.6840 - accuracy: 0.6141 - val_loss: 0.6765 - val_accuracy: 0.6173 - 395ms/epoch - 3ms/step\n",
      "Epoch 23/10000\n",
      "113/113 - 0s - loss: 0.6814 - accuracy: 0.6143 - val_loss: 0.6749 - val_accuracy: 0.6172 - 355ms/epoch - 3ms/step\n",
      "Epoch 24/10000\n",
      "113/113 - 0s - loss: 0.6794 - accuracy: 0.6145 - val_loss: 0.6738 - val_accuracy: 0.6180 - 369ms/epoch - 3ms/step\n",
      "Epoch 25/10000\n",
      "113/113 - 0s - loss: 0.6778 - accuracy: 0.6147 - val_loss: 0.6728 - val_accuracy: 0.6175 - 408ms/epoch - 4ms/step\n",
      "Epoch 26/10000\n",
      "113/113 - 0s - loss: 0.6768 - accuracy: 0.6150 - val_loss: 0.6731 - val_accuracy: 0.6182 - 399ms/epoch - 4ms/step\n",
      "Epoch 27/10000\n",
      "113/113 - 0s - loss: 0.6763 - accuracy: 0.6151 - val_loss: 0.6726 - val_accuracy: 0.6175 - 344ms/epoch - 3ms/step\n",
      "Epoch 28/10000\n",
      "113/113 - 0s - loss: 0.6754 - accuracy: 0.6152 - val_loss: 0.6709 - val_accuracy: 0.6182 - 471ms/epoch - 4ms/step\n",
      "Epoch 29/10000\n",
      "113/113 - 0s - loss: 0.6751 - accuracy: 0.6150 - val_loss: 0.6723 - val_accuracy: 0.6177 - 374ms/epoch - 3ms/step\n",
      "Epoch 30/10000\n",
      "113/113 - 0s - loss: 0.6747 - accuracy: 0.6154 - val_loss: 0.6726 - val_accuracy: 0.6180 - 331ms/epoch - 3ms/step\n",
      "Epoch 31/10000\n",
      "113/113 - 0s - loss: 0.6745 - accuracy: 0.6151 - val_loss: 0.6725 - val_accuracy: 0.6181 - 372ms/epoch - 3ms/step\n",
      "Epoch 32/10000\n",
      "113/113 - 0s - loss: 0.6745 - accuracy: 0.6151 - val_loss: 0.6732 - val_accuracy: 0.6180 - 357ms/epoch - 3ms/step\n",
      "Epoch 33/10000\n",
      "113/113 - 0s - loss: 0.6740 - accuracy: 0.6158 - val_loss: 0.6735 - val_accuracy: 0.6185 - 359ms/epoch - 3ms/step\n",
      "Epoch 34/10000\n",
      "113/113 - 0s - loss: 0.6731 - accuracy: 0.6157 - val_loss: 0.6760 - val_accuracy: 0.6186 - 387ms/epoch - 3ms/step\n",
      "Epoch 35/10000\n",
      "113/113 - 0s - loss: 0.6730 - accuracy: 0.6156 - val_loss: 0.6740 - val_accuracy: 0.6180 - 360ms/epoch - 3ms/step\n",
      "Epoch 36/10000\n",
      "113/113 - 0s - loss: 0.6728 - accuracy: 0.6156 - val_loss: 0.6735 - val_accuracy: 0.6179 - 339ms/epoch - 3ms/step\n",
      "Epoch 37/10000\n",
      "113/113 - 0s - loss: 0.6724 - accuracy: 0.6157 - val_loss: 0.6733 - val_accuracy: 0.6183 - 385ms/epoch - 3ms/step\n",
      "Epoch 38/10000\n",
      "113/113 - 0s - loss: 0.6717 - accuracy: 0.6157 - val_loss: 0.6720 - val_accuracy: 0.6178 - 478ms/epoch - 4ms/step\n",
      "Epoch 39/10000\n",
      "113/113 - 0s - loss: 0.6714 - accuracy: 0.6157 - val_loss: 0.6703 - val_accuracy: 0.6176 - 368ms/epoch - 3ms/step\n",
      "Epoch 40/10000\n",
      "113/113 - 0s - loss: 0.6715 - accuracy: 0.6156 - val_loss: 0.6734 - val_accuracy: 0.6174 - 369ms/epoch - 3ms/step\n",
      "Epoch 41/10000\n",
      "113/113 - 0s - loss: 0.6711 - accuracy: 0.6155 - val_loss: 0.6702 - val_accuracy: 0.6178 - 373ms/epoch - 3ms/step\n",
      "Epoch 42/10000\n",
      "113/113 - 0s - loss: 0.6713 - accuracy: 0.6156 - val_loss: 0.6724 - val_accuracy: 0.6176 - 375ms/epoch - 3ms/step\n",
      "Epoch 43/10000\n",
      "113/113 - 0s - loss: 0.6709 - accuracy: 0.6154 - val_loss: 0.6702 - val_accuracy: 0.6178 - 340ms/epoch - 3ms/step\n",
      "Epoch 44/10000\n",
      "113/113 - 0s - loss: 0.6710 - accuracy: 0.6153 - val_loss: 0.6704 - val_accuracy: 0.6179 - 365ms/epoch - 3ms/step\n",
      "435/435 [==============================] - 1s 2ms/step - loss: 0.6723 - accuracy: 0.6210\n",
      "Larghezza: 50 \n",
      "Profondità: 2 \n",
      "Dimensione del batch: 1000\n",
      "Test loss (binary_crossentropy): 0.6722647547721863\n",
      "Test metrics (accuracy): 0.6209625005722046\n",
      "Epoch 1/10000\n",
      "11260/11260 - 22s - loss: 2436745.7500 - accuracy: 0.5722 - val_loss: 520729.0000 - val_accuracy: 0.6178 - 22s/epoch - 2ms/step\n",
      "Epoch 2/10000\n",
      "11260/11260 - 18s - loss: 895708.5000 - accuracy: 0.5673 - val_loss: 1652218.2500 - val_accuracy: 0.6112 - 18s/epoch - 2ms/step\n",
      "Epoch 3/10000\n",
      "11260/11260 - 22s - loss: 851724.5000 - accuracy: 0.5735 - val_loss: 301234.4062 - val_accuracy: 0.6275 - 22s/epoch - 2ms/step\n",
      "Epoch 4/10000\n",
      "11260/11260 - 26s - loss: 810519.9375 - accuracy: 0.5756 - val_loss: 303794.6875 - val_accuracy: 0.6290 - 26s/epoch - 2ms/step\n",
      "Epoch 5/10000\n",
      "11260/11260 - 25s - loss: 787771.5000 - accuracy: 0.5748 - val_loss: 596409.6875 - val_accuracy: 0.6262 - 25s/epoch - 2ms/step\n",
      "Epoch 6/10000\n",
      "11260/11260 - 21s - loss: 749553.0000 - accuracy: 0.5772 - val_loss: 1659966.5000 - val_accuracy: 0.4271 - 21s/epoch - 2ms/step\n",
      "Epoch 7/10000\n",
      "11260/11260 - 22s - loss: 789326.1250 - accuracy: 0.5803 - val_loss: 697076.3750 - val_accuracy: 0.6227 - 22s/epoch - 2ms/step\n",
      "Epoch 8/10000\n",
      "11260/11260 - 23s - loss: 802107.4375 - accuracy: 0.5788 - val_loss: 2013526.5000 - val_accuracy: 0.6187 - 23s/epoch - 2ms/step\n",
      "Epoch 9/10000\n",
      "11260/11260 - 21s - loss: 740817.6875 - accuracy: 0.5811 - val_loss: 778153.0000 - val_accuracy: 0.4777 - 21s/epoch - 2ms/step\n",
      "Epoch 10/10000\n",
      "11260/11260 - 20s - loss: 720386.3125 - accuracy: 0.5791 - val_loss: 750859.7500 - val_accuracy: 0.6167 - 20s/epoch - 2ms/step\n",
      "Epoch 11/10000\n",
      "11260/11260 - 20s - loss: 776688.1250 - accuracy: 0.5807 - val_loss: 1331664.8750 - val_accuracy: 0.6156 - 20s/epoch - 2ms/step\n",
      "Epoch 12/10000\n",
      "11260/11260 - 20s - loss: 740077.9375 - accuracy: 0.5814 - val_loss: 567342.4375 - val_accuracy: 0.6157 - 20s/epoch - 2ms/step\n",
      "Epoch 13/10000\n",
      "11260/11260 - 20s - loss: 715687.1250 - accuracy: 0.5832 - val_loss: 929965.8750 - val_accuracy: 0.4976 - 20s/epoch - 2ms/step\n",
      "Epoch 14/10000\n",
      "11260/11260 - 23s - loss: 717501.0000 - accuracy: 0.5828 - val_loss: 246840.9062 - val_accuracy: 0.6517 - 23s/epoch - 2ms/step\n",
      "Epoch 15/10000\n",
      "11260/11260 - 23s - loss: 762851.6250 - accuracy: 0.5860 - val_loss: 2064692.7500 - val_accuracy: 0.4190 - 23s/epoch - 2ms/step\n",
      "Epoch 16/10000\n",
      "11260/11260 - 22s - loss: 738342.8125 - accuracy: 0.5860 - val_loss: 1062797.8750 - val_accuracy: 0.6158 - 22s/epoch - 2ms/step\n",
      "Epoch 17/10000\n",
      "11260/11260 - 24s - loss: 750701.2500 - accuracy: 0.5829 - val_loss: 1140053.6250 - val_accuracy: 0.6207 - 24s/epoch - 2ms/step\n",
      "Epoch 18/10000\n",
      "11260/11260 - 20s - loss: 689243.0625 - accuracy: 0.5857 - val_loss: 528708.0625 - val_accuracy: 0.4996 - 20s/epoch - 2ms/step\n",
      "Epoch 19/10000\n",
      "11260/11260 - 19s - loss: 702496.0625 - accuracy: 0.5846 - val_loss: 1399235.0000 - val_accuracy: 0.6184 - 19s/epoch - 2ms/step\n",
      "Epoch 20/10000\n",
      "11260/11260 - 18s - loss: 709097.1875 - accuracy: 0.5847 - val_loss: 255088.7500 - val_accuracy: 0.6316 - 18s/epoch - 2ms/step\n",
      "Epoch 21/10000\n",
      "11260/11260 - 22s - loss: 663400.1875 - accuracy: 0.5849 - val_loss: 216287.8906 - val_accuracy: 0.5569 - 22s/epoch - 2ms/step\n",
      "Epoch 22/10000\n",
      "11260/11260 - 21s - loss: 695937.6250 - accuracy: 0.5873 - val_loss: 474706.4062 - val_accuracy: 0.6331 - 21s/epoch - 2ms/step\n",
      "Epoch 23/10000\n",
      "11260/11260 - 18s - loss: 666076.7500 - accuracy: 0.5875 - val_loss: 229657.4531 - val_accuracy: 0.6299 - 18s/epoch - 2ms/step\n",
      "Epoch 24/10000\n",
      "11260/11260 - 18s - loss: 639915.5000 - accuracy: 0.5874 - val_loss: 410814.5312 - val_accuracy: 0.6262 - 18s/epoch - 2ms/step\n",
      "435/435 [==============================] - 1s 1ms/step - loss: 249339.7969 - accuracy: 0.6469\n",
      "Larghezza: 50 \n",
      "Profondità: 3 \n",
      "Dimensione del batch: 10\n",
      "Test loss (binary_crossentropy): 249339.796875\n",
      "Test metrics (accuracy): 0.6469318866729736\n",
      "Epoch 1/10000\n",
      "1126/1126 - 3s - loss: 220433136.0000 - accuracy: 0.5228 - val_loss: 2428749.7500 - val_accuracy: 0.5740 - 3s/epoch - 2ms/step\n",
      "Epoch 2/10000\n",
      "1126/1126 - 2s - loss: 2048265.8750 - accuracy: 0.5678 - val_loss: 1525418.2500 - val_accuracy: 0.5800 - 2s/epoch - 2ms/step\n",
      "Epoch 3/10000\n",
      "1126/1126 - 2s - loss: 1243736.3750 - accuracy: 0.5660 - val_loss: 916833.3125 - val_accuracy: 0.5774 - 2s/epoch - 2ms/step\n",
      "Epoch 4/10000\n",
      "1126/1126 - 2s - loss: 727973.9375 - accuracy: 0.5644 - val_loss: 447763.0000 - val_accuracy: 0.5835 - 2s/epoch - 2ms/step\n",
      "Epoch 5/10000\n",
      "1126/1126 - 2s - loss: 504277.8125 - accuracy: 0.5715 - val_loss: 301657.3438 - val_accuracy: 0.6018 - 2s/epoch - 2ms/step\n",
      "Epoch 6/10000\n",
      "1126/1126 - 2s - loss: 429194.0625 - accuracy: 0.5749 - val_loss: 425385.2500 - val_accuracy: 0.5107 - 2s/epoch - 2ms/step\n",
      "Epoch 7/10000\n",
      "1126/1126 - 2s - loss: 425883.8438 - accuracy: 0.5721 - val_loss: 632987.1250 - val_accuracy: 0.6149 - 2s/epoch - 2ms/step\n",
      "Epoch 8/10000\n",
      "1126/1126 - 2s - loss: 358134.3750 - accuracy: 0.5758 - val_loss: 206172.0938 - val_accuracy: 0.6230 - 2s/epoch - 2ms/step\n",
      "Epoch 9/10000\n",
      "1126/1126 - 2s - loss: 328333.7812 - accuracy: 0.5749 - val_loss: 210329.6719 - val_accuracy: 0.5828 - 2s/epoch - 2ms/step\n",
      "Epoch 10/10000\n",
      "1126/1126 - 2s - loss: 325613.4062 - accuracy: 0.5757 - val_loss: 841203.6875 - val_accuracy: 0.6164 - 2s/epoch - 2ms/step\n",
      "Epoch 11/10000\n",
      "1126/1126 - 2s - loss: 349044.5938 - accuracy: 0.5721 - val_loss: 150694.7812 - val_accuracy: 0.5993 - 2s/epoch - 2ms/step\n",
      "Epoch 12/10000\n",
      "1126/1126 - 2s - loss: 311772.8438 - accuracy: 0.5760 - val_loss: 118293.7422 - val_accuracy: 0.6099 - 2s/epoch - 2ms/step\n",
      "Epoch 13/10000\n",
      "1126/1126 - 2s - loss: 318645.8438 - accuracy: 0.5759 - val_loss: 555711.6250 - val_accuracy: 0.6166 - 2s/epoch - 2ms/step\n",
      "Epoch 14/10000\n",
      "1126/1126 - 2s - loss: 347856.9375 - accuracy: 0.5724 - val_loss: 323583.4062 - val_accuracy: 0.5014 - 2s/epoch - 2ms/step\n",
      "Epoch 15/10000\n",
      "1126/1126 - 2s - loss: 301296.6562 - accuracy: 0.5764 - val_loss: 755200.3750 - val_accuracy: 0.6179 - 2s/epoch - 2ms/step\n",
      "Epoch 16/10000\n",
      "1126/1126 - 2s - loss: 324168.9688 - accuracy: 0.5750 - val_loss: 100755.1797 - val_accuracy: 0.6235 - 2s/epoch - 2ms/step\n",
      "Epoch 17/10000\n",
      "1126/1126 - 2s - loss: 317354.7188 - accuracy: 0.5747 - val_loss: 96903.5703 - val_accuracy: 0.6334 - 2s/epoch - 2ms/step\n",
      "Epoch 18/10000\n",
      "1126/1126 - 2s - loss: 314876.9688 - accuracy: 0.5783 - val_loss: 273994.8750 - val_accuracy: 0.6160 - 2s/epoch - 2ms/step\n",
      "Epoch 19/10000\n",
      "1126/1126 - 2s - loss: 315348.4062 - accuracy: 0.5770 - val_loss: 96267.4766 - val_accuracy: 0.6394 - 2s/epoch - 2ms/step\n",
      "Epoch 20/10000\n",
      "1126/1126 - 2s - loss: 305233.3438 - accuracy: 0.5757 - val_loss: 151610.0469 - val_accuracy: 0.6140 - 2s/epoch - 2ms/step\n",
      "Epoch 21/10000\n",
      "1126/1126 - 2s - loss: 356892.7188 - accuracy: 0.5739 - val_loss: 87668.4766 - val_accuracy: 0.6288 - 2s/epoch - 2ms/step\n",
      "Epoch 22/10000\n",
      "1126/1126 - 2s - loss: 340637.4062 - accuracy: 0.5772 - val_loss: 265786.0938 - val_accuracy: 0.5355 - 2s/epoch - 2ms/step\n",
      "Epoch 23/10000\n",
      "1126/1126 - 2s - loss: 314292.0938 - accuracy: 0.5755 - val_loss: 107930.2734 - val_accuracy: 0.6319 - 2s/epoch - 2ms/step\n",
      "Epoch 24/10000\n",
      "1126/1126 - 2s - loss: 288073.6250 - accuracy: 0.5737 - val_loss: 378023.8750 - val_accuracy: 0.5056 - 2s/epoch - 2ms/step\n",
      "Epoch 25/10000\n",
      "1126/1126 - 2s - loss: 307947.8750 - accuracy: 0.5762 - val_loss: 184471.8281 - val_accuracy: 0.6216 - 2s/epoch - 2ms/step\n",
      "Epoch 26/10000\n",
      "1126/1126 - 2s - loss: 320612.9062 - accuracy: 0.5800 - val_loss: 158326.5312 - val_accuracy: 0.6216 - 2s/epoch - 2ms/step\n",
      "Epoch 27/10000\n",
      "1126/1126 - 2s - loss: 278375.7188 - accuracy: 0.5801 - val_loss: 78312.7578 - val_accuracy: 0.6470 - 2s/epoch - 2ms/step\n",
      "Epoch 28/10000\n",
      "1126/1126 - 2s - loss: 329949.2500 - accuracy: 0.5757 - val_loss: 297304.8750 - val_accuracy: 0.6195 - 2s/epoch - 2ms/step\n",
      "Epoch 29/10000\n",
      "1126/1126 - 2s - loss: 316889.6250 - accuracy: 0.5773 - val_loss: 518870.3125 - val_accuracy: 0.6154 - 2s/epoch - 2ms/step\n",
      "Epoch 30/10000\n",
      "1126/1126 - 2s - loss: 321221.9688 - accuracy: 0.5764 - val_loss: 385976.9062 - val_accuracy: 0.5019 - 2s/epoch - 2ms/step\n",
      "Epoch 31/10000\n",
      "1126/1126 - 2s - loss: 320100.5938 - accuracy: 0.5764 - val_loss: 260062.6406 - val_accuracy: 0.5207 - 2s/epoch - 2ms/step\n",
      "Epoch 32/10000\n",
      "1126/1126 - 2s - loss: 328155.3438 - accuracy: 0.5769 - val_loss: 272133.9062 - val_accuracy: 0.6207 - 2s/epoch - 2ms/step\n",
      "Epoch 33/10000\n",
      "1126/1126 - 2s - loss: 296009.3125 - accuracy: 0.5820 - val_loss: 746532.9375 - val_accuracy: 0.4345 - 2s/epoch - 2ms/step\n",
      "Epoch 34/10000\n",
      "1126/1126 - 2s - loss: 308712.7812 - accuracy: 0.5787 - val_loss: 116167.8750 - val_accuracy: 0.6387 - 2s/epoch - 2ms/step\n",
      "Epoch 35/10000\n",
      "1126/1126 - 2s - loss: 314902.2812 - accuracy: 0.5805 - val_loss: 177362.9844 - val_accuracy: 0.6247 - 2s/epoch - 2ms/step\n",
      "Epoch 36/10000\n",
      "1126/1126 - 2s - loss: 337247.9688 - accuracy: 0.5768 - val_loss: 425143.6562 - val_accuracy: 0.6164 - 2s/epoch - 2ms/step\n",
      "Epoch 37/10000\n",
      "1126/1126 - 2s - loss: 301646.9375 - accuracy: 0.5797 - val_loss: 125503.0859 - val_accuracy: 0.5944 - 2s/epoch - 2ms/step\n",
      "435/435 [==============================] - 1s 1ms/step - loss: 81096.0312 - accuracy: 0.6462\n",
      "Larghezza: 50 \n",
      "Profondità: 3 \n",
      "Dimensione del batch: 100\n",
      "Test loss (binary_crossentropy): 81096.03125\n",
      "Test metrics (accuracy): 0.6462125182151794\n",
      "Epoch 1/10000\n",
      "113/113 - 1s - loss: 57426252.0000 - accuracy: 0.4225 - val_loss: 11908557.0000 - val_accuracy: 0.5463 - 1s/epoch - 10ms/step\n",
      "Epoch 2/10000\n",
      "113/113 - 0s - loss: 6426082.5000 - accuracy: 0.5424 - val_loss: 3844519.5000 - val_accuracy: 0.5639 - 388ms/epoch - 3ms/step\n",
      "Epoch 3/10000\n",
      "113/113 - 0s - loss: 3168735.7500 - accuracy: 0.5637 - val_loss: 2464148.2500 - val_accuracy: 0.5698 - 332ms/epoch - 3ms/step\n",
      "Epoch 4/10000\n",
      "113/113 - 0s - loss: 2299028.0000 - accuracy: 0.5666 - val_loss: 2044782.2500 - val_accuracy: 0.5619 - 332ms/epoch - 3ms/step\n",
      "Epoch 5/10000\n",
      "113/113 - 0s - loss: 2022043.1250 - accuracy: 0.5662 - val_loss: 1960938.1250 - val_accuracy: 0.5760 - 330ms/epoch - 3ms/step\n",
      "Epoch 6/10000\n",
      "113/113 - 0s - loss: 1867859.2500 - accuracy: 0.5717 - val_loss: 1727509.8750 - val_accuracy: 0.5769 - 362ms/epoch - 3ms/step\n",
      "Epoch 7/10000\n",
      "113/113 - 0s - loss: 1733799.7500 - accuracy: 0.5758 - val_loss: 1648828.3750 - val_accuracy: 0.5877 - 334ms/epoch - 3ms/step\n",
      "Epoch 8/10000\n",
      "113/113 - 0s - loss: 1665670.2500 - accuracy: 0.5790 - val_loss: 1522939.6250 - val_accuracy: 0.5867 - 332ms/epoch - 3ms/step\n",
      "Epoch 9/10000\n",
      "113/113 - 0s - loss: 1711391.8750 - accuracy: 0.5799 - val_loss: 1429732.2500 - val_accuracy: 0.5856 - 318ms/epoch - 3ms/step\n",
      "Epoch 10/10000\n",
      "113/113 - 0s - loss: 1537821.5000 - accuracy: 0.5816 - val_loss: 1317019.2500 - val_accuracy: 0.5788 - 358ms/epoch - 3ms/step\n",
      "Epoch 11/10000\n",
      "113/113 - 0s - loss: 1468253.1250 - accuracy: 0.5768 - val_loss: 1343239.8750 - val_accuracy: 0.5800 - 321ms/epoch - 3ms/step\n",
      "Epoch 12/10000\n",
      "113/113 - 0s - loss: 1376278.0000 - accuracy: 0.5776 - val_loss: 1336050.7500 - val_accuracy: 0.5706 - 331ms/epoch - 3ms/step\n",
      "Epoch 13/10000\n",
      "113/113 - 0s - loss: 1376380.3750 - accuracy: 0.5773 - val_loss: 1567141.8750 - val_accuracy: 0.5972 - 364ms/epoch - 3ms/step\n",
      "Epoch 14/10000\n",
      "113/113 - 0s - loss: 1225505.1250 - accuracy: 0.5778 - val_loss: 1060141.7500 - val_accuracy: 0.5775 - 345ms/epoch - 3ms/step\n",
      "Epoch 15/10000\n",
      "113/113 - 0s - loss: 1215494.0000 - accuracy: 0.5776 - val_loss: 978388.0000 - val_accuracy: 0.5776 - 369ms/epoch - 3ms/step\n",
      "Epoch 16/10000\n",
      "113/113 - 0s - loss: 1186721.0000 - accuracy: 0.5744 - val_loss: 1284237.6250 - val_accuracy: 0.5515 - 353ms/epoch - 3ms/step\n",
      "Epoch 17/10000\n",
      "113/113 - 0s - loss: 994717.3750 - accuracy: 0.5785 - val_loss: 883983.2500 - val_accuracy: 0.5799 - 338ms/epoch - 3ms/step\n",
      "Epoch 18/10000\n",
      "113/113 - 0s - loss: 931496.5000 - accuracy: 0.5771 - val_loss: 1230921.7500 - val_accuracy: 0.5978 - 327ms/epoch - 3ms/step\n",
      "Epoch 19/10000\n",
      "113/113 - 0s - loss: 1017269.3750 - accuracy: 0.5770 - val_loss: 1554337.5000 - val_accuracy: 0.5009 - 356ms/epoch - 3ms/step\n",
      "Epoch 20/10000\n",
      "113/113 - 0s - loss: 956200.7500 - accuracy: 0.5758 - val_loss: 746129.5625 - val_accuracy: 0.5778 - 326ms/epoch - 3ms/step\n",
      "Epoch 21/10000\n",
      "113/113 - 0s - loss: 843252.8125 - accuracy: 0.5776 - val_loss: 721429.1875 - val_accuracy: 0.5663 - 353ms/epoch - 3ms/step\n",
      "Epoch 22/10000\n",
      "113/113 - 0s - loss: 921232.8750 - accuracy: 0.5750 - val_loss: 1248521.0000 - val_accuracy: 0.5196 - 317ms/epoch - 3ms/step\n",
      "Epoch 23/10000\n",
      "113/113 - 0s - loss: 964769.6250 - accuracy: 0.5718 - val_loss: 998797.8750 - val_accuracy: 0.5417 - 302ms/epoch - 3ms/step\n",
      "Epoch 24/10000\n",
      "113/113 - 0s - loss: 925222.3750 - accuracy: 0.5746 - val_loss: 722812.6875 - val_accuracy: 0.5675 - 336ms/epoch - 3ms/step\n",
      "Epoch 25/10000\n",
      "113/113 - 0s - loss: 686965.2500 - accuracy: 0.5808 - val_loss: 608533.3750 - val_accuracy: 0.5783 - 329ms/epoch - 3ms/step\n",
      "Epoch 26/10000\n",
      "113/113 - 0s - loss: 802364.6250 - accuracy: 0.5719 - val_loss: 649093.8125 - val_accuracy: 0.5815 - 318ms/epoch - 3ms/step\n",
      "Epoch 27/10000\n",
      "113/113 - 0s - loss: 1196273.3750 - accuracy: 0.5666 - val_loss: 792733.0000 - val_accuracy: 0.5316 - 329ms/epoch - 3ms/step\n",
      "Epoch 28/10000\n",
      "113/113 - 0s - loss: 760667.1250 - accuracy: 0.5792 - val_loss: 588134.1250 - val_accuracy: 0.5537 - 317ms/epoch - 3ms/step\n",
      "435/435 [==============================] - 1s 1ms/step - loss: 1214381.6250 - accuracy: 0.6010\n",
      "Larghezza: 50 \n",
      "Profondità: 3 \n",
      "Dimensione del batch: 1000\n",
      "Test loss (binary_crossentropy): 1214381.625\n",
      "Test metrics (accuracy): 0.6009639501571655\n",
      "Epoch 1/10000\n",
      "11260/11260 - 17s - loss: 1.4053 - accuracy: 0.6029 - val_loss: 0.6641 - val_accuracy: 0.6163 - 17s/epoch - 1ms/step\n",
      "Epoch 2/10000\n",
      "11260/11260 - 19s - loss: 0.6710 - accuracy: 0.6079 - val_loss: 0.6643 - val_accuracy: 0.6146 - 19s/epoch - 2ms/step\n",
      "Epoch 3/10000\n",
      "11260/11260 - 20s - loss: 0.6642 - accuracy: 0.6077 - val_loss: 0.6672 - val_accuracy: 0.6123 - 20s/epoch - 2ms/step\n",
      "Epoch 4/10000\n",
      "11260/11260 - 20s - loss: 0.6613 - accuracy: 0.6108 - val_loss: 0.6521 - val_accuracy: 0.6165 - 20s/epoch - 2ms/step\n",
      "Epoch 5/10000\n",
      "11260/11260 - 20s - loss: 0.6587 - accuracy: 0.6128 - val_loss: 0.6600 - val_accuracy: 0.6142 - 20s/epoch - 2ms/step\n",
      "Epoch 6/10000\n",
      "11260/11260 - 20s - loss: 0.6578 - accuracy: 0.6137 - val_loss: 0.6593 - val_accuracy: 0.6157 - 20s/epoch - 2ms/step\n",
      "Epoch 7/10000\n",
      "11260/11260 - 20s - loss: 0.6561 - accuracy: 0.6116 - val_loss: 0.6588 - val_accuracy: 0.5993 - 20s/epoch - 2ms/step\n",
      "Epoch 8/10000\n",
      "11260/11260 - 19s - loss: 0.6579 - accuracy: 0.6103 - val_loss: 0.6594 - val_accuracy: 0.6194 - 19s/epoch - 2ms/step\n",
      "Epoch 9/10000\n",
      "11260/11260 - 18s - loss: 0.6590 - accuracy: 0.6133 - val_loss: 0.6550 - val_accuracy: 0.6146 - 18s/epoch - 2ms/step\n",
      "Epoch 10/10000\n",
      "11260/11260 - 17s - loss: 0.6578 - accuracy: 0.6058 - val_loss: 0.6606 - val_accuracy: 0.6161 - 17s/epoch - 2ms/step\n",
      "Epoch 11/10000\n",
      "11260/11260 - 17s - loss: 0.6530 - accuracy: 0.6067 - val_loss: 0.6499 - val_accuracy: 0.6041 - 17s/epoch - 2ms/step\n",
      "Epoch 12/10000\n",
      "11260/11260 - 16s - loss: 0.6528 - accuracy: 0.6075 - val_loss: 0.6484 - val_accuracy: 0.6151 - 16s/epoch - 1ms/step\n",
      "Epoch 13/10000\n",
      "11260/11260 - 16s - loss: 0.6507 - accuracy: 0.6153 - val_loss: 0.6481 - val_accuracy: 0.6228 - 16s/epoch - 1ms/step\n",
      "Epoch 14/10000\n",
      "11260/11260 - 16s - loss: 0.6509 - accuracy: 0.6120 - val_loss: 0.6528 - val_accuracy: 0.6132 - 16s/epoch - 1ms/step\n",
      "Epoch 15/10000\n",
      "11260/11260 - 16s - loss: 0.6493 - accuracy: 0.6169 - val_loss: 0.6487 - val_accuracy: 0.6183 - 16s/epoch - 1ms/step\n",
      "Epoch 16/10000\n",
      "11260/11260 - 16s - loss: 0.6553 - accuracy: 0.6106 - val_loss: 0.6471 - val_accuracy: 0.6188 - 16s/epoch - 1ms/step\n",
      "Epoch 17/10000\n",
      "11260/11260 - 16s - loss: 0.6524 - accuracy: 0.6123 - val_loss: 0.6528 - val_accuracy: 0.6102 - 16s/epoch - 1ms/step\n",
      "Epoch 18/10000\n",
      "11260/11260 - 16s - loss: 0.6555 - accuracy: 0.6138 - val_loss: 0.6489 - val_accuracy: 0.6203 - 16s/epoch - 1ms/step\n",
      "Epoch 19/10000\n",
      "11260/11260 - 17s - loss: 0.6501 - accuracy: 0.6215 - val_loss: 0.6532 - val_accuracy: 0.6130 - 17s/epoch - 1ms/step\n",
      "Epoch 20/10000\n",
      "11260/11260 - 17s - loss: 0.6514 - accuracy: 0.6207 - val_loss: 0.6464 - val_accuracy: 0.6169 - 17s/epoch - 2ms/step\n",
      "Epoch 21/10000\n",
      "11260/11260 - 18s - loss: 0.6479 - accuracy: 0.6185 - val_loss: 0.6469 - val_accuracy: 0.6138 - 18s/epoch - 2ms/step\n",
      "Epoch 22/10000\n",
      "11260/11260 - 18s - loss: 0.6485 - accuracy: 0.6219 - val_loss: 0.6533 - val_accuracy: 0.6219 - 18s/epoch - 2ms/step\n",
      "Epoch 23/10000\n",
      "11260/11260 - 18s - loss: 0.6513 - accuracy: 0.6212 - val_loss: 0.6506 - val_accuracy: 0.6141 - 18s/epoch - 2ms/step\n",
      "435/435 [==============================] - 1s 1ms/step - loss: 0.6461 - accuracy: 0.6200\n",
      "Larghezza: 50 \n",
      "Profondità: 3 \n",
      "Dimensione del batch: 10\n",
      "Test loss (binary_crossentropy): 0.6460729837417603\n",
      "Test metrics (accuracy): 0.6200273633003235\n",
      "Epoch 1/10000\n",
      "1126/1126 - 3s - loss: 13.2628 - accuracy: 0.6080 - val_loss: 2.6560 - val_accuracy: 0.5920 - 3s/epoch - 2ms/step\n",
      "Epoch 2/10000\n",
      "1126/1126 - 2s - loss: 0.8491 - accuracy: 0.6012 - val_loss: 0.6891 - val_accuracy: 0.6060 - 2s/epoch - 2ms/step\n",
      "Epoch 3/10000\n",
      "1126/1126 - 2s - loss: 0.6953 - accuracy: 0.6078 - val_loss: 0.6792 - val_accuracy: 0.6163 - 2s/epoch - 2ms/step\n",
      "Epoch 4/10000\n",
      "1126/1126 - 2s - loss: 0.6829 - accuracy: 0.6143 - val_loss: 0.6736 - val_accuracy: 0.6188 - 2s/epoch - 2ms/step\n",
      "Epoch 5/10000\n",
      "1126/1126 - 2s - loss: 0.6807 - accuracy: 0.6140 - val_loss: 0.6730 - val_accuracy: 0.6150 - 2s/epoch - 2ms/step\n",
      "Epoch 6/10000\n",
      "1126/1126 - 2s - loss: 0.6832 - accuracy: 0.6142 - val_loss: 0.6703 - val_accuracy: 0.6192 - 2s/epoch - 2ms/step\n",
      "Epoch 7/10000\n",
      "1126/1126 - 2s - loss: 0.6765 - accuracy: 0.6147 - val_loss: 0.6728 - val_accuracy: 0.6165 - 2s/epoch - 2ms/step\n",
      "Epoch 8/10000\n",
      "1126/1126 - 2s - loss: 0.6737 - accuracy: 0.6143 - val_loss: 0.6689 - val_accuracy: 0.6182 - 2s/epoch - 2ms/step\n",
      "Epoch 9/10000\n",
      "1126/1126 - 2s - loss: 0.6711 - accuracy: 0.6148 - val_loss: 0.6648 - val_accuracy: 0.6167 - 2s/epoch - 2ms/step\n",
      "Epoch 10/10000\n",
      "1126/1126 - 2s - loss: 0.6658 - accuracy: 0.6151 - val_loss: 0.6663 - val_accuracy: 0.6171 - 2s/epoch - 2ms/step\n",
      "Epoch 11/10000\n",
      "1126/1126 - 2s - loss: 0.6676 - accuracy: 0.6151 - val_loss: 0.6677 - val_accuracy: 0.6188 - 2s/epoch - 2ms/step\n",
      "Epoch 12/10000\n",
      "1126/1126 - 2s - loss: 0.6671 - accuracy: 0.6148 - val_loss: 0.6643 - val_accuracy: 0.6154 - 2s/epoch - 2ms/step\n",
      "Epoch 13/10000\n",
      "1126/1126 - 2s - loss: 0.6721 - accuracy: 0.6147 - val_loss: 0.6692 - val_accuracy: 0.6164 - 2s/epoch - 2ms/step\n",
      "Epoch 14/10000\n",
      "1126/1126 - 2s - loss: 0.6678 - accuracy: 0.6144 - val_loss: 0.6672 - val_accuracy: 0.6135 - 2s/epoch - 2ms/step\n",
      "Epoch 15/10000\n",
      "1126/1126 - 2s - loss: 0.6680 - accuracy: 0.6149 - val_loss: 0.6625 - val_accuracy: 0.6144 - 2s/epoch - 2ms/step\n",
      "Epoch 16/10000\n",
      "1126/1126 - 2s - loss: 0.6668 - accuracy: 0.6146 - val_loss: 0.6613 - val_accuracy: 0.6191 - 2s/epoch - 2ms/step\n",
      "435/435 [==============================] - 1s 1ms/step - loss: 0.6769 - accuracy: 0.6196\n",
      "Larghezza: 50 \n",
      "Profondità: 3 \n",
      "Dimensione del batch: 100\n",
      "Test loss (binary_crossentropy): 0.6769081950187683\n",
      "Test metrics (accuracy): 0.6195957064628601\n",
      "Epoch 1/10000\n",
      "113/113 - 1s - loss: 2.7048 - accuracy: 0.5188 - val_loss: 0.8042 - val_accuracy: 0.6056 - 1s/epoch - 11ms/step\n",
      "Epoch 2/10000\n",
      "113/113 - 0s - loss: 0.7674 - accuracy: 0.6040 - val_loss: 0.7350 - val_accuracy: 0.6099 - 416ms/epoch - 4ms/step\n",
      "Epoch 3/10000\n",
      "113/113 - 0s - loss: 0.7341 - accuracy: 0.6066 - val_loss: 0.7077 - val_accuracy: 0.6114 - 374ms/epoch - 3ms/step\n",
      "Epoch 4/10000\n",
      "113/113 - 0s - loss: 0.6988 - accuracy: 0.6101 - val_loss: 0.6777 - val_accuracy: 0.6155 - 377ms/epoch - 3ms/step\n",
      "Epoch 5/10000\n",
      "113/113 - 0s - loss: 0.6878 - accuracy: 0.6117 - val_loss: 0.6743 - val_accuracy: 0.6140 - 393ms/epoch - 3ms/step\n",
      "Epoch 6/10000\n",
      "113/113 - 0s - loss: 0.6835 - accuracy: 0.6110 - val_loss: 0.6693 - val_accuracy: 0.6153 - 380ms/epoch - 3ms/step\n",
      "Epoch 7/10000\n",
      "113/113 - 0s - loss: 0.6797 - accuracy: 0.6121 - val_loss: 0.6684 - val_accuracy: 0.6156 - 374ms/epoch - 3ms/step\n",
      "Epoch 8/10000\n",
      "113/113 - 0s - loss: 0.6771 - accuracy: 0.6118 - val_loss: 0.6676 - val_accuracy: 0.6158 - 369ms/epoch - 3ms/step\n",
      "Epoch 9/10000\n",
      "113/113 - 0s - loss: 0.6759 - accuracy: 0.6117 - val_loss: 0.6670 - val_accuracy: 0.6154 - 396ms/epoch - 4ms/step\n",
      "Epoch 10/10000\n",
      "113/113 - 0s - loss: 0.6739 - accuracy: 0.6122 - val_loss: 0.6667 - val_accuracy: 0.6159 - 386ms/epoch - 3ms/step\n",
      "Epoch 11/10000\n",
      "113/113 - 0s - loss: 0.6721 - accuracy: 0.6134 - val_loss: 0.6662 - val_accuracy: 0.6161 - 388ms/epoch - 3ms/step\n",
      "Epoch 12/10000\n",
      "113/113 - 0s - loss: 0.6697 - accuracy: 0.6135 - val_loss: 0.6655 - val_accuracy: 0.6158 - 386ms/epoch - 3ms/step\n",
      "Epoch 13/10000\n",
      "113/113 - 0s - loss: 0.6671 - accuracy: 0.6147 - val_loss: 0.6628 - val_accuracy: 0.6163 - 384ms/epoch - 3ms/step\n",
      "Epoch 14/10000\n",
      "113/113 - 0s - loss: 0.6664 - accuracy: 0.6160 - val_loss: 0.6632 - val_accuracy: 0.6195 - 388ms/epoch - 3ms/step\n",
      "Epoch 15/10000\n",
      "113/113 - 0s - loss: 0.6655 - accuracy: 0.6162 - val_loss: 0.6639 - val_accuracy: 0.6177 - 401ms/epoch - 4ms/step\n",
      "Epoch 16/10000\n",
      "113/113 - 0s - loss: 0.6650 - accuracy: 0.6156 - val_loss: 0.6641 - val_accuracy: 0.6175 - 378ms/epoch - 3ms/step\n",
      "Epoch 17/10000\n",
      "113/113 - 0s - loss: 0.6652 - accuracy: 0.6158 - val_loss: 0.6634 - val_accuracy: 0.6174 - 375ms/epoch - 3ms/step\n",
      "Epoch 18/10000\n",
      "113/113 - 0s - loss: 0.6648 - accuracy: 0.6156 - val_loss: 0.6622 - val_accuracy: 0.6171 - 392ms/epoch - 3ms/step\n",
      "Epoch 19/10000\n",
      "113/113 - 0s - loss: 0.6645 - accuracy: 0.6155 - val_loss: 0.6645 - val_accuracy: 0.6175 - 405ms/epoch - 4ms/step\n",
      "Epoch 20/10000\n",
      "113/113 - 0s - loss: 0.6647 - accuracy: 0.6159 - val_loss: 0.6617 - val_accuracy: 0.6199 - 381ms/epoch - 3ms/step\n",
      "Epoch 21/10000\n",
      "113/113 - 0s - loss: 0.6634 - accuracy: 0.6165 - val_loss: 0.6622 - val_accuracy: 0.6197 - 386ms/epoch - 3ms/step\n",
      "Epoch 22/10000\n",
      "113/113 - 0s - loss: 0.6628 - accuracy: 0.6168 - val_loss: 0.6598 - val_accuracy: 0.6209 - 391ms/epoch - 3ms/step\n",
      "Epoch 23/10000\n",
      "113/113 - 0s - loss: 0.6623 - accuracy: 0.6168 - val_loss: 0.6602 - val_accuracy: 0.6207 - 386ms/epoch - 3ms/step\n",
      "Epoch 24/10000\n",
      "113/113 - 0s - loss: 0.6620 - accuracy: 0.6168 - val_loss: 0.6612 - val_accuracy: 0.6199 - 368ms/epoch - 3ms/step\n",
      "Epoch 25/10000\n",
      "113/113 - 0s - loss: 0.6622 - accuracy: 0.6164 - val_loss: 0.6606 - val_accuracy: 0.6195 - 367ms/epoch - 3ms/step\n",
      "Epoch 26/10000\n",
      "113/113 - 0s - loss: 0.6615 - accuracy: 0.6172 - val_loss: 0.6603 - val_accuracy: 0.6215 - 434ms/epoch - 4ms/step\n",
      "Epoch 27/10000\n",
      "113/113 - 0s - loss: 0.6617 - accuracy: 0.6168 - val_loss: 0.6610 - val_accuracy: 0.6189 - 347ms/epoch - 3ms/step\n",
      "Epoch 28/10000\n",
      "113/113 - 0s - loss: 0.6614 - accuracy: 0.6164 - val_loss: 0.6602 - val_accuracy: 0.6187 - 361ms/epoch - 3ms/step\n",
      "Epoch 29/10000\n",
      "113/113 - 0s - loss: 0.6616 - accuracy: 0.6164 - val_loss: 0.6584 - val_accuracy: 0.6224 - 383ms/epoch - 3ms/step\n",
      "Epoch 30/10000\n",
      "113/113 - 0s - loss: 0.6609 - accuracy: 0.6169 - val_loss: 0.6594 - val_accuracy: 0.6189 - 407ms/epoch - 4ms/step\n",
      "Epoch 31/10000\n",
      "113/113 - 0s - loss: 0.6610 - accuracy: 0.6171 - val_loss: 0.6588 - val_accuracy: 0.6209 - 364ms/epoch - 3ms/step\n",
      "Epoch 32/10000\n",
      "113/113 - 0s - loss: 0.6612 - accuracy: 0.6163 - val_loss: 0.6589 - val_accuracy: 0.6218 - 375ms/epoch - 3ms/step\n",
      "Epoch 33/10000\n",
      "113/113 - 0s - loss: 0.6613 - accuracy: 0.6163 - val_loss: 0.6588 - val_accuracy: 0.6198 - 362ms/epoch - 3ms/step\n",
      "Epoch 34/10000\n",
      "113/113 - 0s - loss: 0.6606 - accuracy: 0.6167 - val_loss: 0.6593 - val_accuracy: 0.6215 - 370ms/epoch - 3ms/step\n",
      "Epoch 35/10000\n",
      "113/113 - 0s - loss: 0.6604 - accuracy: 0.6167 - val_loss: 0.6585 - val_accuracy: 0.6202 - 373ms/epoch - 3ms/step\n",
      "Epoch 36/10000\n",
      "113/113 - 0s - loss: 0.6607 - accuracy: 0.6163 - val_loss: 0.6572 - val_accuracy: 0.6223 - 385ms/epoch - 3ms/step\n",
      "Epoch 37/10000\n",
      "113/113 - 0s - loss: 0.6595 - accuracy: 0.6182 - val_loss: 0.6579 - val_accuracy: 0.6220 - 381ms/epoch - 3ms/step\n",
      "Epoch 38/10000\n",
      "113/113 - 0s - loss: 0.6591 - accuracy: 0.6190 - val_loss: 0.6590 - val_accuracy: 0.6214 - 366ms/epoch - 3ms/step\n",
      "Epoch 39/10000\n",
      "113/113 - 0s - loss: 0.6591 - accuracy: 0.6196 - val_loss: 0.6570 - val_accuracy: 0.6247 - 367ms/epoch - 3ms/step\n",
      "Epoch 40/10000\n",
      "113/113 - 0s - loss: 0.6590 - accuracy: 0.6193 - val_loss: 0.6579 - val_accuracy: 0.6247 - 383ms/epoch - 3ms/step\n",
      "Epoch 41/10000\n",
      "113/113 - 0s - loss: 0.6584 - accuracy: 0.6207 - val_loss: 0.6568 - val_accuracy: 0.6253 - 400ms/epoch - 4ms/step\n",
      "Epoch 42/10000\n",
      "113/113 - 0s - loss: 0.6586 - accuracy: 0.6197 - val_loss: 0.6577 - val_accuracy: 0.6232 - 375ms/epoch - 3ms/step\n",
      "Epoch 43/10000\n",
      "113/113 - 0s - loss: 0.6583 - accuracy: 0.6205 - val_loss: 0.6552 - val_accuracy: 0.6257 - 376ms/epoch - 3ms/step\n",
      "Epoch 44/10000\n",
      "113/113 - 0s - loss: 0.6579 - accuracy: 0.6211 - val_loss: 0.6558 - val_accuracy: 0.6260 - 372ms/epoch - 3ms/step\n",
      "Epoch 45/10000\n",
      "113/113 - 0s - loss: 0.6571 - accuracy: 0.6225 - val_loss: 0.6548 - val_accuracy: 0.6280 - 385ms/epoch - 3ms/step\n",
      "Epoch 46/10000\n",
      "113/113 - 0s - loss: 0.6567 - accuracy: 0.6230 - val_loss: 0.6545 - val_accuracy: 0.6287 - 408ms/epoch - 4ms/step\n",
      "Epoch 47/10000\n",
      "113/113 - 0s - loss: 0.6572 - accuracy: 0.6221 - val_loss: 0.6544 - val_accuracy: 0.6270 - 351ms/epoch - 3ms/step\n",
      "Epoch 48/10000\n",
      "113/113 - 0s - loss: 0.6573 - accuracy: 0.6218 - val_loss: 0.6574 - val_accuracy: 0.6235 - 384ms/epoch - 3ms/step\n",
      "Epoch 49/10000\n",
      "113/113 - 0s - loss: 0.6571 - accuracy: 0.6211 - val_loss: 0.6577 - val_accuracy: 0.6250 - 382ms/epoch - 3ms/step\n",
      "Epoch 50/10000\n",
      "113/113 - 0s - loss: 0.6571 - accuracy: 0.6208 - val_loss: 0.6581 - val_accuracy: 0.6241 - 385ms/epoch - 3ms/step\n",
      "Epoch 51/10000\n",
      "113/113 - 0s - loss: 0.6567 - accuracy: 0.6206 - val_loss: 0.6558 - val_accuracy: 0.6245 - 404ms/epoch - 4ms/step\n",
      "Epoch 52/10000\n",
      "113/113 - 0s - loss: 0.6560 - accuracy: 0.6212 - val_loss: 0.6580 - val_accuracy: 0.6239 - 365ms/epoch - 3ms/step\n",
      "Epoch 53/10000\n",
      "113/113 - 0s - loss: 0.6560 - accuracy: 0.6212 - val_loss: 0.6567 - val_accuracy: 0.6235 - 371ms/epoch - 3ms/step\n",
      "Epoch 54/10000\n",
      "113/113 - 0s - loss: 0.6568 - accuracy: 0.6196 - val_loss: 0.6585 - val_accuracy: 0.6208 - 393ms/epoch - 3ms/step\n",
      "Epoch 55/10000\n",
      "113/113 - 0s - loss: 0.6565 - accuracy: 0.6201 - val_loss: 0.6580 - val_accuracy: 0.6222 - 386ms/epoch - 3ms/step\n",
      "Epoch 56/10000\n",
      "113/113 - 0s - loss: 0.6570 - accuracy: 0.6194 - val_loss: 0.6574 - val_accuracy: 0.6219 - 385ms/epoch - 3ms/step\n",
      "435/435 [==============================] - 1s 2ms/step - loss: 0.6556 - accuracy: 0.6219\n",
      "Larghezza: 50 \n",
      "Profondità: 3 \n",
      "Dimensione del batch: 1000\n",
      "Test loss (binary_crossentropy): 0.6555879712104797\n",
      "Test metrics (accuracy): 0.6218976974487305\n",
      "Epoch 1/10000\n",
      "11260/11260 - 15s - loss: 167610.3750 - accuracy: 0.5715 - val_loss: 898.9481 - val_accuracy: 0.5983 - 15s/epoch - 1ms/step\n",
      "Epoch 2/10000\n",
      "11260/11260 - 15s - loss: 1286.8181 - accuracy: 0.5741 - val_loss: 1768.2784 - val_accuracy: 0.4908 - 15s/epoch - 1ms/step\n",
      "Epoch 3/10000\n",
      "11260/11260 - 15s - loss: 1068.4843 - accuracy: 0.5785 - val_loss: 723.0565 - val_accuracy: 0.6192 - 15s/epoch - 1ms/step\n",
      "Epoch 4/10000\n",
      "11260/11260 - 14s - loss: 1064.4806 - accuracy: 0.5795 - val_loss: 1128.3125 - val_accuracy: 0.6127 - 14s/epoch - 1ms/step\n",
      "Epoch 5/10000\n",
      "11260/11260 - 14s - loss: 1061.0090 - accuracy: 0.5807 - val_loss: 915.2476 - val_accuracy: 0.6300 - 14s/epoch - 1ms/step\n",
      "Epoch 6/10000\n",
      "11260/11260 - 13s - loss: 1040.9709 - accuracy: 0.5836 - val_loss: 889.8245 - val_accuracy: 0.5346 - 13s/epoch - 1ms/step\n",
      "Epoch 7/10000\n",
      "11260/11260 - 13s - loss: 987.9680 - accuracy: 0.5877 - val_loss: 3072.3904 - val_accuracy: 0.6146 - 13s/epoch - 1ms/step\n",
      "Epoch 8/10000\n",
      "11260/11260 - 13s - loss: 975.8365 - accuracy: 0.5892 - val_loss: 2351.6658 - val_accuracy: 0.6181 - 13s/epoch - 1ms/step\n",
      "Epoch 9/10000\n",
      "11260/11260 - 13s - loss: 961.9570 - accuracy: 0.5879 - val_loss: 407.5045 - val_accuracy: 0.5941 - 13s/epoch - 1ms/step\n",
      "Epoch 10/10000\n",
      "11260/11260 - 13s - loss: 932.7007 - accuracy: 0.5870 - val_loss: 1499.7794 - val_accuracy: 0.6148 - 13s/epoch - 1ms/step\n",
      "Epoch 11/10000\n",
      "11260/11260 - 12s - loss: 936.9160 - accuracy: 0.5883 - val_loss: 280.3067 - val_accuracy: 0.6406 - 12s/epoch - 1ms/step\n",
      "Epoch 12/10000\n",
      "11260/11260 - 12s - loss: 940.7115 - accuracy: 0.5889 - val_loss: 307.3208 - val_accuracy: 0.6138 - 12s/epoch - 1ms/step\n",
      "Epoch 13/10000\n",
      "11260/11260 - 16s - loss: 943.4230 - accuracy: 0.5887 - val_loss: 864.2476 - val_accuracy: 0.6288 - 16s/epoch - 1ms/step\n",
      "Epoch 14/10000\n",
      "11260/11260 - 25s - loss: 903.0617 - accuracy: 0.5894 - val_loss: 971.0674 - val_accuracy: 0.6226 - 25s/epoch - 2ms/step\n",
      "Epoch 15/10000\n",
      "11260/11260 - 20s - loss: 936.9697 - accuracy: 0.5955 - val_loss: 707.7801 - val_accuracy: 0.6259 - 20s/epoch - 2ms/step\n",
      "Epoch 16/10000\n",
      "11260/11260 - 25s - loss: 911.2625 - accuracy: 0.5941 - val_loss: 758.8853 - val_accuracy: 0.6297 - 25s/epoch - 2ms/step\n",
      "Epoch 17/10000\n",
      "11260/11260 - 16s - loss: 931.0145 - accuracy: 0.5922 - val_loss: 1644.0583 - val_accuracy: 0.6207 - 16s/epoch - 1ms/step\n",
      "Epoch 18/10000\n",
      "11260/11260 - 18s - loss: 897.5119 - accuracy: 0.5938 - val_loss: 2549.1316 - val_accuracy: 0.6158 - 18s/epoch - 2ms/step\n",
      "Epoch 19/10000\n",
      "11260/11260 - 18s - loss: 905.0358 - accuracy: 0.5939 - val_loss: 1174.5255 - val_accuracy: 0.6212 - 18s/epoch - 2ms/step\n",
      "Epoch 20/10000\n",
      "11260/11260 - 17s - loss: 903.5072 - accuracy: 0.5956 - val_loss: 864.6898 - val_accuracy: 0.5109 - 17s/epoch - 1ms/step\n",
      "Epoch 21/10000\n",
      "11260/11260 - 16s - loss: 931.5515 - accuracy: 0.5915 - val_loss: 414.2860 - val_accuracy: 0.5776 - 16s/epoch - 1ms/step\n",
      "435/435 [==============================] - 1s 1ms/step - loss: 289.1232 - accuracy: 0.6274\n",
      "Larghezza: 200 \n",
      "Profondità: 1 \n",
      "Dimensione del batch: 10\n",
      "Test loss (binary_crossentropy): 289.1231994628906\n",
      "Test metrics (accuracy): 0.6273649334907532\n",
      "Epoch 1/10000\n",
      "1126/1126 - 2s - loss: 254664.0000 - accuracy: 0.5437 - val_loss: 2969.1946 - val_accuracy: 0.5856 - 2s/epoch - 2ms/step\n",
      "Epoch 2/10000\n",
      "1126/1126 - 2s - loss: 2282.0593 - accuracy: 0.5706 - val_loss: 1921.0665 - val_accuracy: 0.5975 - 2s/epoch - 2ms/step\n",
      "Epoch 3/10000\n",
      "1126/1126 - 2s - loss: 1276.2073 - accuracy: 0.5696 - val_loss: 790.6287 - val_accuracy: 0.5815 - 2s/epoch - 2ms/step\n",
      "Epoch 4/10000\n",
      "1126/1126 - 2s - loss: 879.6802 - accuracy: 0.5746 - val_loss: 573.4320 - val_accuracy: 0.5995 - 2s/epoch - 1ms/step\n",
      "Epoch 5/10000\n",
      "1126/1126 - 2s - loss: 790.3050 - accuracy: 0.5797 - val_loss: 706.6821 - val_accuracy: 0.6115 - 2s/epoch - 2ms/step\n",
      "Epoch 6/10000\n",
      "1126/1126 - 2s - loss: 697.0924 - accuracy: 0.5856 - val_loss: 546.6389 - val_accuracy: 0.5669 - 2s/epoch - 2ms/step\n",
      "Epoch 7/10000\n",
      "1126/1126 - 2s - loss: 669.0004 - accuracy: 0.5860 - val_loss: 437.4048 - val_accuracy: 0.5690 - 2s/epoch - 1ms/step\n",
      "Epoch 8/10000\n",
      "1126/1126 - 2s - loss: 591.6071 - accuracy: 0.5887 - val_loss: 457.9617 - val_accuracy: 0.6200 - 2s/epoch - 2ms/step\n",
      "Epoch 9/10000\n",
      "1126/1126 - 2s - loss: 549.9393 - accuracy: 0.5897 - val_loss: 344.9974 - val_accuracy: 0.6263 - 2s/epoch - 1ms/step\n",
      "Epoch 10/10000\n",
      "1126/1126 - 2s - loss: 601.4359 - accuracy: 0.5903 - val_loss: 741.6215 - val_accuracy: 0.6168 - 2s/epoch - 2ms/step\n",
      "Epoch 11/10000\n",
      "1126/1126 - 2s - loss: 526.8712 - accuracy: 0.5945 - val_loss: 459.8818 - val_accuracy: 0.5719 - 2s/epoch - 2ms/step\n",
      "Epoch 12/10000\n",
      "1126/1126 - 2s - loss: 526.5659 - accuracy: 0.5910 - val_loss: 444.0080 - val_accuracy: 0.6184 - 2s/epoch - 2ms/step\n",
      "Epoch 13/10000\n",
      "1126/1126 - 2s - loss: 530.5279 - accuracy: 0.5904 - val_loss: 303.2066 - val_accuracy: 0.6133 - 2s/epoch - 2ms/step\n",
      "Epoch 14/10000\n",
      "1126/1126 - 2s - loss: 533.5485 - accuracy: 0.5925 - val_loss: 659.2118 - val_accuracy: 0.6202 - 2s/epoch - 2ms/step\n",
      "Epoch 15/10000\n",
      "1126/1126 - 2s - loss: 521.2449 - accuracy: 0.5922 - val_loss: 516.6281 - val_accuracy: 0.5814 - 2s/epoch - 2ms/step\n",
      "Epoch 16/10000\n",
      "1126/1126 - 2s - loss: 545.7342 - accuracy: 0.5921 - val_loss: 351.9511 - val_accuracy: 0.6179 - 2s/epoch - 2ms/step\n",
      "Epoch 17/10000\n",
      "1126/1126 - 2s - loss: 534.5427 - accuracy: 0.5918 - val_loss: 1043.6310 - val_accuracy: 0.6155 - 2s/epoch - 2ms/step\n",
      "Epoch 18/10000\n",
      "1126/1126 - 2s - loss: 490.4576 - accuracy: 0.5943 - val_loss: 571.9407 - val_accuracy: 0.5553 - 2s/epoch - 2ms/step\n",
      "Epoch 19/10000\n",
      "1126/1126 - 2s - loss: 495.4486 - accuracy: 0.5916 - val_loss: 358.2822 - val_accuracy: 0.5849 - 2s/epoch - 2ms/step\n",
      "435/435 [==============================] - 1s 2ms/step - loss: 351.8501 - accuracy: 0.6238\n",
      "Larghezza: 200 \n",
      "Profondità: 1 \n",
      "Dimensione del batch: 100\n",
      "Test loss (binary_crossentropy): 351.85009765625\n",
      "Test metrics (accuracy): 0.6237680912017822\n",
      "Epoch 1/10000\n",
      "113/113 - 1s - loss: 1744985.6250 - accuracy: 0.6113 - val_loss: 1480905.8750 - val_accuracy: 0.6131 - 1s/epoch - 12ms/step\n",
      "Epoch 2/10000\n",
      "113/113 - 0s - loss: 1212257.6250 - accuracy: 0.6113 - val_loss: 943576.6875 - val_accuracy: 0.6131 - 357ms/epoch - 3ms/step\n",
      "Epoch 3/10000\n",
      "113/113 - 0s - loss: 670884.6250 - accuracy: 0.6113 - val_loss: 394809.1562 - val_accuracy: 0.6131 - 454ms/epoch - 4ms/step\n",
      "Epoch 4/10000\n",
      "113/113 - 0s - loss: 149787.2500 - accuracy: 0.6139 - val_loss: 22084.3809 - val_accuracy: 0.6206 - 386ms/epoch - 3ms/step\n",
      "Epoch 5/10000\n",
      "113/113 - 0s - loss: 18345.9023 - accuracy: 0.6209 - val_loss: 14959.7969 - val_accuracy: 0.6231 - 368ms/epoch - 3ms/step\n",
      "Epoch 6/10000\n",
      "113/113 - 0s - loss: 13255.0264 - accuracy: 0.6159 - val_loss: 10740.5977 - val_accuracy: 0.6062 - 362ms/epoch - 3ms/step\n",
      "Epoch 7/10000\n",
      "113/113 - 0s - loss: 9766.7246 - accuracy: 0.6008 - val_loss: 8875.0615 - val_accuracy: 0.6008 - 357ms/epoch - 3ms/step\n",
      "Epoch 8/10000\n",
      "113/113 - 0s - loss: 8540.2959 - accuracy: 0.6016 - val_loss: 7965.5337 - val_accuracy: 0.6035 - 358ms/epoch - 3ms/step\n",
      "Epoch 9/10000\n",
      "113/113 - 0s - loss: 7739.8989 - accuracy: 0.6000 - val_loss: 7237.3320 - val_accuracy: 0.6034 - 340ms/epoch - 3ms/step\n",
      "Epoch 10/10000\n",
      "113/113 - 0s - loss: 6831.5674 - accuracy: 0.5943 - val_loss: 6199.7949 - val_accuracy: 0.5988 - 466ms/epoch - 4ms/step\n",
      "Epoch 11/10000\n",
      "113/113 - 0s - loss: 5945.6504 - accuracy: 0.5914 - val_loss: 5580.9941 - val_accuracy: 0.5908 - 449ms/epoch - 4ms/step\n",
      "Epoch 12/10000\n",
      "113/113 - 0s - loss: 5237.3579 - accuracy: 0.5870 - val_loss: 4876.1909 - val_accuracy: 0.5864 - 329ms/epoch - 3ms/step\n",
      "Epoch 13/10000\n",
      "113/113 - 0s - loss: 4597.8735 - accuracy: 0.5801 - val_loss: 4230.0776 - val_accuracy: 0.5858 - 435ms/epoch - 4ms/step\n",
      "Epoch 14/10000\n",
      "113/113 - 0s - loss: 3864.3262 - accuracy: 0.5726 - val_loss: 3122.4268 - val_accuracy: 0.5667 - 397ms/epoch - 4ms/step\n",
      "Epoch 15/10000\n",
      "113/113 - 0s - loss: 2685.1787 - accuracy: 0.5525 - val_loss: 2283.4766 - val_accuracy: 0.5649 - 349ms/epoch - 3ms/step\n",
      "435/435 [==============================] - 1s 1ms/step - loss: 14971.8057 - accuracy: 0.6230\n",
      "Larghezza: 200 \n",
      "Profondità: 1 \n",
      "Dimensione del batch: 1000\n",
      "Test loss (binary_crossentropy): 14971.8056640625\n",
      "Test metrics (accuracy): 0.6229767799377441\n",
      "Epoch 1/10000\n",
      "11260/11260 - 21s - loss: 2.3197 - accuracy: 0.5367 - val_loss: 1.4574 - val_accuracy: 0.5586 - 21s/epoch - 2ms/step\n",
      "Epoch 2/10000\n",
      "11260/11260 - 22s - loss: 1.2586 - accuracy: 0.5657 - val_loss: 1.1455 - val_accuracy: 0.5871 - 22s/epoch - 2ms/step\n",
      "Epoch 3/10000\n",
      "11260/11260 - 21s - loss: 1.0194 - accuracy: 0.5841 - val_loss: 0.9379 - val_accuracy: 0.6060 - 21s/epoch - 2ms/step\n",
      "Epoch 4/10000\n",
      "11260/11260 - 24s - loss: 0.8832 - accuracy: 0.6019 - val_loss: 0.8700 - val_accuracy: 0.6139 - 24s/epoch - 2ms/step\n",
      "Epoch 5/10000\n",
      "11260/11260 - 20s - loss: 0.8530 - accuracy: 0.6041 - val_loss: 0.8315 - val_accuracy: 0.6156 - 20s/epoch - 2ms/step\n",
      "Epoch 6/10000\n",
      "11260/11260 - 23s - loss: 0.8330 - accuracy: 0.6043 - val_loss: 0.8349 - val_accuracy: 0.6097 - 23s/epoch - 2ms/step\n",
      "Epoch 7/10000\n",
      "11260/11260 - 20s - loss: 0.8015 - accuracy: 0.6154 - val_loss: 0.7813 - val_accuracy: 0.6169 - 20s/epoch - 2ms/step\n",
      "Epoch 8/10000\n",
      "11260/11260 - 25s - loss: 0.7740 - accuracy: 0.6157 - val_loss: 0.7620 - val_accuracy: 0.6187 - 25s/epoch - 2ms/step\n",
      "Epoch 9/10000\n",
      "11260/11260 - 29s - loss: 0.7601 - accuracy: 0.6134 - val_loss: 0.7562 - val_accuracy: 0.6127 - 29s/epoch - 3ms/step\n",
      "Epoch 10/10000\n",
      "11260/11260 - 25s - loss: 0.7455 - accuracy: 0.6147 - val_loss: 0.7260 - val_accuracy: 0.6266 - 25s/epoch - 2ms/step\n",
      "Epoch 11/10000\n",
      "11260/11260 - 28s - loss: 0.7462 - accuracy: 0.6172 - val_loss: 0.7403 - val_accuracy: 0.6025 - 28s/epoch - 3ms/step\n",
      "Epoch 12/10000\n",
      "11260/11260 - 21s - loss: 0.7237 - accuracy: 0.6141 - val_loss: 0.7205 - val_accuracy: 0.6207 - 21s/epoch - 2ms/step\n",
      "Epoch 13/10000\n",
      "11260/11260 - 18s - loss: 0.7127 - accuracy: 0.6159 - val_loss: 0.7038 - val_accuracy: 0.6181 - 18s/epoch - 2ms/step\n",
      "Epoch 14/10000\n",
      "11260/11260 - 16s - loss: 0.7206 - accuracy: 0.6157 - val_loss: 0.6962 - val_accuracy: 0.6223 - 16s/epoch - 1ms/step\n",
      "Epoch 15/10000\n",
      "11260/11260 - 16s - loss: 0.7149 - accuracy: 0.6157 - val_loss: 0.7051 - val_accuracy: 0.6228 - 16s/epoch - 1ms/step\n",
      "Epoch 16/10000\n",
      "11260/11260 - 19s - loss: 0.7050 - accuracy: 0.6087 - val_loss: 0.6911 - val_accuracy: 0.6197 - 19s/epoch - 2ms/step\n",
      "Epoch 17/10000\n",
      "11260/11260 - 27s - loss: 0.6971 - accuracy: 0.6115 - val_loss: 0.6889 - val_accuracy: 0.6179 - 27s/epoch - 2ms/step\n",
      "Epoch 18/10000\n",
      "11260/11260 - 26s - loss: 0.6939 - accuracy: 0.6149 - val_loss: 0.6874 - val_accuracy: 0.6204 - 26s/epoch - 2ms/step\n",
      "Epoch 19/10000\n",
      "11260/11260 - 30s - loss: 0.6891 - accuracy: 0.6160 - val_loss: 0.6903 - val_accuracy: 0.6202 - 30s/epoch - 3ms/step\n",
      "Epoch 20/10000\n",
      "11260/11260 - 26s - loss: 0.6912 - accuracy: 0.6184 - val_loss: 0.6889 - val_accuracy: 0.6243 - 26s/epoch - 2ms/step\n",
      "435/435 [==============================] - 1s 1ms/step - loss: 0.7294 - accuracy: 0.6259\n",
      "Larghezza: 200 \n",
      "Profondità: 1 \n",
      "Dimensione del batch: 10\n",
      "Test loss (binary_crossentropy): 0.7293853163719177\n",
      "Test metrics (accuracy): 0.6258542537689209\n",
      "Epoch 1/10000\n",
      "1126/1126 - 5s - loss: 11.9111 - accuracy: 0.5805 - val_loss: 2.1841 - val_accuracy: 0.5124 - 5s/epoch - 5ms/step\n",
      "Epoch 2/10000\n",
      "1126/1126 - 3s - loss: 1.9442 - accuracy: 0.5275 - val_loss: 1.6785 - val_accuracy: 0.5217 - 3s/epoch - 2ms/step\n",
      "Epoch 3/10000\n",
      "1126/1126 - 3s - loss: 1.5204 - accuracy: 0.5423 - val_loss: 1.3585 - val_accuracy: 0.5319 - 3s/epoch - 3ms/step\n",
      "Epoch 4/10000\n",
      "1126/1126 - 3s - loss: 1.2882 - accuracy: 0.5643 - val_loss: 1.1962 - val_accuracy: 0.5745 - 3s/epoch - 2ms/step\n",
      "Epoch 5/10000\n",
      "1126/1126 - 2s - loss: 1.1403 - accuracy: 0.5694 - val_loss: 1.1135 - val_accuracy: 0.5695 - 2s/epoch - 2ms/step\n",
      "Epoch 6/10000\n",
      "1126/1126 - 2s - loss: 1.0447 - accuracy: 0.5724 - val_loss: 0.9929 - val_accuracy: 0.5820 - 2s/epoch - 2ms/step\n",
      "Epoch 7/10000\n",
      "1126/1126 - 2s - loss: 0.9605 - accuracy: 0.5784 - val_loss: 0.9391 - val_accuracy: 0.5814 - 2s/epoch - 2ms/step\n",
      "Epoch 8/10000\n",
      "1126/1126 - 2s - loss: 0.9018 - accuracy: 0.5867 - val_loss: 0.8926 - val_accuracy: 0.5958 - 2s/epoch - 2ms/step\n",
      "Epoch 9/10000\n",
      "1126/1126 - 2s - loss: 0.8664 - accuracy: 0.5886 - val_loss: 0.8529 - val_accuracy: 0.5970 - 2s/epoch - 2ms/step\n",
      "Epoch 10/10000\n",
      "1126/1126 - 3s - loss: 0.8349 - accuracy: 0.5962 - val_loss: 0.8270 - val_accuracy: 0.5943 - 3s/epoch - 2ms/step\n",
      "Epoch 11/10000\n",
      "1126/1126 - 3s - loss: 0.8186 - accuracy: 0.5951 - val_loss: 0.8107 - val_accuracy: 0.5991 - 3s/epoch - 2ms/step\n",
      "Epoch 12/10000\n",
      "1126/1126 - 2s - loss: 0.8060 - accuracy: 0.5945 - val_loss: 0.8061 - val_accuracy: 0.5924 - 2s/epoch - 2ms/step\n",
      "Epoch 13/10000\n",
      "1126/1126 - 3s - loss: 0.7910 - accuracy: 0.5985 - val_loss: 0.7947 - val_accuracy: 0.6002 - 3s/epoch - 3ms/step\n",
      "Epoch 14/10000\n",
      "1126/1126 - 2s - loss: 0.7809 - accuracy: 0.6014 - val_loss: 0.7764 - val_accuracy: 0.6030 - 2s/epoch - 2ms/step\n",
      "Epoch 15/10000\n",
      "1126/1126 - 2s - loss: 0.7655 - accuracy: 0.6041 - val_loss: 0.7663 - val_accuracy: 0.6067 - 2s/epoch - 2ms/step\n",
      "Epoch 16/10000\n",
      "1126/1126 - 2s - loss: 0.7518 - accuracy: 0.6094 - val_loss: 0.7622 - val_accuracy: 0.6087 - 2s/epoch - 2ms/step\n",
      "Epoch 17/10000\n",
      "1126/1126 - 2s - loss: 0.7522 - accuracy: 0.6062 - val_loss: 0.7566 - val_accuracy: 0.6076 - 2s/epoch - 2ms/step\n",
      "Epoch 18/10000\n",
      "1126/1126 - 2s - loss: 0.7425 - accuracy: 0.6102 - val_loss: 0.7447 - val_accuracy: 0.6146 - 2s/epoch - 2ms/step\n",
      "Epoch 19/10000\n",
      "1126/1126 - 3s - loss: 0.7378 - accuracy: 0.6146 - val_loss: 0.7469 - val_accuracy: 0.6158 - 3s/epoch - 3ms/step\n",
      "Epoch 20/10000\n",
      "1126/1126 - 4s - loss: 0.7381 - accuracy: 0.6175 - val_loss: 0.7427 - val_accuracy: 0.6141 - 4s/epoch - 3ms/step\n",
      "Epoch 21/10000\n",
      "1126/1126 - 2s - loss: 0.7298 - accuracy: 0.6159 - val_loss: 0.7395 - val_accuracy: 0.6114 - 2s/epoch - 2ms/step\n",
      "Epoch 22/10000\n",
      "1126/1126 - 2s - loss: 0.7284 - accuracy: 0.6118 - val_loss: 0.7344 - val_accuracy: 0.6143 - 2s/epoch - 2ms/step\n",
      "Epoch 23/10000\n",
      "1126/1126 - 2s - loss: 0.7220 - accuracy: 0.6130 - val_loss: 0.7292 - val_accuracy: 0.6134 - 2s/epoch - 2ms/step\n",
      "Epoch 24/10000\n",
      "1126/1126 - 2s - loss: 0.7185 - accuracy: 0.6167 - val_loss: 0.7203 - val_accuracy: 0.6203 - 2s/epoch - 2ms/step\n",
      "Epoch 25/10000\n",
      "1126/1126 - 3s - loss: 0.7198 - accuracy: 0.6187 - val_loss: 0.7213 - val_accuracy: 0.6212 - 3s/epoch - 2ms/step\n",
      "Epoch 26/10000\n",
      "1126/1126 - 2s - loss: 0.7198 - accuracy: 0.6204 - val_loss: 0.7171 - val_accuracy: 0.6323 - 2s/epoch - 2ms/step\n",
      "Epoch 27/10000\n",
      "1126/1126 - 2s - loss: 0.7164 - accuracy: 0.6258 - val_loss: 0.7247 - val_accuracy: 0.6282 - 2s/epoch - 2ms/step\n",
      "Epoch 28/10000\n",
      "1126/1126 - 2s - loss: 0.7128 - accuracy: 0.6272 - val_loss: 0.7200 - val_accuracy: 0.6260 - 2s/epoch - 2ms/step\n",
      "Epoch 29/10000\n",
      "1126/1126 - 3s - loss: 0.7102 - accuracy: 0.6271 - val_loss: 0.7138 - val_accuracy: 0.6235 - 3s/epoch - 3ms/step\n",
      "Epoch 30/10000\n",
      "1126/1126 - 2s - loss: 0.7046 - accuracy: 0.6243 - val_loss: 0.7039 - val_accuracy: 0.6266 - 2s/epoch - 2ms/step\n",
      "Epoch 31/10000\n",
      "1126/1126 - 3s - loss: 0.7070 - accuracy: 0.6257 - val_loss: 0.7082 - val_accuracy: 0.6302 - 3s/epoch - 2ms/step\n",
      "Epoch 32/10000\n",
      "1126/1126 - 3s - loss: 0.7052 - accuracy: 0.6250 - val_loss: 0.7064 - val_accuracy: 0.6306 - 3s/epoch - 2ms/step\n",
      "Epoch 33/10000\n",
      "1126/1126 - 3s - loss: 0.7005 - accuracy: 0.6286 - val_loss: 0.7018 - val_accuracy: 0.6310 - 3s/epoch - 3ms/step\n",
      "Epoch 34/10000\n",
      "1126/1126 - 2s - loss: 0.7008 - accuracy: 0.6288 - val_loss: 0.7085 - val_accuracy: 0.6270 - 2s/epoch - 2ms/step\n",
      "Epoch 35/10000\n",
      "1126/1126 - 2s - loss: 0.6988 - accuracy: 0.6262 - val_loss: 0.7064 - val_accuracy: 0.6262 - 2s/epoch - 2ms/step\n",
      "Epoch 36/10000\n",
      "1126/1126 - 2s - loss: 0.6995 - accuracy: 0.6253 - val_loss: 0.7030 - val_accuracy: 0.6315 - 2s/epoch - 2ms/step\n",
      "435/435 [==============================] - 1s 1ms/step - loss: 0.7153 - accuracy: 0.6290\n",
      "Larghezza: 200 \n",
      "Profondità: 1 \n",
      "Dimensione del batch: 100\n",
      "Test loss (binary_crossentropy): 0.7153183221817017\n",
      "Test metrics (accuracy): 0.6290194988250732\n",
      "Epoch 1/10000\n",
      "113/113 - 4s - loss: 12.2360 - accuracy: 0.6098 - val_loss: 9.8032 - val_accuracy: 0.6091 - 4s/epoch - 40ms/step\n",
      "Epoch 2/10000\n",
      "113/113 - 1s - loss: 7.8784 - accuracy: 0.6021 - val_loss: 5.8168 - val_accuracy: 0.6020 - 592ms/epoch - 5ms/step\n",
      "Epoch 3/10000\n",
      "113/113 - 1s - loss: 4.1912 - accuracy: 0.5869 - val_loss: 2.7808 - val_accuracy: 0.5743 - 638ms/epoch - 6ms/step\n",
      "Epoch 4/10000\n",
      "113/113 - 0s - loss: 2.4822 - accuracy: 0.5747 - val_loss: 2.2396 - val_accuracy: 0.5756 - 499ms/epoch - 4ms/step\n",
      "Epoch 5/10000\n",
      "113/113 - 1s - loss: 2.1373 - accuracy: 0.5784 - val_loss: 1.9854 - val_accuracy: 0.5778 - 577ms/epoch - 5ms/step\n",
      "Epoch 6/10000\n",
      "113/113 - 0s - loss: 1.9220 - accuracy: 0.5790 - val_loss: 1.8007 - val_accuracy: 0.5786 - 472ms/epoch - 4ms/step\n",
      "Epoch 7/10000\n",
      "113/113 - 0s - loss: 1.7611 - accuracy: 0.5780 - val_loss: 1.6441 - val_accuracy: 0.5790 - 487ms/epoch - 4ms/step\n",
      "Epoch 8/10000\n",
      "113/113 - 1s - loss: 1.6328 - accuracy: 0.5783 - val_loss: 1.5432 - val_accuracy: 0.5783 - 564ms/epoch - 5ms/step\n",
      "Epoch 9/10000\n",
      "113/113 - 1s - loss: 1.5342 - accuracy: 0.5797 - val_loss: 1.4507 - val_accuracy: 0.5781 - 650ms/epoch - 6ms/step\n",
      "Epoch 10/10000\n",
      "113/113 - 1s - loss: 1.4598 - accuracy: 0.5791 - val_loss: 1.3966 - val_accuracy: 0.5795 - 539ms/epoch - 5ms/step\n",
      "Epoch 11/10000\n",
      "113/113 - 1s - loss: 1.4038 - accuracy: 0.5805 - val_loss: 1.3541 - val_accuracy: 0.5819 - 837ms/epoch - 7ms/step\n",
      "435/435 [==============================] - 1s 3ms/step - loss: 9.7278 - accuracy: 0.6110\n",
      "Larghezza: 200 \n",
      "Profondità: 1 \n",
      "Dimensione del batch: 1000\n",
      "Test loss (binary_crossentropy): 9.727779388427734\n",
      "Test metrics (accuracy): 0.6109632253646851\n",
      "Epoch 1/10000\n",
      "11260/11260 - 28s - loss: 480564.3750 - accuracy: 0.5735 - val_loss: 86604.1875 - val_accuracy: 0.5988 - 28s/epoch - 2ms/step\n",
      "Epoch 2/10000\n",
      "11260/11260 - 26s - loss: 221311.4844 - accuracy: 0.5765 - val_loss: 155860.4531 - val_accuracy: 0.5602 - 26s/epoch - 2ms/step\n",
      "Epoch 3/10000\n",
      "11260/11260 - 20s - loss: 221549.5781 - accuracy: 0.5775 - val_loss: 171503.9844 - val_accuracy: 0.6155 - 20s/epoch - 2ms/step\n",
      "Epoch 4/10000\n",
      "11260/11260 - 22s - loss: 211665.5469 - accuracy: 0.5793 - val_loss: 82514.2031 - val_accuracy: 0.5475 - 22s/epoch - 2ms/step\n",
      "Epoch 5/10000\n",
      "11260/11260 - 22s - loss: 208410.7969 - accuracy: 0.5803 - val_loss: 446031.5938 - val_accuracy: 0.6223 - 22s/epoch - 2ms/step\n",
      "Epoch 6/10000\n",
      "11260/11260 - 23s - loss: 222900.0938 - accuracy: 0.5804 - val_loss: 67095.5391 - val_accuracy: 0.6268 - 23s/epoch - 2ms/step\n",
      "Epoch 7/10000\n",
      "11260/11260 - 29s - loss: 213588.9844 - accuracy: 0.5814 - val_loss: 434781.9688 - val_accuracy: 0.6144 - 29s/epoch - 3ms/step\n",
      "Epoch 8/10000\n",
      "11260/11260 - 25s - loss: 205401.3125 - accuracy: 0.5811 - val_loss: 328368.9062 - val_accuracy: 0.6231 - 25s/epoch - 2ms/step\n",
      "Epoch 9/10000\n",
      "11260/11260 - 27s - loss: 190185.7812 - accuracy: 0.5853 - val_loss: 87396.8984 - val_accuracy: 0.6306 - 27s/epoch - 2ms/step\n",
      "Epoch 10/10000\n",
      "11260/11260 - 30s - loss: 210229.5938 - accuracy: 0.5841 - val_loss: 218422.7344 - val_accuracy: 0.6233 - 30s/epoch - 3ms/step\n",
      "Epoch 11/10000\n",
      "11260/11260 - 29s - loss: 213141.9531 - accuracy: 0.5846 - val_loss: 61858.8008 - val_accuracy: 0.5880 - 29s/epoch - 3ms/step\n",
      "Epoch 12/10000\n",
      "11260/11260 - 25s - loss: 204122.0625 - accuracy: 0.5837 - val_loss: 54070.6836 - val_accuracy: 0.6045 - 25s/epoch - 2ms/step\n",
      "Epoch 13/10000\n",
      "11260/11260 - 23s - loss: 205823.8906 - accuracy: 0.5856 - val_loss: 216505.7812 - val_accuracy: 0.6235 - 23s/epoch - 2ms/step\n",
      "Epoch 14/10000\n",
      "11260/11260 - 23s - loss: 202777.6250 - accuracy: 0.5850 - val_loss: 877767.1875 - val_accuracy: 0.4049 - 23s/epoch - 2ms/step\n",
      "Epoch 15/10000\n",
      "11260/11260 - 23s - loss: 212581.1562 - accuracy: 0.5879 - val_loss: 54488.0898 - val_accuracy: 0.6196 - 23s/epoch - 2ms/step\n",
      "Epoch 16/10000\n",
      "11260/11260 - 21s - loss: 199973.3750 - accuracy: 0.5881 - val_loss: 102717.0547 - val_accuracy: 0.6362 - 21s/epoch - 2ms/step\n",
      "Epoch 17/10000\n",
      "11260/11260 - 22s - loss: 205634.1875 - accuracy: 0.5849 - val_loss: 408779.2188 - val_accuracy: 0.6144 - 22s/epoch - 2ms/step\n",
      "Epoch 18/10000\n",
      "11260/11260 - 22s - loss: 198087.3906 - accuracy: 0.5861 - val_loss: 501461.5938 - val_accuracy: 0.6138 - 22s/epoch - 2ms/step\n",
      "Epoch 19/10000\n",
      "11260/11260 - 22s - loss: 202878.1094 - accuracy: 0.5880 - val_loss: 134332.9375 - val_accuracy: 0.5698 - 22s/epoch - 2ms/step\n",
      "Epoch 20/10000\n",
      "11260/11260 - 21s - loss: 192998.0938 - accuracy: 0.5855 - val_loss: 109143.1719 - val_accuracy: 0.6248 - 21s/epoch - 2ms/step\n",
      "Epoch 21/10000\n",
      "11260/11260 - 19s - loss: 202570.0938 - accuracy: 0.5880 - val_loss: 70227.8438 - val_accuracy: 0.6085 - 19s/epoch - 2ms/step\n",
      "Epoch 22/10000\n",
      "11260/11260 - 20s - loss: 197669.8281 - accuracy: 0.5879 - val_loss: 133903.2500 - val_accuracy: 0.6310 - 20s/epoch - 2ms/step\n",
      "Epoch 23/10000\n",
      "11260/11260 - 20s - loss: 197197.0156 - accuracy: 0.5874 - val_loss: 44324.7227 - val_accuracy: 0.6543 - 20s/epoch - 2ms/step\n",
      "Epoch 24/10000\n",
      "11260/11260 - 20s - loss: 200905.9688 - accuracy: 0.5894 - val_loss: 159236.8906 - val_accuracy: 0.5497 - 20s/epoch - 2ms/step\n",
      "Epoch 25/10000\n",
      "11260/11260 - 19s - loss: 192519.9844 - accuracy: 0.5873 - val_loss: 139204.9688 - val_accuracy: 0.6305 - 19s/epoch - 2ms/step\n",
      "Epoch 26/10000\n",
      "11260/11260 - 20s - loss: 201792.4062 - accuracy: 0.5904 - val_loss: 161295.7344 - val_accuracy: 0.6293 - 20s/epoch - 2ms/step\n",
      "Epoch 27/10000\n",
      "11260/11260 - 21s - loss: 196165.8594 - accuracy: 0.5897 - val_loss: 315212.2500 - val_accuracy: 0.6153 - 21s/epoch - 2ms/step\n",
      "Epoch 28/10000\n",
      "11260/11260 - 21s - loss: 195551.6719 - accuracy: 0.5887 - val_loss: 227576.6406 - val_accuracy: 0.6262 - 21s/epoch - 2ms/step\n",
      "Epoch 29/10000\n",
      "11260/11260 - 21s - loss: 188500.2812 - accuracy: 0.5903 - val_loss: 54879.3984 - val_accuracy: 0.6381 - 21s/epoch - 2ms/step\n",
      "Epoch 30/10000\n",
      "11260/11260 - 23s - loss: 186469.5000 - accuracy: 0.5876 - val_loss: 284437.9688 - val_accuracy: 0.6256 - 23s/epoch - 2ms/step\n",
      "Epoch 31/10000\n",
      "11260/11260 - 22s - loss: 192769.2031 - accuracy: 0.5868 - val_loss: 41429.7383 - val_accuracy: 0.6505 - 22s/epoch - 2ms/step\n",
      "Epoch 32/10000\n",
      "11260/11260 - 23s - loss: 195513.2656 - accuracy: 0.5898 - val_loss: 739594.8750 - val_accuracy: 0.4283 - 23s/epoch - 2ms/step\n",
      "Epoch 33/10000\n",
      "11260/11260 - 21s - loss: 193425.4844 - accuracy: 0.5862 - val_loss: 480635.6562 - val_accuracy: 0.4729 - 21s/epoch - 2ms/step\n",
      "435/435 [==============================] - 1s 2ms/step - loss: 44165.4844 - accuracy: 0.6515\n",
      "Larghezza: 200 \n",
      "Profondità: 2 \n",
      "Dimensione del batch: 10\n",
      "Test loss (binary_crossentropy): 44165.484375\n",
      "Test metrics (accuracy): 0.6514639258384705\n",
      "Epoch 1/10000\n",
      "1126/1126 - 4s - loss: 9661646.0000 - accuracy: 0.5837 - val_loss: 154248.1094 - val_accuracy: 0.5523 - 4s/epoch - 3ms/step\n",
      "Epoch 2/10000\n",
      "1126/1126 - 3s - loss: 117681.6172 - accuracy: 0.5774 - val_loss: 71350.5703 - val_accuracy: 0.5607 - 3s/epoch - 3ms/step\n",
      "Epoch 3/10000\n",
      "1126/1126 - 3s - loss: 107994.7578 - accuracy: 0.5740 - val_loss: 126995.5234 - val_accuracy: 0.6175 - 3s/epoch - 3ms/step\n",
      "Epoch 4/10000\n",
      "1126/1126 - 3s - loss: 97797.4922 - accuracy: 0.5730 - val_loss: 55511.2383 - val_accuracy: 0.6167 - 3s/epoch - 3ms/step\n",
      "Epoch 5/10000\n",
      "1126/1126 - 3s - loss: 107346.6953 - accuracy: 0.5719 - val_loss: 173756.3594 - val_accuracy: 0.6192 - 3s/epoch - 3ms/step\n",
      "Epoch 6/10000\n",
      "1126/1126 - 3s - loss: 105839.1016 - accuracy: 0.5754 - val_loss: 36361.5625 - val_accuracy: 0.5967 - 3s/epoch - 3ms/step\n",
      "Epoch 7/10000\n",
      "1126/1126 - 3s - loss: 107033.2422 - accuracy: 0.5727 - val_loss: 305107.8438 - val_accuracy: 0.6154 - 3s/epoch - 2ms/step\n",
      "Epoch 8/10000\n",
      "1126/1126 - 3s - loss: 100332.0078 - accuracy: 0.5785 - val_loss: 30221.7598 - val_accuracy: 0.6056 - 3s/epoch - 3ms/step\n",
      "Epoch 9/10000\n",
      "1126/1126 - 3s - loss: 96128.5312 - accuracy: 0.5766 - val_loss: 133658.1250 - val_accuracy: 0.6119 - 3s/epoch - 3ms/step\n",
      "Epoch 10/10000\n",
      "1126/1126 - 3s - loss: 97134.4375 - accuracy: 0.5773 - val_loss: 160661.2656 - val_accuracy: 0.6139 - 3s/epoch - 3ms/step\n",
      "Epoch 11/10000\n",
      "1126/1126 - 3s - loss: 107518.6719 - accuracy: 0.5736 - val_loss: 90749.0234 - val_accuracy: 0.5294 - 3s/epoch - 3ms/step\n",
      "Epoch 12/10000\n",
      "1126/1126 - 3s - loss: 101189.1797 - accuracy: 0.5760 - val_loss: 139937.5938 - val_accuracy: 0.6186 - 3s/epoch - 3ms/step\n",
      "Epoch 13/10000\n",
      "1126/1126 - 3s - loss: 90808.6875 - accuracy: 0.5782 - val_loss: 38004.7422 - val_accuracy: 0.5852 - 3s/epoch - 3ms/step\n",
      "Epoch 14/10000\n",
      "1126/1126 - 3s - loss: 109536.3672 - accuracy: 0.5777 - val_loss: 47658.9023 - val_accuracy: 0.6242 - 3s/epoch - 3ms/step\n",
      "Epoch 15/10000\n",
      "1126/1126 - 3s - loss: 96016.7109 - accuracy: 0.5800 - val_loss: 125806.5391 - val_accuracy: 0.6153 - 3s/epoch - 3ms/step\n",
      "Epoch 16/10000\n",
      "1126/1126 - 3s - loss: 87027.0625 - accuracy: 0.5803 - val_loss: 187419.1875 - val_accuracy: 0.6202 - 3s/epoch - 3ms/step\n",
      "Epoch 17/10000\n",
      "1126/1126 - 3s - loss: 109248.8594 - accuracy: 0.5813 - val_loss: 60220.9609 - val_accuracy: 0.5332 - 3s/epoch - 3ms/step\n",
      "Epoch 18/10000\n",
      "1126/1126 - 4s - loss: 96362.5000 - accuracy: 0.5828 - val_loss: 508260.4062 - val_accuracy: 0.6148 - 4s/epoch - 4ms/step\n",
      "Epoch 19/10000\n",
      "1126/1126 - 4s - loss: 103146.9688 - accuracy: 0.5799 - val_loss: 59833.9141 - val_accuracy: 0.6114 - 4s/epoch - 3ms/step\n",
      "Epoch 20/10000\n",
      "1126/1126 - 4s - loss: 96124.2266 - accuracy: 0.5808 - val_loss: 31752.7520 - val_accuracy: 0.6468 - 4s/epoch - 3ms/step\n",
      "Epoch 21/10000\n",
      "1126/1126 - 3s - loss: 93139.0703 - accuracy: 0.5821 - val_loss: 218555.7031 - val_accuracy: 0.4375 - 3s/epoch - 3ms/step\n",
      "Epoch 22/10000\n",
      "1126/1126 - 3s - loss: 92598.7188 - accuracy: 0.5837 - val_loss: 61919.0391 - val_accuracy: 0.5274 - 3s/epoch - 3ms/step\n",
      "Epoch 23/10000\n",
      "1126/1126 - 3s - loss: 94300.8906 - accuracy: 0.5810 - val_loss: 235420.0000 - val_accuracy: 0.4424 - 3s/epoch - 3ms/step\n",
      "Epoch 24/10000\n",
      "1126/1126 - 3s - loss: 103357.0391 - accuracy: 0.5829 - val_loss: 36910.8359 - val_accuracy: 0.5888 - 3s/epoch - 3ms/step\n",
      "Epoch 25/10000\n",
      "1126/1126 - 3s - loss: 97376.3594 - accuracy: 0.5829 - val_loss: 67202.0078 - val_accuracy: 0.6247 - 3s/epoch - 3ms/step\n",
      "Epoch 26/10000\n",
      "1126/1126 - 3s - loss: 94719.2656 - accuracy: 0.5827 - val_loss: 31990.1504 - val_accuracy: 0.6322 - 3s/epoch - 3ms/step\n",
      "Epoch 27/10000\n",
      "1126/1126 - 3s - loss: 89983.1250 - accuracy: 0.5829 - val_loss: 134250.2500 - val_accuracy: 0.6185 - 3s/epoch - 3ms/step\n",
      "Epoch 28/10000\n",
      "1126/1126 - 3s - loss: 90249.6641 - accuracy: 0.5854 - val_loss: 79434.5781 - val_accuracy: 0.4881 - 3s/epoch - 3ms/step\n",
      "Epoch 29/10000\n",
      "1126/1126 - 3s - loss: 102531.4062 - accuracy: 0.5816 - val_loss: 41440.2539 - val_accuracy: 0.5598 - 3s/epoch - 3ms/step\n",
      "Epoch 30/10000\n",
      "1126/1126 - 3s - loss: 91560.4297 - accuracy: 0.5837 - val_loss: 29829.6738 - val_accuracy: 0.6249 - 3s/epoch - 3ms/step\n",
      "435/435 [==============================] - 1s 2ms/step - loss: 30626.1484 - accuracy: 0.6520\n",
      "Larghezza: 200 \n",
      "Profondità: 2 \n",
      "Dimensione del batch: 100\n",
      "Test loss (binary_crossentropy): 30626.1484375\n",
      "Test metrics (accuracy): 0.6520394086837769\n",
      "Epoch 1/10000\n",
      "113/113 - 1s - loss: 13481397.0000 - accuracy: 0.5803 - val_loss: 4828961.5000 - val_accuracy: 0.5704 - 1s/epoch - 13ms/step\n",
      "Epoch 2/10000\n",
      "113/113 - 1s - loss: 2231585.2500 - accuracy: 0.5740 - val_loss: 634003.3125 - val_accuracy: 0.5828 - 617ms/epoch - 5ms/step\n",
      "Epoch 3/10000\n",
      "113/113 - 1s - loss: 376786.1250 - accuracy: 0.5819 - val_loss: 253259.9688 - val_accuracy: 0.5802 - 644ms/epoch - 6ms/step\n",
      "Epoch 4/10000\n",
      "113/113 - 1s - loss: 202162.5469 - accuracy: 0.5749 - val_loss: 192569.1875 - val_accuracy: 0.5502 - 597ms/epoch - 5ms/step\n",
      "Epoch 5/10000\n",
      "113/113 - 1s - loss: 186146.3281 - accuracy: 0.5692 - val_loss: 132048.2344 - val_accuracy: 0.5811 - 630ms/epoch - 6ms/step\n",
      "Epoch 6/10000\n",
      "113/113 - 1s - loss: 124243.0000 - accuracy: 0.5813 - val_loss: 112615.0391 - val_accuracy: 0.5988 - 586ms/epoch - 5ms/step\n",
      "Epoch 7/10000\n",
      "113/113 - 1s - loss: 153441.4844 - accuracy: 0.5729 - val_loss: 96809.5156 - val_accuracy: 0.6137 - 616ms/epoch - 5ms/step\n",
      "Epoch 8/10000\n",
      "113/113 - 1s - loss: 132082.3125 - accuracy: 0.5808 - val_loss: 102834.5703 - val_accuracy: 0.6089 - 584ms/epoch - 5ms/step\n",
      "Epoch 9/10000\n",
      "113/113 - 1s - loss: 126822.1953 - accuracy: 0.5788 - val_loss: 78285.3672 - val_accuracy: 0.5900 - 594ms/epoch - 5ms/step\n",
      "Epoch 10/10000\n",
      "113/113 - 1s - loss: 105414.8125 - accuracy: 0.5807 - val_loss: 174153.7031 - val_accuracy: 0.6139 - 594ms/epoch - 5ms/step\n",
      "Epoch 11/10000\n",
      "113/113 - 1s - loss: 110105.4766 - accuracy: 0.5795 - val_loss: 118813.8984 - val_accuracy: 0.6117 - 585ms/epoch - 5ms/step\n",
      "Epoch 12/10000\n",
      "113/113 - 1s - loss: 121186.7109 - accuracy: 0.5794 - val_loss: 140485.3125 - val_accuracy: 0.5444 - 591ms/epoch - 5ms/step\n",
      "Epoch 13/10000\n",
      "113/113 - 1s - loss: 108143.4531 - accuracy: 0.5805 - val_loss: 63696.9727 - val_accuracy: 0.5912 - 590ms/epoch - 5ms/step\n",
      "Epoch 14/10000\n",
      "113/113 - 1s - loss: 100766.9609 - accuracy: 0.5798 - val_loss: 102852.0859 - val_accuracy: 0.6132 - 608ms/epoch - 5ms/step\n",
      "Epoch 15/10000\n",
      "113/113 - 1s - loss: 106324.9375 - accuracy: 0.5784 - val_loss: 107214.2891 - val_accuracy: 0.6159 - 700ms/epoch - 6ms/step\n",
      "Epoch 16/10000\n",
      "113/113 - 1s - loss: 94596.5781 - accuracy: 0.5831 - val_loss: 62192.8125 - val_accuracy: 0.5943 - 599ms/epoch - 5ms/step\n",
      "Epoch 17/10000\n",
      "113/113 - 1s - loss: 68984.9141 - accuracy: 0.5841 - val_loss: 61333.7422 - val_accuracy: 0.5971 - 601ms/epoch - 5ms/step\n",
      "Epoch 18/10000\n",
      "113/113 - 1s - loss: 95985.8359 - accuracy: 0.5799 - val_loss: 57327.8906 - val_accuracy: 0.5956 - 620ms/epoch - 5ms/step\n",
      "Epoch 19/10000\n",
      "113/113 - 1s - loss: 102430.3359 - accuracy: 0.5804 - val_loss: 68039.4531 - val_accuracy: 0.6031 - 568ms/epoch - 5ms/step\n",
      "Epoch 20/10000\n",
      "113/113 - 1s - loss: 78282.8828 - accuracy: 0.5840 - val_loss: 54168.0898 - val_accuracy: 0.5966 - 592ms/epoch - 5ms/step\n",
      "Epoch 21/10000\n",
      "113/113 - 1s - loss: 99091.5469 - accuracy: 0.5792 - val_loss: 61895.7461 - val_accuracy: 0.6074 - 598ms/epoch - 5ms/step\n",
      "Epoch 22/10000\n",
      "113/113 - 1s - loss: 93749.6172 - accuracy: 0.5826 - val_loss: 141008.4531 - val_accuracy: 0.6139 - 583ms/epoch - 5ms/step\n",
      "Epoch 23/10000\n",
      "113/113 - 1s - loss: 94734.9141 - accuracy: 0.5830 - val_loss: 98645.7266 - val_accuracy: 0.6154 - 582ms/epoch - 5ms/step\n",
      "Epoch 24/10000\n",
      "113/113 - 1s - loss: 86130.4297 - accuracy: 0.5829 - val_loss: 144412.2656 - val_accuracy: 0.5118 - 597ms/epoch - 5ms/step\n",
      "Epoch 25/10000\n",
      "113/113 - 1s - loss: 85317.2031 - accuracy: 0.5822 - val_loss: 53673.4922 - val_accuracy: 0.5867 - 631ms/epoch - 6ms/step\n",
      "435/435 [==============================] - 1s 2ms/step - loss: 108404.7266 - accuracy: 0.6102\n",
      "Larghezza: 200 \n",
      "Profondità: 2 \n",
      "Dimensione del batch: 1000\n",
      "Test loss (binary_crossentropy): 108404.7265625\n",
      "Test metrics (accuracy): 0.6102438569068909\n",
      "Epoch 1/10000\n",
      "11260/11260 - 21s - loss: 1.0228 - accuracy: 0.5767 - val_loss: 0.8154 - val_accuracy: 0.5532 - 21s/epoch - 2ms/step\n",
      "Epoch 2/10000\n",
      "11260/11260 - 21s - loss: 0.7926 - accuracy: 0.5823 - val_loss: 0.7482 - val_accuracy: 0.5945 - 21s/epoch - 2ms/step\n",
      "Epoch 3/10000\n",
      "11260/11260 - 21s - loss: 0.7743 - accuracy: 0.5835 - val_loss: 0.7490 - val_accuracy: 0.5968 - 21s/epoch - 2ms/step\n",
      "Epoch 4/10000\n",
      "11260/11260 - 21s - loss: 0.7619 - accuracy: 0.5880 - val_loss: 0.7228 - val_accuracy: 0.5600 - 21s/epoch - 2ms/step\n",
      "Epoch 5/10000\n",
      "11260/11260 - 21s - loss: 0.7413 - accuracy: 0.5897 - val_loss: 0.7214 - val_accuracy: 0.5927 - 21s/epoch - 2ms/step\n",
      "Epoch 6/10000\n",
      "11260/11260 - 21s - loss: 0.7400 - accuracy: 0.5913 - val_loss: 0.7654 - val_accuracy: 0.6089 - 21s/epoch - 2ms/step\n",
      "Epoch 7/10000\n",
      "11260/11260 - 19s - loss: 0.7354 - accuracy: 0.5951 - val_loss: 0.7142 - val_accuracy: 0.6130 - 19s/epoch - 2ms/step\n",
      "Epoch 8/10000\n",
      "11260/11260 - 20s - loss: 0.7170 - accuracy: 0.5975 - val_loss: 0.7267 - val_accuracy: 0.5898 - 20s/epoch - 2ms/step\n",
      "Epoch 9/10000\n",
      "11260/11260 - 24s - loss: 0.7213 - accuracy: 0.5957 - val_loss: 0.7068 - val_accuracy: 0.6059 - 24s/epoch - 2ms/step\n",
      "Epoch 10/10000\n",
      "11260/11260 - 26s - loss: 0.7235 - accuracy: 0.5973 - val_loss: 0.7093 - val_accuracy: 0.6098 - 26s/epoch - 2ms/step\n",
      "Epoch 11/10000\n",
      "11260/11260 - 26s - loss: 0.7214 - accuracy: 0.5982 - val_loss: 0.7114 - val_accuracy: 0.6057 - 26s/epoch - 2ms/step\n",
      "Epoch 12/10000\n",
      "11260/11260 - 26s - loss: 0.7360 - accuracy: 0.5973 - val_loss: 0.6900 - val_accuracy: 0.6163 - 26s/epoch - 2ms/step\n",
      "Epoch 13/10000\n",
      "11260/11260 - 27s - loss: 0.7192 - accuracy: 0.5956 - val_loss: 0.7005 - val_accuracy: 0.5753 - 27s/epoch - 2ms/step\n",
      "Epoch 14/10000\n",
      "11260/11260 - 26s - loss: 0.7153 - accuracy: 0.5967 - val_loss: 0.7223 - val_accuracy: 0.5840 - 26s/epoch - 2ms/step\n",
      "Epoch 15/10000\n",
      "11260/11260 - 26s - loss: 0.7154 - accuracy: 0.5962 - val_loss: 0.6895 - val_accuracy: 0.5940 - 26s/epoch - 2ms/step\n",
      "Epoch 16/10000\n",
      "11260/11260 - 27s - loss: 0.7103 - accuracy: 0.5912 - val_loss: 0.7055 - val_accuracy: 0.6079 - 27s/epoch - 2ms/step\n",
      "Epoch 17/10000\n",
      "11260/11260 - 24s - loss: 0.7071 - accuracy: 0.5956 - val_loss: 0.7086 - val_accuracy: 0.6025 - 24s/epoch - 2ms/step\n",
      "Epoch 18/10000\n",
      "11260/11260 - 24s - loss: 0.7029 - accuracy: 0.5995 - val_loss: 0.7090 - val_accuracy: 0.5666 - 24s/epoch - 2ms/step\n",
      "Epoch 19/10000\n",
      "11260/11260 - 25s - loss: 0.6985 - accuracy: 0.6031 - val_loss: 0.6889 - val_accuracy: 0.6183 - 25s/epoch - 2ms/step\n",
      "Epoch 20/10000\n",
      "11260/11260 - 24s - loss: 0.6945 - accuracy: 0.6066 - val_loss: 0.6838 - val_accuracy: 0.6228 - 24s/epoch - 2ms/step\n",
      "Epoch 21/10000\n",
      "11260/11260 - 24s - loss: 0.6978 - accuracy: 0.6042 - val_loss: 0.6989 - val_accuracy: 0.5936 - 24s/epoch - 2ms/step\n",
      "Epoch 22/10000\n",
      "11260/11260 - 23s - loss: 0.6992 - accuracy: 0.6028 - val_loss: 0.7081 - val_accuracy: 0.5840 - 23s/epoch - 2ms/step\n",
      "Epoch 23/10000\n",
      "11260/11260 - 24s - loss: 0.7000 - accuracy: 0.6020 - val_loss: 0.6895 - val_accuracy: 0.5969 - 24s/epoch - 2ms/step\n",
      "Epoch 24/10000\n",
      "11260/11260 - 24s - loss: 0.7031 - accuracy: 0.6045 - val_loss: 0.7170 - val_accuracy: 0.5790 - 24s/epoch - 2ms/step\n",
      "Epoch 25/10000\n",
      "11260/11260 - 21s - loss: 0.6942 - accuracy: 0.6000 - val_loss: 0.6810 - val_accuracy: 0.5881 - 21s/epoch - 2ms/step\n",
      "Epoch 26/10000\n",
      "11260/11260 - 20s - loss: 0.6896 - accuracy: 0.6001 - val_loss: 0.7010 - val_accuracy: 0.6085 - 20s/epoch - 2ms/step\n",
      "Epoch 27/10000\n",
      "11260/11260 - 19s - loss: 0.6876 - accuracy: 0.6031 - val_loss: 0.6719 - val_accuracy: 0.6130 - 19s/epoch - 2ms/step\n",
      "Epoch 28/10000\n",
      "11260/11260 - 19s - loss: 0.6842 - accuracy: 0.6045 - val_loss: 0.6718 - val_accuracy: 0.6141 - 19s/epoch - 2ms/step\n",
      "Epoch 29/10000\n",
      "11260/11260 - 19s - loss: 0.6866 - accuracy: 0.6048 - val_loss: 0.6775 - val_accuracy: 0.6131 - 19s/epoch - 2ms/step\n",
      "Epoch 30/10000\n",
      "11260/11260 - 19s - loss: 0.6840 - accuracy: 0.6137 - val_loss: 0.6812 - val_accuracy: 0.6225 - 19s/epoch - 2ms/step\n",
      "435/435 [==============================] - 1s 2ms/step - loss: 0.6860 - accuracy: 0.6176\n",
      "Larghezza: 200 \n",
      "Profondità: 2 \n",
      "Dimensione del batch: 10\n",
      "Test loss (binary_crossentropy): 0.6860315799713135\n",
      "Test metrics (accuracy): 0.6175814867019653\n",
      "Epoch 1/10000\n",
      "1126/1126 - 3s - loss: 2.7627 - accuracy: 0.5813 - val_loss: 0.9501 - val_accuracy: 0.5805 - 3s/epoch - 3ms/step\n",
      "Epoch 2/10000\n",
      "1126/1126 - 3s - loss: 0.8894 - accuracy: 0.5943 - val_loss: 0.8849 - val_accuracy: 0.6013 - 3s/epoch - 3ms/step\n",
      "Epoch 3/10000\n",
      "1126/1126 - 3s - loss: 0.8481 - accuracy: 0.5988 - val_loss: 0.8468 - val_accuracy: 0.5876 - 3s/epoch - 3ms/step\n",
      "Epoch 4/10000\n",
      "1126/1126 - 3s - loss: 0.8204 - accuracy: 0.5994 - val_loss: 0.8031 - val_accuracy: 0.6113 - 3s/epoch - 3ms/step\n",
      "Epoch 5/10000\n",
      "1126/1126 - 3s - loss: 0.8169 - accuracy: 0.5999 - val_loss: 0.8287 - val_accuracy: 0.6156 - 3s/epoch - 3ms/step\n",
      "Epoch 6/10000\n",
      "1126/1126 - 3s - loss: 0.7982 - accuracy: 0.6032 - val_loss: 0.7708 - val_accuracy: 0.6012 - 3s/epoch - 3ms/step\n",
      "Epoch 7/10000\n",
      "1126/1126 - 3s - loss: 0.7741 - accuracy: 0.6051 - val_loss: 0.7921 - val_accuracy: 0.5981 - 3s/epoch - 2ms/step\n",
      "Epoch 8/10000\n",
      "1126/1126 - 3s - loss: 0.7626 - accuracy: 0.6052 - val_loss: 0.7605 - val_accuracy: 0.6139 - 3s/epoch - 3ms/step\n",
      "Epoch 9/10000\n",
      "1126/1126 - 3s - loss: 0.7606 - accuracy: 0.6046 - val_loss: 0.7860 - val_accuracy: 0.6140 - 3s/epoch - 3ms/step\n",
      "Epoch 10/10000\n",
      "1126/1126 - 3s - loss: 0.7590 - accuracy: 0.6063 - val_loss: 0.7913 - val_accuracy: 0.6087 - 3s/epoch - 3ms/step\n",
      "Epoch 11/10000\n",
      "1126/1126 - 3s - loss: 0.7631 - accuracy: 0.6022 - val_loss: 0.7505 - val_accuracy: 0.6141 - 3s/epoch - 3ms/step\n",
      "Epoch 12/10000\n",
      "1126/1126 - 3s - loss: 0.7401 - accuracy: 0.6010 - val_loss: 0.7440 - val_accuracy: 0.5965 - 3s/epoch - 3ms/step\n",
      "Epoch 13/10000\n",
      "1126/1126 - 3s - loss: 0.7312 - accuracy: 0.6035 - val_loss: 0.7163 - val_accuracy: 0.6188 - 3s/epoch - 3ms/step\n",
      "Epoch 14/10000\n",
      "1126/1126 - 3s - loss: 0.7252 - accuracy: 0.6047 - val_loss: 0.7368 - val_accuracy: 0.6209 - 3s/epoch - 3ms/step\n",
      "Epoch 15/10000\n",
      "1126/1126 - 3s - loss: 0.7300 - accuracy: 0.6053 - val_loss: 0.7288 - val_accuracy: 0.6152 - 3s/epoch - 3ms/step\n",
      "Epoch 16/10000\n",
      "1126/1126 - 3s - loss: 0.7246 - accuracy: 0.6095 - val_loss: 0.7229 - val_accuracy: 0.6234 - 3s/epoch - 3ms/step\n",
      "Epoch 17/10000\n",
      "1126/1126 - 3s - loss: 0.7264 - accuracy: 0.6084 - val_loss: 0.7127 - val_accuracy: 0.6071 - 3s/epoch - 3ms/step\n",
      "Epoch 18/10000\n",
      "1126/1126 - 3s - loss: 0.7194 - accuracy: 0.6084 - val_loss: 0.7181 - val_accuracy: 0.6215 - 3s/epoch - 3ms/step\n",
      "Epoch 19/10000\n",
      "1126/1126 - 3s - loss: 0.7176 - accuracy: 0.6044 - val_loss: 0.7225 - val_accuracy: 0.5969 - 3s/epoch - 3ms/step\n",
      "Epoch 20/10000\n",
      "1126/1126 - 3s - loss: 0.7140 - accuracy: 0.6047 - val_loss: 0.7165 - val_accuracy: 0.5796 - 3s/epoch - 3ms/step\n",
      "Epoch 21/10000\n",
      "1126/1126 - 3s - loss: 0.7116 - accuracy: 0.6053 - val_loss: 0.7157 - val_accuracy: 0.6021 - 3s/epoch - 3ms/step\n",
      "Epoch 22/10000\n",
      "1126/1126 - 3s - loss: 0.7154 - accuracy: 0.6066 - val_loss: 0.7172 - val_accuracy: 0.6165 - 3s/epoch - 3ms/step\n",
      "Epoch 23/10000\n",
      "1126/1126 - 3s - loss: 0.7135 - accuracy: 0.6054 - val_loss: 0.7091 - val_accuracy: 0.5806 - 3s/epoch - 3ms/step\n",
      "Epoch 24/10000\n",
      "1126/1126 - 3s - loss: 0.7099 - accuracy: 0.6053 - val_loss: 0.7148 - val_accuracy: 0.6070 - 3s/epoch - 3ms/step\n",
      "Epoch 25/10000\n",
      "1126/1126 - 3s - loss: 0.7115 - accuracy: 0.6081 - val_loss: 0.7082 - val_accuracy: 0.5917 - 3s/epoch - 3ms/step\n",
      "Epoch 26/10000\n",
      "1126/1126 - 3s - loss: 0.7118 - accuracy: 0.6064 - val_loss: 0.7176 - val_accuracy: 0.6180 - 3s/epoch - 3ms/step\n",
      "435/435 [==============================] - 1s 2ms/step - loss: 0.7171 - accuracy: 0.6243\n",
      "Larghezza: 200 \n",
      "Profondità: 2 \n",
      "Dimensione del batch: 100\n",
      "Test loss (binary_crossentropy): 0.7171127796173096\n",
      "Test metrics (accuracy): 0.6243435740470886\n",
      "Epoch 1/10000\n",
      "113/113 - 1s - loss: 13.7931 - accuracy: 0.5972 - val_loss: 3.0391 - val_accuracy: 0.5510 - 1s/epoch - 12ms/step\n",
      "Epoch 2/10000\n",
      "113/113 - 1s - loss: 2.1818 - accuracy: 0.5724 - val_loss: 1.6362 - val_accuracy: 0.5791 - 618ms/epoch - 5ms/step\n",
      "Epoch 3/10000\n",
      "113/113 - 1s - loss: 1.4861 - accuracy: 0.5823 - val_loss: 1.3810 - val_accuracy: 0.5753 - 749ms/epoch - 7ms/step\n",
      "Epoch 4/10000\n",
      "113/113 - 1s - loss: 1.2437 - accuracy: 0.5815 - val_loss: 1.1339 - val_accuracy: 0.5760 - 588ms/epoch - 5ms/step\n",
      "Epoch 5/10000\n",
      "113/113 - 1s - loss: 1.0480 - accuracy: 0.5876 - val_loss: 0.9771 - val_accuracy: 0.6007 - 563ms/epoch - 5ms/step\n",
      "Epoch 6/10000\n",
      "113/113 - 1s - loss: 0.9408 - accuracy: 0.5948 - val_loss: 0.9140 - val_accuracy: 0.6091 - 600ms/epoch - 5ms/step\n",
      "Epoch 7/10000\n",
      "113/113 - 1s - loss: 0.8980 - accuracy: 0.6031 - val_loss: 0.8840 - val_accuracy: 0.6087 - 587ms/epoch - 5ms/step\n",
      "Epoch 8/10000\n",
      "113/113 - 1s - loss: 0.8712 - accuracy: 0.6024 - val_loss: 0.8420 - val_accuracy: 0.6128 - 613ms/epoch - 5ms/step\n",
      "Epoch 9/10000\n",
      "113/113 - 1s - loss: 0.8519 - accuracy: 0.6045 - val_loss: 0.8268 - val_accuracy: 0.6127 - 581ms/epoch - 5ms/step\n",
      "Epoch 10/10000\n",
      "113/113 - 1s - loss: 0.8374 - accuracy: 0.6049 - val_loss: 0.8295 - val_accuracy: 0.6120 - 582ms/epoch - 5ms/step\n",
      "Epoch 11/10000\n",
      "113/113 - 1s - loss: 0.8242 - accuracy: 0.6040 - val_loss: 0.8174 - val_accuracy: 0.5912 - 583ms/epoch - 5ms/step\n",
      "Epoch 12/10000\n",
      "113/113 - 1s - loss: 0.8115 - accuracy: 0.6044 - val_loss: 0.8044 - val_accuracy: 0.6129 - 584ms/epoch - 5ms/step\n",
      "Epoch 13/10000\n",
      "113/113 - 1s - loss: 0.8045 - accuracy: 0.6041 - val_loss: 0.8003 - val_accuracy: 0.6013 - 585ms/epoch - 5ms/step\n",
      "Epoch 14/10000\n",
      "113/113 - 1s - loss: 0.7964 - accuracy: 0.6037 - val_loss: 0.7865 - val_accuracy: 0.6003 - 580ms/epoch - 5ms/step\n",
      "Epoch 15/10000\n",
      "113/113 - 1s - loss: 0.7949 - accuracy: 0.6056 - val_loss: 0.7799 - val_accuracy: 0.6075 - 591ms/epoch - 5ms/step\n",
      "Epoch 16/10000\n",
      "113/113 - 1s - loss: 0.7939 - accuracy: 0.6042 - val_loss: 0.7730 - val_accuracy: 0.6125 - 570ms/epoch - 5ms/step\n",
      "Epoch 17/10000\n",
      "113/113 - 1s - loss: 0.7890 - accuracy: 0.6055 - val_loss: 0.7823 - val_accuracy: 0.6123 - 585ms/epoch - 5ms/step\n",
      "Epoch 18/10000\n",
      "113/113 - 1s - loss: 0.7779 - accuracy: 0.6049 - val_loss: 0.7660 - val_accuracy: 0.6133 - 564ms/epoch - 5ms/step\n",
      "Epoch 19/10000\n",
      "113/113 - 1s - loss: 0.7702 - accuracy: 0.6048 - val_loss: 0.7680 - val_accuracy: 0.5900 - 584ms/epoch - 5ms/step\n",
      "Epoch 20/10000\n",
      "113/113 - 1s - loss: 0.7665 - accuracy: 0.6059 - val_loss: 0.7504 - val_accuracy: 0.6119 - 583ms/epoch - 5ms/step\n",
      "Epoch 21/10000\n",
      "113/113 - 1s - loss: 0.7606 - accuracy: 0.6046 - val_loss: 0.7523 - val_accuracy: 0.6004 - 584ms/epoch - 5ms/step\n",
      "Epoch 22/10000\n",
      "113/113 - 1s - loss: 0.7577 - accuracy: 0.6050 - val_loss: 0.7427 - val_accuracy: 0.6008 - 583ms/epoch - 5ms/step\n",
      "Epoch 23/10000\n",
      "113/113 - 1s - loss: 0.7492 - accuracy: 0.6066 - val_loss: 0.7417 - val_accuracy: 0.5962 - 600ms/epoch - 5ms/step\n",
      "Epoch 24/10000\n",
      "113/113 - 1s - loss: 0.7493 - accuracy: 0.6048 - val_loss: 0.7371 - val_accuracy: 0.6128 - 684ms/epoch - 6ms/step\n",
      "Epoch 25/10000\n",
      "113/113 - 1s - loss: 0.7487 - accuracy: 0.6066 - val_loss: 0.7412 - val_accuracy: 0.6135 - 550ms/epoch - 5ms/step\n",
      "Epoch 26/10000\n",
      "113/113 - 1s - loss: 0.7449 - accuracy: 0.6084 - val_loss: 0.7377 - val_accuracy: 0.6143 - 604ms/epoch - 5ms/step\n",
      "Epoch 27/10000\n",
      "113/113 - 1s - loss: 0.7449 - accuracy: 0.6061 - val_loss: 0.7424 - val_accuracy: 0.6139 - 619ms/epoch - 5ms/step\n",
      "Epoch 28/10000\n",
      "113/113 - 1s - loss: 0.7389 - accuracy: 0.6090 - val_loss: 0.7195 - val_accuracy: 0.6054 - 614ms/epoch - 5ms/step\n",
      "Epoch 29/10000\n",
      "113/113 - 1s - loss: 0.7350 - accuracy: 0.6092 - val_loss: 0.7200 - val_accuracy: 0.6077 - 595ms/epoch - 5ms/step\n",
      "Epoch 30/10000\n",
      "113/113 - 1s - loss: 0.7314 - accuracy: 0.6096 - val_loss: 0.7239 - val_accuracy: 0.6065 - 558ms/epoch - 5ms/step\n",
      "Epoch 31/10000\n",
      "113/113 - 1s - loss: 0.7277 - accuracy: 0.6079 - val_loss: 0.7111 - val_accuracy: 0.6138 - 598ms/epoch - 5ms/step\n",
      "Epoch 32/10000\n",
      "113/113 - 1s - loss: 0.7241 - accuracy: 0.6095 - val_loss: 0.7252 - val_accuracy: 0.6141 - 583ms/epoch - 5ms/step\n",
      "Epoch 33/10000\n",
      "113/113 - 1s - loss: 0.7204 - accuracy: 0.6094 - val_loss: 0.7182 - val_accuracy: 0.6111 - 586ms/epoch - 5ms/step\n",
      "Epoch 34/10000\n",
      "113/113 - 1s - loss: 0.7194 - accuracy: 0.6077 - val_loss: 0.7135 - val_accuracy: 0.6145 - 587ms/epoch - 5ms/step\n",
      "Epoch 35/10000\n",
      "113/113 - 1s - loss: 0.7155 - accuracy: 0.6117 - val_loss: 0.7161 - val_accuracy: 0.6170 - 594ms/epoch - 5ms/step\n",
      "Epoch 36/10000\n",
      "113/113 - 1s - loss: 0.7127 - accuracy: 0.6084 - val_loss: 0.7149 - val_accuracy: 0.6030 - 587ms/epoch - 5ms/step\n",
      "Epoch 37/10000\n",
      "113/113 - 1s - loss: 0.7081 - accuracy: 0.6104 - val_loss: 0.7099 - val_accuracy: 0.6171 - 567ms/epoch - 5ms/step\n",
      "Epoch 38/10000\n",
      "113/113 - 1s - loss: 0.7101 - accuracy: 0.6099 - val_loss: 0.7163 - val_accuracy: 0.6000 - 584ms/epoch - 5ms/step\n",
      "Epoch 39/10000\n",
      "113/113 - 1s - loss: 0.7088 - accuracy: 0.6115 - val_loss: 0.7140 - val_accuracy: 0.6071 - 571ms/epoch - 5ms/step\n",
      "Epoch 40/10000\n",
      "113/113 - 1s - loss: 0.7045 - accuracy: 0.6113 - val_loss: 0.7086 - val_accuracy: 0.6184 - 568ms/epoch - 5ms/step\n",
      "Epoch 41/10000\n",
      "113/113 - 1s - loss: 0.7075 - accuracy: 0.6097 - val_loss: 0.7133 - val_accuracy: 0.6187 - 600ms/epoch - 5ms/step\n",
      "Epoch 42/10000\n",
      "113/113 - 1s - loss: 0.7069 - accuracy: 0.6090 - val_loss: 0.7168 - val_accuracy: 0.6176 - 567ms/epoch - 5ms/step\n",
      "Epoch 43/10000\n",
      "113/113 - 1s - loss: 0.7077 - accuracy: 0.6115 - val_loss: 0.7161 - val_accuracy: 0.6183 - 587ms/epoch - 5ms/step\n",
      "Epoch 44/10000\n",
      "113/113 - 1s - loss: 0.7043 - accuracy: 0.6121 - val_loss: 0.7180 - val_accuracy: 0.6151 - 580ms/epoch - 5ms/step\n",
      "Epoch 45/10000\n",
      "113/113 - 1s - loss: 0.7011 - accuracy: 0.6110 - val_loss: 0.7140 - val_accuracy: 0.6177 - 588ms/epoch - 5ms/step\n",
      "Epoch 46/10000\n",
      "113/113 - 1s - loss: 0.7009 - accuracy: 0.6107 - val_loss: 0.7144 - val_accuracy: 0.6141 - 562ms/epoch - 5ms/step\n",
      "Epoch 47/10000\n",
      "113/113 - 1s - loss: 0.7000 - accuracy: 0.6113 - val_loss: 0.7171 - val_accuracy: 0.6019 - 567ms/epoch - 5ms/step\n",
      "Epoch 48/10000\n",
      "113/113 - 1s - loss: 0.6982 - accuracy: 0.6113 - val_loss: 0.7114 - val_accuracy: 0.6065 - 600ms/epoch - 5ms/step\n",
      "Epoch 49/10000\n",
      "113/113 - 1s - loss: 0.6942 - accuracy: 0.6136 - val_loss: 0.7060 - val_accuracy: 0.6191 - 576ms/epoch - 5ms/step\n",
      "Epoch 50/10000\n",
      "113/113 - 1s - loss: 0.6940 - accuracy: 0.6114 - val_loss: 0.7044 - val_accuracy: 0.6174 - 560ms/epoch - 5ms/step\n",
      "Epoch 51/10000\n",
      "113/113 - 1s - loss: 0.6916 - accuracy: 0.6116 - val_loss: 0.7029 - val_accuracy: 0.5978 - 650ms/epoch - 6ms/step\n",
      "Epoch 52/10000\n",
      "113/113 - 1s - loss: 0.6911 - accuracy: 0.6100 - val_loss: 0.6998 - val_accuracy: 0.6168 - 616ms/epoch - 5ms/step\n",
      "Epoch 53/10000\n",
      "113/113 - 1s - loss: 0.6923 - accuracy: 0.6104 - val_loss: 0.6980 - val_accuracy: 0.5950 - 585ms/epoch - 5ms/step\n",
      "Epoch 54/10000\n",
      "113/113 - 1s - loss: 0.6916 - accuracy: 0.6120 - val_loss: 0.7060 - val_accuracy: 0.5985 - 563ms/epoch - 5ms/step\n",
      "Epoch 55/10000\n",
      "113/113 - 1s - loss: 0.6933 - accuracy: 0.6104 - val_loss: 0.6964 - val_accuracy: 0.6179 - 554ms/epoch - 5ms/step\n",
      "Epoch 56/10000\n",
      "113/113 - 1s - loss: 0.6901 - accuracy: 0.6103 - val_loss: 0.6998 - val_accuracy: 0.5993 - 600ms/epoch - 5ms/step\n",
      "Epoch 57/10000\n",
      "113/113 - 1s - loss: 0.6871 - accuracy: 0.6102 - val_loss: 0.6939 - val_accuracy: 0.6007 - 584ms/epoch - 5ms/step\n",
      "Epoch 58/10000\n",
      "113/113 - 1s - loss: 0.6845 - accuracy: 0.6115 - val_loss: 0.6958 - val_accuracy: 0.6004 - 566ms/epoch - 5ms/step\n",
      "Epoch 59/10000\n",
      "113/113 - 1s - loss: 0.6889 - accuracy: 0.6100 - val_loss: 0.6988 - val_accuracy: 0.6127 - 608ms/epoch - 5ms/step\n",
      "435/435 [==============================] - 1s 2ms/step - loss: 0.7021 - accuracy: 0.6200\n",
      "Larghezza: 200 \n",
      "Profondità: 2 \n",
      "Dimensione del batch: 1000\n",
      "Test loss (binary_crossentropy): 0.7021111845970154\n",
      "Test metrics (accuracy): 0.6199554204940796\n",
      "Epoch 1/10000\n",
      "11260/11260 - 25s - loss: 38464840.0000 - accuracy: 0.5736 - val_loss: 96369896.0000 - val_accuracy: 0.4036 - 25s/epoch - 2ms/step\n",
      "Epoch 2/10000\n",
      "11260/11260 - 24s - loss: 25563660.0000 - accuracy: 0.5723 - val_loss: 13858680.0000 - val_accuracy: 0.5647 - 24s/epoch - 2ms/step\n",
      "Epoch 3/10000\n",
      "11260/11260 - 24s - loss: 25657732.0000 - accuracy: 0.5748 - val_loss: 18019360.0000 - val_accuracy: 0.6242 - 24s/epoch - 2ms/step\n",
      "Epoch 4/10000\n",
      "11260/11260 - 23s - loss: 23872228.0000 - accuracy: 0.5761 - val_loss: 31125638.0000 - val_accuracy: 0.6192 - 23s/epoch - 2ms/step\n",
      "Epoch 5/10000\n",
      "11260/11260 - 23s - loss: 23502012.0000 - accuracy: 0.5748 - val_loss: 44206304.0000 - val_accuracy: 0.6218 - 23s/epoch - 2ms/step\n",
      "Epoch 6/10000\n",
      "11260/11260 - 23s - loss: 21856308.0000 - accuracy: 0.5800 - val_loss: 9044040.0000 - val_accuracy: 0.6383 - 23s/epoch - 2ms/step\n",
      "Epoch 7/10000\n",
      "11260/11260 - 23s - loss: 21061362.0000 - accuracy: 0.5780 - val_loss: 84665160.0000 - val_accuracy: 0.6131 - 23s/epoch - 2ms/step\n",
      "Epoch 8/10000\n",
      "11260/11260 - 23s - loss: 20440780.0000 - accuracy: 0.5795 - val_loss: 55703048.0000 - val_accuracy: 0.6208 - 23s/epoch - 2ms/step\n",
      "Epoch 9/10000\n",
      "11260/11260 - 24s - loss: 19488468.0000 - accuracy: 0.5805 - val_loss: 20297456.0000 - val_accuracy: 0.4940 - 24s/epoch - 2ms/step\n",
      "Epoch 10/10000\n",
      "11260/11260 - 24s - loss: 19826310.0000 - accuracy: 0.5795 - val_loss: 20298930.0000 - val_accuracy: 0.6203 - 24s/epoch - 2ms/step\n",
      "Epoch 11/10000\n",
      "11260/11260 - 24s - loss: 19218254.0000 - accuracy: 0.5809 - val_loss: 4539119.0000 - val_accuracy: 0.6372 - 24s/epoch - 2ms/step\n",
      "Epoch 12/10000\n",
      "11260/11260 - 24s - loss: 19855894.0000 - accuracy: 0.5810 - val_loss: 10250088.0000 - val_accuracy: 0.5094 - 24s/epoch - 2ms/step\n",
      "Epoch 13/10000\n",
      "11260/11260 - 25s - loss: 18748604.0000 - accuracy: 0.5835 - val_loss: 16319688.0000 - val_accuracy: 0.6229 - 25s/epoch - 2ms/step\n",
      "Epoch 14/10000\n",
      "11260/11260 - 25s - loss: 18022540.0000 - accuracy: 0.5841 - val_loss: 10990207.0000 - val_accuracy: 0.5231 - 25s/epoch - 2ms/step\n",
      "Epoch 15/10000\n",
      "11260/11260 - 25s - loss: 17601916.0000 - accuracy: 0.5853 - val_loss: 14142976.0000 - val_accuracy: 0.6195 - 25s/epoch - 2ms/step\n",
      "Epoch 16/10000\n",
      "11260/11260 - 25s - loss: 16894722.0000 - accuracy: 0.5863 - val_loss: 10982679.0000 - val_accuracy: 0.5914 - 25s/epoch - 2ms/step\n",
      "435/435 [==============================] - 1s 2ms/step - loss: 8977539.0000 - accuracy: 0.6347\n",
      "Larghezza: 200 \n",
      "Profondità: 3 \n",
      "Dimensione del batch: 10\n",
      "Test loss (binary_crossentropy): 8977539.0\n",
      "Test metrics (accuracy): 0.6347025632858276\n",
      "Epoch 1/10000\n",
      "1126/1126 - 4s - loss: 187130768.0000 - accuracy: 0.5503 - val_loss: 7838707.5000 - val_accuracy: 0.5991 - 4s/epoch - 3ms/step\n",
      "Epoch 2/10000\n",
      "1126/1126 - 3s - loss: 9926978.0000 - accuracy: 0.5747 - val_loss: 4613766.5000 - val_accuracy: 0.5940 - 3s/epoch - 3ms/step\n",
      "Epoch 3/10000\n",
      "1126/1126 - 3s - loss: 9809108.0000 - accuracy: 0.5761 - val_loss: 5018660.0000 - val_accuracy: 0.6282 - 3s/epoch - 3ms/step\n",
      "Epoch 4/10000\n",
      "1126/1126 - 3s - loss: 9539507.0000 - accuracy: 0.5752 - val_loss: 10545295.0000 - val_accuracy: 0.6146 - 3s/epoch - 3ms/step\n",
      "Epoch 5/10000\n",
      "1126/1126 - 3s - loss: 9687574.0000 - accuracy: 0.5767 - val_loss: 15078559.0000 - val_accuracy: 0.6154 - 3s/epoch - 3ms/step\n",
      "Epoch 6/10000\n",
      "1126/1126 - 3s - loss: 9379521.0000 - accuracy: 0.5748 - val_loss: 8681311.0000 - val_accuracy: 0.6131 - 3s/epoch - 3ms/step\n",
      "Epoch 7/10000\n",
      "1126/1126 - 3s - loss: 8975593.0000 - accuracy: 0.5766 - val_loss: 11467627.0000 - val_accuracy: 0.5133 - 3s/epoch - 3ms/step\n",
      "Epoch 8/10000\n",
      "1126/1126 - 3s - loss: 9041333.0000 - accuracy: 0.5794 - val_loss: 6877881.5000 - val_accuracy: 0.5163 - 3s/epoch - 3ms/step\n",
      "Epoch 9/10000\n",
      "1126/1126 - 3s - loss: 8248525.5000 - accuracy: 0.5758 - val_loss: 8282879.0000 - val_accuracy: 0.5466 - 3s/epoch - 3ms/step\n",
      "Epoch 10/10000\n",
      "1126/1126 - 3s - loss: 8651784.0000 - accuracy: 0.5773 - val_loss: 13317036.0000 - val_accuracy: 0.4778 - 3s/epoch - 3ms/step\n",
      "Epoch 11/10000\n",
      "1126/1126 - 3s - loss: 9412075.0000 - accuracy: 0.5781 - val_loss: 4559717.5000 - val_accuracy: 0.6253 - 3s/epoch - 3ms/step\n",
      "Epoch 12/10000\n",
      "1126/1126 - 3s - loss: 8811530.0000 - accuracy: 0.5808 - val_loss: 4312957.5000 - val_accuracy: 0.6319 - 3s/epoch - 3ms/step\n",
      "Epoch 13/10000\n",
      "1126/1126 - 3s - loss: 9057540.0000 - accuracy: 0.5772 - val_loss: 9758187.0000 - val_accuracy: 0.6087 - 3s/epoch - 3ms/step\n",
      "Epoch 14/10000\n",
      "1126/1126 - 3s - loss: 8679087.0000 - accuracy: 0.5799 - val_loss: 6209232.0000 - val_accuracy: 0.6194 - 3s/epoch - 3ms/step\n",
      "Epoch 15/10000\n",
      "1126/1126 - 3s - loss: 10380007.0000 - accuracy: 0.5793 - val_loss: 14854303.0000 - val_accuracy: 0.6176 - 3s/epoch - 3ms/step\n",
      "Epoch 16/10000\n",
      "1126/1126 - 3s - loss: 8248983.5000 - accuracy: 0.5821 - val_loss: 3915608.0000 - val_accuracy: 0.6059 - 3s/epoch - 3ms/step\n",
      "Epoch 17/10000\n",
      "1126/1126 - 3s - loss: 8783692.0000 - accuracy: 0.5788 - val_loss: 7738667.5000 - val_accuracy: 0.6168 - 3s/epoch - 3ms/step\n",
      "Epoch 18/10000\n",
      "1126/1126 - 3s - loss: 8062377.5000 - accuracy: 0.5826 - val_loss: 15019779.0000 - val_accuracy: 0.4415 - 3s/epoch - 3ms/step\n",
      "Epoch 19/10000\n",
      "1126/1126 - 3s - loss: 8995779.0000 - accuracy: 0.5812 - val_loss: 6797306.5000 - val_accuracy: 0.6161 - 3s/epoch - 3ms/step\n",
      "Epoch 20/10000\n",
      "1126/1126 - 3s - loss: 8821667.0000 - accuracy: 0.5818 - val_loss: 5174391.5000 - val_accuracy: 0.6386 - 3s/epoch - 3ms/step\n",
      "Epoch 21/10000\n",
      "1126/1126 - 3s - loss: 8069754.5000 - accuracy: 0.5851 - val_loss: 10148696.0000 - val_accuracy: 0.6137 - 3s/epoch - 3ms/step\n",
      "Epoch 22/10000\n",
      "1126/1126 - 3s - loss: 8562234.0000 - accuracy: 0.5840 - val_loss: 5311903.0000 - val_accuracy: 0.4932 - 3s/epoch - 3ms/step\n",
      "Epoch 23/10000\n",
      "1126/1126 - 3s - loss: 8237442.5000 - accuracy: 0.5831 - val_loss: 6600924.0000 - val_accuracy: 0.6171 - 3s/epoch - 3ms/step\n",
      "Epoch 24/10000\n",
      "1126/1126 - 3s - loss: 7979616.5000 - accuracy: 0.5841 - val_loss: 4926045.0000 - val_accuracy: 0.6465 - 3s/epoch - 3ms/step\n",
      "Epoch 25/10000\n",
      "1126/1126 - 3s - loss: 8045399.5000 - accuracy: 0.5858 - val_loss: 4832913.5000 - val_accuracy: 0.6450 - 3s/epoch - 3ms/step\n",
      "Epoch 26/10000\n",
      "1126/1126 - 3s - loss: 9212949.0000 - accuracy: 0.5853 - val_loss: 2684130.2500 - val_accuracy: 0.6246 - 3s/epoch - 3ms/step\n",
      "Epoch 27/10000\n",
      "1126/1126 - 3s - loss: 7916038.0000 - accuracy: 0.5867 - val_loss: 2573789.0000 - val_accuracy: 0.6202 - 3s/epoch - 3ms/step\n",
      "Epoch 28/10000\n",
      "1126/1126 - 3s - loss: 8481460.0000 - accuracy: 0.5880 - val_loss: 3292429.0000 - val_accuracy: 0.6198 - 3s/epoch - 3ms/step\n",
      "Epoch 29/10000\n",
      "1126/1126 - 3s - loss: 8095105.0000 - accuracy: 0.5855 - val_loss: 6958134.0000 - val_accuracy: 0.4896 - 3s/epoch - 3ms/step\n",
      "Epoch 30/10000\n",
      "1126/1126 - 3s - loss: 8819195.0000 - accuracy: 0.5864 - val_loss: 18265260.0000 - val_accuracy: 0.4488 - 3s/epoch - 3ms/step\n",
      "Epoch 31/10000\n",
      "1126/1126 - 3s - loss: 7696978.0000 - accuracy: 0.5913 - val_loss: 4280872.5000 - val_accuracy: 0.5590 - 3s/epoch - 3ms/step\n",
      "Epoch 32/10000\n",
      "1126/1126 - 3s - loss: 9008845.0000 - accuracy: 0.5856 - val_loss: 22871888.0000 - val_accuracy: 0.6163 - 3s/epoch - 3ms/step\n",
      "Epoch 33/10000\n",
      "1126/1126 - 3s - loss: 8425514.0000 - accuracy: 0.5885 - val_loss: 14742739.0000 - val_accuracy: 0.4534 - 3s/epoch - 3ms/step\n",
      "Epoch 34/10000\n",
      "1126/1126 - 3s - loss: 8398904.0000 - accuracy: 0.5878 - val_loss: 5005294.5000 - val_accuracy: 0.6189 - 3s/epoch - 3ms/step\n",
      "435/435 [==============================] - 1s 2ms/step - loss: 4999679.5000 - accuracy: 0.6469\n",
      "Larghezza: 200 \n",
      "Profondità: 3 \n",
      "Dimensione del batch: 100\n",
      "Test loss (binary_crossentropy): 4999679.5\n",
      "Test metrics (accuracy): 0.6469318866729736\n",
      "Epoch 1/10000\n",
      "113/113 - 2s - loss: 521769920.0000 - accuracy: 0.4783 - val_loss: 49561416.0000 - val_accuracy: 0.4912 - 2s/epoch - 14ms/step\n",
      "Epoch 2/10000\n",
      "113/113 - 1s - loss: 23707814.0000 - accuracy: 0.5504 - val_loss: 13651870.0000 - val_accuracy: 0.5974 - 671ms/epoch - 6ms/step\n",
      "Epoch 3/10000\n",
      "113/113 - 1s - loss: 12926903.0000 - accuracy: 0.5872 - val_loss: 21022924.0000 - val_accuracy: 0.6021 - 746ms/epoch - 7ms/step\n",
      "Epoch 4/10000\n",
      "113/113 - 1s - loss: 15128350.0000 - accuracy: 0.5777 - val_loss: 11407914.0000 - val_accuracy: 0.6168 - 783ms/epoch - 7ms/step\n",
      "Epoch 5/10000\n",
      "113/113 - 1s - loss: 9085280.0000 - accuracy: 0.5927 - val_loss: 8330268.0000 - val_accuracy: 0.5972 - 716ms/epoch - 6ms/step\n",
      "Epoch 6/10000\n",
      "113/113 - 1s - loss: 7595436.0000 - accuracy: 0.5927 - val_loss: 6552660.5000 - val_accuracy: 0.5754 - 667ms/epoch - 6ms/step\n",
      "Epoch 7/10000\n",
      "113/113 - 1s - loss: 9791740.0000 - accuracy: 0.5835 - val_loss: 5587206.5000 - val_accuracy: 0.6135 - 652ms/epoch - 6ms/step\n",
      "Epoch 8/10000\n",
      "113/113 - 1s - loss: 9339619.0000 - accuracy: 0.5795 - val_loss: 7993033.5000 - val_accuracy: 0.6199 - 649ms/epoch - 6ms/step\n",
      "Epoch 9/10000\n",
      "113/113 - 1s - loss: 6754351.0000 - accuracy: 0.5903 - val_loss: 5691629.0000 - val_accuracy: 0.5843 - 650ms/epoch - 6ms/step\n",
      "Epoch 10/10000\n",
      "113/113 - 1s - loss: 6931061.5000 - accuracy: 0.5829 - val_loss: 5563183.0000 - val_accuracy: 0.6051 - 651ms/epoch - 6ms/step\n",
      "Epoch 11/10000\n",
      "113/113 - 1s - loss: 9470152.0000 - accuracy: 0.5790 - val_loss: 4763808.0000 - val_accuracy: 0.5914 - 669ms/epoch - 6ms/step\n",
      "Epoch 12/10000\n",
      "113/113 - 1s - loss: 6219000.0000 - accuracy: 0.5834 - val_loss: 5434179.5000 - val_accuracy: 0.5719 - 652ms/epoch - 6ms/step\n",
      "Epoch 13/10000\n",
      "113/113 - 1s - loss: 6270323.0000 - accuracy: 0.5814 - val_loss: 18843294.0000 - val_accuracy: 0.6111 - 674ms/epoch - 6ms/step\n",
      "Epoch 14/10000\n",
      "113/113 - 1s - loss: 9262413.0000 - accuracy: 0.5807 - val_loss: 4614110.0000 - val_accuracy: 0.6087 - 633ms/epoch - 6ms/step\n",
      "Epoch 15/10000\n",
      "113/113 - 1s - loss: 7004667.5000 - accuracy: 0.5803 - val_loss: 5421890.5000 - val_accuracy: 0.6076 - 650ms/epoch - 6ms/step\n",
      "Epoch 16/10000\n",
      "113/113 - 1s - loss: 4883479.0000 - accuracy: 0.5844 - val_loss: 4059353.0000 - val_accuracy: 0.5617 - 650ms/epoch - 6ms/step\n",
      "Epoch 17/10000\n",
      "113/113 - 1s - loss: 9038896.0000 - accuracy: 0.5772 - val_loss: 6692147.0000 - val_accuracy: 0.5446 - 665ms/epoch - 6ms/step\n",
      "Epoch 18/10000\n",
      "113/113 - 1s - loss: 4803181.5000 - accuracy: 0.5856 - val_loss: 8287645.0000 - val_accuracy: 0.6131 - 654ms/epoch - 6ms/step\n",
      "435/435 [==============================] - 1s 2ms/step - loss: 7836258.0000 - accuracy: 0.6171\n",
      "Larghezza: 200 \n",
      "Profondità: 3 \n",
      "Dimensione del batch: 1000\n",
      "Test loss (binary_crossentropy): 7836258.0\n",
      "Test metrics (accuracy): 0.617149829864502\n",
      "Epoch 1/10000\n",
      "11260/11260 - 24s - loss: 1.2656 - accuracy: 0.5814 - val_loss: 1.0163 - val_accuracy: 0.6114 - 24s/epoch - 2ms/step\n",
      "Epoch 2/10000\n",
      "11260/11260 - 24s - loss: 1.0049 - accuracy: 0.5914 - val_loss: 0.9680 - val_accuracy: 0.6000 - 24s/epoch - 2ms/step\n",
      "Epoch 3/10000\n",
      "11260/11260 - 24s - loss: 0.9377 - accuracy: 0.5938 - val_loss: 0.9638 - val_accuracy: 0.5799 - 24s/epoch - 2ms/step\n",
      "Epoch 4/10000\n",
      "11260/11260 - 24s - loss: 0.9042 - accuracy: 0.5969 - val_loss: 0.9612 - val_accuracy: 0.6026 - 24s/epoch - 2ms/step\n",
      "Epoch 5/10000\n",
      "11260/11260 - 23s - loss: 0.8860 - accuracy: 0.5944 - val_loss: 0.8422 - val_accuracy: 0.6116 - 23s/epoch - 2ms/step\n",
      "Epoch 6/10000\n",
      "11260/11260 - 24s - loss: 0.8759 - accuracy: 0.5928 - val_loss: 0.8858 - val_accuracy: 0.6063 - 24s/epoch - 2ms/step\n",
      "Epoch 7/10000\n",
      "11260/11260 - 24s - loss: 0.8512 - accuracy: 0.5924 - val_loss: 0.8232 - val_accuracy: 0.5939 - 24s/epoch - 2ms/step\n",
      "Epoch 8/10000\n",
      "11260/11260 - 27s - loss: 0.8474 - accuracy: 0.5943 - val_loss: 0.8290 - val_accuracy: 0.5932 - 27s/epoch - 2ms/step\n",
      "Epoch 9/10000\n",
      "11260/11260 - 23s - loss: 0.8371 - accuracy: 0.5903 - val_loss: 0.8134 - val_accuracy: 0.6046 - 23s/epoch - 2ms/step\n",
      "Epoch 10/10000\n",
      "11260/11260 - 27s - loss: 0.8226 - accuracy: 0.5989 - val_loss: 0.8331 - val_accuracy: 0.6000 - 27s/epoch - 2ms/step\n",
      "Epoch 11/10000\n",
      "11260/11260 - 26s - loss: 0.8249 - accuracy: 0.5956 - val_loss: 0.7682 - val_accuracy: 0.5933 - 26s/epoch - 2ms/step\n",
      "Epoch 12/10000\n",
      "11260/11260 - 25s - loss: 0.7826 - accuracy: 0.6034 - val_loss: 0.7695 - val_accuracy: 0.6035 - 25s/epoch - 2ms/step\n",
      "Epoch 13/10000\n",
      "11260/11260 - 26s - loss: 0.7778 - accuracy: 0.6044 - val_loss: 0.7690 - val_accuracy: 0.5976 - 26s/epoch - 2ms/step\n",
      "Epoch 14/10000\n",
      "11260/11260 - 27s - loss: 0.7575 - accuracy: 0.6087 - val_loss: 0.7162 - val_accuracy: 0.6281 - 27s/epoch - 2ms/step\n",
      "Epoch 15/10000\n",
      "11260/11260 - 27s - loss: 0.7443 - accuracy: 0.6127 - val_loss: 0.7384 - val_accuracy: 0.6209 - 27s/epoch - 2ms/step\n",
      "Epoch 16/10000\n",
      "11260/11260 - 26s - loss: 0.7435 - accuracy: 0.6037 - val_loss: 0.7288 - val_accuracy: 0.6155 - 26s/epoch - 2ms/step\n",
      "Epoch 17/10000\n",
      "11260/11260 - 26s - loss: 0.7299 - accuracy: 0.6085 - val_loss: 0.7334 - val_accuracy: 0.6170 - 26s/epoch - 2ms/step\n",
      "Epoch 18/10000\n",
      "11260/11260 - 27s - loss: 0.7329 - accuracy: 0.6087 - val_loss: 0.7463 - val_accuracy: 0.6069 - 27s/epoch - 2ms/step\n",
      "Epoch 19/10000\n",
      "11260/11260 - 27s - loss: 0.7298 - accuracy: 0.6041 - val_loss: 0.7196 - val_accuracy: 0.5967 - 27s/epoch - 2ms/step\n",
      "Epoch 20/10000\n",
      "11260/11260 - 26s - loss: 0.7106 - accuracy: 0.6074 - val_loss: 0.6811 - val_accuracy: 0.6176 - 26s/epoch - 2ms/step\n",
      "Epoch 21/10000\n",
      "11260/11260 - 28s - loss: 0.7080 - accuracy: 0.6090 - val_loss: 0.7238 - val_accuracy: 0.6168 - 28s/epoch - 2ms/step\n",
      "Epoch 22/10000\n",
      "11260/11260 - 27s - loss: 0.7059 - accuracy: 0.6132 - val_loss: 0.7193 - val_accuracy: 0.5554 - 27s/epoch - 2ms/step\n",
      "Epoch 23/10000\n",
      "11260/11260 - 27s - loss: 0.7030 - accuracy: 0.6106 - val_loss: 0.7015 - val_accuracy: 0.6231 - 27s/epoch - 2ms/step\n",
      "Epoch 24/10000\n",
      "11260/11260 - 26s - loss: 0.7002 - accuracy: 0.6082 - val_loss: 0.6975 - val_accuracy: 0.5869 - 26s/epoch - 2ms/step\n",
      "435/435 [==============================] - 1s 2ms/step - loss: 0.7322 - accuracy: 0.6298\n",
      "Larghezza: 200 \n",
      "Profondità: 3 \n",
      "Dimensione del batch: 10\n",
      "Test loss (binary_crossentropy): 0.732191801071167\n",
      "Test metrics (accuracy): 0.6298108100891113\n",
      "Epoch 1/10000\n",
      "1126/1126 - 5s - loss: 1.2903 - accuracy: 0.5993 - val_loss: 0.9728 - val_accuracy: 0.6178 - 5s/epoch - 4ms/step\n",
      "Epoch 2/10000\n",
      "1126/1126 - 4s - loss: 0.8878 - accuracy: 0.6077 - val_loss: 0.8894 - val_accuracy: 0.6020 - 4s/epoch - 3ms/step\n",
      "Epoch 3/10000\n",
      "1126/1126 - 4s - loss: 0.8298 - accuracy: 0.6083 - val_loss: 0.8640 - val_accuracy: 0.6145 - 4s/epoch - 3ms/step\n",
      "Epoch 4/10000\n",
      "1126/1126 - 4s - loss: 0.8108 - accuracy: 0.6073 - val_loss: 0.8174 - val_accuracy: 0.6147 - 4s/epoch - 3ms/step\n",
      "Epoch 5/10000\n",
      "1126/1126 - 4s - loss: 0.7849 - accuracy: 0.6073 - val_loss: 0.8088 - val_accuracy: 0.6166 - 4s/epoch - 3ms/step\n",
      "Epoch 6/10000\n",
      "1126/1126 - 4s - loss: 0.7652 - accuracy: 0.6117 - val_loss: 0.7833 - val_accuracy: 0.6111 - 4s/epoch - 3ms/step\n",
      "Epoch 7/10000\n",
      "1126/1126 - 4s - loss: 0.7567 - accuracy: 0.6113 - val_loss: 0.7586 - val_accuracy: 0.6157 - 4s/epoch - 3ms/step\n",
      "Epoch 8/10000\n",
      "1126/1126 - 4s - loss: 0.7536 - accuracy: 0.6142 - val_loss: 0.7637 - val_accuracy: 0.6111 - 4s/epoch - 3ms/step\n",
      "Epoch 9/10000\n",
      "1126/1126 - 4s - loss: 0.7546 - accuracy: 0.6112 - val_loss: 0.7517 - val_accuracy: 0.6282 - 4s/epoch - 3ms/step\n",
      "Epoch 10/10000\n",
      "1126/1126 - 4s - loss: 0.7457 - accuracy: 0.6141 - val_loss: 0.7622 - val_accuracy: 0.6140 - 4s/epoch - 3ms/step\n",
      "Epoch 11/10000\n",
      "1126/1126 - 4s - loss: 0.7545 - accuracy: 0.6110 - val_loss: 0.7632 - val_accuracy: 0.6134 - 4s/epoch - 3ms/step\n",
      "Epoch 12/10000\n",
      "1126/1126 - 4s - loss: 0.7310 - accuracy: 0.6110 - val_loss: 0.7564 - val_accuracy: 0.6142 - 4s/epoch - 3ms/step\n",
      "Epoch 13/10000\n",
      "1126/1126 - 4s - loss: 0.7350 - accuracy: 0.6107 - val_loss: 0.7563 - val_accuracy: 0.6019 - 4s/epoch - 3ms/step\n",
      "Epoch 14/10000\n",
      "1126/1126 - 4s - loss: 0.7391 - accuracy: 0.6117 - val_loss: 0.7388 - val_accuracy: 0.6095 - 4s/epoch - 3ms/step\n",
      "Epoch 15/10000\n",
      "1126/1126 - 4s - loss: 0.7321 - accuracy: 0.6163 - val_loss: 0.7471 - val_accuracy: 0.5945 - 4s/epoch - 3ms/step\n",
      "Epoch 16/10000\n",
      "1126/1126 - 4s - loss: 0.7387 - accuracy: 0.6108 - val_loss: 0.7409 - val_accuracy: 0.6080 - 4s/epoch - 3ms/step\n",
      "Epoch 17/10000\n",
      "1126/1126 - 4s - loss: 0.7285 - accuracy: 0.6122 - val_loss: 0.7272 - val_accuracy: 0.6200 - 4s/epoch - 3ms/step\n",
      "Epoch 18/10000\n",
      "1126/1126 - 4s - loss: 0.7361 - accuracy: 0.6105 - val_loss: 0.8056 - val_accuracy: 0.6092 - 4s/epoch - 3ms/step\n",
      "Epoch 19/10000\n",
      "1126/1126 - 4s - loss: 0.7578 - accuracy: 0.6093 - val_loss: 0.7431 - val_accuracy: 0.6175 - 4s/epoch - 3ms/step\n",
      "435/435 [==============================] - 1s 2ms/step - loss: 0.7415 - accuracy: 0.6284\n",
      "Larghezza: 200 \n",
      "Profondità: 3 \n",
      "Dimensione del batch: 100\n",
      "Test loss (binary_crossentropy): 0.7414926290512085\n",
      "Test metrics (accuracy): 0.6284440159797668\n",
      "Epoch 1/10000\n",
      "113/113 - 2s - loss: 4.5990 - accuracy: 0.5712 - val_loss: 1.3506 - val_accuracy: 0.5728 - 2s/epoch - 18ms/step\n",
      "Epoch 2/10000\n",
      "113/113 - 1s - loss: 1.1626 - accuracy: 0.5879 - val_loss: 1.0231 - val_accuracy: 0.5848 - 931ms/epoch - 8ms/step\n",
      "Epoch 3/10000\n",
      "113/113 - 1s - loss: 0.9722 - accuracy: 0.5971 - val_loss: 0.9226 - val_accuracy: 0.5883 - 834ms/epoch - 7ms/step\n",
      "Epoch 4/10000\n",
      "113/113 - 1s - loss: 0.9013 - accuracy: 0.5987 - val_loss: 0.8629 - val_accuracy: 0.6131 - 916ms/epoch - 8ms/step\n",
      "Epoch 5/10000\n",
      "113/113 - 1s - loss: 0.8579 - accuracy: 0.6035 - val_loss: 0.8248 - val_accuracy: 0.6139 - 881ms/epoch - 8ms/step\n",
      "Epoch 6/10000\n",
      "113/113 - 1s - loss: 0.8273 - accuracy: 0.6048 - val_loss: 0.8118 - val_accuracy: 0.6132 - 906ms/epoch - 8ms/step\n",
      "Epoch 7/10000\n",
      "113/113 - 1s - loss: 0.8053 - accuracy: 0.6046 - val_loss: 0.7813 - val_accuracy: 0.6024 - 898ms/epoch - 8ms/step\n",
      "Epoch 8/10000\n",
      "113/113 - 1s - loss: 0.7877 - accuracy: 0.6032 - val_loss: 0.7692 - val_accuracy: 0.5996 - 1s/epoch - 9ms/step\n",
      "Epoch 9/10000\n",
      "113/113 - 1s - loss: 0.7697 - accuracy: 0.6061 - val_loss: 0.7589 - val_accuracy: 0.5981 - 1s/epoch - 11ms/step\n",
      "Epoch 10/10000\n",
      "113/113 - 1s - loss: 0.7564 - accuracy: 0.6069 - val_loss: 0.7383 - val_accuracy: 0.6160 - 1s/epoch - 10ms/step\n",
      "Epoch 11/10000\n",
      "113/113 - 1s - loss: 0.7479 - accuracy: 0.6065 - val_loss: 0.7327 - val_accuracy: 0.6035 - 1s/epoch - 9ms/step\n",
      "Epoch 12/10000\n",
      "113/113 - 1s - loss: 0.7422 - accuracy: 0.6066 - val_loss: 0.7236 - val_accuracy: 0.6129 - 1s/epoch - 11ms/step\n",
      "Epoch 13/10000\n",
      "113/113 - 1s - loss: 0.7352 - accuracy: 0.6047 - val_loss: 0.7287 - val_accuracy: 0.6016 - 985ms/epoch - 9ms/step\n",
      "Epoch 14/10000\n",
      "113/113 - 1s - loss: 0.7242 - accuracy: 0.6049 - val_loss: 0.7160 - val_accuracy: 0.5959 - 896ms/epoch - 8ms/step\n",
      "Epoch 15/10000\n",
      "113/113 - 1s - loss: 0.7241 - accuracy: 0.6055 - val_loss: 0.7146 - val_accuracy: 0.6123 - 854ms/epoch - 8ms/step\n",
      "Epoch 16/10000\n",
      "113/113 - 1s - loss: 0.7183 - accuracy: 0.6080 - val_loss: 0.7068 - val_accuracy: 0.6166 - 893ms/epoch - 8ms/step\n",
      "Epoch 17/10000\n",
      "113/113 - 1s - loss: 0.7233 - accuracy: 0.6079 - val_loss: 0.7063 - val_accuracy: 0.6137 - 912ms/epoch - 8ms/step\n",
      "Epoch 18/10000\n",
      "113/113 - 1s - loss: 0.7110 - accuracy: 0.6088 - val_loss: 0.7206 - val_accuracy: 0.6127 - 1s/epoch - 10ms/step\n",
      "Epoch 19/10000\n",
      "113/113 - 1s - loss: 0.7117 - accuracy: 0.6070 - val_loss: 0.7036 - val_accuracy: 0.6157 - 1s/epoch - 10ms/step\n",
      "Epoch 20/10000\n",
      "113/113 - 1s - loss: 0.7073 - accuracy: 0.6058 - val_loss: 0.6957 - val_accuracy: 0.6175 - 1s/epoch - 10ms/step\n",
      "Epoch 21/10000\n",
      "113/113 - 1s - loss: 0.7012 - accuracy: 0.6104 - val_loss: 0.7079 - val_accuracy: 0.5984 - 976ms/epoch - 9ms/step\n",
      "Epoch 22/10000\n",
      "113/113 - 1s - loss: 0.6988 - accuracy: 0.6093 - val_loss: 0.6983 - val_accuracy: 0.5903 - 1s/epoch - 10ms/step\n",
      "Epoch 23/10000\n",
      "113/113 - 1s - loss: 0.6970 - accuracy: 0.6105 - val_loss: 0.6937 - val_accuracy: 0.6021 - 1s/epoch - 10ms/step\n",
      "Epoch 24/10000\n",
      "113/113 - 1s - loss: 0.6942 - accuracy: 0.6117 - val_loss: 0.6884 - val_accuracy: 0.6161 - 1s/epoch - 10ms/step\n",
      "Epoch 25/10000\n",
      "113/113 - 1s - loss: 0.6954 - accuracy: 0.6129 - val_loss: 0.7037 - val_accuracy: 0.6184 - 935ms/epoch - 8ms/step\n",
      "Epoch 26/10000\n",
      "113/113 - 1s - loss: 0.6952 - accuracy: 0.6105 - val_loss: 0.7053 - val_accuracy: 0.6113 - 1s/epoch - 9ms/step\n",
      "Epoch 27/10000\n",
      "113/113 - 1s - loss: 0.6950 - accuracy: 0.6099 - val_loss: 0.6981 - val_accuracy: 0.6125 - 1s/epoch - 9ms/step\n",
      "Epoch 28/10000\n",
      "113/113 - 1s - loss: 0.6926 - accuracy: 0.6121 - val_loss: 0.7021 - val_accuracy: 0.6108 - 916ms/epoch - 8ms/step\n",
      "Epoch 29/10000\n",
      "113/113 - 1s - loss: 0.6928 - accuracy: 0.6089 - val_loss: 0.7007 - val_accuracy: 0.6192 - 909ms/epoch - 8ms/step\n",
      "Epoch 30/10000\n",
      "113/113 - 1s - loss: 0.6964 - accuracy: 0.6107 - val_loss: 0.7099 - val_accuracy: 0.6186 - 954ms/epoch - 8ms/step\n",
      "Epoch 31/10000\n",
      "113/113 - 1s - loss: 0.6969 - accuracy: 0.6130 - val_loss: 0.7077 - val_accuracy: 0.6071 - 878ms/epoch - 8ms/step\n",
      "Epoch 32/10000\n",
      "113/113 - 1s - loss: 0.6953 - accuracy: 0.6067 - val_loss: 0.6968 - val_accuracy: 0.6181 - 875ms/epoch - 8ms/step\n",
      "Epoch 33/10000\n",
      "113/113 - 1s - loss: 0.6910 - accuracy: 0.6084 - val_loss: 0.7050 - val_accuracy: 0.5944 - 923ms/epoch - 8ms/step\n",
      "Epoch 34/10000\n",
      "113/113 - 1s - loss: 0.6908 - accuracy: 0.6126 - val_loss: 0.7079 - val_accuracy: 0.5996 - 874ms/epoch - 8ms/step\n",
      "Epoch 35/10000\n",
      "113/113 - 1s - loss: 0.6904 - accuracy: 0.6094 - val_loss: 0.7015 - val_accuracy: 0.6163 - 957ms/epoch - 8ms/step\n",
      "Epoch 36/10000\n",
      "113/113 - 1s - loss: 0.6869 - accuracy: 0.6137 - val_loss: 0.6983 - val_accuracy: 0.6032 - 937ms/epoch - 8ms/step\n",
      "Epoch 37/10000\n",
      "113/113 - 1s - loss: 0.6890 - accuracy: 0.6121 - val_loss: 0.7035 - val_accuracy: 0.6120 - 960ms/epoch - 8ms/step\n",
      "Epoch 38/10000\n",
      "113/113 - 1s - loss: 0.6887 - accuracy: 0.6128 - val_loss: 0.6939 - val_accuracy: 0.6144 - 954ms/epoch - 8ms/step\n",
      "Epoch 39/10000\n",
      "113/113 - 1s - loss: 0.6826 - accuracy: 0.6145 - val_loss: 0.7029 - val_accuracy: 0.6161 - 942ms/epoch - 8ms/step\n",
      "435/435 [==============================] - 1s 3ms/step - loss: 0.6908 - accuracy: 0.6224\n",
      "Larghezza: 200 \n",
      "Profondità: 3 \n",
      "Dimensione del batch: 1000\n",
      "Test loss (binary_crossentropy): 0.6908475160598755\n",
      "Test metrics (accuracy): 0.622401237487793\n",
      "Epoch 1/10000\n",
      "11260/11260 - 26s - loss: 11060.7129 - accuracy: 0.5728 - val_loss: 3705.4790 - val_accuracy: 0.4986 - 26s/epoch - 2ms/step\n",
      "Epoch 2/10000\n",
      "11260/11260 - 21s - loss: 2798.3457 - accuracy: 0.5780 - val_loss: 3571.9336 - val_accuracy: 0.6131 - 21s/epoch - 2ms/step\n",
      "Epoch 3/10000\n",
      "11260/11260 - 22s - loss: 2579.8655 - accuracy: 0.5854 - val_loss: 895.9267 - val_accuracy: 0.6561 - 22s/epoch - 2ms/step\n",
      "Epoch 4/10000\n",
      "11260/11260 - 21s - loss: 2419.5273 - accuracy: 0.5876 - val_loss: 2962.1687 - val_accuracy: 0.6202 - 21s/epoch - 2ms/step\n",
      "Epoch 5/10000\n",
      "11260/11260 - 20s - loss: 2309.1035 - accuracy: 0.5891 - val_loss: 779.0013 - val_accuracy: 0.6353 - 20s/epoch - 2ms/step\n",
      "Epoch 6/10000\n",
      "11260/11260 - 18s - loss: 2391.3535 - accuracy: 0.5886 - val_loss: 1190.4957 - val_accuracy: 0.6270 - 18s/epoch - 2ms/step\n",
      "Epoch 7/10000\n",
      "11260/11260 - 18s - loss: 2383.4048 - accuracy: 0.5892 - val_loss: 4607.4409 - val_accuracy: 0.6203 - 18s/epoch - 2ms/step\n",
      "Epoch 8/10000\n",
      "11260/11260 - 21s - loss: 2387.9500 - accuracy: 0.5906 - val_loss: 4348.2080 - val_accuracy: 0.6236 - 21s/epoch - 2ms/step\n",
      "Epoch 9/10000\n",
      "11260/11260 - 27s - loss: 2309.8113 - accuracy: 0.5901 - val_loss: 1530.3536 - val_accuracy: 0.5902 - 27s/epoch - 2ms/step\n",
      "Epoch 10/10000\n",
      "11260/11260 - 21s - loss: 2292.8953 - accuracy: 0.5950 - val_loss: 5978.2158 - val_accuracy: 0.6160 - 21s/epoch - 2ms/step\n",
      "Epoch 11/10000\n",
      "11260/11260 - 21s - loss: 2362.6157 - accuracy: 0.5926 - val_loss: 5329.7212 - val_accuracy: 0.6151 - 21s/epoch - 2ms/step\n",
      "Epoch 12/10000\n",
      "11260/11260 - 22s - loss: 2282.4351 - accuracy: 0.5919 - val_loss: 970.8289 - val_accuracy: 0.5650 - 22s/epoch - 2ms/step\n",
      "Epoch 13/10000\n",
      "11260/11260 - 23s - loss: 2295.4673 - accuracy: 0.5935 - val_loss: 724.8590 - val_accuracy: 0.6619 - 23s/epoch - 2ms/step\n",
      "Epoch 14/10000\n",
      "11260/11260 - 22s - loss: 2310.2708 - accuracy: 0.5915 - val_loss: 2203.8931 - val_accuracy: 0.6263 - 22s/epoch - 2ms/step\n",
      "Epoch 15/10000\n",
      "11260/11260 - 23s - loss: 2305.0771 - accuracy: 0.5935 - val_loss: 3216.8184 - val_accuracy: 0.4596 - 23s/epoch - 2ms/step\n",
      "Epoch 16/10000\n",
      "11260/11260 - 23s - loss: 2165.7058 - accuracy: 0.5984 - val_loss: 659.1624 - val_accuracy: 0.6542 - 23s/epoch - 2ms/step\n",
      "Epoch 17/10000\n",
      "11260/11260 - 23s - loss: 2164.7422 - accuracy: 0.5958 - val_loss: 1628.1394 - val_accuracy: 0.6257 - 23s/epoch - 2ms/step\n",
      "Epoch 18/10000\n",
      "11260/11260 - 23s - loss: 2172.9048 - accuracy: 0.5963 - val_loss: 1250.7969 - val_accuracy: 0.5506 - 23s/epoch - 2ms/step\n",
      "Epoch 19/10000\n",
      "11260/11260 - 23s - loss: 2192.3635 - accuracy: 0.5946 - val_loss: 1781.0944 - val_accuracy: 0.5682 - 23s/epoch - 2ms/step\n",
      "Epoch 20/10000\n",
      "11260/11260 - 22s - loss: 2262.0930 - accuracy: 0.5954 - val_loss: 746.0903 - val_accuracy: 0.6230 - 22s/epoch - 2ms/step\n",
      "Epoch 21/10000\n",
      "11260/11260 - 22s - loss: 2232.4248 - accuracy: 0.5950 - val_loss: 2431.9858 - val_accuracy: 0.6123 - 22s/epoch - 2ms/step\n",
      "Epoch 22/10000\n",
      "11260/11260 - 22s - loss: 2292.8516 - accuracy: 0.5945 - val_loss: 924.0840 - val_accuracy: 0.6450 - 22s/epoch - 2ms/step\n",
      "Epoch 23/10000\n",
      "11260/11260 - 22s - loss: 2331.4336 - accuracy: 0.5948 - val_loss: 1091.3750 - val_accuracy: 0.6280 - 22s/epoch - 2ms/step\n",
      "435/435 [==============================] - 1s 2ms/step - loss: 709.2742 - accuracy: 0.6626\n",
      "Larghezza: 500 \n",
      "Profondità: 1 \n",
      "Dimensione del batch: 10\n",
      "Test loss (binary_crossentropy): 709.274169921875\n",
      "Test metrics (accuracy): 0.6626142263412476\n",
      "Epoch 1/10000\n",
      "1126/1126 - 3s - loss: 267981.0625 - accuracy: 0.5829 - val_loss: 22081.2637 - val_accuracy: 0.6251 - 3s/epoch - 3ms/step\n",
      "Epoch 2/10000\n",
      "1126/1126 - 3s - loss: 11209.0342 - accuracy: 0.6099 - val_loss: 3070.0747 - val_accuracy: 0.6051 - 3s/epoch - 2ms/step\n",
      "Epoch 3/10000\n",
      "1126/1126 - 3s - loss: 2352.7710 - accuracy: 0.5827 - val_loss: 1747.9023 - val_accuracy: 0.6052 - 3s/epoch - 2ms/step\n",
      "Epoch 4/10000\n",
      "1126/1126 - 3s - loss: 1619.2697 - accuracy: 0.5848 - val_loss: 1538.8195 - val_accuracy: 0.6199 - 3s/epoch - 2ms/step\n",
      "Epoch 5/10000\n",
      "1126/1126 - 2s - loss: 1367.3618 - accuracy: 0.5884 - val_loss: 837.9603 - val_accuracy: 0.6154 - 2s/epoch - 2ms/step\n",
      "Epoch 6/10000\n",
      "1126/1126 - 3s - loss: 1103.4518 - accuracy: 0.5914 - val_loss: 672.3271 - val_accuracy: 0.6222 - 3s/epoch - 2ms/step\n",
      "Epoch 7/10000\n",
      "1126/1126 - 3s - loss: 1170.2313 - accuracy: 0.5933 - val_loss: 994.4329 - val_accuracy: 0.5627 - 3s/epoch - 2ms/step\n",
      "Epoch 8/10000\n",
      "1126/1126 - 3s - loss: 1096.4705 - accuracy: 0.5954 - val_loss: 733.2349 - val_accuracy: 0.6251 - 3s/epoch - 2ms/step\n",
      "Epoch 9/10000\n",
      "1126/1126 - 3s - loss: 961.1034 - accuracy: 0.5944 - val_loss: 547.0495 - val_accuracy: 0.6342 - 3s/epoch - 2ms/step\n",
      "Epoch 10/10000\n",
      "1126/1126 - 3s - loss: 995.9799 - accuracy: 0.5956 - val_loss: 3338.1692 - val_accuracy: 0.6173 - 3s/epoch - 2ms/step\n",
      "Epoch 11/10000\n",
      "1126/1126 - 2s - loss: 1083.2773 - accuracy: 0.5950 - val_loss: 845.7194 - val_accuracy: 0.5709 - 2s/epoch - 2ms/step\n",
      "Epoch 12/10000\n",
      "1126/1126 - 3s - loss: 993.7654 - accuracy: 0.5950 - val_loss: 606.8063 - val_accuracy: 0.6293 - 3s/epoch - 3ms/step\n",
      "Epoch 13/10000\n",
      "1126/1126 - 3s - loss: 1023.9800 - accuracy: 0.5949 - val_loss: 468.8368 - val_accuracy: 0.6250 - 3s/epoch - 2ms/step\n",
      "Epoch 14/10000\n",
      "1126/1126 - 3s - loss: 978.7451 - accuracy: 0.5953 - val_loss: 699.1432 - val_accuracy: 0.6235 - 3s/epoch - 2ms/step\n",
      "Epoch 15/10000\n",
      "1126/1126 - 3s - loss: 966.2339 - accuracy: 0.5964 - val_loss: 437.0476 - val_accuracy: 0.6512 - 3s/epoch - 2ms/step\n",
      "Epoch 16/10000\n",
      "1126/1126 - 3s - loss: 1002.3812 - accuracy: 0.5942 - val_loss: 916.3181 - val_accuracy: 0.5948 - 3s/epoch - 2ms/step\n",
      "Epoch 17/10000\n",
      "1126/1126 - 3s - loss: 973.4500 - accuracy: 0.5954 - val_loss: 493.7746 - val_accuracy: 0.5830 - 3s/epoch - 2ms/step\n",
      "Epoch 18/10000\n",
      "1126/1126 - 3s - loss: 996.9247 - accuracy: 0.5956 - val_loss: 972.6156 - val_accuracy: 0.6250 - 3s/epoch - 2ms/step\n",
      "Epoch 19/10000\n",
      "1126/1126 - 3s - loss: 938.0916 - accuracy: 0.5980 - val_loss: 450.3710 - val_accuracy: 0.6545 - 3s/epoch - 2ms/step\n",
      "Epoch 20/10000\n",
      "1126/1126 - 3s - loss: 852.5851 - accuracy: 0.5997 - val_loss: 436.1865 - val_accuracy: 0.6394 - 3s/epoch - 2ms/step\n",
      "Epoch 21/10000\n",
      "1126/1126 - 3s - loss: 890.1895 - accuracy: 0.5993 - val_loss: 445.2014 - val_accuracy: 0.6306 - 3s/epoch - 2ms/step\n",
      "Epoch 22/10000\n",
      "1126/1126 - 3s - loss: 904.2902 - accuracy: 0.5999 - val_loss: 1232.5767 - val_accuracy: 0.5633 - 3s/epoch - 2ms/step\n",
      "Epoch 23/10000\n",
      "1126/1126 - 3s - loss: 922.2600 - accuracy: 0.5985 - val_loss: 1040.1300 - val_accuracy: 0.6235 - 3s/epoch - 2ms/step\n",
      "Epoch 24/10000\n",
      "1126/1126 - 3s - loss: 952.2325 - accuracy: 0.5978 - val_loss: 695.3467 - val_accuracy: 0.6292 - 3s/epoch - 2ms/step\n",
      "Epoch 25/10000\n",
      "1126/1126 - 3s - loss: 875.1990 - accuracy: 0.5989 - val_loss: 3505.9944 - val_accuracy: 0.4512 - 3s/epoch - 2ms/step\n",
      "Epoch 26/10000\n",
      "1126/1126 - 3s - loss: 935.1907 - accuracy: 0.5977 - val_loss: 1298.8104 - val_accuracy: 0.5275 - 3s/epoch - 2ms/step\n",
      "Epoch 27/10000\n",
      "1126/1126 - 3s - loss: 863.5493 - accuracy: 0.5985 - val_loss: 381.9908 - val_accuracy: 0.6354 - 3s/epoch - 2ms/step\n",
      "Epoch 28/10000\n",
      "1126/1126 - 3s - loss: 857.8046 - accuracy: 0.5991 - val_loss: 1280.9113 - val_accuracy: 0.4885 - 3s/epoch - 2ms/step\n",
      "Epoch 29/10000\n",
      "1126/1126 - 3s - loss: 934.5651 - accuracy: 0.5985 - val_loss: 464.2012 - val_accuracy: 0.6325 - 3s/epoch - 2ms/step\n",
      "435/435 [==============================] - 1s 2ms/step - loss: 484.8492 - accuracy: 0.6492\n",
      "Larghezza: 500 \n",
      "Profondità: 1 \n",
      "Dimensione del batch: 100\n",
      "Test loss (binary_crossentropy): 484.8492126464844\n",
      "Test metrics (accuracy): 0.649233877658844\n",
      "Epoch 1/10000\n",
      "113/113 - 2s - loss: 2966536.5000 - accuracy: 0.3887 - val_loss: 2154935.2500 - val_accuracy: 0.3869 - 2s/epoch - 13ms/step\n",
      "Epoch 2/10000\n",
      "113/113 - 1s - loss: 1314518.6250 - accuracy: 0.3987 - val_loss: 517994.1875 - val_accuracy: 0.4454 - 702ms/epoch - 6ms/step\n",
      "Epoch 3/10000\n",
      "113/113 - 1s - loss: 186389.2812 - accuracy: 0.5606 - val_loss: 113149.6953 - val_accuracy: 0.5867 - 778ms/epoch - 7ms/step\n",
      "Epoch 4/10000\n",
      "113/113 - 1s - loss: 98047.2891 - accuracy: 0.5896 - val_loss: 81670.1406 - val_accuracy: 0.5908 - 732ms/epoch - 6ms/step\n",
      "Epoch 5/10000\n",
      "113/113 - 1s - loss: 66177.9297 - accuracy: 0.5941 - val_loss: 49743.1484 - val_accuracy: 0.6024 - 744ms/epoch - 7ms/step\n",
      "Epoch 6/10000\n",
      "113/113 - 1s - loss: 36742.7969 - accuracy: 0.6024 - val_loss: 23462.9629 - val_accuracy: 0.6080 - 702ms/epoch - 6ms/step\n",
      "Epoch 7/10000\n",
      "113/113 - 1s - loss: 16749.4492 - accuracy: 0.5909 - val_loss: 12947.4746 - val_accuracy: 0.5737 - 752ms/epoch - 7ms/step\n",
      "Epoch 8/10000\n",
      "113/113 - 1s - loss: 11715.0771 - accuracy: 0.5740 - val_loss: 10138.1611 - val_accuracy: 0.5789 - 716ms/epoch - 6ms/step\n",
      "Epoch 9/10000\n",
      "113/113 - 1s - loss: 9227.3994 - accuracy: 0.5770 - val_loss: 7995.0483 - val_accuracy: 0.5785 - 703ms/epoch - 6ms/step\n",
      "Epoch 10/10000\n",
      "113/113 - 1s - loss: 7277.1226 - accuracy: 0.5773 - val_loss: 6238.2778 - val_accuracy: 0.5793 - 685ms/epoch - 6ms/step\n",
      "Epoch 11/10000\n",
      "113/113 - 1s - loss: 5666.1748 - accuracy: 0.5766 - val_loss: 4993.5806 - val_accuracy: 0.5697 - 721ms/epoch - 6ms/step\n",
      "Epoch 12/10000\n",
      "113/113 - 1s - loss: 4396.1475 - accuracy: 0.5725 - val_loss: 3780.5642 - val_accuracy: 0.5749 - 702ms/epoch - 6ms/step\n",
      "Epoch 13/10000\n",
      "113/113 - 1s - loss: 3474.0417 - accuracy: 0.5708 - val_loss: 2996.9841 - val_accuracy: 0.5709 - 732ms/epoch - 6ms/step\n",
      "Epoch 14/10000\n",
      "113/113 - 1s - loss: 2782.9846 - accuracy: 0.5699 - val_loss: 2383.7708 - val_accuracy: 0.5715 - 711ms/epoch - 6ms/step\n",
      "Epoch 15/10000\n",
      "113/113 - 1s - loss: 2348.2874 - accuracy: 0.5695 - val_loss: 2426.5798 - val_accuracy: 0.5454 - 702ms/epoch - 6ms/step\n",
      "Epoch 16/10000\n",
      "113/113 - 1s - loss: 2142.4561 - accuracy: 0.5744 - val_loss: 1673.3979 - val_accuracy: 0.5824 - 701ms/epoch - 6ms/step\n",
      "435/435 [==============================] - 1s 2ms/step - loss: 23983.8633 - accuracy: 0.6034\n",
      "Larghezza: 500 \n",
      "Profondità: 1 \n",
      "Dimensione del batch: 1000\n",
      "Test loss (binary_crossentropy): 23983.86328125\n",
      "Test metrics (accuracy): 0.6034098267555237\n",
      "Epoch 1/10000\n",
      "11260/11260 - 23s - loss: 2.5100 - accuracy: 0.5632 - val_loss: 1.6857 - val_accuracy: 0.5683 - 23s/epoch - 2ms/step\n",
      "Epoch 2/10000\n",
      "11260/11260 - 24s - loss: 1.5964 - accuracy: 0.5666 - val_loss: 1.5224 - val_accuracy: 0.5359 - 24s/epoch - 2ms/step\n",
      "Epoch 3/10000\n",
      "11260/11260 - 22s - loss: 1.4267 - accuracy: 0.5637 - val_loss: 1.3162 - val_accuracy: 0.5737 - 22s/epoch - 2ms/step\n",
      "Epoch 4/10000\n",
      "11260/11260 - 23s - loss: 1.2886 - accuracy: 0.5703 - val_loss: 1.2209 - val_accuracy: 0.5716 - 23s/epoch - 2ms/step\n",
      "Epoch 5/10000\n",
      "11260/11260 - 22s - loss: 1.1797 - accuracy: 0.5771 - val_loss: 1.1181 - val_accuracy: 0.5801 - 22s/epoch - 2ms/step\n",
      "Epoch 6/10000\n",
      "11260/11260 - 22s - loss: 1.0943 - accuracy: 0.5848 - val_loss: 1.0395 - val_accuracy: 0.5987 - 22s/epoch - 2ms/step\n",
      "Epoch 7/10000\n",
      "11260/11260 - 22s - loss: 1.0423 - accuracy: 0.5859 - val_loss: 1.0387 - val_accuracy: 0.5740 - 22s/epoch - 2ms/step\n",
      "Epoch 8/10000\n",
      "11260/11260 - 21s - loss: 1.0268 - accuracy: 0.5842 - val_loss: 0.9822 - val_accuracy: 0.5960 - 21s/epoch - 2ms/step\n",
      "Epoch 9/10000\n",
      "11260/11260 - 21s - loss: 0.9537 - accuracy: 0.5899 - val_loss: 0.9620 - val_accuracy: 0.5969 - 21s/epoch - 2ms/step\n",
      "Epoch 10/10000\n",
      "11260/11260 - 21s - loss: 0.9060 - accuracy: 0.5996 - val_loss: 0.8599 - val_accuracy: 0.6056 - 21s/epoch - 2ms/step\n",
      "Epoch 11/10000\n",
      "11260/11260 - 21s - loss: 0.8878 - accuracy: 0.5979 - val_loss: 0.8772 - val_accuracy: 0.6083 - 21s/epoch - 2ms/step\n",
      "Epoch 12/10000\n",
      "11260/11260 - 21s - loss: 0.8623 - accuracy: 0.5982 - val_loss: 0.8597 - val_accuracy: 0.6132 - 21s/epoch - 2ms/step\n",
      "Epoch 13/10000\n",
      "11260/11260 - 21s - loss: 0.8446 - accuracy: 0.5983 - val_loss: 0.8151 - val_accuracy: 0.6047 - 21s/epoch - 2ms/step\n",
      "Epoch 14/10000\n",
      "11260/11260 - 27s - loss: 0.8372 - accuracy: 0.6006 - val_loss: 0.8140 - val_accuracy: 0.6100 - 27s/epoch - 2ms/step\n",
      "Epoch 15/10000\n",
      "11260/11260 - 20s - loss: 0.8260 - accuracy: 0.6030 - val_loss: 0.7963 - val_accuracy: 0.6107 - 20s/epoch - 2ms/step\n",
      "Epoch 16/10000\n",
      "11260/11260 - 20s - loss: 0.8223 - accuracy: 0.6053 - val_loss: 0.7957 - val_accuracy: 0.6227 - 20s/epoch - 2ms/step\n",
      "Epoch 17/10000\n",
      "11260/11260 - 24s - loss: 0.7975 - accuracy: 0.6106 - val_loss: 0.7577 - val_accuracy: 0.6117 - 24s/epoch - 2ms/step\n",
      "Epoch 18/10000\n",
      "11260/11260 - 23s - loss: 0.7797 - accuracy: 0.6079 - val_loss: 0.7954 - val_accuracy: 0.5298 - 23s/epoch - 2ms/step\n",
      "Epoch 19/10000\n",
      "11260/11260 - 22s - loss: 0.7667 - accuracy: 0.6098 - val_loss: 0.7494 - val_accuracy: 0.6163 - 22s/epoch - 2ms/step\n",
      "Epoch 20/10000\n",
      "11260/11260 - 22s - loss: 0.7710 - accuracy: 0.6096 - val_loss: 0.7590 - val_accuracy: 0.6162 - 22s/epoch - 2ms/step\n",
      "Epoch 21/10000\n",
      "11260/11260 - 23s - loss: 0.7626 - accuracy: 0.6126 - val_loss: 0.7711 - val_accuracy: 0.6152 - 23s/epoch - 2ms/step\n",
      "Epoch 22/10000\n",
      "11260/11260 - 22s - loss: 0.7644 - accuracy: 0.6116 - val_loss: 0.7753 - val_accuracy: 0.5887 - 22s/epoch - 2ms/step\n",
      "Epoch 23/10000\n",
      "11260/11260 - 22s - loss: 0.7609 - accuracy: 0.6104 - val_loss: 0.7363 - val_accuracy: 0.6235 - 22s/epoch - 2ms/step\n",
      "Epoch 24/10000\n",
      "11260/11260 - 23s - loss: 0.7507 - accuracy: 0.6175 - val_loss: 0.7909 - val_accuracy: 0.5470 - 23s/epoch - 2ms/step\n",
      "Epoch 25/10000\n",
      "11260/11260 - 22s - loss: 0.7453 - accuracy: 0.6185 - val_loss: 0.7399 - val_accuracy: 0.6322 - 22s/epoch - 2ms/step\n",
      "Epoch 26/10000\n",
      "11260/11260 - 22s - loss: 0.7422 - accuracy: 0.6176 - val_loss: 0.7483 - val_accuracy: 0.6159 - 22s/epoch - 2ms/step\n",
      "Epoch 27/10000\n",
      "11260/11260 - 23s - loss: 0.7413 - accuracy: 0.6167 - val_loss: 0.7431 - val_accuracy: 0.6195 - 23s/epoch - 2ms/step\n",
      "Epoch 28/10000\n",
      "11260/11260 - 22s - loss: 0.7425 - accuracy: 0.6158 - val_loss: 0.7489 - val_accuracy: 0.6210 - 22s/epoch - 2ms/step\n",
      "Epoch 29/10000\n",
      "11260/11260 - 23s - loss: 0.7442 - accuracy: 0.6212 - val_loss: 0.7802 - val_accuracy: 0.5445 - 23s/epoch - 2ms/step\n",
      "Epoch 30/10000\n",
      "11260/11260 - 22s - loss: 0.7352 - accuracy: 0.6210 - val_loss: 0.7253 - val_accuracy: 0.6270 - 22s/epoch - 2ms/step\n",
      "Epoch 31/10000\n",
      "11260/11260 - 21s - loss: 0.7407 - accuracy: 0.6203 - val_loss: 0.7705 - val_accuracy: 0.5841 - 21s/epoch - 2ms/step\n",
      "Epoch 32/10000\n",
      "11260/11260 - 23s - loss: 0.7336 - accuracy: 0.6192 - val_loss: 0.7210 - val_accuracy: 0.6340 - 23s/epoch - 2ms/step\n",
      "Epoch 33/10000\n",
      "11260/11260 - 24s - loss: 0.7283 - accuracy: 0.6185 - val_loss: 0.7211 - val_accuracy: 0.6219 - 24s/epoch - 2ms/step\n",
      "Epoch 34/10000\n",
      "11260/11260 - 23s - loss: 0.7262 - accuracy: 0.6165 - val_loss: 0.7398 - val_accuracy: 0.6183 - 23s/epoch - 2ms/step\n",
      "Epoch 35/10000\n",
      "11260/11260 - 23s - loss: 0.7298 - accuracy: 0.6134 - val_loss: 0.7387 - val_accuracy: 0.6230 - 23s/epoch - 2ms/step\n",
      "Epoch 36/10000\n",
      "11260/11260 - 22s - loss: 0.7244 - accuracy: 0.6150 - val_loss: 0.7125 - val_accuracy: 0.6194 - 22s/epoch - 2ms/step\n",
      "Epoch 37/10000\n",
      "11260/11260 - 22s - loss: 0.7280 - accuracy: 0.6174 - val_loss: 0.7145 - val_accuracy: 0.6301 - 22s/epoch - 2ms/step\n",
      "Epoch 38/10000\n",
      "11260/11260 - 22s - loss: 0.7292 - accuracy: 0.6153 - val_loss: 0.7268 - val_accuracy: 0.6215 - 22s/epoch - 2ms/step\n",
      "Epoch 39/10000\n",
      "11260/11260 - 20s - loss: 0.7283 - accuracy: 0.6122 - val_loss: 0.7144 - val_accuracy: 0.6231 - 20s/epoch - 2ms/step\n",
      "Epoch 40/10000\n",
      "11260/11260 - 29s - loss: 0.7194 - accuracy: 0.6131 - val_loss: 0.7033 - val_accuracy: 0.6267 - 29s/epoch - 3ms/step\n",
      "Epoch 41/10000\n",
      "11260/11260 - 21s - loss: 0.7108 - accuracy: 0.6159 - val_loss: 0.7157 - val_accuracy: 0.6247 - 21s/epoch - 2ms/step\n",
      "Epoch 42/10000\n",
      "11260/11260 - 20s - loss: 0.7124 - accuracy: 0.6143 - val_loss: 0.7013 - val_accuracy: 0.6167 - 20s/epoch - 2ms/step\n",
      "435/435 [==============================] - 1s 2ms/step - loss: 0.7096 - accuracy: 0.6313\n",
      "Larghezza: 500 \n",
      "Profondità: 1 \n",
      "Dimensione del batch: 10\n",
      "Test loss (binary_crossentropy): 0.7096090316772461\n",
      "Test metrics (accuracy): 0.6313214898109436\n",
      "Epoch 1/10000\n",
      "1126/1126 - 4s - loss: 7.8908 - accuracy: 0.4974 - val_loss: 2.5098 - val_accuracy: 0.5614 - 4s/epoch - 3ms/step\n",
      "Epoch 2/10000\n",
      "1126/1126 - 4s - loss: 2.2422 - accuracy: 0.5616 - val_loss: 2.0585 - val_accuracy: 0.5833 - 4s/epoch - 3ms/step\n",
      "Epoch 3/10000\n",
      "1126/1126 - 3s - loss: 2.0407 - accuracy: 0.5704 - val_loss: 1.9506 - val_accuracy: 0.5597 - 3s/epoch - 3ms/step\n",
      "Epoch 4/10000\n",
      "1126/1126 - 3s - loss: 1.9195 - accuracy: 0.5696 - val_loss: 1.7788 - val_accuracy: 0.5510 - 3s/epoch - 3ms/step\n",
      "Epoch 5/10000\n",
      "1126/1126 - 3s - loss: 1.7768 - accuracy: 0.5660 - val_loss: 1.6699 - val_accuracy: 0.5796 - 3s/epoch - 3ms/step\n",
      "Epoch 6/10000\n",
      "1126/1126 - 3s - loss: 1.6798 - accuracy: 0.5725 - val_loss: 1.6045 - val_accuracy: 0.5905 - 3s/epoch - 3ms/step\n",
      "Epoch 7/10000\n",
      "1126/1126 - 3s - loss: 1.5833 - accuracy: 0.5788 - val_loss: 1.5055 - val_accuracy: 0.5975 - 3s/epoch - 3ms/step\n",
      "Epoch 8/10000\n",
      "1126/1126 - 3s - loss: 1.5006 - accuracy: 0.5906 - val_loss: 1.4751 - val_accuracy: 0.5935 - 3s/epoch - 3ms/step\n",
      "Epoch 9/10000\n",
      "1126/1126 - 4s - loss: 1.4720 - accuracy: 0.5930 - val_loss: 1.4138 - val_accuracy: 0.6039 - 4s/epoch - 3ms/step\n",
      "Epoch 10/10000\n",
      "1126/1126 - 3s - loss: 1.4032 - accuracy: 0.5855 - val_loss: 1.3356 - val_accuracy: 0.5919 - 3s/epoch - 3ms/step\n",
      "Epoch 11/10000\n",
      "1126/1126 - 3s - loss: 1.3301 - accuracy: 0.5888 - val_loss: 1.2757 - val_accuracy: 0.5950 - 3s/epoch - 2ms/step\n",
      "Epoch 12/10000\n",
      "1126/1126 - 3s - loss: 1.3069 - accuracy: 0.5897 - val_loss: 1.2795 - val_accuracy: 0.5894 - 3s/epoch - 2ms/step\n",
      "Epoch 13/10000\n",
      "1126/1126 - 3s - loss: 1.2711 - accuracy: 0.5893 - val_loss: 1.2517 - val_accuracy: 0.5847 - 3s/epoch - 3ms/step\n",
      "Epoch 14/10000\n",
      "1126/1126 - 3s - loss: 1.2439 - accuracy: 0.5846 - val_loss: 1.1932 - val_accuracy: 0.5760 - 3s/epoch - 3ms/step\n",
      "Epoch 15/10000\n",
      "1126/1126 - 3s - loss: 1.2001 - accuracy: 0.5892 - val_loss: 1.1682 - val_accuracy: 0.5896 - 3s/epoch - 3ms/step\n",
      "Epoch 16/10000\n",
      "1126/1126 - 3s - loss: 1.1937 - accuracy: 0.5872 - val_loss: 1.1886 - val_accuracy: 0.5872 - 3s/epoch - 2ms/step\n",
      "Epoch 17/10000\n",
      "1126/1126 - 2s - loss: 1.1675 - accuracy: 0.5818 - val_loss: 1.1434 - val_accuracy: 0.5858 - 2s/epoch - 2ms/step\n",
      "Epoch 18/10000\n",
      "1126/1126 - 2s - loss: 1.1153 - accuracy: 0.5810 - val_loss: 1.1052 - val_accuracy: 0.5980 - 2s/epoch - 2ms/step\n",
      "Epoch 19/10000\n",
      "1126/1126 - 2s - loss: 1.0944 - accuracy: 0.5869 - val_loss: 1.0906 - val_accuracy: 0.5993 - 2s/epoch - 2ms/step\n",
      "435/435 [==============================] - 1s 2ms/step - loss: 1.4278 - accuracy: 0.6026\n",
      "Larghezza: 500 \n",
      "Profondità: 1 \n",
      "Dimensione del batch: 100\n",
      "Test loss (binary_crossentropy): 1.4278043508529663\n",
      "Test metrics (accuracy): 0.6026185154914856\n",
      "Epoch 1/10000\n",
      "113/113 - 2s - loss: 13.6639 - accuracy: 0.6048 - val_loss: 6.9442 - val_accuracy: 0.5869 - 2s/epoch - 16ms/step\n",
      "Epoch 2/10000\n",
      "113/113 - 1s - loss: 4.2082 - accuracy: 0.5321 - val_loss: 3.3511 - val_accuracy: 0.5151 - 655ms/epoch - 6ms/step\n",
      "Epoch 3/10000\n",
      "113/113 - 1s - loss: 3.1373 - accuracy: 0.4983 - val_loss: 2.9760 - val_accuracy: 0.4861 - 675ms/epoch - 6ms/step\n",
      "Epoch 4/10000\n",
      "113/113 - 1s - loss: 2.7971 - accuracy: 0.4951 - val_loss: 2.6433 - val_accuracy: 0.4947 - 716ms/epoch - 6ms/step\n",
      "Epoch 5/10000\n",
      "113/113 - 1s - loss: 2.5277 - accuracy: 0.5062 - val_loss: 2.4121 - val_accuracy: 0.5050 - 657ms/epoch - 6ms/step\n",
      "Epoch 6/10000\n",
      "113/113 - 1s - loss: 2.3436 - accuracy: 0.5145 - val_loss: 2.2634 - val_accuracy: 0.5170 - 661ms/epoch - 6ms/step\n",
      "Epoch 7/10000\n",
      "113/113 - 1s - loss: 2.2198 - accuracy: 0.5213 - val_loss: 2.1375 - val_accuracy: 0.5240 - 670ms/epoch - 6ms/step\n",
      "Epoch 8/10000\n",
      "113/113 - 1s - loss: 2.1297 - accuracy: 0.5276 - val_loss: 2.0506 - val_accuracy: 0.5298 - 654ms/epoch - 6ms/step\n",
      "Epoch 9/10000\n",
      "113/113 - 1s - loss: 2.0671 - accuracy: 0.5317 - val_loss: 1.9928 - val_accuracy: 0.5356 - 665ms/epoch - 6ms/step\n",
      "Epoch 10/10000\n",
      "113/113 - 1s - loss: 2.0150 - accuracy: 0.5369 - val_loss: 1.9343 - val_accuracy: 0.5374 - 662ms/epoch - 6ms/step\n",
      "Epoch 11/10000\n",
      "113/113 - 1s - loss: 1.9471 - accuracy: 0.5451 - val_loss: 1.8782 - val_accuracy: 0.5478 - 666ms/epoch - 6ms/step\n",
      "435/435 [==============================] - 1s 2ms/step - loss: 6.9729 - accuracy: 0.5878\n",
      "Larghezza: 500 \n",
      "Profondità: 1 \n",
      "Dimensione del batch: 1000\n",
      "Test loss (binary_crossentropy): 6.972936630249023\n",
      "Test metrics (accuracy): 0.5877994298934937\n",
      "Epoch 1/10000\n",
      "11260/11260 - 55s - loss: 1455124.6250 - accuracy: 0.5715 - val_loss: 557338.6875 - val_accuracy: 0.6387 - 55s/epoch - 5ms/step\n",
      "Epoch 2/10000\n",
      "11260/11260 - 49s - loss: 1172426.5000 - accuracy: 0.5708 - val_loss: 2604804.0000 - val_accuracy: 0.6135 - 49s/epoch - 4ms/step\n",
      "Epoch 3/10000\n",
      "11260/11260 - 59s - loss: 1168906.3750 - accuracy: 0.5751 - val_loss: 834290.3125 - val_accuracy: 0.6276 - 59s/epoch - 5ms/step\n",
      "Epoch 4/10000\n",
      "11260/11260 - 51s - loss: 1139589.2500 - accuracy: 0.5768 - val_loss: 790014.3750 - val_accuracy: 0.5171 - 51s/epoch - 5ms/step\n",
      "Epoch 5/10000\n",
      "11260/11260 - 46s - loss: 1075886.6250 - accuracy: 0.5784 - val_loss: 425097.3125 - val_accuracy: 0.6448 - 46s/epoch - 4ms/step\n",
      "Epoch 6/10000\n",
      "11260/11260 - 39s - loss: 1190190.2500 - accuracy: 0.5788 - val_loss: 398384.5625 - val_accuracy: 0.6585 - 39s/epoch - 3ms/step\n",
      "Epoch 7/10000\n",
      "11260/11260 - 38s - loss: 1123328.5000 - accuracy: 0.5818 - val_loss: 332463.0000 - val_accuracy: 0.6344 - 38s/epoch - 3ms/step\n",
      "Epoch 8/10000\n",
      "11260/11260 - 38s - loss: 1088842.7500 - accuracy: 0.5820 - val_loss: 1196614.2500 - val_accuracy: 0.6143 - 38s/epoch - 3ms/step\n",
      "Epoch 9/10000\n",
      "11260/11260 - 37s - loss: 1113568.2500 - accuracy: 0.5825 - val_loss: 807227.0625 - val_accuracy: 0.5339 - 37s/epoch - 3ms/step\n",
      "Epoch 10/10000\n",
      "11260/11260 - 45s - loss: 1075458.0000 - accuracy: 0.5834 - val_loss: 3049343.7500 - val_accuracy: 0.6149 - 45s/epoch - 4ms/step\n",
      "Epoch 11/10000\n",
      "11260/11260 - 39s - loss: 1129138.2500 - accuracy: 0.5823 - val_loss: 3666390.7500 - val_accuracy: 0.6145 - 39s/epoch - 3ms/step\n",
      "Epoch 12/10000\n",
      "11260/11260 - 39s - loss: 1014105.9375 - accuracy: 0.5874 - val_loss: 435879.6250 - val_accuracy: 0.6227 - 39s/epoch - 3ms/step\n",
      "Epoch 13/10000\n",
      "11260/11260 - 39s - loss: 1090454.6250 - accuracy: 0.5842 - val_loss: 1303274.1250 - val_accuracy: 0.6200 - 39s/epoch - 3ms/step\n",
      "Epoch 14/10000\n",
      "11260/11260 - 37s - loss: 968750.5000 - accuracy: 0.5887 - val_loss: 344247.7188 - val_accuracy: 0.6145 - 37s/epoch - 3ms/step\n",
      "Epoch 15/10000\n",
      "11260/11260 - 37s - loss: 1024257.6875 - accuracy: 0.5889 - val_loss: 334679.6875 - val_accuracy: 0.5995 - 37s/epoch - 3ms/step\n",
      "Epoch 16/10000\n",
      "11260/11260 - 37s - loss: 1005786.4375 - accuracy: 0.5875 - val_loss: 456172.4375 - val_accuracy: 0.6487 - 37s/epoch - 3ms/step\n",
      "435/435 [==============================] - 1s 3ms/step - loss: 404760.7188 - accuracy: 0.6506\n",
      "Larghezza: 500 \n",
      "Profondità: 2 \n",
      "Dimensione del batch: 10\n",
      "Test loss (binary_crossentropy): 404760.71875\n",
      "Test metrics (accuracy): 0.6506006717681885\n",
      "Epoch 1/10000\n",
      "1126/1126 - 9s - loss: 16157995.0000 - accuracy: 0.5833 - val_loss: 1176965.8750 - val_accuracy: 0.6143 - 9s/epoch - 8ms/step\n",
      "Epoch 2/10000\n",
      "1126/1126 - 6s - loss: 784344.1875 - accuracy: 0.5768 - val_loss: 861625.1875 - val_accuracy: 0.6150 - 6s/epoch - 6ms/step\n",
      "Epoch 3/10000\n",
      "1126/1126 - 6s - loss: 731189.3750 - accuracy: 0.5784 - val_loss: 721520.9375 - val_accuracy: 0.4539 - 6s/epoch - 5ms/step\n",
      "Epoch 4/10000\n",
      "1126/1126 - 6s - loss: 694287.5625 - accuracy: 0.5749 - val_loss: 616700.3125 - val_accuracy: 0.6180 - 6s/epoch - 5ms/step\n",
      "Epoch 5/10000\n",
      "1126/1126 - 6s - loss: 657101.8750 - accuracy: 0.5746 - val_loss: 168826.2031 - val_accuracy: 0.6228 - 6s/epoch - 6ms/step\n",
      "Epoch 6/10000\n",
      "1126/1126 - 7s - loss: 645001.5000 - accuracy: 0.5778 - val_loss: 324424.8750 - val_accuracy: 0.5639 - 7s/epoch - 6ms/step\n",
      "Epoch 7/10000\n",
      "1126/1126 - 7s - loss: 653918.8750 - accuracy: 0.5756 - val_loss: 197091.0000 - val_accuracy: 0.6203 - 7s/epoch - 6ms/step\n",
      "Epoch 8/10000\n",
      "1126/1126 - 6s - loss: 744230.1250 - accuracy: 0.5735 - val_loss: 1420687.1250 - val_accuracy: 0.6135 - 6s/epoch - 6ms/step\n",
      "Epoch 9/10000\n",
      "1126/1126 - 6s - loss: 600501.3125 - accuracy: 0.5790 - val_loss: 287627.3750 - val_accuracy: 0.6139 - 6s/epoch - 6ms/step\n",
      "Epoch 10/10000\n",
      "1126/1126 - 6s - loss: 671579.8750 - accuracy: 0.5770 - val_loss: 155470.0156 - val_accuracy: 0.6312 - 6s/epoch - 6ms/step\n",
      "Epoch 11/10000\n",
      "1126/1126 - 6s - loss: 603955.6250 - accuracy: 0.5797 - val_loss: 332898.3125 - val_accuracy: 0.6254 - 6s/epoch - 6ms/step\n",
      "Epoch 12/10000\n",
      "1126/1126 - 6s - loss: 653586.1250 - accuracy: 0.5773 - val_loss: 376263.5625 - val_accuracy: 0.5980 - 6s/epoch - 6ms/step\n",
      "Epoch 13/10000\n",
      "1126/1126 - 6s - loss: 658642.4375 - accuracy: 0.5790 - val_loss: 482574.1875 - val_accuracy: 0.4949 - 6s/epoch - 6ms/step\n",
      "Epoch 14/10000\n",
      "1126/1126 - 6s - loss: 654725.8750 - accuracy: 0.5798 - val_loss: 196163.8750 - val_accuracy: 0.6481 - 6s/epoch - 5ms/step\n",
      "Epoch 15/10000\n",
      "1126/1126 - 6s - loss: 664250.5000 - accuracy: 0.5810 - val_loss: 1294149.0000 - val_accuracy: 0.6199 - 6s/epoch - 5ms/step\n",
      "Epoch 16/10000\n",
      "1126/1126 - 6s - loss: 629729.8750 - accuracy: 0.5804 - val_loss: 650554.1875 - val_accuracy: 0.5646 - 6s/epoch - 5ms/step\n",
      "Epoch 17/10000\n",
      "1126/1126 - 6s - loss: 622202.6250 - accuracy: 0.5813 - val_loss: 346580.6562 - val_accuracy: 0.6273 - 6s/epoch - 6ms/step\n",
      "Epoch 18/10000\n",
      "1126/1126 - 6s - loss: 715422.4375 - accuracy: 0.5793 - val_loss: 455934.1250 - val_accuracy: 0.6367 - 6s/epoch - 6ms/step\n",
      "Epoch 19/10000\n",
      "1126/1126 - 6s - loss: 708831.1250 - accuracy: 0.5799 - val_loss: 452326.6562 - val_accuracy: 0.6263 - 6s/epoch - 5ms/step\n",
      "Epoch 20/10000\n",
      "1126/1126 - 6s - loss: 648598.1875 - accuracy: 0.5805 - val_loss: 639397.0000 - val_accuracy: 0.5122 - 6s/epoch - 5ms/step\n",
      "Epoch 21/10000\n",
      "1126/1126 - 6s - loss: 681895.5000 - accuracy: 0.5816 - val_loss: 172630.4062 - val_accuracy: 0.6432 - 6s/epoch - 5ms/step\n",
      "Epoch 22/10000\n",
      "1126/1126 - 6s - loss: 549866.8750 - accuracy: 0.5844 - val_loss: 725269.3125 - val_accuracy: 0.6239 - 6s/epoch - 5ms/step\n",
      "Epoch 23/10000\n",
      "1126/1126 - 6s - loss: 615738.5625 - accuracy: 0.5830 - val_loss: 305533.1562 - val_accuracy: 0.6402 - 6s/epoch - 5ms/step\n",
      "Epoch 24/10000\n",
      "1126/1126 - 6s - loss: 638601.3125 - accuracy: 0.5824 - val_loss: 790359.3125 - val_accuracy: 0.4964 - 6s/epoch - 5ms/step\n",
      "435/435 [==============================] - 1s 3ms/step - loss: 193651.0312 - accuracy: 0.6477\n",
      "Larghezza: 500 \n",
      "Profondità: 2 \n",
      "Dimensione del batch: 100\n",
      "Test loss (binary_crossentropy): 193651.03125\n",
      "Test metrics (accuracy): 0.6476512551307678\n",
      "Epoch 1/10000\n",
      "113/113 - 3s - loss: 8193003.0000 - accuracy: 0.5042 - val_loss: 1283751.7500 - val_accuracy: 0.5685 - 3s/epoch - 30ms/step\n",
      "Epoch 2/10000\n",
      "113/113 - 2s - loss: 1317816.1250 - accuracy: 0.5567 - val_loss: 854931.6250 - val_accuracy: 0.5808 - 2s/epoch - 20ms/step\n",
      "Epoch 3/10000\n",
      "113/113 - 2s - loss: 957862.6250 - accuracy: 0.5543 - val_loss: 1445672.6250 - val_accuracy: 0.6080 - 2s/epoch - 19ms/step\n",
      "Epoch 4/10000\n",
      "113/113 - 2s - loss: 542741.6875 - accuracy: 0.5572 - val_loss: 336183.2500 - val_accuracy: 0.5747 - 2s/epoch - 18ms/step\n",
      "Epoch 5/10000\n",
      "113/113 - 2s - loss: 707862.3125 - accuracy: 0.5606 - val_loss: 1030507.6250 - val_accuracy: 0.6162 - 2s/epoch - 18ms/step\n",
      "Epoch 6/10000\n",
      "113/113 - 2s - loss: 766021.0000 - accuracy: 0.5679 - val_loss: 1333885.6250 - val_accuracy: 0.5123 - 2s/epoch - 18ms/step\n",
      "Epoch 7/10000\n",
      "113/113 - 2s - loss: 634047.6875 - accuracy: 0.5721 - val_loss: 1415528.1250 - val_accuracy: 0.6170 - 2s/epoch - 18ms/step\n",
      "Epoch 8/10000\n",
      "113/113 - 2s - loss: 621412.3750 - accuracy: 0.5706 - val_loss: 1735051.1250 - val_accuracy: 0.6141 - 2s/epoch - 18ms/step\n",
      "Epoch 9/10000\n",
      "113/113 - 2s - loss: 662703.1875 - accuracy: 0.5738 - val_loss: 627725.8125 - val_accuracy: 0.4846 - 2s/epoch - 18ms/step\n",
      "Epoch 10/10000\n",
      "113/113 - 2s - loss: 714201.1250 - accuracy: 0.5743 - val_loss: 587130.3750 - val_accuracy: 0.4890 - 2s/epoch - 18ms/step\n",
      "Epoch 11/10000\n",
      "113/113 - 2s - loss: 354887.5312 - accuracy: 0.5764 - val_loss: 233180.8125 - val_accuracy: 0.6162 - 2s/epoch - 17ms/step\n",
      "Epoch 12/10000\n",
      "113/113 - 2s - loss: 583907.8750 - accuracy: 0.5728 - val_loss: 792047.7500 - val_accuracy: 0.6182 - 2s/epoch - 18ms/step\n",
      "Epoch 13/10000\n",
      "113/113 - 2s - loss: 442665.8125 - accuracy: 0.5778 - val_loss: 146164.0781 - val_accuracy: 0.6156 - 2s/epoch - 17ms/step\n",
      "Epoch 14/10000\n",
      "113/113 - 2s - loss: 446481.9375 - accuracy: 0.5746 - val_loss: 124954.1328 - val_accuracy: 0.6068 - 2s/epoch - 18ms/step\n",
      "Epoch 15/10000\n",
      "113/113 - 2s - loss: 630684.2500 - accuracy: 0.5700 - val_loss: 1706378.5000 - val_accuracy: 0.6150 - 2s/epoch - 19ms/step\n",
      "Epoch 16/10000\n",
      "113/113 - 2s - loss: 575742.0625 - accuracy: 0.5760 - val_loss: 369918.0938 - val_accuracy: 0.6217 - 2s/epoch - 18ms/step\n",
      "Epoch 17/10000\n",
      "113/113 - 2s - loss: 468834.1875 - accuracy: 0.5722 - val_loss: 803949.9375 - val_accuracy: 0.6144 - 2s/epoch - 17ms/step\n",
      "Epoch 18/10000\n",
      "113/113 - 2s - loss: 508448.4062 - accuracy: 0.5774 - val_loss: 386852.0312 - val_accuracy: 0.6261 - 2s/epoch - 17ms/step\n",
      "Epoch 19/10000\n",
      "113/113 - 2s - loss: 689830.8125 - accuracy: 0.5705 - val_loss: 329259.9375 - val_accuracy: 0.6397 - 2s/epoch - 18ms/step\n",
      "Epoch 20/10000\n",
      "113/113 - 2s - loss: 390232.5312 - accuracy: 0.5812 - val_loss: 124900.7891 - val_accuracy: 0.6173 - 2s/epoch - 18ms/step\n",
      "Epoch 21/10000\n",
      "113/113 - 2s - loss: 672968.9375 - accuracy: 0.5737 - val_loss: 662064.2500 - val_accuracy: 0.4517 - 2s/epoch - 18ms/step\n",
      "Epoch 22/10000\n",
      "113/113 - 2s - loss: 438870.0312 - accuracy: 0.5781 - val_loss: 203458.7031 - val_accuracy: 0.6223 - 2s/epoch - 17ms/step\n",
      "Epoch 23/10000\n",
      "113/113 - 2s - loss: 545794.3750 - accuracy: 0.5764 - val_loss: 625767.8750 - val_accuracy: 0.6135 - 2s/epoch - 17ms/step\n",
      "Epoch 24/10000\n",
      "113/113 - 2s - loss: 546151.5625 - accuracy: 0.5748 - val_loss: 1139969.0000 - val_accuracy: 0.6134 - 2s/epoch - 18ms/step\n",
      "Epoch 25/10000\n",
      "113/113 - 2s - loss: 510856.4375 - accuracy: 0.5826 - val_loss: 232111.5781 - val_accuracy: 0.6205 - 2s/epoch - 17ms/step\n",
      "Epoch 26/10000\n",
      "113/113 - 2s - loss: 495152.5312 - accuracy: 0.5767 - val_loss: 720220.0000 - val_accuracy: 0.4459 - 2s/epoch - 17ms/step\n",
      "Epoch 27/10000\n",
      "113/113 - 2s - loss: 550658.5625 - accuracy: 0.5762 - val_loss: 292478.5312 - val_accuracy: 0.6273 - 2s/epoch - 17ms/step\n",
      "Epoch 28/10000\n",
      "113/113 - 2s - loss: 473395.9688 - accuracy: 0.5774 - val_loss: 595624.4375 - val_accuracy: 0.6143 - 2s/epoch - 18ms/step\n",
      "Epoch 29/10000\n",
      "113/113 - 2s - loss: 680764.6250 - accuracy: 0.5738 - val_loss: 2161379.0000 - val_accuracy: 0.4200 - 2s/epoch - 17ms/step\n",
      "435/435 [==============================] - 1s 3ms/step - loss: 330891.5938 - accuracy: 0.6371\n",
      "Larghezza: 500 \n",
      "Profondità: 2 \n",
      "Dimensione del batch: 1000\n",
      "Test loss (binary_crossentropy): 330891.59375\n",
      "Test metrics (accuracy): 0.637148380279541\n",
      "Epoch 1/10000\n",
      "11260/11260 - 52s - loss: 2.0061 - accuracy: 0.5659 - val_loss: 1.5928 - val_accuracy: 0.5851 - 52s/epoch - 5ms/step\n",
      "Epoch 2/10000\n",
      "11260/11260 - 47s - loss: 1.5303 - accuracy: 0.5697 - val_loss: 1.5125 - val_accuracy: 0.5740 - 47s/epoch - 4ms/step\n",
      "Epoch 3/10000\n",
      "11260/11260 - 48s - loss: 1.4395 - accuracy: 0.5716 - val_loss: 1.3121 - val_accuracy: 0.5959 - 48s/epoch - 4ms/step\n",
      "Epoch 4/10000\n",
      "11260/11260 - 47s - loss: 1.3201 - accuracy: 0.5744 - val_loss: 1.3866 - val_accuracy: 0.5612 - 47s/epoch - 4ms/step\n",
      "Epoch 5/10000\n",
      "11260/11260 - 48s - loss: 1.2385 - accuracy: 0.5733 - val_loss: 1.1855 - val_accuracy: 0.5569 - 48s/epoch - 4ms/step\n",
      "Epoch 6/10000\n",
      "11260/11260 - 49s - loss: 1.1277 - accuracy: 0.5734 - val_loss: 1.0776 - val_accuracy: 0.5850 - 49s/epoch - 4ms/step\n",
      "Epoch 7/10000\n",
      "11260/11260 - 48s - loss: 1.0851 - accuracy: 0.5708 - val_loss: 1.0522 - val_accuracy: 0.5688 - 48s/epoch - 4ms/step\n",
      "Epoch 8/10000\n",
      "11260/11260 - 47s - loss: 1.0869 - accuracy: 0.5753 - val_loss: 1.0463 - val_accuracy: 0.5787 - 47s/epoch - 4ms/step\n",
      "Epoch 9/10000\n",
      "11260/11260 - 46s - loss: 1.0614 - accuracy: 0.5778 - val_loss: 1.0121 - val_accuracy: 0.5789 - 46s/epoch - 4ms/step\n",
      "Epoch 10/10000\n",
      "11260/11260 - 47s - loss: 0.9796 - accuracy: 0.5813 - val_loss: 0.9316 - val_accuracy: 0.5895 - 47s/epoch - 4ms/step\n",
      "Epoch 11/10000\n",
      "11260/11260 - 48s - loss: 0.9597 - accuracy: 0.5850 - val_loss: 0.8996 - val_accuracy: 0.5916 - 48s/epoch - 4ms/step\n",
      "Epoch 12/10000\n",
      "11260/11260 - 48s - loss: 0.9123 - accuracy: 0.5850 - val_loss: 0.9589 - val_accuracy: 0.5826 - 48s/epoch - 4ms/step\n",
      "Epoch 13/10000\n",
      "11260/11260 - 47s - loss: 0.9008 - accuracy: 0.5851 - val_loss: 0.8541 - val_accuracy: 0.5936 - 47s/epoch - 4ms/step\n",
      "435/435 [==============================] - 2s 3ms/step - loss: 1.3301 - accuracy: 0.5940\n",
      "Larghezza: 500 \n",
      "Profondità: 2 \n",
      "Dimensione del batch: 10\n",
      "Test loss (binary_crossentropy): 1.3301223516464233\n",
      "Test metrics (accuracy): 0.5939860343933105\n",
      "Epoch 1/10000\n",
      "1126/1126 - 8s - loss: 2.1604 - accuracy: 0.5663 - val_loss: 1.5466 - val_accuracy: 0.5953 - 8s/epoch - 8ms/step\n",
      "Epoch 2/10000\n",
      "1126/1126 - 7s - loss: 1.5181 - accuracy: 0.5783 - val_loss: 1.4209 - val_accuracy: 0.6138 - 7s/epoch - 6ms/step\n",
      "Epoch 3/10000\n",
      "1126/1126 - 7s - loss: 1.4039 - accuracy: 0.5828 - val_loss: 1.3522 - val_accuracy: 0.5671 - 7s/epoch - 6ms/step\n",
      "Epoch 4/10000\n",
      "1126/1126 - 7s - loss: 1.3293 - accuracy: 0.5839 - val_loss: 1.3193 - val_accuracy: 0.5928 - 7s/epoch - 6ms/step\n",
      "Epoch 5/10000\n",
      "1126/1126 - 7s - loss: 1.2739 - accuracy: 0.5856 - val_loss: 1.2526 - val_accuracy: 0.5825 - 7s/epoch - 6ms/step\n",
      "Epoch 6/10000\n",
      "1126/1126 - 7s - loss: 1.2496 - accuracy: 0.5849 - val_loss: 1.2619 - val_accuracy: 0.5972 - 7s/epoch - 6ms/step\n",
      "Epoch 7/10000\n",
      "1126/1126 - 7s - loss: 1.2067 - accuracy: 0.5869 - val_loss: 1.2193 - val_accuracy: 0.5450 - 7s/epoch - 6ms/step\n",
      "Epoch 8/10000\n",
      "1126/1126 - 7s - loss: 1.1635 - accuracy: 0.5876 - val_loss: 1.1812 - val_accuracy: 0.5987 - 7s/epoch - 6ms/step\n",
      "Epoch 9/10000\n",
      "1126/1126 - 7s - loss: 1.1196 - accuracy: 0.5928 - val_loss: 1.1211 - val_accuracy: 0.6221 - 7s/epoch - 6ms/step\n",
      "Epoch 10/10000\n",
      "1126/1126 - 7s - loss: 1.1139 - accuracy: 0.5910 - val_loss: 1.0860 - val_accuracy: 0.5956 - 7s/epoch - 6ms/step\n",
      "Epoch 11/10000\n",
      "1126/1126 - 7s - loss: 1.0849 - accuracy: 0.5934 - val_loss: 1.1213 - val_accuracy: 0.6149 - 7s/epoch - 6ms/step\n",
      "Epoch 12/10000\n",
      "1126/1126 - 7s - loss: 1.0826 - accuracy: 0.5952 - val_loss: 1.1353 - val_accuracy: 0.5854 - 7s/epoch - 6ms/step\n",
      "Epoch 13/10000\n",
      "1126/1126 - 7s - loss: 1.0649 - accuracy: 0.5967 - val_loss: 1.0781 - val_accuracy: 0.5930 - 7s/epoch - 6ms/step\n",
      "Epoch 14/10000\n",
      "1126/1126 - 7s - loss: 1.0603 - accuracy: 0.5948 - val_loss: 1.1223 - val_accuracy: 0.6082 - 7s/epoch - 6ms/step\n",
      "Epoch 15/10000\n",
      "1126/1126 - 7s - loss: 1.0374 - accuracy: 0.5971 - val_loss: 1.0804 - val_accuracy: 0.5824 - 7s/epoch - 6ms/step\n",
      "Epoch 16/10000\n",
      "1126/1126 - 7s - loss: 1.0369 - accuracy: 0.5959 - val_loss: 1.0531 - val_accuracy: 0.5880 - 7s/epoch - 6ms/step\n",
      "Epoch 17/10000\n",
      "1126/1126 - 7s - loss: 1.0390 - accuracy: 0.5958 - val_loss: 1.0727 - val_accuracy: 0.6000 - 7s/epoch - 6ms/step\n",
      "Epoch 18/10000\n",
      "1126/1126 - 7s - loss: 1.0153 - accuracy: 0.5991 - val_loss: 1.0083 - val_accuracy: 0.6072 - 7s/epoch - 6ms/step\n",
      "Epoch 19/10000\n",
      "1126/1126 - 7s - loss: 1.0060 - accuracy: 0.5961 - val_loss: 1.0338 - val_accuracy: 0.5973 - 7s/epoch - 6ms/step\n",
      "435/435 [==============================] - 1s 3ms/step - loss: 1.0766 - accuracy: 0.6123\n",
      "Larghezza: 500 \n",
      "Profondità: 2 \n",
      "Dimensione del batch: 100\n",
      "Test loss (binary_crossentropy): 1.0766382217407227\n",
      "Test metrics (accuracy): 0.6122581362724304\n",
      "Epoch 1/10000\n",
      "113/113 - 3s - loss: 5.6279 - accuracy: 0.5442 - val_loss: 2.6040 - val_accuracy: 0.5905 - 3s/epoch - 30ms/step\n",
      "Epoch 2/10000\n",
      "113/113 - 2s - loss: 2.3357 - accuracy: 0.5757 - val_loss: 2.0404 - val_accuracy: 0.5837 - 2s/epoch - 20ms/step\n",
      "Epoch 3/10000\n",
      "113/113 - 2s - loss: 1.9521 - accuracy: 0.5799 - val_loss: 1.7895 - val_accuracy: 0.5625 - 2s/epoch - 20ms/step\n",
      "Epoch 4/10000\n",
      "113/113 - 2s - loss: 1.7685 - accuracy: 0.5849 - val_loss: 1.6631 - val_accuracy: 0.5883 - 2s/epoch - 19ms/step\n",
      "Epoch 5/10000\n",
      "113/113 - 2s - loss: 1.6644 - accuracy: 0.5863 - val_loss: 1.5899 - val_accuracy: 0.5935 - 2s/epoch - 19ms/step\n",
      "Epoch 6/10000\n",
      "113/113 - 2s - loss: 1.5711 - accuracy: 0.5910 - val_loss: 1.5419 - val_accuracy: 0.5889 - 2s/epoch - 19ms/step\n",
      "Epoch 7/10000\n",
      "113/113 - 2s - loss: 1.5130 - accuracy: 0.5939 - val_loss: 1.4718 - val_accuracy: 0.5962 - 2s/epoch - 19ms/step\n",
      "Epoch 8/10000\n",
      "113/113 - 2s - loss: 1.4734 - accuracy: 0.5916 - val_loss: 1.4184 - val_accuracy: 0.5797 - 2s/epoch - 19ms/step\n",
      "Epoch 9/10000\n",
      "113/113 - 2s - loss: 1.4257 - accuracy: 0.5922 - val_loss: 1.3773 - val_accuracy: 0.6061 - 2s/epoch - 19ms/step\n",
      "Epoch 10/10000\n",
      "113/113 - 2s - loss: 1.3694 - accuracy: 0.5968 - val_loss: 1.3535 - val_accuracy: 0.5916 - 2s/epoch - 19ms/step\n",
      "Epoch 11/10000\n",
      "113/113 - 2s - loss: 1.3477 - accuracy: 0.5962 - val_loss: 1.3267 - val_accuracy: 0.6009 - 2s/epoch - 19ms/step\n",
      "Epoch 12/10000\n",
      "113/113 - 2s - loss: 1.3125 - accuracy: 0.5973 - val_loss: 1.3007 - val_accuracy: 0.5979 - 2s/epoch - 19ms/step\n",
      "Epoch 13/10000\n",
      "113/113 - 2s - loss: 1.2900 - accuracy: 0.6007 - val_loss: 1.3248 - val_accuracy: 0.5854 - 2s/epoch - 19ms/step\n",
      "Epoch 14/10000\n",
      "113/113 - 2s - loss: 1.2901 - accuracy: 0.5959 - val_loss: 1.2828 - val_accuracy: 0.6022 - 2s/epoch - 19ms/step\n",
      "Epoch 15/10000\n",
      "113/113 - 2s - loss: 1.2736 - accuracy: 0.5972 - val_loss: 1.2635 - val_accuracy: 0.5955 - 2s/epoch - 21ms/step\n",
      "Epoch 16/10000\n",
      "113/113 - 2s - loss: 1.2537 - accuracy: 0.6001 - val_loss: 1.2664 - val_accuracy: 0.5893 - 2s/epoch - 20ms/step\n",
      "Epoch 17/10000\n",
      "113/113 - 2s - loss: 1.2548 - accuracy: 0.5969 - val_loss: 1.2301 - val_accuracy: 0.5826 - 2s/epoch - 20ms/step\n",
      "Epoch 18/10000\n",
      "113/113 - 2s - loss: 1.2131 - accuracy: 0.5997 - val_loss: 1.1894 - val_accuracy: 0.5898 - 2s/epoch - 19ms/step\n",
      "Epoch 19/10000\n",
      "113/113 - 2s - loss: 1.1792 - accuracy: 0.5990 - val_loss: 1.1944 - val_accuracy: 0.6036 - 2s/epoch - 19ms/step\n",
      "435/435 [==============================] - 1s 3ms/step - loss: 1.3937 - accuracy: 0.6121\n",
      "Larghezza: 500 \n",
      "Profondità: 2 \n",
      "Dimensione del batch: 1000\n",
      "Test loss (binary_crossentropy): 1.3936911821365356\n",
      "Test metrics (accuracy): 0.6121142506599426\n",
      "Epoch 1/10000\n",
      "11260/11260 - 77s - loss: 280171200.0000 - accuracy: 0.5718 - val_loss: 75515200.0000 - val_accuracy: 0.6322 - 77s/epoch - 7ms/step\n",
      "Epoch 2/10000\n",
      "11260/11260 - 75s - loss: 199444768.0000 - accuracy: 0.5706 - val_loss: 506592288.0000 - val_accuracy: 0.6139 - 75s/epoch - 7ms/step\n",
      "Epoch 3/10000\n",
      "11260/11260 - 74s - loss: 184805584.0000 - accuracy: 0.5740 - val_loss: 153841648.0000 - val_accuracy: 0.6203 - 74s/epoch - 7ms/step\n",
      "Epoch 4/10000\n",
      "11260/11260 - 73s - loss: 170569520.0000 - accuracy: 0.5716 - val_loss: 119473232.0000 - val_accuracy: 0.6229 - 73s/epoch - 6ms/step\n",
      "Epoch 5/10000\n",
      "11260/11260 - 74s - loss: 150619136.0000 - accuracy: 0.5765 - val_loss: 62167728.0000 - val_accuracy: 0.5960 - 74s/epoch - 7ms/step\n",
      "Epoch 6/10000\n",
      "11260/11260 - 75s - loss: 145600672.0000 - accuracy: 0.5785 - val_loss: 74873256.0000 - val_accuracy: 0.6333 - 75s/epoch - 7ms/step\n",
      "Epoch 7/10000\n",
      "11260/11260 - 81s - loss: 138320560.0000 - accuracy: 0.5788 - val_loss: 46367488.0000 - val_accuracy: 0.5503 - 81s/epoch - 7ms/step\n",
      "Epoch 8/10000\n",
      "11260/11260 - 81s - loss: 137649552.0000 - accuracy: 0.5779 - val_loss: 107304984.0000 - val_accuracy: 0.6230 - 81s/epoch - 7ms/step\n",
      "Epoch 9/10000\n",
      "11260/11260 - 82s - loss: 119888056.0000 - accuracy: 0.5778 - val_loss: 194460304.0000 - val_accuracy: 0.4283 - 82s/epoch - 7ms/step\n",
      "Epoch 10/10000\n",
      "11260/11260 - 80s - loss: 115157032.0000 - accuracy: 0.5801 - val_loss: 160017072.0000 - val_accuracy: 0.6183 - 80s/epoch - 7ms/step\n",
      "Epoch 11/10000\n",
      "11260/11260 - 78s - loss: 110983512.0000 - accuracy: 0.5811 - val_loss: 104133496.0000 - val_accuracy: 0.5301 - 78s/epoch - 7ms/step\n",
      "Epoch 12/10000\n",
      "11260/11260 - 78s - loss: 109661488.0000 - accuracy: 0.5811 - val_loss: 34259560.0000 - val_accuracy: 0.5526 - 78s/epoch - 7ms/step\n",
      "Epoch 13/10000\n",
      "11260/11260 - 74s - loss: 101874448.0000 - accuracy: 0.5806 - val_loss: 113297512.0000 - val_accuracy: 0.6207 - 74s/epoch - 7ms/step\n",
      "Epoch 14/10000\n",
      "11260/11260 - 72s - loss: 91248800.0000 - accuracy: 0.5836 - val_loss: 28953688.0000 - val_accuracy: 0.6301 - 72s/epoch - 6ms/step\n",
      "Epoch 15/10000\n",
      "11260/11260 - 75s - loss: 91376480.0000 - accuracy: 0.5853 - val_loss: 25736648.0000 - val_accuracy: 0.6201 - 75s/epoch - 7ms/step\n",
      "Epoch 16/10000\n",
      "11260/11260 - 73s - loss: 87559488.0000 - accuracy: 0.5853 - val_loss: 22192928.0000 - val_accuracy: 0.6632 - 73s/epoch - 6ms/step\n",
      "Epoch 17/10000\n",
      "11260/11260 - 74s - loss: 81239176.0000 - accuracy: 0.5839 - val_loss: 222282528.0000 - val_accuracy: 0.6167 - 74s/epoch - 7ms/step\n",
      "Epoch 18/10000\n",
      "11260/11260 - 76s - loss: 80674520.0000 - accuracy: 0.5830 - val_loss: 78416416.0000 - val_accuracy: 0.4687 - 76s/epoch - 7ms/step\n",
      "Epoch 19/10000\n",
      "11260/11260 - 78s - loss: 73596512.0000 - accuracy: 0.5841 - val_loss: 41161844.0000 - val_accuracy: 0.6004 - 78s/epoch - 7ms/step\n",
      "Epoch 20/10000\n",
      "11260/11260 - 78s - loss: 69859560.0000 - accuracy: 0.5858 - val_loss: 51619168.0000 - val_accuracy: 0.6285 - 78s/epoch - 7ms/step\n",
      "Epoch 21/10000\n",
      "11260/11260 - 92s - loss: 69292784.0000 - accuracy: 0.5864 - val_loss: 26717848.0000 - val_accuracy: 0.5816 - 92s/epoch - 8ms/step\n",
      "Epoch 22/10000\n",
      "11260/11260 - 95s - loss: 64384388.0000 - accuracy: 0.5867 - val_loss: 52885184.0000 - val_accuracy: 0.6238 - 95s/epoch - 8ms/step\n",
      "Epoch 23/10000\n",
      "11260/11260 - 90s - loss: 63766760.0000 - accuracy: 0.5868 - val_loss: 16336615.0000 - val_accuracy: 0.6557 - 90s/epoch - 8ms/step\n",
      "Epoch 24/10000\n",
      "11260/11260 - 89s - loss: 58425564.0000 - accuracy: 0.5875 - val_loss: 50042432.0000 - val_accuracy: 0.5079 - 89s/epoch - 8ms/step\n",
      "Epoch 25/10000\n",
      "11260/11260 - 85s - loss: 55489668.0000 - accuracy: 0.5847 - val_loss: 51442856.0000 - val_accuracy: 0.5718 - 85s/epoch - 8ms/step\n",
      "Epoch 26/10000\n",
      "11260/11260 - 88s - loss: 53532264.0000 - accuracy: 0.5874 - val_loss: 56370752.0000 - val_accuracy: 0.5345 - 88s/epoch - 8ms/step\n",
      "435/435 [==============================] - 2s 5ms/step - loss: 22109192.0000 - accuracy: 0.6563\n",
      "Larghezza: 500 \n",
      "Profondità: 3 \n",
      "Dimensione del batch: 10\n",
      "Test loss (binary_crossentropy): 22109192.0\n",
      "Test metrics (accuracy): 0.6562837362289429\n",
      "Epoch 1/10000\n",
      "1126/1126 - 16s - loss: 572655936.0000 - accuracy: 0.5478 - val_loss: 129598264.0000 - val_accuracy: 0.4486 - 16s/epoch - 14ms/step\n",
      "Epoch 2/10000\n",
      "1126/1126 - 13s - loss: 105362416.0000 - accuracy: 0.5673 - val_loss: 65599500.0000 - val_accuracy: 0.5155 - 13s/epoch - 11ms/step\n",
      "Epoch 3/10000\n",
      "1126/1126 - 12s - loss: 103220304.0000 - accuracy: 0.5697 - val_loss: 26707546.0000 - val_accuracy: 0.6374 - 12s/epoch - 11ms/step\n",
      "Epoch 4/10000\n",
      "1126/1126 - 12s - loss: 106924096.0000 - accuracy: 0.5699 - val_loss: 309385280.0000 - val_accuracy: 0.4964 - 12s/epoch - 11ms/step\n",
      "Epoch 5/10000\n",
      "1126/1126 - 12s - loss: 104152000.0000 - accuracy: 0.5733 - val_loss: 76340120.0000 - val_accuracy: 0.5526 - 12s/epoch - 11ms/step\n",
      "Epoch 6/10000\n",
      "1126/1126 - 12s - loss: 104627512.0000 - accuracy: 0.5713 - val_loss: 45711012.0000 - val_accuracy: 0.5991 - 12s/epoch - 10ms/step\n",
      "Epoch 7/10000\n",
      "1126/1126 - 12s - loss: 105562928.0000 - accuracy: 0.5737 - val_loss: 46060460.0000 - val_accuracy: 0.6326 - 12s/epoch - 10ms/step\n",
      "Epoch 8/10000\n",
      "1126/1126 - 12s - loss: 100722256.0000 - accuracy: 0.5742 - val_loss: 325475328.0000 - val_accuracy: 0.6131 - 12s/epoch - 10ms/step\n",
      "Epoch 9/10000\n",
      "1126/1126 - 12s - loss: 108659824.0000 - accuracy: 0.5732 - val_loss: 169717104.0000 - val_accuracy: 0.6143 - 12s/epoch - 10ms/step\n",
      "Epoch 10/10000\n",
      "1126/1126 - 11s - loss: 95691024.0000 - accuracy: 0.5752 - val_loss: 75031352.0000 - val_accuracy: 0.5579 - 11s/epoch - 10ms/step\n",
      "Epoch 11/10000\n",
      "1126/1126 - 12s - loss: 91596304.0000 - accuracy: 0.5772 - val_loss: 174439600.0000 - val_accuracy: 0.5065 - 12s/epoch - 11ms/step\n",
      "Epoch 12/10000\n",
      "1126/1126 - 12s - loss: 112941232.0000 - accuracy: 0.5724 - val_loss: 145984960.0000 - val_accuracy: 0.5017 - 12s/epoch - 11ms/step\n",
      "Epoch 13/10000\n",
      "1126/1126 - 11s - loss: 106111264.0000 - accuracy: 0.5760 - val_loss: 62298692.0000 - val_accuracy: 0.5749 - 11s/epoch - 10ms/step\n",
      "435/435 [==============================] - 2s 5ms/step - loss: 26630504.0000 - accuracy: 0.6379\n",
      "Larghezza: 500 \n",
      "Profondità: 3 \n",
      "Dimensione del batch: 100\n",
      "Test loss (binary_crossentropy): 26630504.0\n",
      "Test metrics (accuracy): 0.63786780834198\n",
      "Epoch 1/10000\n",
      "113/113 - 6s - loss: 5349569024.0000 - accuracy: 0.5455 - val_loss: 118175776.0000 - val_accuracy: 0.5264 - 6s/epoch - 50ms/step\n",
      "Epoch 2/10000\n",
      "113/113 - 4s - loss: 111172792.0000 - accuracy: 0.5652 - val_loss: 114710040.0000 - val_accuracy: 0.6087 - 4s/epoch - 36ms/step\n",
      "Epoch 3/10000\n",
      "113/113 - 4s - loss: 70978832.0000 - accuracy: 0.5758 - val_loss: 37597236.0000 - val_accuracy: 0.5677 - 4s/epoch - 34ms/step\n",
      "Epoch 4/10000\n",
      "113/113 - 4s - loss: 82197656.0000 - accuracy: 0.5693 - val_loss: 66242816.0000 - val_accuracy: 0.5350 - 4s/epoch - 32ms/step\n",
      "Epoch 5/10000\n",
      "113/113 - 4s - loss: 77444976.0000 - accuracy: 0.5723 - val_loss: 82282648.0000 - val_accuracy: 0.4436 - 4s/epoch - 32ms/step\n",
      "Epoch 6/10000\n",
      "113/113 - 4s - loss: 64857776.0000 - accuracy: 0.5732 - val_loss: 35382024.0000 - val_accuracy: 0.6247 - 4s/epoch - 32ms/step\n",
      "Epoch 7/10000\n",
      "113/113 - 4s - loss: 78939728.0000 - accuracy: 0.5714 - val_loss: 115477960.0000 - val_accuracy: 0.6135 - 4s/epoch - 32ms/step\n",
      "Epoch 8/10000\n",
      "113/113 - 4s - loss: 61462548.0000 - accuracy: 0.5775 - val_loss: 40924776.0000 - val_accuracy: 0.6256 - 4s/epoch - 32ms/step\n",
      "Epoch 9/10000\n",
      "113/113 - 4s - loss: 41508848.0000 - accuracy: 0.5761 - val_loss: 93138032.0000 - val_accuracy: 0.6139 - 4s/epoch - 32ms/step\n",
      "Epoch 10/10000\n",
      "113/113 - 4s - loss: 83378296.0000 - accuracy: 0.5745 - val_loss: 91995960.0000 - val_accuracy: 0.4396 - 4s/epoch - 32ms/step\n",
      "Epoch 11/10000\n",
      "113/113 - 4s - loss: 101776184.0000 - accuracy: 0.5695 - val_loss: 104943656.0000 - val_accuracy: 0.5605 - 4s/epoch - 32ms/step\n",
      "Epoch 12/10000\n",
      "113/113 - 4s - loss: 88148168.0000 - accuracy: 0.5742 - val_loss: 32689924.0000 - val_accuracy: 0.6073 - 4s/epoch - 32ms/step\n",
      "Epoch 13/10000\n",
      "113/113 - 4s - loss: 86285688.0000 - accuracy: 0.5741 - val_loss: 70568312.0000 - val_accuracy: 0.5057 - 4s/epoch - 32ms/step\n",
      "Epoch 14/10000\n",
      "113/113 - 4s - loss: 80875056.0000 - accuracy: 0.5704 - val_loss: 180405504.0000 - val_accuracy: 0.6148 - 4s/epoch - 32ms/step\n",
      "Epoch 15/10000\n",
      "113/113 - 4s - loss: 62223628.0000 - accuracy: 0.5793 - val_loss: 91860224.0000 - val_accuracy: 0.5257 - 4s/epoch - 32ms/step\n",
      "Epoch 16/10000\n",
      "113/113 - 4s - loss: 61051660.0000 - accuracy: 0.5826 - val_loss: 27931594.0000 - val_accuracy: 0.5410 - 4s/epoch - 32ms/step\n",
      "Epoch 17/10000\n",
      "113/113 - 4s - loss: 72611944.0000 - accuracy: 0.5725 - val_loss: 148848400.0000 - val_accuracy: 0.4307 - 4s/epoch - 34ms/step\n",
      "Epoch 18/10000\n",
      "113/113 - 4s - loss: 85753856.0000 - accuracy: 0.5745 - val_loss: 250378048.0000 - val_accuracy: 0.4606 - 4s/epoch - 33ms/step\n",
      "435/435 [==============================] - 2s 4ms/step - loss: 39233980.0000 - accuracy: 0.6255\n",
      "Larghezza: 500 \n",
      "Profondità: 3 \n",
      "Dimensione del batch: 1000\n",
      "Test loss (binary_crossentropy): 39233980.0\n",
      "Test metrics (accuracy): 0.6254945397377014\n",
      "Epoch 1/10000\n",
      "11260/11260 - 70s - loss: 3.2354 - accuracy: 0.5582 - val_loss: 2.0253 - val_accuracy: 0.5898 - 70s/epoch - 6ms/step\n",
      "Epoch 2/10000\n",
      "11260/11260 - 70s - loss: 1.7129 - accuracy: 0.5658 - val_loss: 1.4516 - val_accuracy: 0.5938 - 70s/epoch - 6ms/step\n",
      "Epoch 3/10000\n",
      "11260/11260 - 75s - loss: 1.2219 - accuracy: 0.5720 - val_loss: 0.9961 - val_accuracy: 0.5907 - 75s/epoch - 7ms/step\n",
      "Epoch 4/10000\n",
      "11260/11260 - 74s - loss: 0.9493 - accuracy: 0.5814 - val_loss: 0.8938 - val_accuracy: 0.5753 - 74s/epoch - 7ms/step\n",
      "Epoch 5/10000\n",
      "11260/11260 - 75s - loss: 0.8339 - accuracy: 0.5939 - val_loss: 0.8029 - val_accuracy: 0.5641 - 75s/epoch - 7ms/step\n",
      "Epoch 6/10000\n",
      "11260/11260 - 75s - loss: 0.7934 - accuracy: 0.5984 - val_loss: 0.7419 - val_accuracy: 0.6193 - 75s/epoch - 7ms/step\n",
      "Epoch 7/10000\n",
      "11260/11260 - 75s - loss: 0.7496 - accuracy: 0.5999 - val_loss: 0.7119 - val_accuracy: 0.6120 - 75s/epoch - 7ms/step\n",
      "Epoch 8/10000\n",
      "11260/11260 - 74s - loss: 0.7351 - accuracy: 0.5985 - val_loss: 0.7066 - val_accuracy: 0.6099 - 74s/epoch - 7ms/step\n",
      "Epoch 9/10000\n",
      "11260/11260 - 68s - loss: 0.7189 - accuracy: 0.6010 - val_loss: 0.7095 - val_accuracy: 0.6135 - 68s/epoch - 6ms/step\n",
      "Epoch 10/10000\n",
      "11260/11260 - 68s - loss: 0.7058 - accuracy: 0.6058 - val_loss: 0.6892 - val_accuracy: 0.6138 - 68s/epoch - 6ms/step\n",
      "Epoch 11/10000\n",
      "11260/11260 - 68s - loss: 0.6963 - accuracy: 0.6038 - val_loss: 0.6736 - val_accuracy: 0.6147 - 68s/epoch - 6ms/step\n",
      "Epoch 12/10000\n",
      "11260/11260 - 78s - loss: 0.6849 - accuracy: 0.6118 - val_loss: 0.6823 - val_accuracy: 0.6125 - 78s/epoch - 7ms/step\n",
      "Epoch 13/10000\n",
      "11260/11260 - 73s - loss: 0.6835 - accuracy: 0.6119 - val_loss: 0.6771 - val_accuracy: 0.6262 - 73s/epoch - 7ms/step\n",
      "Epoch 14/10000\n",
      "11260/11260 - 71s - loss: 0.6844 - accuracy: 0.6150 - val_loss: 0.6715 - val_accuracy: 0.6284 - 71s/epoch - 6ms/step\n",
      "Epoch 15/10000\n",
      "11260/11260 - 72s - loss: 0.6724 - accuracy: 0.6154 - val_loss: 0.6529 - val_accuracy: 0.6373 - 72s/epoch - 6ms/step\n",
      "Epoch 16/10000\n",
      "11260/11260 - 82s - loss: 0.6720 - accuracy: 0.6166 - val_loss: 0.6563 - val_accuracy: 0.6212 - 82s/epoch - 7ms/step\n",
      "Epoch 17/10000\n",
      "11260/11260 - 81s - loss: 0.6751 - accuracy: 0.6198 - val_loss: 0.6524 - val_accuracy: 0.6338 - 81s/epoch - 7ms/step\n",
      "Epoch 18/10000\n",
      "11260/11260 - 89s - loss: 0.6730 - accuracy: 0.6172 - val_loss: 0.6882 - val_accuracy: 0.6155 - 89s/epoch - 8ms/step\n",
      "Epoch 19/10000\n",
      "11260/11260 - 73s - loss: 0.6697 - accuracy: 0.6215 - val_loss: 0.6663 - val_accuracy: 0.6259 - 73s/epoch - 6ms/step\n",
      "Epoch 20/10000\n",
      "11260/11260 - 73s - loss: 0.6759 - accuracy: 0.6208 - val_loss: 0.6811 - val_accuracy: 0.6124 - 73s/epoch - 6ms/step\n",
      "Epoch 21/10000\n",
      "11260/11260 - 76s - loss: 0.6722 - accuracy: 0.6204 - val_loss: 0.6637 - val_accuracy: 0.6288 - 76s/epoch - 7ms/step\n",
      "Epoch 22/10000\n",
      "11260/11260 - 76s - loss: 0.6702 - accuracy: 0.6213 - val_loss: 0.6811 - val_accuracy: 0.6095 - 76s/epoch - 7ms/step\n",
      "Epoch 23/10000\n",
      "11260/11260 - 76s - loss: 0.6691 - accuracy: 0.6182 - val_loss: 0.6594 - val_accuracy: 0.6169 - 76s/epoch - 7ms/step\n",
      "Epoch 24/10000\n",
      "11260/11260 - 95s - loss: 0.6706 - accuracy: 0.6153 - val_loss: 0.7024 - val_accuracy: 0.5677 - 95s/epoch - 8ms/step\n",
      "Epoch 25/10000\n",
      "11260/11260 - 88s - loss: 0.6705 - accuracy: 0.6135 - val_loss: 0.6688 - val_accuracy: 0.6126 - 88s/epoch - 8ms/step\n",
      "435/435 [==============================] - 3s 6ms/step - loss: 0.6544 - accuracy: 0.6365\n",
      "Larghezza: 500 \n",
      "Profondità: 3 \n",
      "Dimensione del batch: 10\n",
      "Test loss (binary_crossentropy): 0.6544204354286194\n",
      "Test metrics (accuracy): 0.6365009546279907\n",
      "Epoch 1/10000\n",
      "1126/1126 - 16s - loss: 3.6010 - accuracy: 0.5643 - val_loss: 2.3866 - val_accuracy: 0.5796 - 16s/epoch - 14ms/step\n",
      "Epoch 2/10000\n",
      "1126/1126 - 13s - loss: 2.0966 - accuracy: 0.5723 - val_loss: 2.1110 - val_accuracy: 0.5624 - 13s/epoch - 11ms/step\n",
      "Epoch 3/10000\n",
      "1126/1126 - 13s - loss: 1.9367 - accuracy: 0.5725 - val_loss: 1.9319 - val_accuracy: 0.5619 - 13s/epoch - 11ms/step\n",
      "Epoch 4/10000\n",
      "1126/1126 - 17s - loss: 1.8143 - accuracy: 0.5752 - val_loss: 1.7552 - val_accuracy: 0.5754 - 17s/epoch - 15ms/step\n",
      "Epoch 5/10000\n",
      "1126/1126 - 13s - loss: 1.7482 - accuracy: 0.5753 - val_loss: 1.6918 - val_accuracy: 0.6010 - 13s/epoch - 11ms/step\n",
      "Epoch 6/10000\n",
      "1126/1126 - 18s - loss: 1.6901 - accuracy: 0.5749 - val_loss: 1.6309 - val_accuracy: 0.5621 - 18s/epoch - 16ms/step\n",
      "Epoch 7/10000\n",
      "1126/1126 - 15s - loss: 1.6555 - accuracy: 0.5753 - val_loss: 1.6737 - val_accuracy: 0.5552 - 15s/epoch - 13ms/step\n",
      "Epoch 8/10000\n",
      "1126/1126 - 13s - loss: 1.5325 - accuracy: 0.5783 - val_loss: 1.5089 - val_accuracy: 0.5725 - 13s/epoch - 11ms/step\n",
      "Epoch 9/10000\n",
      "1126/1126 - 11s - loss: 1.5765 - accuracy: 0.5772 - val_loss: 1.5936 - val_accuracy: 0.5661 - 11s/epoch - 10ms/step\n",
      "Epoch 10/10000\n",
      "1126/1126 - 11s - loss: 1.5182 - accuracy: 0.5736 - val_loss: 1.5431 - val_accuracy: 0.5727 - 11s/epoch - 10ms/step\n",
      "Epoch 11/10000\n",
      "1126/1126 - 11s - loss: 1.5005 - accuracy: 0.5784 - val_loss: 1.5968 - val_accuracy: 0.5749 - 11s/epoch - 10ms/step\n",
      "Epoch 12/10000\n",
      "1126/1126 - 11s - loss: 1.4904 - accuracy: 0.5776 - val_loss: 1.4269 - val_accuracy: 0.5886 - 11s/epoch - 10ms/step\n",
      "Epoch 13/10000\n",
      "1126/1126 - 11s - loss: 1.4420 - accuracy: 0.5805 - val_loss: 1.3721 - val_accuracy: 0.5977 - 11s/epoch - 10ms/step\n",
      "Epoch 14/10000\n",
      "1126/1126 - 11s - loss: 1.4189 - accuracy: 0.5786 - val_loss: 1.4761 - val_accuracy: 0.5863 - 11s/epoch - 10ms/step\n",
      "Epoch 15/10000\n",
      "1126/1126 - 11s - loss: 1.4410 - accuracy: 0.5776 - val_loss: 1.4689 - val_accuracy: 0.5733 - 11s/epoch - 10ms/step\n",
      "435/435 [==============================] - 2s 5ms/step - loss: 1.7084 - accuracy: 0.5966\n",
      "Larghezza: 500 \n",
      "Profondità: 3 \n",
      "Dimensione del batch: 100\n",
      "Test loss (binary_crossentropy): 1.7084044218063354\n",
      "Test metrics (accuracy): 0.5966477394104004\n",
      "Epoch 1/10000\n",
      "113/113 - 6s - loss: 3.9011 - accuracy: 0.5521 - val_loss: 2.2315 - val_accuracy: 0.5877 - 6s/epoch - 49ms/step\n",
      "Epoch 2/10000\n",
      "113/113 - 4s - loss: 2.0249 - accuracy: 0.5793 - val_loss: 1.8920 - val_accuracy: 0.5836 - 4s/epoch - 38ms/step\n",
      "Epoch 3/10000\n",
      "113/113 - 4s - loss: 1.7975 - accuracy: 0.5850 - val_loss: 1.7783 - val_accuracy: 0.5791 - 4s/epoch - 38ms/step\n",
      "Epoch 4/10000\n",
      "113/113 - 4s - loss: 1.6686 - accuracy: 0.5852 - val_loss: 1.6511 - val_accuracy: 0.5988 - 4s/epoch - 35ms/step\n",
      "Epoch 5/10000\n",
      "113/113 - 4s - loss: 1.5381 - accuracy: 0.5898 - val_loss: 1.5651 - val_accuracy: 0.5968 - 4s/epoch - 34ms/step\n",
      "Epoch 6/10000\n",
      "113/113 - 4s - loss: 1.4535 - accuracy: 0.5922 - val_loss: 1.5395 - val_accuracy: 0.5907 - 4s/epoch - 36ms/step\n",
      "Epoch 7/10000\n",
      "113/113 - 4s - loss: 1.4325 - accuracy: 0.5906 - val_loss: 1.5296 - val_accuracy: 0.5769 - 4s/epoch - 35ms/step\n",
      "Epoch 8/10000\n",
      "113/113 - 4s - loss: 1.4097 - accuracy: 0.5929 - val_loss: 1.5271 - val_accuracy: 0.5690 - 4s/epoch - 36ms/step\n",
      "Epoch 9/10000\n",
      "113/113 - 4s - loss: 1.3999 - accuracy: 0.5949 - val_loss: 1.4457 - val_accuracy: 0.6091 - 4s/epoch - 35ms/step\n",
      "Epoch 10/10000\n",
      "113/113 - 4s - loss: 1.3530 - accuracy: 0.5952 - val_loss: 1.4359 - val_accuracy: 0.6008 - 4s/epoch - 37ms/step\n",
      "Epoch 11/10000\n",
      "113/113 - 4s - loss: 1.2984 - accuracy: 0.5975 - val_loss: 1.4143 - val_accuracy: 0.6044 - 4s/epoch - 37ms/step\n",
      "Epoch 12/10000\n",
      "113/113 - 4s - loss: 1.2945 - accuracy: 0.5984 - val_loss: 1.4305 - val_accuracy: 0.6036 - 4s/epoch - 36ms/step\n",
      "Epoch 13/10000\n",
      "113/113 - 4s - loss: 1.3306 - accuracy: 0.5958 - val_loss: 1.4370 - val_accuracy: 0.5833 - 4s/epoch - 35ms/step\n",
      "Epoch 14/10000\n",
      "113/113 - 4s - loss: 1.3118 - accuracy: 0.5992 - val_loss: 1.4258 - val_accuracy: 0.6038 - 4s/epoch - 37ms/step\n",
      "Epoch 15/10000\n",
      "113/113 - 4s - loss: 1.2919 - accuracy: 0.5997 - val_loss: 1.3687 - val_accuracy: 0.5893 - 4s/epoch - 36ms/step\n",
      "Epoch 16/10000\n",
      "113/113 - 4s - loss: 1.2710 - accuracy: 0.6006 - val_loss: 1.3625 - val_accuracy: 0.5898 - 4s/epoch - 36ms/step\n",
      "Epoch 17/10000\n",
      "113/113 - 4s - loss: 1.2682 - accuracy: 0.6009 - val_loss: 1.3741 - val_accuracy: 0.6129 - 4s/epoch - 38ms/step\n",
      "Epoch 18/10000\n",
      "113/113 - 4s - loss: 1.2648 - accuracy: 0.6015 - val_loss: 1.3523 - val_accuracy: 0.6059 - 4s/epoch - 36ms/step\n",
      "Epoch 19/10000\n",
      "113/113 - 4s - loss: 1.2371 - accuracy: 0.6020 - val_loss: 1.3115 - val_accuracy: 0.5942 - 4s/epoch - 36ms/step\n",
      "Epoch 20/10000\n",
      "113/113 - 4s - loss: 1.2172 - accuracy: 0.6022 - val_loss: 1.2920 - val_accuracy: 0.5991 - 4s/epoch - 35ms/step\n",
      "Epoch 21/10000\n",
      "113/113 - 4s - loss: 1.1934 - accuracy: 0.6014 - val_loss: 1.2520 - val_accuracy: 0.6101 - 4s/epoch - 37ms/step\n",
      "Epoch 22/10000\n",
      "113/113 - 4s - loss: 1.1920 - accuracy: 0.6035 - val_loss: 1.2802 - val_accuracy: 0.5873 - 4s/epoch - 35ms/step\n",
      "Epoch 23/10000\n",
      "113/113 - 4s - loss: 1.1727 - accuracy: 0.6030 - val_loss: 1.2503 - val_accuracy: 0.5789 - 4s/epoch - 37ms/step\n",
      "Epoch 24/10000\n",
      "113/113 - 4s - loss: 1.1581 - accuracy: 0.6037 - val_loss: 1.2343 - val_accuracy: 0.5734 - 4s/epoch - 36ms/step\n",
      "Epoch 25/10000\n",
      "113/113 - 4s - loss: 1.1424 - accuracy: 0.6051 - val_loss: 1.1949 - val_accuracy: 0.6091 - 4s/epoch - 36ms/step\n",
      "Epoch 26/10000\n",
      "113/113 - 4s - loss: 1.1466 - accuracy: 0.6032 - val_loss: 1.2062 - val_accuracy: 0.6092 - 4s/epoch - 38ms/step\n",
      "Epoch 27/10000\n",
      "113/113 - 4s - loss: 1.1452 - accuracy: 0.6049 - val_loss: 1.2275 - val_accuracy: 0.6107 - 4s/epoch - 36ms/step\n",
      "435/435 [==============================] - 2s 5ms/step - loss: 1.3343 - accuracy: 0.6007\n",
      "Larghezza: 500 \n",
      "Profondità: 3 \n",
      "Dimensione del batch: 1000\n",
      "Test loss (binary_crossentropy): 1.3342899084091187\n",
      "Test metrics (accuracy): 0.6006762385368347\n",
      "Epoch 1/10000\n",
      "11260/11260 - 22s - loss: 12124.0684 - accuracy: 0.5738 - val_loss: 1860.2659 - val_accuracy: 0.6309 - 22s/epoch - 2ms/step\n",
      "Epoch 2/10000\n",
      "11260/11260 - 23s - loss: 4356.0786 - accuracy: 0.5797 - val_loss: 5875.8599 - val_accuracy: 0.6175 - 23s/epoch - 2ms/step\n",
      "Epoch 3/10000\n",
      "11260/11260 - 23s - loss: 4513.5894 - accuracy: 0.5854 - val_loss: 2297.7556 - val_accuracy: 0.6374 - 23s/epoch - 2ms/step\n",
      "Epoch 4/10000\n",
      "11260/11260 - 22s - loss: 4249.7603 - accuracy: 0.5878 - val_loss: 1280.5011 - val_accuracy: 0.6517 - 22s/epoch - 2ms/step\n",
      "Epoch 5/10000\n",
      "11260/11260 - 22s - loss: 4252.0088 - accuracy: 0.5872 - val_loss: 2871.8118 - val_accuracy: 0.6077 - 22s/epoch - 2ms/step\n",
      "Epoch 6/10000\n",
      "11260/11260 - 24s - loss: 4343.2534 - accuracy: 0.5908 - val_loss: 1614.7550 - val_accuracy: 0.6459 - 24s/epoch - 2ms/step\n",
      "Epoch 7/10000\n",
      "11260/11260 - 27s - loss: 4220.4810 - accuracy: 0.5920 - val_loss: 1619.9043 - val_accuracy: 0.6487 - 27s/epoch - 2ms/step\n",
      "Epoch 8/10000\n",
      "11260/11260 - 30s - loss: 4325.6880 - accuracy: 0.5909 - val_loss: 3791.1560 - val_accuracy: 0.6273 - 30s/epoch - 3ms/step\n",
      "Epoch 9/10000\n",
      "11260/11260 - 29s - loss: 4060.8684 - accuracy: 0.5929 - val_loss: 1593.7755 - val_accuracy: 0.6455 - 29s/epoch - 3ms/step\n",
      "Epoch 10/10000\n",
      "11260/11260 - 29s - loss: 4231.4404 - accuracy: 0.5910 - val_loss: 5065.9995 - val_accuracy: 0.6259 - 29s/epoch - 3ms/step\n",
      "Epoch 11/10000\n",
      "11260/11260 - 29s - loss: 4182.2881 - accuracy: 0.5937 - val_loss: 6640.8027 - val_accuracy: 0.6140 - 29s/epoch - 3ms/step\n",
      "Epoch 12/10000\n",
      "11260/11260 - 29s - loss: 4002.8923 - accuracy: 0.5933 - val_loss: 1117.5394 - val_accuracy: 0.6423 - 29s/epoch - 3ms/step\n",
      "Epoch 13/10000\n",
      "11260/11260 - 31s - loss: 4351.9058 - accuracy: 0.5917 - val_loss: 3090.5229 - val_accuracy: 0.5772 - 31s/epoch - 3ms/step\n",
      "Epoch 14/10000\n",
      "11260/11260 - 29s - loss: 4166.4033 - accuracy: 0.5924 - val_loss: 20348.7051 - val_accuracy: 0.4152 - 29s/epoch - 3ms/step\n",
      "435/435 [==============================] - 2s 4ms/step - loss: 1334.5956 - accuracy: 0.6510\n",
      "Larghezza: 1000 \n",
      "Profondità: 1 \n",
      "Dimensione del batch: 10\n",
      "Test loss (binary_crossentropy): 1334.5955810546875\n",
      "Test metrics (accuracy): 0.6510323286056519\n",
      "Epoch 1/10000\n",
      "1126/1126 - 7s - loss: 27434.7266 - accuracy: 0.5872 - val_loss: 2649.8984 - val_accuracy: 0.6031 - 7s/epoch - 6ms/step\n",
      "Epoch 2/10000\n",
      "1126/1126 - 4s - loss: 2815.6689 - accuracy: 0.5724 - val_loss: 2112.1052 - val_accuracy: 0.5264 - 4s/epoch - 4ms/step\n",
      "Epoch 3/10000\n",
      "1126/1126 - 4s - loss: 2434.3816 - accuracy: 0.5759 - val_loss: 1619.0552 - val_accuracy: 0.5201 - 4s/epoch - 4ms/step\n",
      "Epoch 4/10000\n",
      "1126/1126 - 4s - loss: 2195.6157 - accuracy: 0.5785 - val_loss: 1878.2782 - val_accuracy: 0.6323 - 4s/epoch - 4ms/step\n",
      "Epoch 5/10000\n",
      "1126/1126 - 4s - loss: 2195.0959 - accuracy: 0.5806 - val_loss: 1633.5331 - val_accuracy: 0.6385 - 4s/epoch - 4ms/step\n",
      "Epoch 6/10000\n",
      "1126/1126 - 4s - loss: 2003.2250 - accuracy: 0.5857 - val_loss: 3525.3015 - val_accuracy: 0.6257 - 4s/epoch - 4ms/step\n",
      "Epoch 7/10000\n",
      "1126/1126 - 4s - loss: 1908.7115 - accuracy: 0.5850 - val_loss: 5455.0391 - val_accuracy: 0.6192 - 4s/epoch - 4ms/step\n",
      "Epoch 8/10000\n",
      "1126/1126 - 4s - loss: 2117.3140 - accuracy: 0.5861 - val_loss: 762.5429 - val_accuracy: 0.6258 - 4s/epoch - 4ms/step\n",
      "Epoch 9/10000\n",
      "1126/1126 - 5s - loss: 1847.9653 - accuracy: 0.5894 - val_loss: 1672.9496 - val_accuracy: 0.6258 - 5s/epoch - 4ms/step\n",
      "Epoch 10/10000\n",
      "1126/1126 - 4s - loss: 2024.1631 - accuracy: 0.5883 - val_loss: 1449.7570 - val_accuracy: 0.5272 - 4s/epoch - 4ms/step\n",
      "Epoch 11/10000\n",
      "1126/1126 - 4s - loss: 1801.7535 - accuracy: 0.5916 - val_loss: 678.8130 - val_accuracy: 0.6335 - 4s/epoch - 4ms/step\n",
      "Epoch 12/10000\n",
      "1126/1126 - 4s - loss: 2199.8906 - accuracy: 0.5891 - val_loss: 1079.5461 - val_accuracy: 0.6352 - 4s/epoch - 4ms/step\n",
      "Epoch 13/10000\n",
      "1126/1126 - 4s - loss: 1917.3976 - accuracy: 0.5926 - val_loss: 1008.0151 - val_accuracy: 0.5900 - 4s/epoch - 3ms/step\n",
      "Epoch 14/10000\n",
      "1126/1126 - 4s - loss: 1916.3649 - accuracy: 0.5910 - val_loss: 743.8265 - val_accuracy: 0.6253 - 4s/epoch - 3ms/step\n",
      "Epoch 15/10000\n",
      "1126/1126 - 4s - loss: 2002.5024 - accuracy: 0.5917 - val_loss: 748.1838 - val_accuracy: 0.6497 - 4s/epoch - 4ms/step\n",
      "Epoch 16/10000\n",
      "1126/1126 - 4s - loss: 2107.6279 - accuracy: 0.5915 - val_loss: 611.6592 - val_accuracy: 0.6452 - 4s/epoch - 3ms/step\n",
      "Epoch 17/10000\n",
      "1126/1126 - 4s - loss: 1790.7227 - accuracy: 0.5940 - val_loss: 3437.3423 - val_accuracy: 0.6283 - 4s/epoch - 3ms/step\n",
      "Epoch 18/10000\n",
      "1126/1126 - 4s - loss: 1891.1066 - accuracy: 0.5948 - val_loss: 771.9385 - val_accuracy: 0.6379 - 4s/epoch - 3ms/step\n",
      "Epoch 19/10000\n",
      "1126/1126 - 4s - loss: 1686.4297 - accuracy: 0.5956 - val_loss: 601.0541 - val_accuracy: 0.6474 - 4s/epoch - 3ms/step\n",
      "Epoch 20/10000\n",
      "1126/1126 - 4s - loss: 1736.4104 - accuracy: 0.5972 - val_loss: 988.2639 - val_accuracy: 0.6255 - 4s/epoch - 3ms/step\n",
      "Epoch 21/10000\n",
      "1126/1126 - 4s - loss: 1621.2797 - accuracy: 0.5966 - val_loss: 855.9177 - val_accuracy: 0.6557 - 4s/epoch - 4ms/step\n",
      "Epoch 22/10000\n",
      "1126/1126 - 4s - loss: 1902.8098 - accuracy: 0.5971 - val_loss: 737.2492 - val_accuracy: 0.6151 - 4s/epoch - 3ms/step\n",
      "Epoch 23/10000\n",
      "1126/1126 - 4s - loss: 1918.9885 - accuracy: 0.5954 - val_loss: 1886.8794 - val_accuracy: 0.5202 - 4s/epoch - 4ms/step\n",
      "Epoch 24/10000\n",
      "1126/1126 - 4s - loss: 1797.4872 - accuracy: 0.5958 - val_loss: 819.1559 - val_accuracy: 0.6046 - 4s/epoch - 3ms/step\n",
      "Epoch 25/10000\n",
      "1126/1126 - 4s - loss: 1694.2004 - accuracy: 0.5959 - val_loss: 642.2404 - val_accuracy: 0.6560 - 4s/epoch - 4ms/step\n",
      "Epoch 26/10000\n",
      "1126/1126 - 4s - loss: 1808.3057 - accuracy: 0.5964 - val_loss: 800.8171 - val_accuracy: 0.6213 - 4s/epoch - 4ms/step\n",
      "Epoch 27/10000\n",
      "1126/1126 - 4s - loss: 1701.9915 - accuracy: 0.5978 - val_loss: 565.1059 - val_accuracy: 0.6517 - 4s/epoch - 4ms/step\n",
      "Epoch 28/10000\n",
      "1126/1126 - 4s - loss: 1928.4440 - accuracy: 0.5951 - val_loss: 1408.7563 - val_accuracy: 0.6327 - 4s/epoch - 4ms/step\n",
      "Epoch 29/10000\n",
      "1126/1126 - 4s - loss: 1847.5205 - accuracy: 0.5965 - val_loss: 515.9441 - val_accuracy: 0.6427 - 4s/epoch - 3ms/step\n",
      "Epoch 30/10000\n",
      "1126/1126 - 4s - loss: 1731.4617 - accuracy: 0.5969 - val_loss: 3434.4780 - val_accuracy: 0.5082 - 4s/epoch - 4ms/step\n",
      "Epoch 31/10000\n",
      "1126/1126 - 4s - loss: 1824.0332 - accuracy: 0.5980 - val_loss: 2323.1812 - val_accuracy: 0.5229 - 4s/epoch - 4ms/step\n",
      "Epoch 32/10000\n",
      "1126/1126 - 4s - loss: 1886.7946 - accuracy: 0.5957 - val_loss: 1249.9160 - val_accuracy: 0.5660 - 4s/epoch - 4ms/step\n",
      "Epoch 33/10000\n",
      "1126/1126 - 4s - loss: 1764.3016 - accuracy: 0.5983 - val_loss: 1359.6586 - val_accuracy: 0.5835 - 4s/epoch - 4ms/step\n",
      "Epoch 34/10000\n",
      "1126/1126 - 4s - loss: 1794.1887 - accuracy: 0.5956 - val_loss: 628.5457 - val_accuracy: 0.6412 - 4s/epoch - 4ms/step\n",
      "Epoch 35/10000\n",
      "1126/1126 - 4s - loss: 1818.1442 - accuracy: 0.5990 - val_loss: 573.3517 - val_accuracy: 0.6414 - 4s/epoch - 4ms/step\n",
      "435/435 [==============================] - 1s 3ms/step - loss: 679.3583 - accuracy: 0.6474\n",
      "Larghezza: 1000 \n",
      "Profondità: 1 \n",
      "Dimensione del batch: 100\n",
      "Test loss (binary_crossentropy): 679.3583374023438\n",
      "Test metrics (accuracy): 0.6473634839057922\n",
      "Epoch 1/10000\n",
      "113/113 - 4s - loss: 7729640.5000 - accuracy: 0.3887 - val_loss: 5989134.5000 - val_accuracy: 0.3869 - 4s/epoch - 32ms/step\n",
      "Epoch 2/10000\n",
      "113/113 - 2s - loss: 4173238.7500 - accuracy: 0.3886 - val_loss: 2397920.0000 - val_accuracy: 0.3850 - 2s/epoch - 17ms/step\n",
      "Epoch 3/10000\n",
      "113/113 - 2s - loss: 874141.5625 - accuracy: 0.4742 - val_loss: 130226.3125 - val_accuracy: 0.5924 - 2s/epoch - 17ms/step\n",
      "Epoch 4/10000\n",
      "113/113 - 2s - loss: 100902.3594 - accuracy: 0.5971 - val_loss: 72299.2656 - val_accuracy: 0.6024 - 2s/epoch - 19ms/step\n",
      "Epoch 5/10000\n",
      "113/113 - 2s - loss: 47960.8555 - accuracy: 0.6088 - val_loss: 24798.7168 - val_accuracy: 0.6174 - 2s/epoch - 13ms/step\n",
      "Epoch 6/10000\n",
      "113/113 - 2s - loss: 16096.4150 - accuracy: 0.6034 - val_loss: 12446.4990 - val_accuracy: 0.5969 - 2s/epoch - 15ms/step\n",
      "Epoch 7/10000\n",
      "113/113 - 2s - loss: 11427.0479 - accuracy: 0.5916 - val_loss: 10030.0449 - val_accuracy: 0.5948 - 2s/epoch - 15ms/step\n",
      "Epoch 8/10000\n",
      "113/113 - 2s - loss: 9059.0049 - accuracy: 0.5850 - val_loss: 7794.3364 - val_accuracy: 0.5884 - 2s/epoch - 15ms/step\n",
      "Epoch 9/10000\n",
      "113/113 - 2s - loss: 6983.1108 - accuracy: 0.5781 - val_loss: 5876.5298 - val_accuracy: 0.5787 - 2s/epoch - 14ms/step\n",
      "Epoch 10/10000\n",
      "113/113 - 2s - loss: 5178.2388 - accuracy: 0.5679 - val_loss: 4228.1802 - val_accuracy: 0.5641 - 2s/epoch - 14ms/step\n",
      "Epoch 11/10000\n",
      "113/113 - 1s - loss: 3783.3655 - accuracy: 0.5598 - val_loss: 2983.2815 - val_accuracy: 0.5686 - 1s/epoch - 13ms/step\n",
      "Epoch 12/10000\n",
      "113/113 - 1s - loss: 3163.6016 - accuracy: 0.5603 - val_loss: 3560.7795 - val_accuracy: 0.6114 - 1s/epoch - 12ms/step\n",
      "Epoch 13/10000\n",
      "113/113 - 2s - loss: 3413.8135 - accuracy: 0.5713 - val_loss: 1924.0709 - val_accuracy: 0.5981 - 2s/epoch - 14ms/step\n",
      "Epoch 14/10000\n",
      "113/113 - 1s - loss: 3469.1426 - accuracy: 0.5736 - val_loss: 2470.5281 - val_accuracy: 0.6135 - 1s/epoch - 12ms/step\n",
      "Epoch 15/10000\n",
      "113/113 - 2s - loss: 3082.9480 - accuracy: 0.5720 - val_loss: 3735.7695 - val_accuracy: 0.6116 - 2s/epoch - 16ms/step\n",
      "435/435 [==============================] - 2s 4ms/step - loss: 25153.6016 - accuracy: 0.6154\n",
      "Larghezza: 1000 \n",
      "Profondità: 1 \n",
      "Dimensione del batch: 1000\n",
      "Test loss (binary_crossentropy): 25153.6015625\n",
      "Test metrics (accuracy): 0.6153514385223389\n",
      "Epoch 1/10000\n",
      "11260/11260 - 32s - loss: 3.0456 - accuracy: 0.5362 - val_loss: 2.1638 - val_accuracy: 0.5697 - 32s/epoch - 3ms/step\n",
      "Epoch 2/10000\n",
      "11260/11260 - 29s - loss: 2.1128 - accuracy: 0.5642 - val_loss: 1.9544 - val_accuracy: 0.5749 - 29s/epoch - 3ms/step\n",
      "Epoch 3/10000\n",
      "11260/11260 - 30s - loss: 1.8804 - accuracy: 0.5688 - val_loss: 1.7286 - val_accuracy: 0.5708 - 30s/epoch - 3ms/step\n",
      "Epoch 4/10000\n",
      "11260/11260 - 28s - loss: 1.7505 - accuracy: 0.5652 - val_loss: 1.6649 - val_accuracy: 0.5585 - 28s/epoch - 2ms/step\n",
      "Epoch 5/10000\n",
      "11260/11260 - 29s - loss: 1.6464 - accuracy: 0.5694 - val_loss: 1.5643 - val_accuracy: 0.5693 - 29s/epoch - 3ms/step\n",
      "Epoch 6/10000\n",
      "11260/11260 - 30s - loss: 1.5567 - accuracy: 0.5703 - val_loss: 1.5233 - val_accuracy: 0.5996 - 30s/epoch - 3ms/step\n",
      "Epoch 7/10000\n",
      "11260/11260 - 30s - loss: 1.4493 - accuracy: 0.5733 - val_loss: 1.4005 - val_accuracy: 0.5828 - 30s/epoch - 3ms/step\n",
      "Epoch 8/10000\n",
      "11260/11260 - 30s - loss: 1.3892 - accuracy: 0.5709 - val_loss: 1.4083 - val_accuracy: 0.5422 - 30s/epoch - 3ms/step\n",
      "Epoch 9/10000\n",
      "11260/11260 - 30s - loss: 1.3235 - accuracy: 0.5738 - val_loss: 1.2378 - val_accuracy: 0.5940 - 30s/epoch - 3ms/step\n",
      "Epoch 10/10000\n",
      "11260/11260 - 29s - loss: 1.2800 - accuracy: 0.5758 - val_loss: 1.2066 - val_accuracy: 0.5693 - 29s/epoch - 3ms/step\n",
      "Epoch 11/10000\n",
      "11260/11260 - 29s - loss: 1.2213 - accuracy: 0.5758 - val_loss: 1.1333 - val_accuracy: 0.5831 - 29s/epoch - 3ms/step\n",
      "Epoch 12/10000\n",
      "11260/11260 - 32s - loss: 1.1404 - accuracy: 0.5774 - val_loss: 1.1689 - val_accuracy: 0.6008 - 32s/epoch - 3ms/step\n",
      "Epoch 13/10000\n",
      "11260/11260 - 30s - loss: 1.1004 - accuracy: 0.5825 - val_loss: 1.0090 - val_accuracy: 0.6007 - 30s/epoch - 3ms/step\n",
      "Epoch 14/10000\n",
      "11260/11260 - 37s - loss: 1.0720 - accuracy: 0.5820 - val_loss: 1.0251 - val_accuracy: 0.5888 - 37s/epoch - 3ms/step\n",
      "Epoch 15/10000\n",
      "11260/11260 - 30s - loss: 1.0416 - accuracy: 0.5852 - val_loss: 0.9806 - val_accuracy: 0.5994 - 30s/epoch - 3ms/step\n",
      "Epoch 16/10000\n",
      "11260/11260 - 30s - loss: 0.9961 - accuracy: 0.5860 - val_loss: 0.9369 - val_accuracy: 0.6042 - 30s/epoch - 3ms/step\n",
      "Epoch 17/10000\n",
      "11260/11260 - 29s - loss: 0.9720 - accuracy: 0.5895 - val_loss: 0.9771 - val_accuracy: 0.5664 - 29s/epoch - 3ms/step\n",
      "Epoch 18/10000\n",
      "11260/11260 - 28s - loss: 0.9659 - accuracy: 0.5922 - val_loss: 0.9353 - val_accuracy: 0.5967 - 28s/epoch - 2ms/step\n",
      "Epoch 19/10000\n",
      "11260/11260 - 29s - loss: 0.9377 - accuracy: 0.5952 - val_loss: 0.9060 - val_accuracy: 0.6107 - 29s/epoch - 3ms/step\n",
      "Epoch 20/10000\n",
      "11260/11260 - 29s - loss: 0.9162 - accuracy: 0.5967 - val_loss: 0.9080 - val_accuracy: 0.6090 - 29s/epoch - 3ms/step\n",
      "Epoch 21/10000\n",
      "11260/11260 - 29s - loss: 0.9110 - accuracy: 0.5929 - val_loss: 0.9090 - val_accuracy: 0.5877 - 29s/epoch - 3ms/step\n",
      "Epoch 22/10000\n",
      "11260/11260 - 29s - loss: 0.9071 - accuracy: 0.5940 - val_loss: 0.9358 - val_accuracy: 0.5210 - 29s/epoch - 3ms/step\n",
      "Epoch 23/10000\n",
      "11260/11260 - 29s - loss: 0.8901 - accuracy: 0.5976 - val_loss: 0.8751 - val_accuracy: 0.5804 - 29s/epoch - 3ms/step\n",
      "Epoch 24/10000\n",
      "11260/11260 - 29s - loss: 0.8973 - accuracy: 0.5960 - val_loss: 0.8888 - val_accuracy: 0.5742 - 29s/epoch - 3ms/step\n",
      "Epoch 25/10000\n",
      "11260/11260 - 29s - loss: 0.8922 - accuracy: 0.5937 - val_loss: 0.8737 - val_accuracy: 0.6138 - 29s/epoch - 3ms/step\n",
      "Epoch 26/10000\n",
      "11260/11260 - 29s - loss: 0.8678 - accuracy: 0.5973 - val_loss: 0.8463 - val_accuracy: 0.6143 - 29s/epoch - 3ms/step\n",
      "Epoch 27/10000\n",
      "11260/11260 - 30s - loss: 0.8483 - accuracy: 0.6041 - val_loss: 0.8472 - val_accuracy: 0.6215 - 30s/epoch - 3ms/step\n",
      "Epoch 28/10000\n",
      "11260/11260 - 35s - loss: 0.8481 - accuracy: 0.6021 - val_loss: 0.8250 - val_accuracy: 0.6241 - 35s/epoch - 3ms/step\n",
      "Epoch 29/10000\n",
      "11260/11260 - 29s - loss: 0.8369 - accuracy: 0.6011 - val_loss: 0.9244 - val_accuracy: 0.5139 - 29s/epoch - 3ms/step\n",
      "Epoch 30/10000\n",
      "11260/11260 - 31s - loss: 0.8374 - accuracy: 0.6011 - val_loss: 0.8506 - val_accuracy: 0.6119 - 31s/epoch - 3ms/step\n",
      "Epoch 31/10000\n",
      "11260/11260 - 35s - loss: 0.8405 - accuracy: 0.6007 - val_loss: 0.9236 - val_accuracy: 0.5171 - 35s/epoch - 3ms/step\n",
      "Epoch 32/10000\n",
      "11260/11260 - 28s - loss: 0.8477 - accuracy: 0.5996 - val_loss: 0.8302 - val_accuracy: 0.6149 - 28s/epoch - 3ms/step\n",
      "Epoch 33/10000\n",
      "11260/11260 - 30s - loss: 0.8295 - accuracy: 0.6025 - val_loss: 0.8951 - val_accuracy: 0.5186 - 30s/epoch - 3ms/step\n",
      "Epoch 34/10000\n",
      "11260/11260 - 29s - loss: 0.8197 - accuracy: 0.6064 - val_loss: 0.9070 - val_accuracy: 0.6080 - 29s/epoch - 3ms/step\n",
      "Epoch 35/10000\n",
      "11260/11260 - 28s - loss: 0.8130 - accuracy: 0.6073 - val_loss: 0.8121 - val_accuracy: 0.6140 - 28s/epoch - 3ms/step\n",
      "Epoch 36/10000\n",
      "11260/11260 - 30s - loss: 0.8128 - accuracy: 0.6070 - val_loss: 0.7849 - val_accuracy: 0.6283 - 30s/epoch - 3ms/step\n",
      "Epoch 37/10000\n",
      "11260/11260 - 30s - loss: 0.8034 - accuracy: 0.6098 - val_loss: 0.8120 - val_accuracy: 0.6162 - 30s/epoch - 3ms/step\n",
      "Epoch 38/10000\n",
      "11260/11260 - 30s - loss: 0.7973 - accuracy: 0.6101 - val_loss: 0.7819 - val_accuracy: 0.6156 - 30s/epoch - 3ms/step\n",
      "Epoch 39/10000\n",
      "11260/11260 - 29s - loss: 0.7963 - accuracy: 0.6132 - val_loss: 0.7789 - val_accuracy: 0.6266 - 29s/epoch - 3ms/step\n",
      "Epoch 40/10000\n",
      "11260/11260 - 30s - loss: 0.7945 - accuracy: 0.6136 - val_loss: 0.7751 - val_accuracy: 0.6281 - 30s/epoch - 3ms/step\n",
      "Epoch 41/10000\n",
      "11260/11260 - 29s - loss: 0.7896 - accuracy: 0.6118 - val_loss: 0.7664 - val_accuracy: 0.6296 - 29s/epoch - 3ms/step\n",
      "Epoch 42/10000\n",
      "11260/11260 - 29s - loss: 0.7891 - accuracy: 0.6096 - val_loss: 0.7655 - val_accuracy: 0.6282 - 29s/epoch - 3ms/step\n",
      "Epoch 43/10000\n",
      "11260/11260 - 29s - loss: 0.7842 - accuracy: 0.6132 - val_loss: 0.7778 - val_accuracy: 0.6225 - 29s/epoch - 3ms/step\n",
      "Epoch 44/10000\n",
      "11260/11260 - 29s - loss: 0.7896 - accuracy: 0.6128 - val_loss: 0.8042 - val_accuracy: 0.6199 - 29s/epoch - 3ms/step\n",
      "Epoch 45/10000\n",
      "11260/11260 - 28s - loss: 0.7848 - accuracy: 0.6133 - val_loss: 0.7914 - val_accuracy: 0.6235 - 28s/epoch - 3ms/step\n",
      "Epoch 46/10000\n",
      "11260/11260 - 29s - loss: 0.7861 - accuracy: 0.6100 - val_loss: 0.7864 - val_accuracy: 0.6225 - 29s/epoch - 3ms/step\n",
      "Epoch 47/10000\n",
      "11260/11260 - 30s - loss: 0.7764 - accuracy: 0.6144 - val_loss: 0.7587 - val_accuracy: 0.6266 - 30s/epoch - 3ms/step\n",
      "Epoch 48/10000\n",
      "11260/11260 - 29s - loss: 0.7718 - accuracy: 0.6150 - val_loss: 0.7817 - val_accuracy: 0.6255 - 29s/epoch - 3ms/step\n",
      "Epoch 49/10000\n",
      "11260/11260 - 30s - loss: 0.7693 - accuracy: 0.6179 - val_loss: 0.7516 - val_accuracy: 0.6310 - 30s/epoch - 3ms/step\n",
      "Epoch 50/10000\n",
      "11260/11260 - 29s - loss: 0.7652 - accuracy: 0.6162 - val_loss: 0.7507 - val_accuracy: 0.6247 - 29s/epoch - 3ms/step\n",
      "Epoch 51/10000\n",
      "11260/11260 - 29s - loss: 0.7672 - accuracy: 0.6162 - val_loss: 0.8038 - val_accuracy: 0.6121 - 29s/epoch - 3ms/step\n",
      "Epoch 52/10000\n",
      "11260/11260 - 30s - loss: 0.7728 - accuracy: 0.6137 - val_loss: 0.7623 - val_accuracy: 0.6316 - 30s/epoch - 3ms/step\n",
      "Epoch 53/10000\n",
      "11260/11260 - 29s - loss: 0.7627 - accuracy: 0.6172 - val_loss: 0.7646 - val_accuracy: 0.6268 - 29s/epoch - 3ms/step\n",
      "Epoch 54/10000\n",
      "11260/11260 - 30s - loss: 0.7633 - accuracy: 0.6155 - val_loss: 0.7621 - val_accuracy: 0.6302 - 30s/epoch - 3ms/step\n",
      "Epoch 55/10000\n",
      "11260/11260 - 29s - loss: 0.7632 - accuracy: 0.6134 - val_loss: 0.7801 - val_accuracy: 0.6217 - 29s/epoch - 3ms/step\n",
      "Epoch 56/10000\n",
      "11260/11260 - 28s - loss: 0.7637 - accuracy: 0.6111 - val_loss: 0.8172 - val_accuracy: 0.6166 - 28s/epoch - 2ms/step\n",
      "Epoch 57/10000\n",
      "11260/11260 - 27s - loss: 0.7681 - accuracy: 0.6120 - val_loss: 0.7483 - val_accuracy: 0.6249 - 27s/epoch - 2ms/step\n",
      "Epoch 58/10000\n",
      "11260/11260 - 33s - loss: 0.7654 - accuracy: 0.6132 - val_loss: 0.7867 - val_accuracy: 0.6274 - 33s/epoch - 3ms/step\n",
      "Epoch 59/10000\n",
      "11260/11260 - 31s - loss: 0.7710 - accuracy: 0.6165 - val_loss: 0.7637 - val_accuracy: 0.6272 - 31s/epoch - 3ms/step\n",
      "Epoch 60/10000\n",
      "11260/11260 - 37s - loss: 0.7608 - accuracy: 0.6174 - val_loss: 0.7342 - val_accuracy: 0.6341 - 37s/epoch - 3ms/step\n",
      "Epoch 61/10000\n",
      "11260/11260 - 34s - loss: 0.7585 - accuracy: 0.6137 - val_loss: 0.7934 - val_accuracy: 0.6233 - 34s/epoch - 3ms/step\n",
      "Epoch 62/10000\n",
      "11260/11260 - 30s - loss: 0.7635 - accuracy: 0.6146 - val_loss: 0.7740 - val_accuracy: 0.6095 - 30s/epoch - 3ms/step\n",
      "Epoch 63/10000\n",
      "11260/11260 - 26s - loss: 0.7637 - accuracy: 0.6122 - val_loss: 0.7963 - val_accuracy: 0.6259 - 26s/epoch - 2ms/step\n",
      "Epoch 64/10000\n",
      "11260/11260 - 29s - loss: 0.7592 - accuracy: 0.6147 - val_loss: 0.7401 - val_accuracy: 0.6283 - 29s/epoch - 3ms/step\n",
      "Epoch 65/10000\n",
      "11260/11260 - 25s - loss: 0.7556 - accuracy: 0.6118 - val_loss: 0.7463 - val_accuracy: 0.6232 - 25s/epoch - 2ms/step\n",
      "Epoch 66/10000\n",
      "11260/11260 - 28s - loss: 0.7571 - accuracy: 0.6161 - val_loss: 0.7369 - val_accuracy: 0.6329 - 28s/epoch - 2ms/step\n",
      "Epoch 67/10000\n",
      "11260/11260 - 26s - loss: 0.7477 - accuracy: 0.6171 - val_loss: 0.7567 - val_accuracy: 0.6215 - 26s/epoch - 2ms/step\n",
      "Epoch 68/10000\n",
      "11260/11260 - 36s - loss: 0.7466 - accuracy: 0.6158 - val_loss: 0.8012 - val_accuracy: 0.6172 - 36s/epoch - 3ms/step\n",
      "Epoch 69/10000\n",
      "11260/11260 - 30s - loss: 0.7514 - accuracy: 0.6153 - val_loss: 0.8096 - val_accuracy: 0.6224 - 30s/epoch - 3ms/step\n",
      "Epoch 70/10000\n",
      "11260/11260 - 26s - loss: 0.7481 - accuracy: 0.6160 - val_loss: 0.7453 - val_accuracy: 0.6252 - 26s/epoch - 2ms/step\n",
      "435/435 [==============================] - 2s 4ms/step - loss: 0.7385 - accuracy: 0.6297\n",
      "Larghezza: 1000 \n",
      "Profondità: 1 \n",
      "Dimensione del batch: 10\n",
      "Test loss (binary_crossentropy): 0.7384920120239258\n",
      "Test metrics (accuracy): 0.6296669244766235\n",
      "Epoch 1/10000\n",
      "1126/1126 - 6s - loss: 10.0974 - accuracy: 0.5624 - val_loss: 2.8066 - val_accuracy: 0.5528 - 6s/epoch - 6ms/step\n",
      "Epoch 2/10000\n",
      "1126/1126 - 4s - loss: 2.5416 - accuracy: 0.5588 - val_loss: 2.4039 - val_accuracy: 0.5548 - 4s/epoch - 4ms/step\n",
      "Epoch 3/10000\n",
      "1126/1126 - 4s - loss: 2.2202 - accuracy: 0.5531 - val_loss: 2.1566 - val_accuracy: 0.5478 - 4s/epoch - 3ms/step\n",
      "Epoch 4/10000\n",
      "1126/1126 - 4s - loss: 2.0856 - accuracy: 0.5560 - val_loss: 2.0756 - val_accuracy: 0.5651 - 4s/epoch - 4ms/step\n",
      "Epoch 5/10000\n",
      "1126/1126 - 4s - loss: 1.9633 - accuracy: 0.5635 - val_loss: 1.9523 - val_accuracy: 0.5744 - 4s/epoch - 4ms/step\n",
      "Epoch 6/10000\n",
      "1126/1126 - 4s - loss: 1.8801 - accuracy: 0.5653 - val_loss: 1.8707 - val_accuracy: 0.5620 - 4s/epoch - 3ms/step\n",
      "Epoch 7/10000\n",
      "1126/1126 - 4s - loss: 1.8010 - accuracy: 0.5568 - val_loss: 1.8226 - val_accuracy: 0.5491 - 4s/epoch - 3ms/step\n",
      "Epoch 8/10000\n",
      "1126/1126 - 3s - loss: 1.7226 - accuracy: 0.5573 - val_loss: 1.7199 - val_accuracy: 0.5500 - 3s/epoch - 3ms/step\n",
      "Epoch 9/10000\n",
      "1126/1126 - 4s - loss: 1.6534 - accuracy: 0.5613 - val_loss: 1.6650 - val_accuracy: 0.5798 - 4s/epoch - 3ms/step\n",
      "Epoch 10/10000\n",
      "1126/1126 - 4s - loss: 1.6001 - accuracy: 0.5660 - val_loss: 1.6400 - val_accuracy: 0.5803 - 4s/epoch - 3ms/step\n",
      "Epoch 11/10000\n",
      "1126/1126 - 3s - loss: 1.5722 - accuracy: 0.5656 - val_loss: 1.5924 - val_accuracy: 0.5585 - 3s/epoch - 3ms/step\n",
      "Epoch 12/10000\n",
      "1126/1126 - 4s - loss: 1.5447 - accuracy: 0.5605 - val_loss: 1.5180 - val_accuracy: 0.5613 - 4s/epoch - 3ms/step\n",
      "Epoch 13/10000\n",
      "1126/1126 - 4s - loss: 1.5140 - accuracy: 0.5625 - val_loss: 1.5056 - val_accuracy: 0.5594 - 4s/epoch - 4ms/step\n",
      "Epoch 14/10000\n",
      "1126/1126 - 4s - loss: 1.4508 - accuracy: 0.5687 - val_loss: 1.4549 - val_accuracy: 0.5779 - 4s/epoch - 3ms/step\n",
      "Epoch 15/10000\n",
      "1126/1126 - 4s - loss: 1.4243 - accuracy: 0.5663 - val_loss: 1.4247 - val_accuracy: 0.5743 - 4s/epoch - 3ms/step\n",
      "Epoch 16/10000\n",
      "1126/1126 - 4s - loss: 1.3983 - accuracy: 0.5698 - val_loss: 1.4065 - val_accuracy: 0.5441 - 4s/epoch - 3ms/step\n",
      "Epoch 17/10000\n",
      "1126/1126 - 4s - loss: 1.3731 - accuracy: 0.5702 - val_loss: 1.4077 - val_accuracy: 0.5619 - 4s/epoch - 3ms/step\n",
      "Epoch 18/10000\n",
      "1126/1126 - 4s - loss: 1.3471 - accuracy: 0.5735 - val_loss: 1.4000 - val_accuracy: 0.5818 - 4s/epoch - 4ms/step\n",
      "Epoch 19/10000\n",
      "1126/1126 - 4s - loss: 1.3217 - accuracy: 0.5753 - val_loss: 1.3646 - val_accuracy: 0.5796 - 4s/epoch - 4ms/step\n",
      "Epoch 20/10000\n",
      "1126/1126 - 5s - loss: 1.3106 - accuracy: 0.5743 - val_loss: 1.3057 - val_accuracy: 0.5777 - 5s/epoch - 4ms/step\n",
      "Epoch 21/10000\n",
      "1126/1126 - 5s - loss: 1.2713 - accuracy: 0.5766 - val_loss: 1.2870 - val_accuracy: 0.5741 - 5s/epoch - 4ms/step\n",
      "Epoch 22/10000\n",
      "1126/1126 - 5s - loss: 1.2629 - accuracy: 0.5740 - val_loss: 1.2728 - val_accuracy: 0.5714 - 5s/epoch - 4ms/step\n",
      "Epoch 23/10000\n",
      "1126/1126 - 4s - loss: 1.2618 - accuracy: 0.5709 - val_loss: 1.2925 - val_accuracy: 0.5583 - 4s/epoch - 3ms/step\n",
      "Epoch 24/10000\n",
      "1126/1126 - 4s - loss: 1.2447 - accuracy: 0.5718 - val_loss: 1.2479 - val_accuracy: 0.5646 - 4s/epoch - 4ms/step\n",
      "Epoch 25/10000\n",
      "1126/1126 - 4s - loss: 1.2249 - accuracy: 0.5742 - val_loss: 1.2825 - val_accuracy: 0.5635 - 4s/epoch - 4ms/step\n",
      "Epoch 26/10000\n",
      "1126/1126 - 3s - loss: 1.2251 - accuracy: 0.5765 - val_loss: 1.2414 - val_accuracy: 0.5861 - 3s/epoch - 3ms/step\n",
      "Epoch 27/10000\n",
      "1126/1126 - 4s - loss: 1.1965 - accuracy: 0.5823 - val_loss: 1.2512 - val_accuracy: 0.5832 - 4s/epoch - 3ms/step\n",
      "Epoch 28/10000\n",
      "1126/1126 - 4s - loss: 1.1862 - accuracy: 0.5850 - val_loss: 1.2279 - val_accuracy: 0.5838 - 4s/epoch - 3ms/step\n",
      "Epoch 29/10000\n",
      "1126/1126 - 4s - loss: 1.1653 - accuracy: 0.5853 - val_loss: 1.1920 - val_accuracy: 0.5941 - 4s/epoch - 3ms/step\n",
      "Epoch 30/10000\n",
      "1126/1126 - 4s - loss: 1.1550 - accuracy: 0.5858 - val_loss: 1.1746 - val_accuracy: 0.5936 - 4s/epoch - 4ms/step\n",
      "Epoch 31/10000\n",
      "1126/1126 - 4s - loss: 1.1465 - accuracy: 0.5883 - val_loss: 1.1847 - val_accuracy: 0.5371 - 4s/epoch - 3ms/step\n",
      "Epoch 32/10000\n",
      "1126/1126 - 5s - loss: 1.1599 - accuracy: 0.5840 - val_loss: 1.1805 - val_accuracy: 0.5960 - 5s/epoch - 4ms/step\n",
      "Epoch 33/10000\n",
      "1126/1126 - 4s - loss: 1.1229 - accuracy: 0.5863 - val_loss: 1.1585 - val_accuracy: 0.5737 - 4s/epoch - 4ms/step\n",
      "Epoch 34/10000\n",
      "1126/1126 - 4s - loss: 1.1016 - accuracy: 0.5817 - val_loss: 1.1132 - val_accuracy: 0.5729 - 4s/epoch - 4ms/step\n",
      "Epoch 35/10000\n",
      "1126/1126 - 4s - loss: 1.1063 - accuracy: 0.5825 - val_loss: 1.1300 - val_accuracy: 0.5763 - 4s/epoch - 3ms/step\n",
      "Epoch 36/10000\n",
      "1126/1126 - 4s - loss: 1.0856 - accuracy: 0.5838 - val_loss: 1.0891 - val_accuracy: 0.5851 - 4s/epoch - 3ms/step\n",
      "Epoch 37/10000\n",
      "1126/1126 - 5s - loss: 1.0813 - accuracy: 0.5884 - val_loss: 1.0863 - val_accuracy: 0.5992 - 5s/epoch - 4ms/step\n",
      "Epoch 38/10000\n",
      "1126/1126 - 4s - loss: 1.0700 - accuracy: 0.5863 - val_loss: 1.0841 - val_accuracy: 0.5717 - 4s/epoch - 4ms/step\n",
      "Epoch 39/10000\n",
      "1126/1126 - 7s - loss: 1.0549 - accuracy: 0.5877 - val_loss: 1.0521 - val_accuracy: 0.5645 - 7s/epoch - 6ms/step\n",
      "Epoch 40/10000\n",
      "1126/1126 - 6s - loss: 1.0437 - accuracy: 0.5870 - val_loss: 1.0702 - val_accuracy: 0.5870 - 6s/epoch - 6ms/step\n",
      "Epoch 41/10000\n",
      "1126/1126 - 4s - loss: 1.0408 - accuracy: 0.5906 - val_loss: 1.0611 - val_accuracy: 0.5854 - 4s/epoch - 4ms/step\n",
      "Epoch 42/10000\n",
      "1126/1126 - 4s - loss: 1.0375 - accuracy: 0.5945 - val_loss: 1.0546 - val_accuracy: 0.5900 - 4s/epoch - 4ms/step\n",
      "Epoch 43/10000\n",
      "1126/1126 - 5s - loss: 1.0236 - accuracy: 0.5885 - val_loss: 1.0367 - val_accuracy: 0.5855 - 5s/epoch - 5ms/step\n",
      "Epoch 44/10000\n",
      "1126/1126 - 6s - loss: 1.0067 - accuracy: 0.5958 - val_loss: 1.0406 - val_accuracy: 0.5919 - 6s/epoch - 5ms/step\n",
      "Epoch 45/10000\n",
      "1126/1126 - 3s - loss: 0.9878 - accuracy: 0.5943 - val_loss: 1.0315 - val_accuracy: 0.5937 - 3s/epoch - 3ms/step\n",
      "Epoch 46/10000\n",
      "1126/1126 - 3s - loss: 0.9812 - accuracy: 0.5990 - val_loss: 1.0095 - val_accuracy: 0.5733 - 3s/epoch - 3ms/step\n",
      "Epoch 47/10000\n",
      "1126/1126 - 6s - loss: 0.9743 - accuracy: 0.5981 - val_loss: 1.0407 - val_accuracy: 0.5943 - 6s/epoch - 6ms/step\n",
      "435/435 [==============================] - 3s 6ms/step - loss: 1.1200 - accuracy: 0.5937\n",
      "Larghezza: 1000 \n",
      "Profondità: 1 \n",
      "Dimensione del batch: 100\n",
      "Test loss (binary_crossentropy): 1.1200166940689087\n",
      "Test metrics (accuracy): 0.5936983227729797\n",
      "Epoch 1/10000\n",
      "113/113 - 4s - loss: 62.2141 - accuracy: 0.6113 - val_loss: 47.8735 - val_accuracy: 0.6130 - 4s/epoch - 36ms/step\n",
      "Epoch 2/10000\n",
      "113/113 - 2s - loss: 34.8360 - accuracy: 0.6020 - val_loss: 22.8258 - val_accuracy: 0.5729 - 2s/epoch - 15ms/step\n",
      "Epoch 3/10000\n",
      "113/113 - 2s - loss: 16.3509 - accuracy: 0.5300 - val_loss: 12.7316 - val_accuracy: 0.4924 - 2s/epoch - 14ms/step\n",
      "Epoch 4/10000\n",
      "113/113 - 2s - loss: 10.5431 - accuracy: 0.4884 - val_loss: 8.9319 - val_accuracy: 0.4912 - 2s/epoch - 14ms/step\n",
      "Epoch 5/10000\n",
      "113/113 - 1s - loss: 7.3962 - accuracy: 0.4989 - val_loss: 6.3932 - val_accuracy: 0.5005 - 1s/epoch - 13ms/step\n",
      "Epoch 6/10000\n",
      "113/113 - 2s - loss: 5.4109 - accuracy: 0.5071 - val_loss: 4.8239 - val_accuracy: 0.5080 - 2s/epoch - 13ms/step\n",
      "Epoch 7/10000\n",
      "113/113 - 3s - loss: 4.2345 - accuracy: 0.5167 - val_loss: 3.9132 - val_accuracy: 0.5156 - 3s/epoch - 23ms/step\n",
      "Epoch 8/10000\n",
      "113/113 - 2s - loss: 3.4504 - accuracy: 0.5274 - val_loss: 3.2521 - val_accuracy: 0.5346 - 2s/epoch - 21ms/step\n",
      "Epoch 9/10000\n",
      "113/113 - 2s - loss: 3.0451 - accuracy: 0.5377 - val_loss: 2.9631 - val_accuracy: 0.5375 - 2s/epoch - 22ms/step\n",
      "Epoch 10/10000\n",
      "113/113 - 2s - loss: 2.8000 - accuracy: 0.5412 - val_loss: 2.7413 - val_accuracy: 0.5381 - 2s/epoch - 18ms/step\n",
      "Epoch 11/10000\n",
      "113/113 - 3s - loss: 2.6311 - accuracy: 0.5427 - val_loss: 2.6254 - val_accuracy: 0.5380 - 3s/epoch - 29ms/step\n",
      "435/435 [==============================] - 3s 6ms/step - loss: 47.4010 - accuracy: 0.6151\n",
      "Larghezza: 1000 \n",
      "Profondità: 1 \n",
      "Dimensione del batch: 1000\n",
      "Test loss (binary_crossentropy): 47.401004791259766\n",
      "Test metrics (accuracy): 0.6150636672973633\n",
      "Epoch 1/10000\n",
      "11260/11260 - 281s - loss: 6353532.5000 - accuracy: 0.5666 - val_loss: 5517309.0000 - val_accuracy: 0.6140 - 281s/epoch - 25ms/step\n",
      "Epoch 2/10000\n",
      "11260/11260 - 280s - loss: 5138705.5000 - accuracy: 0.5680 - val_loss: 2317588.5000 - val_accuracy: 0.6304 - 280s/epoch - 25ms/step\n",
      "Epoch 3/10000\n",
      "11260/11260 - 336s - loss: 4773277.5000 - accuracy: 0.5743 - val_loss: 3928302.2500 - val_accuracy: 0.6182 - 336s/epoch - 30ms/step\n",
      "Epoch 4/10000\n",
      "11260/11260 - 518s - loss: 4856208.5000 - accuracy: 0.5766 - val_loss: 1726420.7500 - val_accuracy: 0.6345 - 518s/epoch - 46ms/step\n",
      "Epoch 5/10000\n",
      "11260/11260 - 407s - loss: 4980847.0000 - accuracy: 0.5781 - val_loss: 1860341.2500 - val_accuracy: 0.6422 - 407s/epoch - 36ms/step\n",
      "Epoch 6/10000\n",
      "11260/11260 - 553s - loss: 4722323.0000 - accuracy: 0.5761 - val_loss: 1697510.5000 - val_accuracy: 0.6227 - 553s/epoch - 49ms/step\n",
      "Epoch 7/10000\n",
      "11260/11260 - 428s - loss: 4693109.0000 - accuracy: 0.5795 - val_loss: 22884456.0000 - val_accuracy: 0.6140 - 428s/epoch - 38ms/step\n",
      "Epoch 8/10000\n",
      "11260/11260 - 429s - loss: 4683733.5000 - accuracy: 0.5804 - val_loss: 4787195.5000 - val_accuracy: 0.6220 - 429s/epoch - 38ms/step\n",
      "Epoch 9/10000\n",
      "11260/11260 - 422s - loss: 4258226.0000 - accuracy: 0.5800 - val_loss: 1796793.6250 - val_accuracy: 0.5601 - 422s/epoch - 37ms/step\n",
      "Epoch 10/10000\n",
      "11260/11260 - 404s - loss: 4621502.0000 - accuracy: 0.5827 - val_loss: 4230939.5000 - val_accuracy: 0.6234 - 404s/epoch - 36ms/step\n",
      "Epoch 11/10000\n",
      "11260/11260 - 355s - loss: 4381950.5000 - accuracy: 0.5853 - val_loss: 2772348.7500 - val_accuracy: 0.5562 - 355s/epoch - 32ms/step\n",
      "Epoch 12/10000\n",
      "11260/11260 - 378s - loss: 4064308.2500 - accuracy: 0.5864 - val_loss: 2273910.2500 - val_accuracy: 0.4877 - 378s/epoch - 34ms/step\n",
      "Epoch 13/10000\n",
      "11260/11260 - 422s - loss: 4275905.0000 - accuracy: 0.5843 - val_loss: 1403037.2500 - val_accuracy: 0.6465 - 422s/epoch - 37ms/step\n",
      "Epoch 14/10000\n",
      "11260/11260 - 392s - loss: 4277211.5000 - accuracy: 0.5864 - val_loss: 23016544.0000 - val_accuracy: 0.3916 - 392s/epoch - 35ms/step\n",
      "Epoch 15/10000\n",
      "11260/11260 - 342s - loss: 4319956.0000 - accuracy: 0.5880 - val_loss: 9627315.0000 - val_accuracy: 0.4142 - 342s/epoch - 30ms/step\n",
      "Epoch 16/10000\n",
      "11260/11260 - 340s - loss: 4431517.0000 - accuracy: 0.5884 - val_loss: 2180028.2500 - val_accuracy: 0.6455 - 340s/epoch - 30ms/step\n",
      "Epoch 17/10000\n",
      "11260/11260 - 341s - loss: 4190459.7500 - accuracy: 0.5876 - val_loss: 5310684.0000 - val_accuracy: 0.6214 - 341s/epoch - 30ms/step\n",
      "Epoch 18/10000\n",
      "11260/11260 - 443s - loss: 3962824.7500 - accuracy: 0.5874 - val_loss: 3390495.0000 - val_accuracy: 0.4778 - 443s/epoch - 39ms/step\n",
      "Epoch 19/10000\n",
      "11260/11260 - 445s - loss: 3922597.7500 - accuracy: 0.5906 - val_loss: 2618160.7500 - val_accuracy: 0.5327 - 445s/epoch - 39ms/step\n",
      "Epoch 20/10000\n",
      "11260/11260 - 363s - loss: 4060452.7500 - accuracy: 0.5903 - val_loss: 2145466.7500 - val_accuracy: 0.5704 - 363s/epoch - 32ms/step\n",
      "Epoch 21/10000\n",
      "11260/11260 - 357s - loss: 4028609.7500 - accuracy: 0.5899 - val_loss: 904587.1875 - val_accuracy: 0.6664 - 357s/epoch - 32ms/step\n",
      "Epoch 22/10000\n",
      "11260/11260 - 377s - loss: 3759605.5000 - accuracy: 0.5908 - val_loss: 3837236.0000 - val_accuracy: 0.6248 - 377s/epoch - 34ms/step\n",
      "Epoch 23/10000\n",
      "11260/11260 - 316s - loss: 3874290.2500 - accuracy: 0.5909 - val_loss: 1708255.1250 - val_accuracy: 0.6481 - 316s/epoch - 28ms/step\n",
      "Epoch 24/10000\n",
      "11260/11260 - 345s - loss: 3823916.5000 - accuracy: 0.5915 - val_loss: 1917135.1250 - val_accuracy: 0.6346 - 345s/epoch - 31ms/step\n",
      "Epoch 25/10000\n",
      "11260/11260 - 209s - loss: 3718275.2500 - accuracy: 0.5913 - val_loss: 2064736.8750 - val_accuracy: 0.5123 - 209s/epoch - 19ms/step\n",
      "Epoch 26/10000\n",
      "11260/11260 - 194s - loss: 3847912.5000 - accuracy: 0.5915 - val_loss: 2983030.5000 - val_accuracy: 0.5822 - 194s/epoch - 17ms/step\n",
      "Epoch 27/10000\n",
      "11260/11260 - 211s - loss: 3717633.2500 - accuracy: 0.5927 - val_loss: 2384475.2500 - val_accuracy: 0.6375 - 211s/epoch - 19ms/step\n",
      "Epoch 28/10000\n",
      "11260/11260 - 209s - loss: 3707388.7500 - accuracy: 0.5934 - val_loss: 1373809.6250 - val_accuracy: 0.6420 - 209s/epoch - 19ms/step\n",
      "Epoch 29/10000\n",
      "11260/11260 - 235s - loss: 3637121.0000 - accuracy: 0.5937 - val_loss: 1500702.3750 - val_accuracy: 0.6460 - 235s/epoch - 21ms/step\n",
      "Epoch 30/10000\n",
      "11260/11260 - 245s - loss: 3492125.0000 - accuracy: 0.5927 - val_loss: 28800472.0000 - val_accuracy: 0.6132 - 245s/epoch - 22ms/step\n",
      "Epoch 31/10000\n",
      "11260/11260 - 249s - loss: 3606787.2500 - accuracy: 0.5932 - val_loss: 3899445.7500 - val_accuracy: 0.4749 - 249s/epoch - 22ms/step\n",
      "435/435 [==============================] - 2s 5ms/step - loss: 889223.1250 - accuracy: 0.6690\n",
      "Larghezza: 1000 \n",
      "Profondità: 2 \n",
      "Dimensione del batch: 10\n",
      "Test loss (binary_crossentropy): 889223.125\n",
      "Test metrics (accuracy): 0.6690165996551514\n",
      "Epoch 1/10000\n",
      "1126/1126 - 34s - loss: 5612156.5000 - accuracy: 0.5728 - val_loss: 1009847.6875 - val_accuracy: 0.6002 - 34s/epoch - 31ms/step\n",
      "Epoch 2/10000\n",
      "1126/1126 - 34s - loss: 2507369.7500 - accuracy: 0.5706 - val_loss: 3347171.5000 - val_accuracy: 0.4366 - 34s/epoch - 30ms/step\n",
      "Epoch 3/10000\n",
      "1126/1126 - 27s - loss: 2443582.7500 - accuracy: 0.5758 - val_loss: 1879310.3750 - val_accuracy: 0.4563 - 27s/epoch - 24ms/step\n",
      "Epoch 4/10000\n",
      "1126/1126 - 28s - loss: 1946693.3750 - accuracy: 0.5724 - val_loss: 2810605.5000 - val_accuracy: 0.6162 - 28s/epoch - 24ms/step\n",
      "Epoch 5/10000\n",
      "1126/1126 - 27s - loss: 1909862.8750 - accuracy: 0.5772 - val_loss: 1454590.2500 - val_accuracy: 0.5264 - 27s/epoch - 24ms/step\n",
      "Epoch 6/10000\n",
      "1126/1126 - 27s - loss: 2056630.8750 - accuracy: 0.5767 - val_loss: 1032576.2500 - val_accuracy: 0.6314 - 27s/epoch - 24ms/step\n",
      "Epoch 7/10000\n",
      "1126/1126 - 27s - loss: 2506810.5000 - accuracy: 0.5744 - val_loss: 703516.8750 - val_accuracy: 0.6418 - 27s/epoch - 24ms/step\n",
      "Epoch 8/10000\n",
      "1126/1126 - 27s - loss: 2133754.0000 - accuracy: 0.5763 - val_loss: 1102175.5000 - val_accuracy: 0.6346 - 27s/epoch - 24ms/step\n",
      "Epoch 9/10000\n",
      "1126/1126 - 28s - loss: 2155664.2500 - accuracy: 0.5761 - val_loss: 2278766.5000 - val_accuracy: 0.4558 - 28s/epoch - 25ms/step\n",
      "Epoch 10/10000\n",
      "1126/1126 - 28s - loss: 2234266.5000 - accuracy: 0.5769 - val_loss: 1543118.3750 - val_accuracy: 0.6309 - 28s/epoch - 25ms/step\n",
      "Epoch 11/10000\n",
      "1126/1126 - 26s - loss: 2172925.5000 - accuracy: 0.5810 - val_loss: 1197438.2500 - val_accuracy: 0.5858 - 26s/epoch - 23ms/step\n",
      "Epoch 12/10000\n",
      "1126/1126 - 25s - loss: 2092467.2500 - accuracy: 0.5818 - val_loss: 639314.3750 - val_accuracy: 0.6418 - 25s/epoch - 23ms/step\n",
      "Epoch 13/10000\n",
      "1126/1126 - 26s - loss: 2428037.7500 - accuracy: 0.5803 - val_loss: 1513800.8750 - val_accuracy: 0.6292 - 26s/epoch - 23ms/step\n",
      "Epoch 14/10000\n",
      "1126/1126 - 25s - loss: 2109964.0000 - accuracy: 0.5823 - val_loss: 3422445.5000 - val_accuracy: 0.6161 - 25s/epoch - 22ms/step\n",
      "Epoch 15/10000\n",
      "1126/1126 - 25s - loss: 2137632.0000 - accuracy: 0.5817 - val_loss: 2489247.0000 - val_accuracy: 0.6236 - 25s/epoch - 23ms/step\n",
      "Epoch 16/10000\n",
      "1126/1126 - 26s - loss: 2179931.7500 - accuracy: 0.5826 - val_loss: 3465332.0000 - val_accuracy: 0.6203 - 26s/epoch - 23ms/step\n",
      "Epoch 17/10000\n",
      "1126/1126 - 25s - loss: 2097582.7500 - accuracy: 0.5829 - val_loss: 3867909.2500 - val_accuracy: 0.6157 - 25s/epoch - 22ms/step\n",
      "435/435 [==============================] - 2s 4ms/step - loss: 695034.4375 - accuracy: 0.6444\n",
      "Larghezza: 1000 \n",
      "Profondità: 2 \n",
      "Dimensione del batch: 100\n",
      "Test loss (binary_crossentropy): 695034.4375\n",
      "Test metrics (accuracy): 0.6444140672683716\n",
      "Epoch 1/10000\n",
      "113/113 - 7s - loss: 12946894.0000 - accuracy: 0.4837 - val_loss: 2463616.7500 - val_accuracy: 0.5515 - 7s/epoch - 64ms/step\n",
      "Epoch 2/10000\n",
      "113/113 - 6s - loss: 4261737.0000 - accuracy: 0.5442 - val_loss: 5835738.5000 - val_accuracy: 0.6031 - 6s/epoch - 56ms/step\n",
      "Epoch 3/10000\n",
      "113/113 - 7s - loss: 2198764.5000 - accuracy: 0.5744 - val_loss: 2091046.1250 - val_accuracy: 0.6012 - 7s/epoch - 65ms/step\n",
      "Epoch 4/10000\n",
      "113/113 - 8s - loss: 2224213.0000 - accuracy: 0.5688 - val_loss: 854346.5000 - val_accuracy: 0.6237 - 8s/epoch - 70ms/step\n",
      "Epoch 5/10000\n",
      "113/113 - 6s - loss: 3748175.2500 - accuracy: 0.5653 - val_loss: 1677485.6250 - val_accuracy: 0.6138 - 6s/epoch - 56ms/step\n",
      "Epoch 6/10000\n",
      "113/113 - 7s - loss: 1983248.2500 - accuracy: 0.5778 - val_loss: 754958.5000 - val_accuracy: 0.5908 - 7s/epoch - 60ms/step\n",
      "Epoch 7/10000\n",
      "113/113 - 7s - loss: 1975697.5000 - accuracy: 0.5701 - val_loss: 7737290.0000 - val_accuracy: 0.4163 - 7s/epoch - 59ms/step\n",
      "Epoch 8/10000\n",
      "113/113 - 7s - loss: 3180815.5000 - accuracy: 0.5634 - val_loss: 2026761.3750 - val_accuracy: 0.5634 - 7s/epoch - 61ms/step\n",
      "Epoch 9/10000\n",
      "113/113 - 7s - loss: 2955921.0000 - accuracy: 0.5678 - val_loss: 3373186.5000 - val_accuracy: 0.5171 - 7s/epoch - 60ms/step\n",
      "Epoch 10/10000\n",
      "113/113 - 7s - loss: 2274466.7500 - accuracy: 0.5759 - val_loss: 1027784.3125 - val_accuracy: 0.6313 - 7s/epoch - 61ms/step\n",
      "Epoch 11/10000\n",
      "113/113 - 7s - loss: 1930026.1250 - accuracy: 0.5751 - val_loss: 607957.5625 - val_accuracy: 0.6354 - 7s/epoch - 61ms/step\n",
      "Epoch 12/10000\n",
      "113/113 - 7s - loss: 2395240.2500 - accuracy: 0.5683 - val_loss: 4037227.5000 - val_accuracy: 0.6131 - 7s/epoch - 58ms/step\n",
      "Epoch 13/10000\n",
      "113/113 - 7s - loss: 1911122.5000 - accuracy: 0.5772 - val_loss: 4121758.2500 - val_accuracy: 0.6202 - 7s/epoch - 61ms/step\n",
      "Epoch 14/10000\n",
      "113/113 - 7s - loss: 2730495.0000 - accuracy: 0.5712 - val_loss: 510418.7812 - val_accuracy: 0.6251 - 7s/epoch - 60ms/step\n",
      "Epoch 15/10000\n",
      "113/113 - 7s - loss: 2153458.0000 - accuracy: 0.5723 - val_loss: 601549.0000 - val_accuracy: 0.6426 - 7s/epoch - 58ms/step\n",
      "Epoch 16/10000\n",
      "113/113 - 7s - loss: 2279966.0000 - accuracy: 0.5725 - val_loss: 1950091.8750 - val_accuracy: 0.5877 - 7s/epoch - 60ms/step\n",
      "Epoch 17/10000\n",
      "113/113 - 7s - loss: 2575077.7500 - accuracy: 0.5751 - val_loss: 1043421.6250 - val_accuracy: 0.6327 - 7s/epoch - 60ms/step\n",
      "Epoch 18/10000\n",
      "113/113 - 7s - loss: 2040556.2500 - accuracy: 0.5783 - val_loss: 1461069.8750 - val_accuracy: 0.6311 - 7s/epoch - 62ms/step\n",
      "Epoch 19/10000\n",
      "113/113 - 9s - loss: 2841780.0000 - accuracy: 0.5693 - val_loss: 6507834.5000 - val_accuracy: 0.4180 - 9s/epoch - 77ms/step\n",
      "Epoch 20/10000\n",
      "113/113 - 13s - loss: 1365928.5000 - accuracy: 0.5826 - val_loss: 2131343.2500 - val_accuracy: 0.4379 - 13s/epoch - 119ms/step\n",
      "Epoch 21/10000\n",
      "113/113 - 12s - loss: 1882735.0000 - accuracy: 0.5681 - val_loss: 6713690.0000 - val_accuracy: 0.6127 - 12s/epoch - 104ms/step\n",
      "Epoch 22/10000\n",
      "113/113 - 12s - loss: 2668111.2500 - accuracy: 0.5758 - val_loss: 1006910.3125 - val_accuracy: 0.6298 - 12s/epoch - 105ms/step\n",
      "Epoch 23/10000\n",
      "113/113 - 12s - loss: 2862477.0000 - accuracy: 0.5706 - val_loss: 4296095.5000 - val_accuracy: 0.6130 - 12s/epoch - 102ms/step\n",
      "Epoch 24/10000\n",
      "113/113 - 11s - loss: 2792062.5000 - accuracy: 0.5739 - val_loss: 2266693.2500 - val_accuracy: 0.6180 - 11s/epoch - 93ms/step\n",
      "Epoch 25/10000\n",
      "113/113 - 11s - loss: 2275532.7500 - accuracy: 0.5742 - val_loss: 2771096.7500 - val_accuracy: 0.6217 - 11s/epoch - 94ms/step\n",
      "435/435 [==============================] - 3s 6ms/step - loss: 600719.3125 - accuracy: 0.6405\n",
      "Larghezza: 1000 \n",
      "Profondità: 2 \n",
      "Dimensione del batch: 1000\n",
      "Test loss (binary_crossentropy): 600719.3125\n",
      "Test metrics (accuracy): 0.6404575109481812\n",
      "Epoch 1/10000\n",
      "11260/11260 - 317s - loss: 2.9228 - accuracy: 0.5639 - val_loss: 2.3639 - val_accuracy: 0.5777 - 317s/epoch - 28ms/step\n",
      "Epoch 2/10000\n",
      "11260/11260 - 261s - loss: 2.0694 - accuracy: 0.5703 - val_loss: 1.7896 - val_accuracy: 0.5571 - 261s/epoch - 23ms/step\n",
      "Epoch 3/10000\n",
      "11260/11260 - 271s - loss: 1.6565 - accuracy: 0.5671 - val_loss: 1.4218 - val_accuracy: 0.5649 - 271s/epoch - 24ms/step\n",
      "Epoch 4/10000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\qj1aleimbria\\Desktop\\File VS Code\\11 - 16 10 23 (ML su Prestitempo)\\Reti neurali custom con più variabili.ipynb Cell 99\u001b[0m line \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/qj1aleimbria/Desktop/File%20VS%20Code/11%20-%2016%2010%2023%20%28ML%20su%20Prestitempo%29/Reti%20neurali%20custom%20con%20pi%C3%B9%20variabili.ipynb#Y166sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m early_stopping \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mcallbacks\u001b[39m.\u001b[39mEarlyStopping(monitor\u001b[39m=\u001b[39mmonitor,patience\u001b[39m=\u001b[39mpatience, restore_best_weights\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/qj1aleimbria/Desktop/File%20VS%20Code/11%20-%2016%2010%2023%20%28ML%20su%20Prestitempo%29/Reti%20neurali%20custom%20con%20pi%C3%B9%20variabili.ipynb#Y166sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m \u001b[39m# Allenamento del modello\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/qj1aleimbria/Desktop/File%20VS%20Code/11%20-%2016%2010%2023%20%28ML%20su%20Prestitempo%29/Reti%20neurali%20custom%20con%20pi%C3%B9%20variabili.ipynb#Y166sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(X_train,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/qj1aleimbria/Desktop/File%20VS%20Code/11%20-%2016%2010%2023%20%28ML%20su%20Prestitempo%29/Reti%20neurali%20custom%20con%20pi%C3%B9%20variabili.ipynb#Y166sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m         y_train,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/qj1aleimbria/Desktop/File%20VS%20Code/11%20-%2016%2010%2023%20%28ML%20su%20Prestitempo%29/Reti%20neurali%20custom%20con%20pi%C3%B9%20variabili.ipynb#Y166sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m         batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/qj1aleimbria/Desktop/File%20VS%20Code/11%20-%2016%2010%2023%20%28ML%20su%20Prestitempo%29/Reti%20neurali%20custom%20con%20pi%C3%B9%20variabili.ipynb#Y166sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m         epochs\u001b[39m=\u001b[39;49mmax_epochs,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/qj1aleimbria/Desktop/File%20VS%20Code/11%20-%2016%2010%2023%20%28ML%20su%20Prestitempo%29/Reti%20neurali%20custom%20con%20pi%C3%B9%20variabili.ipynb#Y166sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m         callbacks\u001b[39m=\u001b[39;49m[early_stopping],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/qj1aleimbria/Desktop/File%20VS%20Code/11%20-%2016%2010%2023%20%28ML%20su%20Prestitempo%29/Reti%20neurali%20custom%20con%20pi%C3%B9%20variabili.ipynb#Y166sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m         validation_data\u001b[39m=\u001b[39;49m(X_val, y_val),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/qj1aleimbria/Desktop/File%20VS%20Code/11%20-%2016%2010%2023%20%28ML%20su%20Prestitempo%29/Reti%20neurali%20custom%20con%20pi%C3%B9%20variabili.ipynb#Y166sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m         verbose\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/qj1aleimbria/Desktop/File%20VS%20Code/11%20-%2016%2010%2023%20%28ML%20su%20Prestitempo%29/Reti%20neurali%20custom%20con%20pi%C3%B9%20variabili.ipynb#Y166sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m test_loss, test_metrics \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mevaluate(X_test, y_test)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/qj1aleimbria/Desktop/File%20VS%20Code/11%20-%2016%2010%2023%20%28ML%20su%20Prestitempo%29/Reti%20neurali%20custom%20con%20pi%C3%B9%20variabili.ipynb#Y166sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLarghezza: \u001b[39m\u001b[39m{\u001b[39;00mhidden_layer_size\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mProfondità: \u001b[39m\u001b[39m{\u001b[39;00mnum_hidden_layers\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mDimensione del batch: \u001b[39m\u001b[39m{\u001b[39;00mbatch_size\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mTest loss (\u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m}\u001b[39;00m\u001b[39m): \u001b[39m\u001b[39m{\u001b[39;00mtest_loss\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mTest metrics (\u001b[39m\u001b[39m{\u001b[39;00mmetrics\u001b[39m}\u001b[39;00m\u001b[39m): \u001b[39m\u001b[39m{\u001b[39;00mtest_metrics\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\qj1aleimbria\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\qj1aleimbria\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py:1783\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1775\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1776\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1777\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1780\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1781\u001b[0m ):\n\u001b[0;32m   1782\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1783\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1784\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1785\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\qj1aleimbria\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\qj1aleimbria\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:831\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    828\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    830\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 831\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    833\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    834\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\qj1aleimbria\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:867\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    864\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    865\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    866\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 867\u001b[0m   \u001b[39mreturn\u001b[39;00m tracing_compilation\u001b[39m.\u001b[39;49mcall_function(\n\u001b[0;32m    868\u001b[0m       args, kwds, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_config\n\u001b[0;32m    869\u001b[0m   )\n\u001b[0;32m    870\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_config \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    871\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    872\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    873\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\qj1aleimbria\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[39m=\u001b[39m function\u001b[39m.\u001b[39mfunction_type\u001b[39m.\u001b[39mbind(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[39m=\u001b[39m function\u001b[39m.\u001b[39mfunction_type\u001b[39m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[39mreturn\u001b[39;00m function\u001b[39m.\u001b[39;49m_call_flat(  \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m     flat_inputs, captured_inputs\u001b[39m=\u001b[39;49mfunction\u001b[39m.\u001b[39;49mcaptured_inputs\n\u001b[0;32m    141\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\qj1aleimbria\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1264\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1260\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1261\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1262\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1263\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1264\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mflat_call(args)\n\u001b[0;32m   1265\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1266\u001b[0m     args,\n\u001b[0;32m   1267\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1268\u001b[0m     executing_eagerly)\n\u001b[0;32m   1269\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\qj1aleimbria\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:217\u001b[0m, in \u001b[0;36mAtomicFunction.flat_call\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mflat_call\u001b[39m(\u001b[39mself\u001b[39m, args: Sequence[core\u001b[39m.\u001b[39mTensor]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    216\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Calls with tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 217\u001b[0m   flat_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\u001b[39m*\u001b[39;49margs)\n\u001b[0;32m    218\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction_type\u001b[39m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32mc:\\Users\\qj1aleimbria\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:252\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[39mwith\u001b[39;00m record\u001b[39m.\u001b[39mstop_recording():\n\u001b[0;32m    251\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 252\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_bound_context\u001b[39m.\u001b[39;49mcall_function(\n\u001b[0;32m    253\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname,\n\u001b[0;32m    254\u001b[0m         \u001b[39mlist\u001b[39;49m(args),\n\u001b[0;32m    255\u001b[0m         \u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction_type\u001b[39m.\u001b[39;49mflat_outputs),\n\u001b[0;32m    256\u001b[0m     )\n\u001b[0;32m    257\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    258\u001b[0m     outputs \u001b[39m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    259\u001b[0m         \u001b[39mself\u001b[39m,\n\u001b[0;32m    260\u001b[0m         \u001b[39mlist\u001b[39m(args),\n\u001b[0;32m    261\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mfunction_call_options\u001b[39m.\u001b[39mas_attrs(),\n\u001b[0;32m    262\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\qj1aleimbria\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\context.py:1479\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1477\u001b[0m cancellation_context \u001b[39m=\u001b[39m cancellation\u001b[39m.\u001b[39mcontext()\n\u001b[0;32m   1478\u001b[0m \u001b[39mif\u001b[39;00m cancellation_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1479\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m   1480\u001b[0m       name\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1481\u001b[0m       num_outputs\u001b[39m=\u001b[39;49mnum_outputs,\n\u001b[0;32m   1482\u001b[0m       inputs\u001b[39m=\u001b[39;49mtensor_inputs,\n\u001b[0;32m   1483\u001b[0m       attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m   1484\u001b[0m       ctx\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m   1485\u001b[0m   )\n\u001b[0;32m   1486\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1487\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1488\u001b[0m       name\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m   1489\u001b[0m       num_outputs\u001b[39m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1493\u001b[0m       cancellation_manager\u001b[39m=\u001b[39mcancellation_context,\n\u001b[0;32m   1494\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\qj1aleimbria\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:60\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     53\u001b[0m   \u001b[39m# Convert any objects of type core_types.Tensor to Tensor.\u001b[39;00m\n\u001b[0;32m     54\u001b[0m   inputs \u001b[39m=\u001b[39m [\n\u001b[0;32m     55\u001b[0m       tensor_conversion_registry\u001b[39m.\u001b[39mconvert(t)\n\u001b[0;32m     56\u001b[0m       \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(t, core_types\u001b[39m.\u001b[39mTensor)\n\u001b[0;32m     57\u001b[0m       \u001b[39melse\u001b[39;00m t\n\u001b[0;32m     58\u001b[0m       \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m inputs\n\u001b[0;32m     59\u001b[0m   ]\n\u001b[1;32m---> 60\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     61\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     62\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     63\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "output_size = 1\n",
    "\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Parametri della griglia\n",
    "param_grid = {\n",
    "    'hidden_layer_size': [50, 200, 500, 1000],\n",
    "    'minval': [-10],\n",
    "    'maxval': [10],\n",
    "    'num_hidden_layers': [1, 2, 3],\n",
    "    'activation_hidden': ['relu', 'sigmoid'],\n",
    "    'activation_output': ['sigmoid'],\n",
    "    'batch_size': [10, 100, 1000],\n",
    "    'epochs': [10000],\n",
    "    'monitor': ['val_accuracy'],\n",
    "    'patience': [10],\n",
    "    'optimizer': ['adam'],\n",
    "    'loss': ['binary_crossentropy'],\n",
    "    'metrics': ['accuracy']\n",
    "}\n",
    "\n",
    "# Itera su tutte le combinazioni dei parametri\n",
    "all_params = [dict(zip(param_grid.keys(), v)) for v in itertools.product(*param_grid.values())]\n",
    "\n",
    "# Creo una nuova cartella per salvare i modelli, con un nuovo nome se la cartella esiste già\n",
    "current_time = datetime.datetime.now().strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
    "model_dir = 'Modelli con reti neurali'\n",
    "counter = 1\n",
    "\n",
    "# Elenco di tutte le directory nella directory corrente\n",
    "existing_dirs = os.listdir()\n",
    "\n",
    "while any(name.startswith(f\"{counter} - Modelli con reti neurali\") for name in existing_dirs):\n",
    "    counter += 1\n",
    "\n",
    "new_dir = f'{counter} - {model_dir} {current_time} (custom+)'\n",
    "\n",
    "os.makedirs(new_dir)\n",
    "\n",
    "\n",
    "\n",
    "for i, params in enumerate(all_params):\n",
    "        \n",
    "    hidden_layer_size, minval, maxval, num_hidden_layers, activation_hidden, activation_output, batch_size, max_epochs, monitor, patience, optimizer, loss, metrics = tuple(params.values())\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "    for _ in range(num_hidden_layers):\n",
    "        model.add(tf.keras.layers.Dense(hidden_layer_size,\n",
    "                                        activation=activation_hidden,\n",
    "                                        kernel_initializer=tf.random_uniform_initializer(minval=minval, maxval=maxval),\n",
    "                                        bias_initializer=tf.random_uniform_initializer(minval=minval, maxval=maxval)))\n",
    "    model.add(tf.keras.layers.Dense(output_size, activation=activation_output,\n",
    "                                    kernel_initializer=tf.random_uniform_initializer(minval=minval, maxval=maxval),\n",
    "                                    bias_initializer=tf.random_uniform_initializer(minval=minval, maxval=maxval)))\n",
    "\n",
    "    # Compilazione del modello\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=[metrics])\n",
    "\n",
    "    # Callback per l'early stopping\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor=monitor,patience=patience, restore_best_weights=True)\n",
    "\n",
    "    # Allenamento del modello\n",
    "    model.fit(X_train,\n",
    "            y_train,\n",
    "            batch_size=batch_size,\n",
    "            epochs=max_epochs,\n",
    "            callbacks=[early_stopping],\n",
    "            validation_data=(X_val, y_val),\n",
    "            verbose=2)\n",
    "\n",
    "    test_loss, test_metrics = model.evaluate(X_test, y_test)\n",
    "    print(f\"Larghezza: {hidden_layer_size} \\nProfondità: {num_hidden_layers} \\nDimensione del batch: {batch_size}\\nTest loss ({loss}): {test_loss}\\nTest metrics ({metrics}): {test_metrics}\")\n",
    "\n",
    "    # Salvataggio del modello\n",
    "    current_time = datetime.datetime.now().strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
    "    model_name = f\"{i+1} - A{round(test_metrics*100,2)}%, L{hidden_layer_size}, P{num_hidden_layers}, AH{activation_hidden}, AO{activation_output}, B{batch_size} ({current_time}).h5\"\n",
    "    model_path = os.path.join(new_dir, model_name)\n",
    "    model.save(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6> Grid search con varie metriche </h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Short list usata:\n",
      "['Target_95', 'Denominazione Regione', 'AgeingGestioneGg', 'DESCRIZIONE PRODOTTO', 'NUMERO RATE', 'Ratio_Rate_Imp1', 'TOT_AFF', 'AgeingErogazioneGg', 'Eta_Debitore', 'IMP_FINANZIATO', 'RATIO_ONERI_AFF_TOT_AFF']\n",
      "\n",
      "Epoch 1/10000\n",
      "1126/1126 - 3s - loss: 167172.3125 - accuracy: 0.4876 - val_loss: 28595.2949 - val_accuracy: 0.5823 - 3s/epoch - 3ms/step\n",
      "Epoch 2/10000\n",
      "1126/1126 - 3s - loss: 21827.8359 - accuracy: 0.5875 - val_loss: 14558.7607 - val_accuracy: 0.5870 - 3s/epoch - 2ms/step\n",
      "Epoch 3/10000\n",
      "1126/1126 - 3s - loss: 7166.3369 - accuracy: 0.5984 - val_loss: 1690.6709 - val_accuracy: 0.6018 - 3s/epoch - 2ms/step\n",
      "Epoch 4/10000\n",
      "1126/1126 - 2s - loss: 1324.0790 - accuracy: 0.5957 - val_loss: 1075.2994 - val_accuracy: 0.5996 - 2s/epoch - 2ms/step\n",
      "Epoch 5/10000\n",
      "1126/1126 - 2s - loss: 736.2258 - accuracy: 0.5882 - val_loss: 457.2039 - val_accuracy: 0.5932 - 2s/epoch - 2ms/step\n",
      "Epoch 6/10000\n",
      "1126/1126 - 2s - loss: 322.7021 - accuracy: 0.5592 - val_loss: 234.4862 - val_accuracy: 0.5482 - 2s/epoch - 2ms/step\n",
      "Epoch 7/10000\n",
      "1126/1126 - 2s - loss: 228.7322 - accuracy: 0.5449 - val_loss: 213.0856 - val_accuracy: 0.5051 - 2s/epoch - 2ms/step\n",
      "Epoch 8/10000\n",
      "1126/1126 - 2s - loss: 182.0831 - accuracy: 0.5471 - val_loss: 161.1973 - val_accuracy: 0.5796 - 2s/epoch - 2ms/step\n",
      "Epoch 9/10000\n",
      "1126/1126 - 2s - loss: 148.1292 - accuracy: 0.5507 - val_loss: 114.8390 - val_accuracy: 0.5670 - 2s/epoch - 2ms/step\n",
      "Epoch 10/10000\n",
      "1126/1126 - 2s - loss: 128.4490 - accuracy: 0.5533 - val_loss: 153.1024 - val_accuracy: 0.6038 - 2s/epoch - 2ms/step\n",
      "Epoch 11/10000\n",
      "1126/1126 - 2s - loss: 108.4887 - accuracy: 0.5587 - val_loss: 128.7128 - val_accuracy: 0.4701 - 2s/epoch - 2ms/step\n",
      "Epoch 12/10000\n",
      "1126/1126 - 2s - loss: 100.5323 - accuracy: 0.5626 - val_loss: 116.3730 - val_accuracy: 0.4772 - 2s/epoch - 2ms/step\n",
      "Epoch 13/10000\n",
      "1126/1126 - 2s - loss: 95.9205 - accuracy: 0.5677 - val_loss: 68.3674 - val_accuracy: 0.6141 - 2s/epoch - 2ms/step\n",
      "Epoch 14/10000\n",
      "1126/1126 - 2s - loss: 87.7194 - accuracy: 0.5741 - val_loss: 53.6418 - val_accuracy: 0.5800 - 2s/epoch - 2ms/step\n",
      "Epoch 15/10000\n",
      "1126/1126 - 2s - loss: 83.5084 - accuracy: 0.5741 - val_loss: 46.0751 - val_accuracy: 0.6295 - 2s/epoch - 2ms/step\n",
      "Epoch 16/10000\n",
      "1126/1126 - 2s - loss: 82.9429 - accuracy: 0.5752 - val_loss: 85.2272 - val_accuracy: 0.4916 - 2s/epoch - 2ms/step\n",
      "Epoch 17/10000\n",
      "1126/1126 - 2s - loss: 86.9176 - accuracy: 0.5774 - val_loss: 70.2356 - val_accuracy: 0.4981 - 2s/epoch - 2ms/step\n",
      "Epoch 18/10000\n",
      "1126/1126 - 2s - loss: 88.2232 - accuracy: 0.5754 - val_loss: 246.5396 - val_accuracy: 0.6167 - 2s/epoch - 2ms/step\n",
      "Epoch 19/10000\n",
      "1126/1126 - 2s - loss: 82.4780 - accuracy: 0.5781 - val_loss: 42.4199 - val_accuracy: 0.6370 - 2s/epoch - 2ms/step\n",
      "Epoch 20/10000\n",
      "1126/1126 - 2s - loss: 83.2954 - accuracy: 0.5789 - val_loss: 39.8559 - val_accuracy: 0.6127 - 2s/epoch - 2ms/step\n",
      "Epoch 21/10000\n",
      "1126/1126 - 2s - loss: 79.5926 - accuracy: 0.5778 - val_loss: 55.4967 - val_accuracy: 0.6313 - 2s/epoch - 2ms/step\n",
      "Epoch 22/10000\n",
      "1126/1126 - 2s - loss: 81.6585 - accuracy: 0.5795 - val_loss: 113.4204 - val_accuracy: 0.6179 - 2s/epoch - 2ms/step\n",
      "Epoch 23/10000\n",
      "1126/1126 - 2s - loss: 70.2491 - accuracy: 0.5828 - val_loss: 35.7774 - val_accuracy: 0.6370 - 2s/epoch - 2ms/step\n",
      "Epoch 24/10000\n",
      "1126/1126 - 2s - loss: 78.8878 - accuracy: 0.5793 - val_loss: 35.3628 - val_accuracy: 0.6422 - 2s/epoch - 2ms/step\n",
      "Epoch 25/10000\n",
      "1126/1126 - 2s - loss: 75.6848 - accuracy: 0.5800 - val_loss: 189.8974 - val_accuracy: 0.4383 - 2s/epoch - 2ms/step\n",
      "Epoch 26/10000\n",
      "1126/1126 - 2s - loss: 72.7892 - accuracy: 0.5813 - val_loss: 90.6769 - val_accuracy: 0.6194 - 2s/epoch - 2ms/step\n",
      "Epoch 27/10000\n",
      "1126/1126 - 2s - loss: 78.6158 - accuracy: 0.5790 - val_loss: 108.1364 - val_accuracy: 0.4908 - 2s/epoch - 2ms/step\n",
      "Epoch 28/10000\n",
      "1126/1126 - 2s - loss: 76.7788 - accuracy: 0.5813 - val_loss: 43.7557 - val_accuracy: 0.5550 - 2s/epoch - 2ms/step\n",
      "Epoch 29/10000\n",
      "1126/1126 - 2s - loss: 77.3207 - accuracy: 0.5775 - val_loss: 61.0742 - val_accuracy: 0.6268 - 2s/epoch - 2ms/step\n",
      "Epoch 30/10000\n",
      "1126/1126 - 2s - loss: 79.5737 - accuracy: 0.5785 - val_loss: 35.4380 - val_accuracy: 0.5887 - 2s/epoch - 2ms/step\n",
      "Epoch 31/10000\n",
      "1126/1126 - 2s - loss: 73.1954 - accuracy: 0.5789 - val_loss: 138.7615 - val_accuracy: 0.4888 - 2s/epoch - 2ms/step\n",
      "Epoch 32/10000\n",
      "1126/1126 - 2s - loss: 77.4289 - accuracy: 0.5787 - val_loss: 46.7544 - val_accuracy: 0.6282 - 2s/epoch - 2ms/step\n",
      "Epoch 33/10000\n",
      "1126/1126 - 2s - loss: 75.6927 - accuracy: 0.5784 - val_loss: 99.3962 - val_accuracy: 0.6183 - 2s/epoch - 2ms/step\n",
      "Epoch 34/10000\n",
      "1126/1126 - 2s - loss: 78.1155 - accuracy: 0.5783 - val_loss: 29.6077 - val_accuracy: 0.6594 - 2s/epoch - 2ms/step\n",
      "Epoch 35/10000\n",
      "1126/1126 - 2s - loss: 74.9447 - accuracy: 0.5788 - val_loss: 36.3019 - val_accuracy: 0.6420 - 2s/epoch - 2ms/step\n",
      "Epoch 36/10000\n",
      "1126/1126 - 2s - loss: 77.8558 - accuracy: 0.5798 - val_loss: 39.3330 - val_accuracy: 0.6333 - 2s/epoch - 2ms/step\n",
      "Epoch 37/10000\n",
      "1126/1126 - 2s - loss: 71.7702 - accuracy: 0.5817 - val_loss: 37.0933 - val_accuracy: 0.6426 - 2s/epoch - 2ms/step\n",
      "Epoch 38/10000\n",
      "1126/1126 - 2s - loss: 70.8776 - accuracy: 0.5789 - val_loss: 75.1521 - val_accuracy: 0.5077 - 2s/epoch - 2ms/step\n",
      "Epoch 39/10000\n",
      "1126/1126 - 2s - loss: 73.7545 - accuracy: 0.5782 - val_loss: 66.9516 - val_accuracy: 0.5257 - 2s/epoch - 2ms/step\n",
      "Epoch 40/10000\n",
      "1126/1126 - 2s - loss: 74.8998 - accuracy: 0.5784 - val_loss: 28.5446 - val_accuracy: 0.6492 - 2s/epoch - 2ms/step\n",
      "Epoch 41/10000\n",
      "1126/1126 - 2s - loss: 71.2044 - accuracy: 0.5805 - val_loss: 59.7511 - val_accuracy: 0.6207 - 2s/epoch - 2ms/step\n",
      "Epoch 42/10000\n",
      "1126/1126 - 2s - loss: 63.8063 - accuracy: 0.5810 - val_loss: 37.7621 - val_accuracy: 0.6353 - 2s/epoch - 2ms/step\n",
      "Epoch 43/10000\n",
      "1126/1126 - 2s - loss: 65.3763 - accuracy: 0.5795 - val_loss: 29.5882 - val_accuracy: 0.5699 - 2s/epoch - 2ms/step\n",
      "Epoch 44/10000\n",
      "1126/1126 - 2s - loss: 70.4193 - accuracy: 0.5791 - val_loss: 70.6024 - val_accuracy: 0.5044 - 2s/epoch - 2ms/step\n",
      "391/391 [==============================] - 1s 1ms/step\n",
      "\n",
      "Iterazione 1/128 (03-11-2023_10-15-29)\n",
      "hidden_layer_size: 20\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 1\n",
      "activation_hidden: relu\n",
      "activation_output: sigmoid\n",
      "batch_size: 100\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 10\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 65.94%\n",
      "La recall di questo modello sul validation set è: 49.82%\n",
      "La f1 di questo modello sul validation set è: 53.1%\n",
      "La balanced accuracy di questo modello sul validation set è: 62.97%\n",
      "La precision di questo modello sul validation set è: 56.83%\n",
      "La AUC di questo modello sul validation set è: 68.11%\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.71    0.76      0.73  7,670.00\n",
      "1                  0.57    0.50      0.53  4,841.00\n",
      "accuracy           0.66    0.66      0.66      0.66\n",
      "macro avg          0.64    0.63      0.63 12,511.00\n",
      "weighted avg       0.65    0.66      0.65 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         5838         1832\n",
      "Actual 1         2429         2412\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 65.94% con il modello:\n",
      "1 - AC65.94%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 49.82% con il modello:\n",
      "1 - RE49.82%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 53.1% con il modello:\n",
      "1 - F153.1%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 62.97% con il modello:\n",
      "1 - BA62.97%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 56.83% con il modello:\n",
      "1 - PR56.83%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "1126/1126 - 3s - loss: 167172.3125 - accuracy: 0.4876 - val_loss: 28595.2949 - val_accuracy: 0.5823 - 3s/epoch - 3ms/step\n",
      "Epoch 2/10000\n",
      "1126/1126 - 2s - loss: 21827.8359 - accuracy: 0.5875 - val_loss: 14558.7607 - val_accuracy: 0.5870 - 2s/epoch - 2ms/step\n",
      "Epoch 3/10000\n",
      "1126/1126 - 2s - loss: 7166.3369 - accuracy: 0.5984 - val_loss: 1690.6709 - val_accuracy: 0.6018 - 2s/epoch - 2ms/step\n",
      "Epoch 4/10000\n",
      "1126/1126 - 2s - loss: 1324.0790 - accuracy: 0.5957 - val_loss: 1075.2994 - val_accuracy: 0.5996 - 2s/epoch - 2ms/step\n",
      "Epoch 5/10000\n",
      "1126/1126 - 2s - loss: 736.2258 - accuracy: 0.5882 - val_loss: 457.2039 - val_accuracy: 0.5932 - 2s/epoch - 2ms/step\n",
      "Epoch 6/10000\n",
      "1126/1126 - 2s - loss: 322.7021 - accuracy: 0.5592 - val_loss: 234.4862 - val_accuracy: 0.5482 - 2s/epoch - 2ms/step\n",
      "Epoch 7/10000\n",
      "1126/1126 - 2s - loss: 228.7322 - accuracy: 0.5449 - val_loss: 213.0856 - val_accuracy: 0.5051 - 2s/epoch - 2ms/step\n",
      "Epoch 8/10000\n",
      "1126/1126 - 2s - loss: 182.0831 - accuracy: 0.5471 - val_loss: 161.1973 - val_accuracy: 0.5796 - 2s/epoch - 2ms/step\n",
      "Epoch 9/10000\n",
      "1126/1126 - 2s - loss: 148.1292 - accuracy: 0.5507 - val_loss: 114.8390 - val_accuracy: 0.5670 - 2s/epoch - 2ms/step\n",
      "Epoch 10/10000\n",
      "1126/1126 - 2s - loss: 128.4490 - accuracy: 0.5533 - val_loss: 153.1024 - val_accuracy: 0.6038 - 2s/epoch - 2ms/step\n",
      "Epoch 11/10000\n",
      "1126/1126 - 2s - loss: 108.4887 - accuracy: 0.5587 - val_loss: 128.7128 - val_accuracy: 0.4701 - 2s/epoch - 2ms/step\n",
      "Epoch 12/10000\n",
      "1126/1126 - 2s - loss: 100.5323 - accuracy: 0.5626 - val_loss: 116.3730 - val_accuracy: 0.4772 - 2s/epoch - 2ms/step\n",
      "Epoch 13/10000\n",
      "1126/1126 - 2s - loss: 95.9205 - accuracy: 0.5677 - val_loss: 68.3674 - val_accuracy: 0.6141 - 2s/epoch - 2ms/step\n",
      "Epoch 14/10000\n",
      "1126/1126 - 2s - loss: 87.7194 - accuracy: 0.5741 - val_loss: 53.6418 - val_accuracy: 0.5800 - 2s/epoch - 2ms/step\n",
      "Epoch 15/10000\n",
      "1126/1126 - 2s - loss: 83.5084 - accuracy: 0.5741 - val_loss: 46.0751 - val_accuracy: 0.6295 - 2s/epoch - 2ms/step\n",
      "Epoch 16/10000\n",
      "1126/1126 - 2s - loss: 82.9429 - accuracy: 0.5752 - val_loss: 85.2272 - val_accuracy: 0.4916 - 2s/epoch - 2ms/step\n",
      "Epoch 17/10000\n",
      "1126/1126 - 2s - loss: 86.9176 - accuracy: 0.5774 - val_loss: 70.2356 - val_accuracy: 0.4981 - 2s/epoch - 2ms/step\n",
      "Epoch 18/10000\n",
      "1126/1126 - 2s - loss: 88.2232 - accuracy: 0.5754 - val_loss: 246.5396 - val_accuracy: 0.6167 - 2s/epoch - 2ms/step\n",
      "Epoch 19/10000\n",
      "1126/1126 - 2s - loss: 82.4780 - accuracy: 0.5781 - val_loss: 42.4199 - val_accuracy: 0.6370 - 2s/epoch - 2ms/step\n",
      "Epoch 20/10000\n",
      "1126/1126 - 2s - loss: 83.2954 - accuracy: 0.5789 - val_loss: 39.8559 - val_accuracy: 0.6127 - 2s/epoch - 2ms/step\n",
      "Epoch 21/10000\n",
      "1126/1126 - 2s - loss: 79.5926 - accuracy: 0.5778 - val_loss: 55.4967 - val_accuracy: 0.6313 - 2s/epoch - 2ms/step\n",
      "Epoch 22/10000\n",
      "1126/1126 - 2s - loss: 81.6585 - accuracy: 0.5795 - val_loss: 113.4204 - val_accuracy: 0.6179 - 2s/epoch - 2ms/step\n",
      "Epoch 23/10000\n",
      "1126/1126 - 2s - loss: 70.2491 - accuracy: 0.5828 - val_loss: 35.7774 - val_accuracy: 0.6370 - 2s/epoch - 2ms/step\n",
      "Epoch 24/10000\n",
      "1126/1126 - 2s - loss: 78.8878 - accuracy: 0.5793 - val_loss: 35.3628 - val_accuracy: 0.6422 - 2s/epoch - 2ms/step\n",
      "Epoch 25/10000\n",
      "1126/1126 - 2s - loss: 75.6848 - accuracy: 0.5800 - val_loss: 189.8974 - val_accuracy: 0.4383 - 2s/epoch - 2ms/step\n",
      "Epoch 26/10000\n",
      "1126/1126 - 2s - loss: 72.7892 - accuracy: 0.5813 - val_loss: 90.6769 - val_accuracy: 0.6194 - 2s/epoch - 2ms/step\n",
      "Epoch 27/10000\n",
      "1126/1126 - 2s - loss: 78.6158 - accuracy: 0.5790 - val_loss: 108.1364 - val_accuracy: 0.4908 - 2s/epoch - 2ms/step\n",
      "Epoch 28/10000\n",
      "1126/1126 - 2s - loss: 76.7788 - accuracy: 0.5813 - val_loss: 43.7557 - val_accuracy: 0.5550 - 2s/epoch - 2ms/step\n",
      "Epoch 29/10000\n",
      "1126/1126 - 2s - loss: 77.3207 - accuracy: 0.5775 - val_loss: 61.0742 - val_accuracy: 0.6268 - 2s/epoch - 2ms/step\n",
      "Epoch 30/10000\n",
      "1126/1126 - 2s - loss: 79.5737 - accuracy: 0.5785 - val_loss: 35.4380 - val_accuracy: 0.5887 - 2s/epoch - 2ms/step\n",
      "Epoch 31/10000\n",
      "1126/1126 - 2s - loss: 73.1954 - accuracy: 0.5789 - val_loss: 138.7615 - val_accuracy: 0.4888 - 2s/epoch - 2ms/step\n",
      "Epoch 32/10000\n",
      "1126/1126 - 2s - loss: 77.4289 - accuracy: 0.5787 - val_loss: 46.7544 - val_accuracy: 0.6282 - 2s/epoch - 2ms/step\n",
      "Epoch 33/10000\n",
      "1126/1126 - 2s - loss: 75.6927 - accuracy: 0.5784 - val_loss: 99.3962 - val_accuracy: 0.6183 - 2s/epoch - 2ms/step\n",
      "Epoch 34/10000\n",
      "1126/1126 - 2s - loss: 78.1155 - accuracy: 0.5783 - val_loss: 29.6077 - val_accuracy: 0.6594 - 2s/epoch - 2ms/step\n",
      "Epoch 35/10000\n",
      "1126/1126 - 2s - loss: 74.9447 - accuracy: 0.5788 - val_loss: 36.3019 - val_accuracy: 0.6420 - 2s/epoch - 2ms/step\n",
      "Epoch 36/10000\n",
      "1126/1126 - 2s - loss: 77.8558 - accuracy: 0.5798 - val_loss: 39.3330 - val_accuracy: 0.6333 - 2s/epoch - 2ms/step\n",
      "Epoch 37/10000\n",
      "1126/1126 - 2s - loss: 71.7702 - accuracy: 0.5817 - val_loss: 37.0933 - val_accuracy: 0.6426 - 2s/epoch - 2ms/step\n",
      "Epoch 38/10000\n",
      "1126/1126 - 2s - loss: 70.8776 - accuracy: 0.5789 - val_loss: 75.1521 - val_accuracy: 0.5077 - 2s/epoch - 2ms/step\n",
      "Epoch 39/10000\n",
      "1126/1126 - 2s - loss: 73.7545 - accuracy: 0.5782 - val_loss: 66.9516 - val_accuracy: 0.5257 - 2s/epoch - 2ms/step\n",
      "Epoch 40/10000\n",
      "1126/1126 - 2s - loss: 74.8998 - accuracy: 0.5784 - val_loss: 28.5446 - val_accuracy: 0.6492 - 2s/epoch - 2ms/step\n",
      "Epoch 41/10000\n",
      "1126/1126 - 2s - loss: 71.2044 - accuracy: 0.5805 - val_loss: 59.7511 - val_accuracy: 0.6207 - 2s/epoch - 2ms/step\n",
      "Epoch 42/10000\n",
      "1126/1126 - 2s - loss: 63.8063 - accuracy: 0.5810 - val_loss: 37.7621 - val_accuracy: 0.6353 - 2s/epoch - 2ms/step\n",
      "Epoch 43/10000\n",
      "1126/1126 - 2s - loss: 65.3763 - accuracy: 0.5795 - val_loss: 29.5882 - val_accuracy: 0.5699 - 2s/epoch - 2ms/step\n",
      "Epoch 44/10000\n",
      "1126/1126 - 2s - loss: 70.4193 - accuracy: 0.5791 - val_loss: 70.6024 - val_accuracy: 0.5044 - 2s/epoch - 2ms/step\n",
      "Epoch 45/10000\n",
      "1126/1126 - 2s - loss: 60.9699 - accuracy: 0.5821 - val_loss: 38.4731 - val_accuracy: 0.6274 - 2s/epoch - 2ms/step\n",
      "Epoch 46/10000\n",
      "1126/1126 - 2s - loss: 67.2811 - accuracy: 0.5797 - val_loss: 124.1573 - val_accuracy: 0.6170 - 2s/epoch - 2ms/step\n",
      "Epoch 47/10000\n",
      "1126/1126 - 2s - loss: 65.6089 - accuracy: 0.5777 - val_loss: 83.3981 - val_accuracy: 0.4997 - 2s/epoch - 2ms/step\n",
      "Epoch 48/10000\n",
      "1126/1126 - 2s - loss: 61.3946 - accuracy: 0.5820 - val_loss: 90.9458 - val_accuracy: 0.6191 - 2s/epoch - 2ms/step\n",
      "Epoch 49/10000\n",
      "1126/1126 - 2s - loss: 63.1894 - accuracy: 0.5775 - val_loss: 98.0302 - val_accuracy: 0.6172 - 2s/epoch - 2ms/step\n",
      "Epoch 50/10000\n",
      "1126/1126 - 2s - loss: 61.2688 - accuracy: 0.5805 - val_loss: 21.2558 - val_accuracy: 0.6501 - 2s/epoch - 2ms/step\n",
      "Epoch 51/10000\n",
      "1126/1126 - 2s - loss: 61.6440 - accuracy: 0.5812 - val_loss: 73.2160 - val_accuracy: 0.6215 - 2s/epoch - 2ms/step\n",
      "Epoch 52/10000\n",
      "1126/1126 - 2s - loss: 65.0240 - accuracy: 0.5798 - val_loss: 92.2874 - val_accuracy: 0.4928 - 2s/epoch - 2ms/step\n",
      "Epoch 53/10000\n",
      "1126/1126 - 2s - loss: 58.5369 - accuracy: 0.5808 - val_loss: 37.0825 - val_accuracy: 0.6225 - 2s/epoch - 2ms/step\n",
      "Epoch 54/10000\n",
      "1126/1126 - 2s - loss: 67.4713 - accuracy: 0.5781 - val_loss: 66.1827 - val_accuracy: 0.4838 - 2s/epoch - 2ms/step\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Iterazione 2/128 (03-11-2023_10-17-15)\n",
      "hidden_layer_size: 20\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 1\n",
      "activation_hidden: relu\n",
      "activation_output: sigmoid\n",
      "batch_size: 100\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 20\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 65.94%\n",
      "La recall di questo modello sul validation set è: 49.82%\n",
      "La f1 di questo modello sul validation set è: 53.1%\n",
      "La balanced accuracy di questo modello sul validation set è: 62.97%\n",
      "La precision di questo modello sul validation set è: 56.83%\n",
      "La AUC di questo modello sul validation set è: 68.11%\n",
      "391/391 [==============================] - 1s 1ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.71    0.76      0.73  7,670.00\n",
      "1                  0.57    0.50      0.53  4,841.00\n",
      "accuracy           0.66    0.66      0.66      0.66\n",
      "macro avg          0.64    0.63      0.63 12,511.00\n",
      "weighted avg       0.65    0.66      0.65 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         5838         1832\n",
      "Actual 1         2429         2412\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 65.94% con il modello:\n",
      "1 - AC65.94%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 49.82% con il modello:\n",
      "1 - RE49.82%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 53.1% con il modello:\n",
      "1 - F153.1%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 62.97% con il modello:\n",
      "1 - BA62.97%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 56.83% con il modello:\n",
      "1 - PR56.83%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "113/113 - 1s - loss: 477538.8125 - accuracy: 0.3879 - val_loss: 435933.0625 - val_accuracy: 0.3847 - 1s/epoch - 9ms/step\n",
      "Epoch 2/10000\n",
      "113/113 - 0s - loss: 384440.9688 - accuracy: 0.3879 - val_loss: 342696.8438 - val_accuracy: 0.3849 - 287ms/epoch - 3ms/step\n",
      "Epoch 3/10000\n",
      "113/113 - 0s - loss: 292846.6875 - accuracy: 0.3922 - val_loss: 251201.1250 - val_accuracy: 0.3968 - 265ms/epoch - 2ms/step\n",
      "Epoch 4/10000\n",
      "113/113 - 0s - loss: 203500.7812 - accuracy: 0.4094 - val_loss: 162776.9219 - val_accuracy: 0.4241 - 271ms/epoch - 2ms/step\n",
      "Epoch 5/10000\n",
      "113/113 - 0s - loss: 119919.4141 - accuracy: 0.4544 - val_loss: 84766.3984 - val_accuracy: 0.4890 - 262ms/epoch - 2ms/step\n",
      "Epoch 6/10000\n",
      "113/113 - 0s - loss: 57396.6133 - accuracy: 0.5335 - val_loss: 40423.1133 - val_accuracy: 0.5636 - 284ms/epoch - 3ms/step\n",
      "Epoch 7/10000\n",
      "113/113 - 0s - loss: 34305.9258 - accuracy: 0.5767 - val_loss: 32167.5117 - val_accuracy: 0.5805 - 277ms/epoch - 2ms/step\n",
      "Epoch 8/10000\n",
      "113/113 - 0s - loss: 31169.2969 - accuracy: 0.5829 - val_loss: 30732.3926 - val_accuracy: 0.5818 - 277ms/epoch - 2ms/step\n",
      "Epoch 9/10000\n",
      "113/113 - 0s - loss: 30002.5078 - accuracy: 0.5843 - val_loss: 29569.3203 - val_accuracy: 0.5823 - 280ms/epoch - 2ms/step\n",
      "Epoch 10/10000\n",
      "113/113 - 0s - loss: 28839.5723 - accuracy: 0.5852 - val_loss: 28364.6172 - val_accuracy: 0.5824 - 296ms/epoch - 3ms/step\n",
      "Epoch 11/10000\n",
      "113/113 - 0s - loss: 27620.2754 - accuracy: 0.5857 - val_loss: 27110.8730 - val_accuracy: 0.5832 - 271ms/epoch - 2ms/step\n",
      "Epoch 12/10000\n",
      "113/113 - 0s - loss: 26348.0215 - accuracy: 0.5862 - val_loss: 25789.2715 - val_accuracy: 0.5831 - 289ms/epoch - 3ms/step\n",
      "Epoch 13/10000\n",
      "113/113 - 0s - loss: 25026.6621 - accuracy: 0.5866 - val_loss: 24437.2949 - val_accuracy: 0.5829 - 301ms/epoch - 3ms/step\n",
      "Epoch 14/10000\n",
      "113/113 - 0s - loss: 23656.2812 - accuracy: 0.5870 - val_loss: 23025.4824 - val_accuracy: 0.5838 - 254ms/epoch - 2ms/step\n",
      "Epoch 15/10000\n",
      "113/113 - 0s - loss: 22245.8340 - accuracy: 0.5875 - val_loss: 21588.8242 - val_accuracy: 0.5840 - 250ms/epoch - 2ms/step\n",
      "Epoch 16/10000\n",
      "113/113 - 0s - loss: 20798.1367 - accuracy: 0.5881 - val_loss: 20110.5156 - val_accuracy: 0.5851 - 299ms/epoch - 3ms/step\n",
      "Epoch 17/10000\n",
      "113/113 - 0s - loss: 19317.7090 - accuracy: 0.5887 - val_loss: 18602.8457 - val_accuracy: 0.5860 - 282ms/epoch - 2ms/step\n",
      "Epoch 18/10000\n",
      "113/113 - 0s - loss: 17813.8223 - accuracy: 0.5900 - val_loss: 17064.4492 - val_accuracy: 0.5865 - 271ms/epoch - 2ms/step\n",
      "Epoch 19/10000\n",
      "113/113 - 0s - loss: 16287.1836 - accuracy: 0.5910 - val_loss: 15522.9131 - val_accuracy: 0.5878 - 300ms/epoch - 3ms/step\n",
      "Epoch 20/10000\n",
      "113/113 - 0s - loss: 14743.4561 - accuracy: 0.5918 - val_loss: 13956.5479 - val_accuracy: 0.5887 - 319ms/epoch - 3ms/step\n",
      "Epoch 21/10000\n",
      "113/113 - 0s - loss: 13179.0898 - accuracy: 0.5925 - val_loss: 12380.4053 - val_accuracy: 0.5887 - 286ms/epoch - 3ms/step\n",
      "Epoch 22/10000\n",
      "113/113 - 0s - loss: 11592.5508 - accuracy: 0.5931 - val_loss: 10780.9951 - val_accuracy: 0.5904 - 284ms/epoch - 3ms/step\n",
      "Epoch 23/10000\n",
      "113/113 - 0s - loss: 9984.9346 - accuracy: 0.5937 - val_loss: 9164.7061 - val_accuracy: 0.5917 - 279ms/epoch - 2ms/step\n",
      "Epoch 24/10000\n",
      "113/113 - 0s - loss: 8362.5957 - accuracy: 0.5952 - val_loss: 7530.6074 - val_accuracy: 0.5932 - 295ms/epoch - 3ms/step\n",
      "Epoch 25/10000\n",
      "113/113 - 0s - loss: 6729.8984 - accuracy: 0.5970 - val_loss: 5919.6606 - val_accuracy: 0.5930 - 270ms/epoch - 2ms/step\n",
      "Epoch 26/10000\n",
      "113/113 - 0s - loss: 5107.9282 - accuracy: 0.5996 - val_loss: 4297.7705 - val_accuracy: 0.5994 - 297ms/epoch - 3ms/step\n",
      "Epoch 27/10000\n",
      "113/113 - 0s - loss: 3543.9661 - accuracy: 0.6030 - val_loss: 2803.5261 - val_accuracy: 0.6070 - 294ms/epoch - 3ms/step\n",
      "Epoch 28/10000\n",
      "113/113 - 0s - loss: 2237.7356 - accuracy: 0.6098 - val_loss: 1803.9351 - val_accuracy: 0.6103 - 276ms/epoch - 2ms/step\n",
      "Epoch 29/10000\n",
      "113/113 - 0s - loss: 1680.7974 - accuracy: 0.6066 - val_loss: 1605.2323 - val_accuracy: 0.6042 - 294ms/epoch - 3ms/step\n",
      "Epoch 30/10000\n",
      "113/113 - 0s - loss: 1568.1349 - accuracy: 0.6002 - val_loss: 1527.5775 - val_accuracy: 0.6004 - 281ms/epoch - 2ms/step\n",
      "Epoch 31/10000\n",
      "113/113 - 0s - loss: 1500.4709 - accuracy: 0.5981 - val_loss: 1476.9281 - val_accuracy: 0.5951 - 288ms/epoch - 3ms/step\n",
      "Epoch 32/10000\n",
      "113/113 - 0s - loss: 1432.5809 - accuracy: 0.5971 - val_loss: 1392.8252 - val_accuracy: 0.5966 - 250ms/epoch - 2ms/step\n",
      "Epoch 33/10000\n",
      "113/113 - 0s - loss: 1363.1097 - accuracy: 0.5962 - val_loss: 1324.4385 - val_accuracy: 0.5968 - 276ms/epoch - 2ms/step\n",
      "Epoch 34/10000\n",
      "113/113 - 0s - loss: 1292.6362 - accuracy: 0.5958 - val_loss: 1256.2388 - val_accuracy: 0.5958 - 267ms/epoch - 2ms/step\n",
      "Epoch 35/10000\n",
      "113/113 - 0s - loss: 1224.6375 - accuracy: 0.5964 - val_loss: 1185.6570 - val_accuracy: 0.5964 - 295ms/epoch - 3ms/step\n",
      "Epoch 36/10000\n",
      "113/113 - 0s - loss: 1155.5017 - accuracy: 0.5953 - val_loss: 1120.5121 - val_accuracy: 0.5934 - 272ms/epoch - 2ms/step\n",
      "Epoch 37/10000\n",
      "113/113 - 0s - loss: 1089.7565 - accuracy: 0.5953 - val_loss: 1050.4998 - val_accuracy: 0.5952 - 277ms/epoch - 2ms/step\n",
      "Epoch 38/10000\n",
      "113/113 - 0s - loss: 1018.6259 - accuracy: 0.5939 - val_loss: 981.3695 - val_accuracy: 0.5960 - 269ms/epoch - 2ms/step\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Iterazione 3/128 (03-11-2023_10-17-29)\n",
      "hidden_layer_size: 20\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 1\n",
      "activation_hidden: relu\n",
      "activation_output: sigmoid\n",
      "batch_size: 1000\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 10\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 61.03%\n",
      "La recall di questo modello sul validation set è: 28.67%\n",
      "La f1 di questo modello sul validation set è: 36.28%\n",
      "La balanced accuracy di questo modello sul validation set è: 55.07%\n",
      "La precision di questo modello sul validation set è: 49.4%\n",
      "La AUC di questo modello sul validation set è: 55.03%\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.64    0.81      0.72  7,670.00\n",
      "1                  0.49    0.29      0.36  4,841.00\n",
      "accuracy           0.61    0.61      0.61      0.61\n",
      "macro avg          0.57    0.55      0.54 12,511.00\n",
      "weighted avg       0.59    0.61      0.58 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         6248         1422\n",
      "Actual 1         3453         1388\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 65.94% con il modello:\n",
      "1 - AC65.94%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 49.82% con il modello:\n",
      "1 - RE49.82%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 53.1% con il modello:\n",
      "1 - F153.1%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 62.97% con il modello:\n",
      "1 - BA62.97%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 56.83% con il modello:\n",
      "1 - PR56.83%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "113/113 - 1s - loss: 477538.8125 - accuracy: 0.3879 - val_loss: 435933.0625 - val_accuracy: 0.3847 - 980ms/epoch - 9ms/step\n",
      "Epoch 2/10000\n",
      "113/113 - 0s - loss: 384440.9688 - accuracy: 0.3879 - val_loss: 342696.8438 - val_accuracy: 0.3849 - 280ms/epoch - 2ms/step\n",
      "Epoch 3/10000\n",
      "113/113 - 0s - loss: 292846.6875 - accuracy: 0.3922 - val_loss: 251201.1250 - val_accuracy: 0.3968 - 287ms/epoch - 3ms/step\n",
      "Epoch 4/10000\n",
      "113/113 - 0s - loss: 203500.7812 - accuracy: 0.4094 - val_loss: 162776.9219 - val_accuracy: 0.4241 - 273ms/epoch - 2ms/step\n",
      "Epoch 5/10000\n",
      "113/113 - 0s - loss: 119919.4141 - accuracy: 0.4544 - val_loss: 84766.3984 - val_accuracy: 0.4890 - 267ms/epoch - 2ms/step\n",
      "Epoch 6/10000\n",
      "113/113 - 0s - loss: 57396.6133 - accuracy: 0.5335 - val_loss: 40423.1133 - val_accuracy: 0.5636 - 267ms/epoch - 2ms/step\n",
      "Epoch 7/10000\n",
      "113/113 - 0s - loss: 34305.9258 - accuracy: 0.5767 - val_loss: 32167.5117 - val_accuracy: 0.5805 - 261ms/epoch - 2ms/step\n",
      "Epoch 8/10000\n",
      "113/113 - 0s - loss: 31169.2969 - accuracy: 0.5829 - val_loss: 30732.3926 - val_accuracy: 0.5818 - 268ms/epoch - 2ms/step\n",
      "Epoch 9/10000\n",
      "113/113 - 0s - loss: 30002.5078 - accuracy: 0.5843 - val_loss: 29569.3203 - val_accuracy: 0.5823 - 289ms/epoch - 3ms/step\n",
      "Epoch 10/10000\n",
      "113/113 - 0s - loss: 28839.5723 - accuracy: 0.5852 - val_loss: 28364.6172 - val_accuracy: 0.5824 - 292ms/epoch - 3ms/step\n",
      "Epoch 11/10000\n",
      "113/113 - 0s - loss: 27620.2754 - accuracy: 0.5857 - val_loss: 27110.8730 - val_accuracy: 0.5832 - 298ms/epoch - 3ms/step\n",
      "Epoch 12/10000\n",
      "113/113 - 0s - loss: 26348.0215 - accuracy: 0.5862 - val_loss: 25789.2715 - val_accuracy: 0.5831 - 277ms/epoch - 2ms/step\n",
      "Epoch 13/10000\n",
      "113/113 - 0s - loss: 25026.6621 - accuracy: 0.5866 - val_loss: 24437.2949 - val_accuracy: 0.5829 - 298ms/epoch - 3ms/step\n",
      "Epoch 14/10000\n",
      "113/113 - 0s - loss: 23656.2812 - accuracy: 0.5870 - val_loss: 23025.4824 - val_accuracy: 0.5838 - 278ms/epoch - 2ms/step\n",
      "Epoch 15/10000\n",
      "113/113 - 0s - loss: 22245.8340 - accuracy: 0.5875 - val_loss: 21588.8242 - val_accuracy: 0.5840 - 300ms/epoch - 3ms/step\n",
      "Epoch 16/10000\n",
      "113/113 - 0s - loss: 20798.1367 - accuracy: 0.5881 - val_loss: 20110.5156 - val_accuracy: 0.5851 - 316ms/epoch - 3ms/step\n",
      "Epoch 17/10000\n",
      "113/113 - 0s - loss: 19317.7090 - accuracy: 0.5887 - val_loss: 18602.8457 - val_accuracy: 0.5860 - 303ms/epoch - 3ms/step\n",
      "Epoch 18/10000\n",
      "113/113 - 0s - loss: 17813.8223 - accuracy: 0.5900 - val_loss: 17064.4492 - val_accuracy: 0.5865 - 291ms/epoch - 3ms/step\n",
      "Epoch 19/10000\n",
      "113/113 - 0s - loss: 16287.1836 - accuracy: 0.5910 - val_loss: 15522.9131 - val_accuracy: 0.5878 - 290ms/epoch - 3ms/step\n",
      "Epoch 20/10000\n",
      "113/113 - 0s - loss: 14743.4561 - accuracy: 0.5918 - val_loss: 13956.5479 - val_accuracy: 0.5887 - 305ms/epoch - 3ms/step\n",
      "Epoch 21/10000\n",
      "113/113 - 0s - loss: 13179.0898 - accuracy: 0.5925 - val_loss: 12380.4053 - val_accuracy: 0.5887 - 354ms/epoch - 3ms/step\n",
      "Epoch 22/10000\n",
      "113/113 - 0s - loss: 11592.5508 - accuracy: 0.5931 - val_loss: 10780.9951 - val_accuracy: 0.5904 - 302ms/epoch - 3ms/step\n",
      "Epoch 23/10000\n",
      "113/113 - 0s - loss: 9984.9346 - accuracy: 0.5937 - val_loss: 9164.7061 - val_accuracy: 0.5917 - 300ms/epoch - 3ms/step\n",
      "Epoch 24/10000\n",
      "113/113 - 0s - loss: 8362.5957 - accuracy: 0.5952 - val_loss: 7530.6074 - val_accuracy: 0.5932 - 311ms/epoch - 3ms/step\n",
      "Epoch 25/10000\n",
      "113/113 - 0s - loss: 6729.8984 - accuracy: 0.5970 - val_loss: 5919.6606 - val_accuracy: 0.5930 - 319ms/epoch - 3ms/step\n",
      "Epoch 26/10000\n",
      "113/113 - 0s - loss: 5107.9282 - accuracy: 0.5996 - val_loss: 4297.7705 - val_accuracy: 0.5994 - 319ms/epoch - 3ms/step\n",
      "Epoch 27/10000\n",
      "113/113 - 0s - loss: 3543.9661 - accuracy: 0.6030 - val_loss: 2803.5261 - val_accuracy: 0.6070 - 301ms/epoch - 3ms/step\n",
      "Epoch 28/10000\n",
      "113/113 - 0s - loss: 2237.7356 - accuracy: 0.6098 - val_loss: 1803.9351 - val_accuracy: 0.6103 - 295ms/epoch - 3ms/step\n",
      "Epoch 29/10000\n",
      "113/113 - 0s - loss: 1680.7974 - accuracy: 0.6066 - val_loss: 1605.2323 - val_accuracy: 0.6042 - 305ms/epoch - 3ms/step\n",
      "Epoch 30/10000\n",
      "113/113 - 0s - loss: 1568.1349 - accuracy: 0.6002 - val_loss: 1527.5775 - val_accuracy: 0.6004 - 295ms/epoch - 3ms/step\n",
      "Epoch 31/10000\n",
      "113/113 - 0s - loss: 1500.4709 - accuracy: 0.5981 - val_loss: 1476.9281 - val_accuracy: 0.5951 - 296ms/epoch - 3ms/step\n",
      "Epoch 32/10000\n",
      "113/113 - 0s - loss: 1432.5809 - accuracy: 0.5971 - val_loss: 1392.8252 - val_accuracy: 0.5966 - 291ms/epoch - 3ms/step\n",
      "Epoch 33/10000\n",
      "113/113 - 0s - loss: 1363.1097 - accuracy: 0.5962 - val_loss: 1324.4385 - val_accuracy: 0.5968 - 299ms/epoch - 3ms/step\n",
      "Epoch 34/10000\n",
      "113/113 - 0s - loss: 1292.6362 - accuracy: 0.5958 - val_loss: 1256.2388 - val_accuracy: 0.5958 - 289ms/epoch - 3ms/step\n",
      "Epoch 35/10000\n",
      "113/113 - 0s - loss: 1224.6375 - accuracy: 0.5964 - val_loss: 1185.6570 - val_accuracy: 0.5964 - 299ms/epoch - 3ms/step\n",
      "Epoch 36/10000\n",
      "113/113 - 0s - loss: 1155.5017 - accuracy: 0.5953 - val_loss: 1120.5121 - val_accuracy: 0.5934 - 287ms/epoch - 3ms/step\n",
      "Epoch 37/10000\n",
      "113/113 - 0s - loss: 1089.7565 - accuracy: 0.5953 - val_loss: 1050.4998 - val_accuracy: 0.5952 - 291ms/epoch - 3ms/step\n",
      "Epoch 38/10000\n",
      "113/113 - 0s - loss: 1018.6259 - accuracy: 0.5939 - val_loss: 981.3695 - val_accuracy: 0.5960 - 302ms/epoch - 3ms/step\n",
      "Epoch 39/10000\n",
      "113/113 - 0s - loss: 949.5175 - accuracy: 0.5949 - val_loss: 911.0478 - val_accuracy: 0.5949 - 308ms/epoch - 3ms/step\n",
      "Epoch 40/10000\n",
      "113/113 - 0s - loss: 881.3879 - accuracy: 0.5938 - val_loss: 843.7412 - val_accuracy: 0.5953 - 318ms/epoch - 3ms/step\n",
      "Epoch 41/10000\n",
      "113/113 - 0s - loss: 814.3546 - accuracy: 0.5931 - val_loss: 776.3983 - val_accuracy: 0.5956 - 298ms/epoch - 3ms/step\n",
      "Epoch 42/10000\n",
      "113/113 - 0s - loss: 746.2228 - accuracy: 0.5930 - val_loss: 715.9940 - val_accuracy: 0.5889 - 304ms/epoch - 3ms/step\n",
      "Epoch 43/10000\n",
      "113/113 - 0s - loss: 681.5305 - accuracy: 0.5910 - val_loss: 647.5394 - val_accuracy: 0.5896 - 294ms/epoch - 3ms/step\n",
      "Epoch 44/10000\n",
      "113/113 - 0s - loss: 618.6847 - accuracy: 0.5901 - val_loss: 588.6205 - val_accuracy: 0.5861 - 292ms/epoch - 3ms/step\n",
      "Epoch 45/10000\n",
      "113/113 - 0s - loss: 551.1244 - accuracy: 0.5893 - val_loss: 524.6527 - val_accuracy: 0.5933 - 297ms/epoch - 3ms/step\n",
      "Epoch 46/10000\n",
      "113/113 - 0s - loss: 487.7473 - accuracy: 0.5869 - val_loss: 458.8705 - val_accuracy: 0.5820 - 297ms/epoch - 3ms/step\n",
      "Epoch 47/10000\n",
      "113/113 - 0s - loss: 432.5486 - accuracy: 0.5828 - val_loss: 409.6544 - val_accuracy: 0.5710 - 295ms/epoch - 3ms/step\n",
      "Epoch 48/10000\n",
      "113/113 - 0s - loss: 370.7506 - accuracy: 0.5805 - val_loss: 361.9694 - val_accuracy: 0.5879 - 349ms/epoch - 3ms/step\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Iterazione 4/128 (03-11-2023_10-17-46)\n",
      "hidden_layer_size: 20\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 1\n",
      "activation_hidden: relu\n",
      "activation_output: sigmoid\n",
      "batch_size: 1000\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 20\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 61.03%\n",
      "La recall di questo modello sul validation set è: 28.67%\n",
      "La f1 di questo modello sul validation set è: 36.28%\n",
      "La balanced accuracy di questo modello sul validation set è: 55.07%\n",
      "La precision di questo modello sul validation set è: 49.4%\n",
      "La AUC di questo modello sul validation set è: 55.03%\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.64    0.81      0.72  7,670.00\n",
      "1                  0.49    0.29      0.36  4,841.00\n",
      "accuracy           0.61    0.61      0.61      0.61\n",
      "macro avg          0.57    0.55      0.54 12,511.00\n",
      "weighted avg       0.59    0.61      0.58 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         6248         1422\n",
      "Actual 1         3453         1388\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 65.94% con il modello:\n",
      "1 - AC65.94%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 49.82% con il modello:\n",
      "1 - RE49.82%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 53.1% con il modello:\n",
      "1 - F153.1%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 62.97% con il modello:\n",
      "1 - BA62.97%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 56.83% con il modello:\n",
      "1 - PR56.83%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "1126/1126 - 3s - loss: 8.9628 - accuracy: 0.3880 - val_loss: 4.4579 - val_accuracy: 0.3848 - 3s/epoch - 2ms/step\n",
      "Epoch 2/10000\n",
      "1126/1126 - 2s - loss: 1.7081 - accuracy: 0.4566 - val_loss: 0.9340 - val_accuracy: 0.5643 - 2s/epoch - 2ms/step\n",
      "Epoch 3/10000\n",
      "1126/1126 - 2s - loss: 0.8999 - accuracy: 0.5664 - val_loss: 0.8720 - val_accuracy: 0.5745 - 2s/epoch - 2ms/step\n",
      "Epoch 4/10000\n",
      "1126/1126 - 2s - loss: 0.8375 - accuracy: 0.5771 - val_loss: 0.8106 - val_accuracy: 0.5853 - 2s/epoch - 2ms/step\n",
      "Epoch 5/10000\n",
      "1126/1126 - 2s - loss: 0.7893 - accuracy: 0.5853 - val_loss: 0.7755 - val_accuracy: 0.5901 - 2s/epoch - 2ms/step\n",
      "Epoch 6/10000\n",
      "1126/1126 - 2s - loss: 0.7558 - accuracy: 0.5910 - val_loss: 0.7465 - val_accuracy: 0.5955 - 2s/epoch - 2ms/step\n",
      "Epoch 7/10000\n",
      "1126/1126 - 2s - loss: 0.7311 - accuracy: 0.5968 - val_loss: 0.7256 - val_accuracy: 0.6009 - 2s/epoch - 2ms/step\n",
      "Epoch 8/10000\n",
      "1126/1126 - 2s - loss: 0.7155 - accuracy: 0.6000 - val_loss: 0.7083 - val_accuracy: 0.6035 - 2s/epoch - 2ms/step\n",
      "Epoch 9/10000\n",
      "1126/1126 - 2s - loss: 0.7032 - accuracy: 0.6025 - val_loss: 0.7011 - val_accuracy: 0.6043 - 2s/epoch - 2ms/step\n",
      "Epoch 10/10000\n",
      "1126/1126 - 2s - loss: 0.6927 - accuracy: 0.6055 - val_loss: 0.6915 - val_accuracy: 0.6067 - 2s/epoch - 2ms/step\n",
      "Epoch 11/10000\n",
      "1126/1126 - 2s - loss: 0.6867 - accuracy: 0.6068 - val_loss: 0.6861 - val_accuracy: 0.6092 - 2s/epoch - 2ms/step\n",
      "Epoch 12/10000\n",
      "1126/1126 - 2s - loss: 0.6823 - accuracy: 0.6073 - val_loss: 0.6814 - val_accuracy: 0.6096 - 2s/epoch - 2ms/step\n",
      "Epoch 13/10000\n",
      "1126/1126 - 2s - loss: 0.6798 - accuracy: 0.6077 - val_loss: 0.6813 - val_accuracy: 0.6092 - 2s/epoch - 2ms/step\n",
      "Epoch 14/10000\n",
      "1126/1126 - 2s - loss: 0.6790 - accuracy: 0.6084 - val_loss: 0.6791 - val_accuracy: 0.6108 - 2s/epoch - 2ms/step\n",
      "Epoch 15/10000\n",
      "1126/1126 - 2s - loss: 0.6772 - accuracy: 0.6083 - val_loss: 0.6772 - val_accuracy: 0.6119 - 2s/epoch - 2ms/step\n",
      "Epoch 16/10000\n",
      "1126/1126 - 2s - loss: 0.6761 - accuracy: 0.6085 - val_loss: 0.6758 - val_accuracy: 0.6114 - 2s/epoch - 2ms/step\n",
      "Epoch 17/10000\n",
      "1126/1126 - 2s - loss: 0.6751 - accuracy: 0.6082 - val_loss: 0.6732 - val_accuracy: 0.6118 - 2s/epoch - 2ms/step\n",
      "Epoch 18/10000\n",
      "1126/1126 - 2s - loss: 0.6735 - accuracy: 0.6091 - val_loss: 0.6724 - val_accuracy: 0.6119 - 2s/epoch - 2ms/step\n",
      "Epoch 19/10000\n",
      "1126/1126 - 2s - loss: 0.6726 - accuracy: 0.6094 - val_loss: 0.6725 - val_accuracy: 0.6116 - 2s/epoch - 2ms/step\n",
      "Epoch 20/10000\n",
      "1126/1126 - 2s - loss: 0.6720 - accuracy: 0.6091 - val_loss: 0.6713 - val_accuracy: 0.6119 - 2s/epoch - 2ms/step\n",
      "Epoch 21/10000\n",
      "1126/1126 - 2s - loss: 0.6715 - accuracy: 0.6092 - val_loss: 0.6695 - val_accuracy: 0.6118 - 2s/epoch - 2ms/step\n",
      "Epoch 22/10000\n",
      "1126/1126 - 2s - loss: 0.6708 - accuracy: 0.6094 - val_loss: 0.6685 - val_accuracy: 0.6126 - 2s/epoch - 2ms/step\n",
      "Epoch 23/10000\n",
      "1126/1126 - 2s - loss: 0.6700 - accuracy: 0.6096 - val_loss: 0.6675 - val_accuracy: 0.6121 - 2s/epoch - 2ms/step\n",
      "Epoch 24/10000\n",
      "1126/1126 - 2s - loss: 0.6693 - accuracy: 0.6092 - val_loss: 0.6668 - val_accuracy: 0.6122 - 2s/epoch - 2ms/step\n",
      "Epoch 25/10000\n",
      "1126/1126 - 2s - loss: 0.6680 - accuracy: 0.6084 - val_loss: 0.6658 - val_accuracy: 0.6113 - 2s/epoch - 2ms/step\n",
      "Epoch 26/10000\n",
      "1126/1126 - 2s - loss: 0.6665 - accuracy: 0.6084 - val_loss: 0.6657 - val_accuracy: 0.6110 - 2s/epoch - 2ms/step\n",
      "Epoch 27/10000\n",
      "1126/1126 - 2s - loss: 0.6657 - accuracy: 0.6085 - val_loss: 0.6649 - val_accuracy: 0.6119 - 2s/epoch - 2ms/step\n",
      "Epoch 28/10000\n",
      "1126/1126 - 2s - loss: 0.6651 - accuracy: 0.6090 - val_loss: 0.6644 - val_accuracy: 0.6119 - 2s/epoch - 2ms/step\n",
      "Epoch 29/10000\n",
      "1126/1126 - 2s - loss: 0.6651 - accuracy: 0.6088 - val_loss: 0.6641 - val_accuracy: 0.6107 - 2s/epoch - 2ms/step\n",
      "Epoch 30/10000\n",
      "1126/1126 - 2s - loss: 0.6647 - accuracy: 0.6084 - val_loss: 0.6639 - val_accuracy: 0.6114 - 2s/epoch - 2ms/step\n",
      "Epoch 31/10000\n",
      "1126/1126 - 2s - loss: 0.6638 - accuracy: 0.6094 - val_loss: 0.6645 - val_accuracy: 0.6123 - 2s/epoch - 2ms/step\n",
      "Epoch 32/10000\n",
      "1126/1126 - 2s - loss: 0.6636 - accuracy: 0.6092 - val_loss: 0.6647 - val_accuracy: 0.6101 - 2s/epoch - 2ms/step\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Iterazione 5/128 (03-11-2023_10-18-51)\n",
      "hidden_layer_size: 20\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 1\n",
      "activation_hidden: sigmoid\n",
      "activation_output: sigmoid\n",
      "batch_size: 100\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 10\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 61.26%\n",
      "La recall di questo modello sul validation set è: 0.89%\n",
      "La f1 di questo modello sul validation set è: 1.74%\n",
      "La balanced accuracy di questo modello sul validation set è: 50.12%\n",
      "La precision di questo modello sul validation set è: 46.74%\n",
      "La AUC di questo modello sul validation set è: 55.76%\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.61    0.99      0.76  7,670.00\n",
      "1                  0.47    0.01      0.02  4,841.00\n",
      "accuracy           0.61    0.61      0.61      0.61\n",
      "macro avg          0.54    0.50      0.39 12,511.00\n",
      "weighted avg       0.56    0.61      0.47 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         7621           49\n",
      "Actual 1         4798           43\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 65.94% con il modello:\n",
      "1 - AC65.94%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 49.82% con il modello:\n",
      "1 - RE49.82%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 53.1% con il modello:\n",
      "1 - F153.1%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 62.97% con il modello:\n",
      "1 - BA62.97%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 56.83% con il modello:\n",
      "1 - PR56.83%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "1126/1126 - 3s - loss: 8.9628 - accuracy: 0.3880 - val_loss: 4.4579 - val_accuracy: 0.3848 - 3s/epoch - 2ms/step\n",
      "Epoch 2/10000\n",
      "1126/1126 - 2s - loss: 1.7081 - accuracy: 0.4566 - val_loss: 0.9340 - val_accuracy: 0.5643 - 2s/epoch - 2ms/step\n",
      "Epoch 3/10000\n",
      "1126/1126 - 2s - loss: 0.8999 - accuracy: 0.5664 - val_loss: 0.8720 - val_accuracy: 0.5745 - 2s/epoch - 2ms/step\n",
      "Epoch 4/10000\n",
      "1126/1126 - 2s - loss: 0.8375 - accuracy: 0.5771 - val_loss: 0.8106 - val_accuracy: 0.5853 - 2s/epoch - 2ms/step\n",
      "Epoch 5/10000\n",
      "1126/1126 - 2s - loss: 0.7893 - accuracy: 0.5853 - val_loss: 0.7755 - val_accuracy: 0.5901 - 2s/epoch - 2ms/step\n",
      "Epoch 6/10000\n",
      "1126/1126 - 2s - loss: 0.7558 - accuracy: 0.5910 - val_loss: 0.7465 - val_accuracy: 0.5955 - 2s/epoch - 2ms/step\n",
      "Epoch 7/10000\n",
      "1126/1126 - 2s - loss: 0.7311 - accuracy: 0.5968 - val_loss: 0.7256 - val_accuracy: 0.6009 - 2s/epoch - 2ms/step\n",
      "Epoch 8/10000\n",
      "1126/1126 - 2s - loss: 0.7155 - accuracy: 0.6000 - val_loss: 0.7083 - val_accuracy: 0.6035 - 2s/epoch - 2ms/step\n",
      "Epoch 9/10000\n",
      "1126/1126 - 2s - loss: 0.7032 - accuracy: 0.6025 - val_loss: 0.7011 - val_accuracy: 0.6043 - 2s/epoch - 2ms/step\n",
      "Epoch 10/10000\n",
      "1126/1126 - 2s - loss: 0.6927 - accuracy: 0.6055 - val_loss: 0.6915 - val_accuracy: 0.6067 - 2s/epoch - 2ms/step\n",
      "Epoch 11/10000\n",
      "1126/1126 - 2s - loss: 0.6867 - accuracy: 0.6068 - val_loss: 0.6861 - val_accuracy: 0.6092 - 2s/epoch - 2ms/step\n",
      "Epoch 12/10000\n",
      "1126/1126 - 2s - loss: 0.6823 - accuracy: 0.6073 - val_loss: 0.6814 - val_accuracy: 0.6096 - 2s/epoch - 2ms/step\n",
      "Epoch 13/10000\n",
      "1126/1126 - 2s - loss: 0.6798 - accuracy: 0.6077 - val_loss: 0.6813 - val_accuracy: 0.6092 - 2s/epoch - 2ms/step\n",
      "Epoch 14/10000\n",
      "1126/1126 - 2s - loss: 0.6790 - accuracy: 0.6084 - val_loss: 0.6791 - val_accuracy: 0.6108 - 2s/epoch - 2ms/step\n",
      "Epoch 15/10000\n",
      "1126/1126 - 2s - loss: 0.6772 - accuracy: 0.6083 - val_loss: 0.6772 - val_accuracy: 0.6119 - 2s/epoch - 2ms/step\n",
      "Epoch 16/10000\n",
      "1126/1126 - 2s - loss: 0.6761 - accuracy: 0.6085 - val_loss: 0.6758 - val_accuracy: 0.6114 - 2s/epoch - 2ms/step\n",
      "Epoch 17/10000\n",
      "1126/1126 - 2s - loss: 0.6751 - accuracy: 0.6082 - val_loss: 0.6732 - val_accuracy: 0.6118 - 2s/epoch - 2ms/step\n",
      "Epoch 18/10000\n",
      "1126/1126 - 2s - loss: 0.6735 - accuracy: 0.6091 - val_loss: 0.6724 - val_accuracy: 0.6119 - 2s/epoch - 2ms/step\n",
      "Epoch 19/10000\n",
      "1126/1126 - 2s - loss: 0.6726 - accuracy: 0.6094 - val_loss: 0.6725 - val_accuracy: 0.6116 - 2s/epoch - 2ms/step\n",
      "Epoch 20/10000\n",
      "1126/1126 - 2s - loss: 0.6720 - accuracy: 0.6091 - val_loss: 0.6713 - val_accuracy: 0.6119 - 2s/epoch - 2ms/step\n",
      "Epoch 21/10000\n",
      "1126/1126 - 2s - loss: 0.6715 - accuracy: 0.6092 - val_loss: 0.6695 - val_accuracy: 0.6118 - 2s/epoch - 2ms/step\n",
      "Epoch 22/10000\n",
      "1126/1126 - 2s - loss: 0.6708 - accuracy: 0.6094 - val_loss: 0.6685 - val_accuracy: 0.6126 - 2s/epoch - 2ms/step\n",
      "Epoch 23/10000\n",
      "1126/1126 - 2s - loss: 0.6700 - accuracy: 0.6096 - val_loss: 0.6675 - val_accuracy: 0.6121 - 2s/epoch - 2ms/step\n",
      "Epoch 24/10000\n",
      "1126/1126 - 2s - loss: 0.6693 - accuracy: 0.6092 - val_loss: 0.6668 - val_accuracy: 0.6122 - 2s/epoch - 2ms/step\n",
      "Epoch 25/10000\n",
      "1126/1126 - 2s - loss: 0.6680 - accuracy: 0.6084 - val_loss: 0.6658 - val_accuracy: 0.6113 - 2s/epoch - 2ms/step\n",
      "Epoch 26/10000\n",
      "1126/1126 - 2s - loss: 0.6665 - accuracy: 0.6084 - val_loss: 0.6657 - val_accuracy: 0.6110 - 2s/epoch - 2ms/step\n",
      "Epoch 27/10000\n",
      "1126/1126 - 2s - loss: 0.6657 - accuracy: 0.6085 - val_loss: 0.6649 - val_accuracy: 0.6119 - 2s/epoch - 2ms/step\n",
      "Epoch 28/10000\n",
      "1126/1126 - 2s - loss: 0.6651 - accuracy: 0.6090 - val_loss: 0.6644 - val_accuracy: 0.6119 - 2s/epoch - 2ms/step\n",
      "Epoch 29/10000\n",
      "1126/1126 - 2s - loss: 0.6651 - accuracy: 0.6088 - val_loss: 0.6641 - val_accuracy: 0.6107 - 2s/epoch - 2ms/step\n",
      "Epoch 30/10000\n",
      "1126/1126 - 2s - loss: 0.6647 - accuracy: 0.6084 - val_loss: 0.6639 - val_accuracy: 0.6114 - 2s/epoch - 2ms/step\n",
      "Epoch 31/10000\n",
      "1126/1126 - 2s - loss: 0.6638 - accuracy: 0.6094 - val_loss: 0.6645 - val_accuracy: 0.6123 - 2s/epoch - 2ms/step\n",
      "Epoch 32/10000\n",
      "1126/1126 - 2s - loss: 0.6636 - accuracy: 0.6092 - val_loss: 0.6647 - val_accuracy: 0.6101 - 2s/epoch - 2ms/step\n",
      "Epoch 33/10000\n",
      "1126/1126 - 2s - loss: 0.6634 - accuracy: 0.6088 - val_loss: 0.6641 - val_accuracy: 0.6108 - 2s/epoch - 2ms/step\n",
      "Epoch 34/10000\n",
      "1126/1126 - 2s - loss: 0.6633 - accuracy: 0.6090 - val_loss: 0.6641 - val_accuracy: 0.6117 - 2s/epoch - 2ms/step\n",
      "Epoch 35/10000\n",
      "1126/1126 - 2s - loss: 0.6631 - accuracy: 0.6091 - val_loss: 0.6626 - val_accuracy: 0.6117 - 2s/epoch - 2ms/step\n",
      "Epoch 36/10000\n",
      "1126/1126 - 2s - loss: 0.6629 - accuracy: 0.6091 - val_loss: 0.6629 - val_accuracy: 0.6118 - 2s/epoch - 2ms/step\n",
      "Epoch 37/10000\n",
      "1126/1126 - 2s - loss: 0.6628 - accuracy: 0.6092 - val_loss: 0.6629 - val_accuracy: 0.6117 - 2s/epoch - 2ms/step\n",
      "Epoch 38/10000\n",
      "1126/1126 - 2s - loss: 0.6626 - accuracy: 0.6094 - val_loss: 0.6630 - val_accuracy: 0.6120 - 2s/epoch - 2ms/step\n",
      "Epoch 39/10000\n",
      "1126/1126 - 2s - loss: 0.6625 - accuracy: 0.6094 - val_loss: 0.6632 - val_accuracy: 0.6107 - 2s/epoch - 2ms/step\n",
      "Epoch 40/10000\n",
      "1126/1126 - 2s - loss: 0.6626 - accuracy: 0.6094 - val_loss: 0.6633 - val_accuracy: 0.6105 - 2s/epoch - 2ms/step\n",
      "Epoch 41/10000\n",
      "1126/1126 - 3s - loss: 0.6628 - accuracy: 0.6086 - val_loss: 0.6628 - val_accuracy: 0.6099 - 3s/epoch - 3ms/step\n",
      "Epoch 42/10000\n",
      "1126/1126 - 2s - loss: 0.6624 - accuracy: 0.6089 - val_loss: 0.6625 - val_accuracy: 0.6106 - 2s/epoch - 2ms/step\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Iterazione 6/128 (03-11-2023_10-20-20)\n",
      "hidden_layer_size: 20\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 1\n",
      "activation_hidden: sigmoid\n",
      "activation_output: sigmoid\n",
      "batch_size: 100\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 20\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 61.26%\n",
      "La recall di questo modello sul validation set è: 0.89%\n",
      "La f1 di questo modello sul validation set è: 1.74%\n",
      "La balanced accuracy di questo modello sul validation set è: 50.12%\n",
      "La precision di questo modello sul validation set è: 46.74%\n",
      "La AUC di questo modello sul validation set è: 55.76%\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.61    0.99      0.76  7,670.00\n",
      "1                  0.47    0.01      0.02  4,841.00\n",
      "accuracy           0.61    0.61      0.61      0.61\n",
      "macro avg          0.54    0.50      0.39 12,511.00\n",
      "weighted avg       0.56    0.61      0.47 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         7621           49\n",
      "Actual 1         4798           43\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 65.94% con il modello:\n",
      "1 - AC65.94%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 49.82% con il modello:\n",
      "1 - RE49.82%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 53.1% con il modello:\n",
      "1 - F153.1%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 62.97% con il modello:\n",
      "1 - BA62.97%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 56.83% con il modello:\n",
      "1 - PR56.83%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "113/113 - 1s - loss: 13.0397 - accuracy: 0.3883 - val_loss: 12.6035 - val_accuracy: 0.3856 - 1s/epoch - 9ms/step\n",
      "Epoch 2/10000\n",
      "113/113 - 0s - loss: 12.0673 - accuracy: 0.3879 - val_loss: 11.6257 - val_accuracy: 0.3856 - 351ms/epoch - 3ms/step\n",
      "Epoch 3/10000\n",
      "113/113 - 0s - loss: 11.0929 - accuracy: 0.3880 - val_loss: 10.6407 - val_accuracy: 0.3855 - 317ms/epoch - 3ms/step\n",
      "Epoch 4/10000\n",
      "113/113 - 0s - loss: 10.1376 - accuracy: 0.3880 - val_loss: 9.6888 - val_accuracy: 0.3855 - 293ms/epoch - 3ms/step\n",
      "Epoch 5/10000\n",
      "113/113 - 0s - loss: 9.1898 - accuracy: 0.3880 - val_loss: 8.7464 - val_accuracy: 0.3855 - 271ms/epoch - 2ms/step\n",
      "Epoch 6/10000\n",
      "113/113 - 0s - loss: 8.2444 - accuracy: 0.3880 - val_loss: 7.7930 - val_accuracy: 0.3855 - 275ms/epoch - 2ms/step\n",
      "Epoch 7/10000\n",
      "113/113 - 0s - loss: 7.3092 - accuracy: 0.3880 - val_loss: 6.8524 - val_accuracy: 0.3855 - 298ms/epoch - 3ms/step\n",
      "Epoch 8/10000\n",
      "113/113 - 0s - loss: 6.3642 - accuracy: 0.3880 - val_loss: 5.8950 - val_accuracy: 0.3848 - 344ms/epoch - 3ms/step\n",
      "Epoch 9/10000\n",
      "113/113 - 0s - loss: 5.4262 - accuracy: 0.3878 - val_loss: 4.9706 - val_accuracy: 0.3849 - 288ms/epoch - 3ms/step\n",
      "Epoch 10/10000\n",
      "113/113 - 0s - loss: 4.5071 - accuracy: 0.3879 - val_loss: 4.0566 - val_accuracy: 0.3848 - 288ms/epoch - 3ms/step\n",
      "Epoch 11/10000\n",
      "113/113 - 0s - loss: 3.5885 - accuracy: 0.3877 - val_loss: 3.1346 - val_accuracy: 0.3841 - 288ms/epoch - 3ms/step\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Iterazione 7/128 (03-11-2023_10-20-26)\n",
      "hidden_layer_size: 20\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 1\n",
      "activation_hidden: sigmoid\n",
      "activation_output: sigmoid\n",
      "batch_size: 1000\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 10\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 38.56%\n",
      "La recall di questo modello sul validation set è: 99.28%\n",
      "La f1 di questo modello sul validation set è: 55.56%\n",
      "La balanced accuracy di questo modello sul validation set è: 49.76%\n",
      "La precision di questo modello sul validation set è: 38.58%\n",
      "La AUC di questo modello sul validation set è: 49.42%\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.34    0.00      0.00  7,670.00\n",
      "1                  0.39    0.99      0.56  4,841.00\n",
      "accuracy           0.39    0.39      0.39      0.39\n",
      "macro avg          0.36    0.50      0.28 12,511.00\n",
      "weighted avg       0.36    0.39      0.22 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0           18         7652\n",
      "Actual 1           35         4806\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 65.94% con il modello:\n",
      "1 - AC65.94%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 62.97% con il modello:\n",
      "1 - BA62.97%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 56.83% con il modello:\n",
      "1 - PR56.83%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "113/113 - 1s - loss: 13.0397 - accuracy: 0.3883 - val_loss: 12.6035 - val_accuracy: 0.3856 - 1s/epoch - 9ms/step\n",
      "Epoch 2/10000\n",
      "113/113 - 0s - loss: 12.0673 - accuracy: 0.3879 - val_loss: 11.6257 - val_accuracy: 0.3856 - 298ms/epoch - 3ms/step\n",
      "Epoch 3/10000\n",
      "113/113 - 0s - loss: 11.0929 - accuracy: 0.3880 - val_loss: 10.6407 - val_accuracy: 0.3855 - 354ms/epoch - 3ms/step\n",
      "Epoch 4/10000\n",
      "113/113 - 0s - loss: 10.1376 - accuracy: 0.3880 - val_loss: 9.6888 - val_accuracy: 0.3855 - 266ms/epoch - 2ms/step\n",
      "Epoch 5/10000\n",
      "113/113 - 0s - loss: 9.1898 - accuracy: 0.3880 - val_loss: 8.7464 - val_accuracy: 0.3855 - 292ms/epoch - 3ms/step\n",
      "Epoch 6/10000\n",
      "113/113 - 0s - loss: 8.2444 - accuracy: 0.3880 - val_loss: 7.7930 - val_accuracy: 0.3855 - 277ms/epoch - 2ms/step\n",
      "Epoch 7/10000\n",
      "113/113 - 0s - loss: 7.3092 - accuracy: 0.3880 - val_loss: 6.8524 - val_accuracy: 0.3855 - 282ms/epoch - 2ms/step\n",
      "Epoch 8/10000\n",
      "113/113 - 0s - loss: 6.3642 - accuracy: 0.3880 - val_loss: 5.8950 - val_accuracy: 0.3848 - 269ms/epoch - 2ms/step\n",
      "Epoch 9/10000\n",
      "113/113 - 0s - loss: 5.4262 - accuracy: 0.3878 - val_loss: 4.9706 - val_accuracy: 0.3849 - 295ms/epoch - 3ms/step\n",
      "Epoch 10/10000\n",
      "113/113 - 0s - loss: 4.5071 - accuracy: 0.3879 - val_loss: 4.0566 - val_accuracy: 0.3848 - 291ms/epoch - 3ms/step\n",
      "Epoch 11/10000\n",
      "113/113 - 0s - loss: 3.5885 - accuracy: 0.3877 - val_loss: 3.1346 - val_accuracy: 0.3841 - 285ms/epoch - 3ms/step\n",
      "Epoch 12/10000\n",
      "113/113 - 0s - loss: 2.6869 - accuracy: 0.3876 - val_loss: 2.2583 - val_accuracy: 0.3841 - 306ms/epoch - 3ms/step\n",
      "Epoch 13/10000\n",
      "113/113 - 0s - loss: 1.8696 - accuracy: 0.3876 - val_loss: 1.5229 - val_accuracy: 0.3841 - 301ms/epoch - 3ms/step\n",
      "Epoch 14/10000\n",
      "113/113 - 0s - loss: 1.2616 - accuracy: 0.3878 - val_loss: 1.0605 - val_accuracy: 0.3842 - 297ms/epoch - 3ms/step\n",
      "Epoch 15/10000\n",
      "113/113 - 0s - loss: 0.9490 - accuracy: 0.3878 - val_loss: 0.8786 - val_accuracy: 0.3841 - 289ms/epoch - 3ms/step\n",
      "Epoch 16/10000\n",
      "113/113 - 0s - loss: 0.8362 - accuracy: 0.5161 - val_loss: 0.8180 - val_accuracy: 0.5926 - 297ms/epoch - 3ms/step\n",
      "Epoch 17/10000\n",
      "113/113 - 0s - loss: 0.7990 - accuracy: 0.5940 - val_loss: 0.7972 - val_accuracy: 0.5940 - 312ms/epoch - 3ms/step\n",
      "Epoch 18/10000\n",
      "113/113 - 0s - loss: 0.7838 - accuracy: 0.5955 - val_loss: 0.7876 - val_accuracy: 0.5948 - 306ms/epoch - 3ms/step\n",
      "Epoch 19/10000\n",
      "113/113 - 0s - loss: 0.7755 - accuracy: 0.5965 - val_loss: 0.7795 - val_accuracy: 0.5962 - 296ms/epoch - 3ms/step\n",
      "Epoch 20/10000\n",
      "113/113 - 0s - loss: 0.7657 - accuracy: 0.5976 - val_loss: 0.7701 - val_accuracy: 0.5976 - 299ms/epoch - 3ms/step\n",
      "Epoch 21/10000\n",
      "113/113 - 0s - loss: 0.7567 - accuracy: 0.5990 - val_loss: 0.7573 - val_accuracy: 0.6000 - 301ms/epoch - 3ms/step\n",
      "Epoch 22/10000\n",
      "113/113 - 0s - loss: 0.7466 - accuracy: 0.6010 - val_loss: 0.7530 - val_accuracy: 0.6007 - 295ms/epoch - 3ms/step\n",
      "Epoch 23/10000\n",
      "113/113 - 0s - loss: 0.7436 - accuracy: 0.6011 - val_loss: 0.7485 - val_accuracy: 0.6007 - 295ms/epoch - 3ms/step\n",
      "Epoch 24/10000\n",
      "113/113 - 0s - loss: 0.7370 - accuracy: 0.6021 - val_loss: 0.7407 - val_accuracy: 0.6013 - 298ms/epoch - 3ms/step\n",
      "Epoch 25/10000\n",
      "113/113 - 0s - loss: 0.7307 - accuracy: 0.6029 - val_loss: 0.7349 - val_accuracy: 0.6026 - 312ms/epoch - 3ms/step\n",
      "Epoch 26/10000\n",
      "113/113 - 0s - loss: 0.7249 - accuracy: 0.6037 - val_loss: 0.7313 - val_accuracy: 0.6032 - 314ms/epoch - 3ms/step\n",
      "Epoch 27/10000\n",
      "113/113 - 0s - loss: 0.7209 - accuracy: 0.6045 - val_loss: 0.7278 - val_accuracy: 0.6039 - 302ms/epoch - 3ms/step\n",
      "Epoch 28/10000\n",
      "113/113 - 0s - loss: 0.7191 - accuracy: 0.6047 - val_loss: 0.7263 - val_accuracy: 0.6042 - 293ms/epoch - 3ms/step\n",
      "Epoch 29/10000\n",
      "113/113 - 0s - loss: 0.7174 - accuracy: 0.6050 - val_loss: 0.7254 - val_accuracy: 0.6039 - 309ms/epoch - 3ms/step\n",
      "Epoch 30/10000\n",
      "113/113 - 0s - loss: 0.7162 - accuracy: 0.6050 - val_loss: 0.7234 - val_accuracy: 0.6041 - 299ms/epoch - 3ms/step\n",
      "Epoch 31/10000\n",
      "113/113 - 0s - loss: 0.7152 - accuracy: 0.6049 - val_loss: 0.7218 - val_accuracy: 0.6042 - 310ms/epoch - 3ms/step\n",
      "Epoch 32/10000\n",
      "113/113 - 0s - loss: 0.7142 - accuracy: 0.6050 - val_loss: 0.7205 - val_accuracy: 0.6043 - 302ms/epoch - 3ms/step\n",
      "Epoch 33/10000\n",
      "113/113 - 0s - loss: 0.7128 - accuracy: 0.6051 - val_loss: 0.7194 - val_accuracy: 0.6043 - 318ms/epoch - 3ms/step\n",
      "Epoch 34/10000\n",
      "113/113 - 0s - loss: 0.7120 - accuracy: 0.6051 - val_loss: 0.7175 - val_accuracy: 0.6043 - 285ms/epoch - 3ms/step\n",
      "Epoch 35/10000\n",
      "113/113 - 0s - loss: 0.7113 - accuracy: 0.6050 - val_loss: 0.7168 - val_accuracy: 0.6039 - 316ms/epoch - 3ms/step\n",
      "Epoch 36/10000\n",
      "113/113 - 0s - loss: 0.7107 - accuracy: 0.6051 - val_loss: 0.7165 - val_accuracy: 0.6041 - 286ms/epoch - 3ms/step\n",
      "Epoch 37/10000\n",
      "113/113 - 0s - loss: 0.7099 - accuracy: 0.6051 - val_loss: 0.7161 - val_accuracy: 0.6042 - 301ms/epoch - 3ms/step\n",
      "Epoch 38/10000\n",
      "113/113 - 0s - loss: 0.7097 - accuracy: 0.6051 - val_loss: 0.7134 - val_accuracy: 0.6049 - 291ms/epoch - 3ms/step\n",
      "Epoch 39/10000\n",
      "113/113 - 0s - loss: 0.7090 - accuracy: 0.6053 - val_loss: 0.7127 - val_accuracy: 0.6047 - 344ms/epoch - 3ms/step\n",
      "Epoch 40/10000\n",
      "113/113 - 0s - loss: 0.7085 - accuracy: 0.6051 - val_loss: 0.7124 - val_accuracy: 0.6044 - 278ms/epoch - 2ms/step\n",
      "Epoch 41/10000\n",
      "113/113 - 0s - loss: 0.7076 - accuracy: 0.6051 - val_loss: 0.7123 - val_accuracy: 0.6043 - 278ms/epoch - 2ms/step\n",
      "Epoch 42/10000\n",
      "113/113 - 0s - loss: 0.7069 - accuracy: 0.6052 - val_loss: 0.7122 - val_accuracy: 0.6041 - 289ms/epoch - 3ms/step\n",
      "Epoch 43/10000\n",
      "113/113 - 0s - loss: 0.7064 - accuracy: 0.6052 - val_loss: 0.7110 - val_accuracy: 0.6042 - 299ms/epoch - 3ms/step\n",
      "Epoch 44/10000\n",
      "113/113 - 0s - loss: 0.7059 - accuracy: 0.6052 - val_loss: 0.7094 - val_accuracy: 0.6043 - 321ms/epoch - 3ms/step\n",
      "Epoch 45/10000\n",
      "113/113 - 0s - loss: 0.7056 - accuracy: 0.6050 - val_loss: 0.7095 - val_accuracy: 0.6042 - 302ms/epoch - 3ms/step\n",
      "Epoch 46/10000\n",
      "113/113 - 0s - loss: 0.7050 - accuracy: 0.6050 - val_loss: 0.7087 - val_accuracy: 0.6043 - 318ms/epoch - 3ms/step\n",
      "Epoch 47/10000\n",
      "113/113 - 0s - loss: 0.7041 - accuracy: 0.6050 - val_loss: 0.7074 - val_accuracy: 0.6042 - 286ms/epoch - 3ms/step\n",
      "Epoch 48/10000\n",
      "113/113 - 0s - loss: 0.7036 - accuracy: 0.6051 - val_loss: 0.7063 - val_accuracy: 0.6043 - 300ms/epoch - 3ms/step\n",
      "Epoch 49/10000\n",
      "113/113 - 0s - loss: 0.7030 - accuracy: 0.6051 - val_loss: 0.7054 - val_accuracy: 0.6045 - 290ms/epoch - 3ms/step\n",
      "Epoch 50/10000\n",
      "113/113 - 0s - loss: 0.7025 - accuracy: 0.6052 - val_loss: 0.7047 - val_accuracy: 0.6043 - 298ms/epoch - 3ms/step\n",
      "Epoch 51/10000\n",
      "113/113 - 0s - loss: 0.7019 - accuracy: 0.6052 - val_loss: 0.7043 - val_accuracy: 0.6044 - 285ms/epoch - 3ms/step\n",
      "Epoch 52/10000\n",
      "113/113 - 0s - loss: 0.7013 - accuracy: 0.6052 - val_loss: 0.7038 - val_accuracy: 0.6043 - 315ms/epoch - 3ms/step\n",
      "Epoch 53/10000\n",
      "113/113 - 0s - loss: 0.7009 - accuracy: 0.6053 - val_loss: 0.7035 - val_accuracy: 0.6043 - 310ms/epoch - 3ms/step\n",
      "Epoch 54/10000\n",
      "113/113 - 0s - loss: 0.7006 - accuracy: 0.6053 - val_loss: 0.7030 - val_accuracy: 0.6044 - 295ms/epoch - 3ms/step\n",
      "Epoch 55/10000\n",
      "113/113 - 0s - loss: 0.7000 - accuracy: 0.6052 - val_loss: 0.7026 - val_accuracy: 0.6043 - 303ms/epoch - 3ms/step\n",
      "Epoch 56/10000\n",
      "113/113 - 0s - loss: 0.6997 - accuracy: 0.6052 - val_loss: 0.7023 - val_accuracy: 0.6043 - 293ms/epoch - 3ms/step\n",
      "Epoch 57/10000\n",
      "113/113 - 0s - loss: 0.6990 - accuracy: 0.6051 - val_loss: 0.7028 - val_accuracy: 0.6043 - 286ms/epoch - 3ms/step\n",
      "Epoch 58/10000\n",
      "113/113 - 0s - loss: 0.6986 - accuracy: 0.6052 - val_loss: 0.7019 - val_accuracy: 0.6042 - 299ms/epoch - 3ms/step\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Iterazione 8/128 (03-11-2023_10-20-46)\n",
      "hidden_layer_size: 20\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 1\n",
      "activation_hidden: sigmoid\n",
      "activation_output: sigmoid\n",
      "batch_size: 1000\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 20\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 60.49%\n",
      "La recall di questo modello sul validation set è: 4.15%\n",
      "La f1 di questo modello sul validation set è: 7.52%\n",
      "La balanced accuracy di questo modello sul validation set è: 50.1%\n",
      "La precision di questo modello sul validation set è: 39.88%\n",
      "La AUC di questo modello sul validation set è: 49.45%\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.61    0.96      0.75  7,670.00\n",
      "1                  0.40    0.04      0.08  4,841.00\n",
      "accuracy           0.60    0.60      0.60      0.60\n",
      "macro avg          0.51    0.50      0.41 12,511.00\n",
      "weighted avg       0.53    0.60      0.49 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         7367          303\n",
      "Actual 1         4640          201\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 65.94% con il modello:\n",
      "1 - AC65.94%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 62.97% con il modello:\n",
      "1 - BA62.97%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 56.83% con il modello:\n",
      "1 - PR56.83%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "1126/1126 - 3s - loss: 11858903.0000 - accuracy: 0.3994 - val_loss: 88742.4062 - val_accuracy: 0.4729 - 3s/epoch - 3ms/step\n",
      "Epoch 2/10000\n",
      "1126/1126 - 2s - loss: 61351.8320 - accuracy: 0.4823 - val_loss: 37820.0547 - val_accuracy: 0.5020 - 2s/epoch - 2ms/step\n",
      "Epoch 3/10000\n",
      "1126/1126 - 2s - loss: 27158.2402 - accuracy: 0.5277 - val_loss: 19188.4688 - val_accuracy: 0.5498 - 2s/epoch - 2ms/step\n",
      "Epoch 4/10000\n",
      "1126/1126 - 2s - loss: 14982.6533 - accuracy: 0.5649 - val_loss: 12317.1328 - val_accuracy: 0.5767 - 2s/epoch - 2ms/step\n",
      "Epoch 5/10000\n",
      "1126/1126 - 2s - loss: 9878.3770 - accuracy: 0.5687 - val_loss: 8116.6167 - val_accuracy: 0.5858 - 2s/epoch - 2ms/step\n",
      "Epoch 6/10000\n",
      "1126/1126 - 2s - loss: 7093.8853 - accuracy: 0.5567 - val_loss: 6085.5088 - val_accuracy: 0.5079 - 2s/epoch - 2ms/step\n",
      "Epoch 7/10000\n",
      "1126/1126 - 2s - loss: 5655.9614 - accuracy: 0.5553 - val_loss: 4798.6650 - val_accuracy: 0.5952 - 2s/epoch - 2ms/step\n",
      "Epoch 8/10000\n",
      "1126/1126 - 2s - loss: 4602.8472 - accuracy: 0.5580 - val_loss: 3055.1042 - val_accuracy: 0.6012 - 2s/epoch - 2ms/step\n",
      "Epoch 9/10000\n",
      "1126/1126 - 2s - loss: 4113.6875 - accuracy: 0.5587 - val_loss: 2620.8875 - val_accuracy: 0.6008 - 2s/epoch - 2ms/step\n",
      "Epoch 10/10000\n",
      "1126/1126 - 2s - loss: 3926.1406 - accuracy: 0.5599 - val_loss: 2658.2771 - val_accuracy: 0.6038 - 2s/epoch - 2ms/step\n",
      "Epoch 11/10000\n",
      "1126/1126 - 2s - loss: 3791.2549 - accuracy: 0.5595 - val_loss: 7342.1694 - val_accuracy: 0.4631 - 2s/epoch - 2ms/step\n",
      "Epoch 12/10000\n",
      "1126/1126 - 2s - loss: 3570.2639 - accuracy: 0.5611 - val_loss: 6231.1055 - val_accuracy: 0.6020 - 2s/epoch - 2ms/step\n",
      "Epoch 13/10000\n",
      "1126/1126 - 2s - loss: 3776.9124 - accuracy: 0.5607 - val_loss: 2183.0969 - val_accuracy: 0.6140 - 2s/epoch - 2ms/step\n",
      "Epoch 14/10000\n",
      "1126/1126 - 2s - loss: 3311.9231 - accuracy: 0.5606 - val_loss: 2142.7305 - val_accuracy: 0.5418 - 2s/epoch - 2ms/step\n",
      "Epoch 15/10000\n",
      "1126/1126 - 2s - loss: 3483.4321 - accuracy: 0.5601 - val_loss: 6893.5020 - val_accuracy: 0.6059 - 2s/epoch - 2ms/step\n",
      "Epoch 16/10000\n",
      "1126/1126 - 2s - loss: 3143.6992 - accuracy: 0.5624 - val_loss: 1552.4740 - val_accuracy: 0.5803 - 2s/epoch - 2ms/step\n",
      "Epoch 17/10000\n",
      "1126/1126 - 2s - loss: 3339.0498 - accuracy: 0.5624 - val_loss: 2997.5212 - val_accuracy: 0.6095 - 2s/epoch - 2ms/step\n",
      "Epoch 18/10000\n",
      "1126/1126 - 2s - loss: 3026.5151 - accuracy: 0.5650 - val_loss: 1812.7979 - val_accuracy: 0.5398 - 2s/epoch - 2ms/step\n",
      "Epoch 19/10000\n",
      "1126/1126 - 2s - loss: 2928.8318 - accuracy: 0.5640 - val_loss: 3116.3088 - val_accuracy: 0.6051 - 2s/epoch - 2ms/step\n",
      "Epoch 20/10000\n",
      "1126/1126 - 2s - loss: 2985.0320 - accuracy: 0.5652 - val_loss: 1874.7549 - val_accuracy: 0.6209 - 2s/epoch - 2ms/step\n",
      "Epoch 21/10000\n",
      "1126/1126 - 2s - loss: 2768.1086 - accuracy: 0.5678 - val_loss: 2272.4028 - val_accuracy: 0.6103 - 2s/epoch - 2ms/step\n",
      "Epoch 22/10000\n",
      "1126/1126 - 2s - loss: 2778.5037 - accuracy: 0.5663 - val_loss: 3044.8108 - val_accuracy: 0.6084 - 2s/epoch - 2ms/step\n",
      "Epoch 23/10000\n",
      "1126/1126 - 2s - loss: 2504.2249 - accuracy: 0.5695 - val_loss: 2397.2300 - val_accuracy: 0.4971 - 2s/epoch - 2ms/step\n",
      "Epoch 24/10000\n",
      "1126/1126 - 2s - loss: 2660.0051 - accuracy: 0.5702 - val_loss: 1207.9612 - val_accuracy: 0.6219 - 2s/epoch - 2ms/step\n",
      "Epoch 25/10000\n",
      "1126/1126 - 2s - loss: 2582.3540 - accuracy: 0.5703 - val_loss: 5761.3481 - val_accuracy: 0.6103 - 2s/epoch - 2ms/step\n",
      "Epoch 26/10000\n",
      "1126/1126 - 2s - loss: 2653.8501 - accuracy: 0.5691 - val_loss: 1359.3225 - val_accuracy: 0.6163 - 2s/epoch - 2ms/step\n",
      "Epoch 27/10000\n",
      "1126/1126 - 2s - loss: 2414.4077 - accuracy: 0.5703 - val_loss: 1687.9545 - val_accuracy: 0.6197 - 2s/epoch - 2ms/step\n",
      "Epoch 28/10000\n",
      "1126/1126 - 2s - loss: 2639.8367 - accuracy: 0.5679 - val_loss: 1262.3986 - val_accuracy: 0.5493 - 2s/epoch - 2ms/step\n",
      "Epoch 29/10000\n",
      "1126/1126 - 2s - loss: 2542.0908 - accuracy: 0.5696 - val_loss: 4462.0137 - val_accuracy: 0.4753 - 2s/epoch - 2ms/step\n",
      "Epoch 30/10000\n",
      "1126/1126 - 2s - loss: 2288.5691 - accuracy: 0.5730 - val_loss: 2018.9735 - val_accuracy: 0.6104 - 2s/epoch - 2ms/step\n",
      "Epoch 31/10000\n",
      "1126/1126 - 2s - loss: 2419.1924 - accuracy: 0.5711 - val_loss: 5043.6401 - val_accuracy: 0.4514 - 2s/epoch - 2ms/step\n",
      "Epoch 32/10000\n",
      "1126/1126 - 2s - loss: 2340.6824 - accuracy: 0.5722 - val_loss: 4378.9829 - val_accuracy: 0.6127 - 2s/epoch - 2ms/step\n",
      "Epoch 33/10000\n",
      "1126/1126 - 2s - loss: 2320.8794 - accuracy: 0.5736 - val_loss: 2335.1492 - val_accuracy: 0.4797 - 2s/epoch - 2ms/step\n",
      "Epoch 34/10000\n",
      "1126/1126 - 2s - loss: 2293.2156 - accuracy: 0.5691 - val_loss: 2519.4612 - val_accuracy: 0.4764 - 2s/epoch - 2ms/step\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Iterazione 9/128 (03-11-2023_10-22-00)\n",
      "hidden_layer_size: 20\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 2\n",
      "activation_hidden: relu\n",
      "activation_output: sigmoid\n",
      "batch_size: 100\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 10\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 62.19%\n",
      "La recall di questo modello sul validation set è: 17.27%\n",
      "La f1 di questo modello sul validation set è: 26.12%\n",
      "La balanced accuracy di questo modello sul validation set è: 53.91%\n",
      "La precision di questo modello sul validation set è: 53.56%\n",
      "La AUC di questo modello sul validation set è: 53.95%\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.63    0.91      0.75  7,670.00\n",
      "1                  0.54    0.17      0.26  4,841.00\n",
      "accuracy           0.62    0.62      0.62      0.62\n",
      "macro avg          0.58    0.54      0.50 12,511.00\n",
      "weighted avg       0.60    0.62      0.56 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         6945          725\n",
      "Actual 1         4005          836\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 65.94% con il modello:\n",
      "1 - AC65.94%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 62.97% con il modello:\n",
      "1 - BA62.97%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 56.83% con il modello:\n",
      "1 - PR56.83%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "1126/1126 - 3s - loss: 11858903.0000 - accuracy: 0.3994 - val_loss: 88742.4062 - val_accuracy: 0.4729 - 3s/epoch - 3ms/step\n",
      "Epoch 2/10000\n",
      "1126/1126 - 2s - loss: 61351.8320 - accuracy: 0.4823 - val_loss: 37820.0547 - val_accuracy: 0.5020 - 2s/epoch - 2ms/step\n",
      "Epoch 3/10000\n",
      "1126/1126 - 2s - loss: 27158.2402 - accuracy: 0.5277 - val_loss: 19188.4688 - val_accuracy: 0.5498 - 2s/epoch - 2ms/step\n",
      "Epoch 4/10000\n",
      "1126/1126 - 2s - loss: 14982.6533 - accuracy: 0.5649 - val_loss: 12317.1328 - val_accuracy: 0.5767 - 2s/epoch - 2ms/step\n",
      "Epoch 5/10000\n",
      "1126/1126 - 2s - loss: 9878.3770 - accuracy: 0.5687 - val_loss: 8116.6167 - val_accuracy: 0.5858 - 2s/epoch - 2ms/step\n",
      "Epoch 6/10000\n",
      "1126/1126 - 2s - loss: 7093.8853 - accuracy: 0.5567 - val_loss: 6085.5088 - val_accuracy: 0.5079 - 2s/epoch - 2ms/step\n",
      "Epoch 7/10000\n",
      "1126/1126 - 2s - loss: 5655.9614 - accuracy: 0.5553 - val_loss: 4798.6650 - val_accuracy: 0.5952 - 2s/epoch - 2ms/step\n",
      "Epoch 8/10000\n",
      "1126/1126 - 2s - loss: 4602.8472 - accuracy: 0.5580 - val_loss: 3055.1042 - val_accuracy: 0.6012 - 2s/epoch - 2ms/step\n",
      "Epoch 9/10000\n",
      "1126/1126 - 2s - loss: 4113.6875 - accuracy: 0.5587 - val_loss: 2620.8875 - val_accuracy: 0.6008 - 2s/epoch - 2ms/step\n",
      "Epoch 10/10000\n",
      "1126/1126 - 2s - loss: 3926.1406 - accuracy: 0.5599 - val_loss: 2658.2771 - val_accuracy: 0.6038 - 2s/epoch - 2ms/step\n",
      "Epoch 11/10000\n",
      "1126/1126 - 2s - loss: 3791.2549 - accuracy: 0.5595 - val_loss: 7342.1694 - val_accuracy: 0.4631 - 2s/epoch - 2ms/step\n",
      "Epoch 12/10000\n",
      "1126/1126 - 2s - loss: 3570.2639 - accuracy: 0.5611 - val_loss: 6231.1055 - val_accuracy: 0.6020 - 2s/epoch - 2ms/step\n",
      "Epoch 13/10000\n",
      "1126/1126 - 2s - loss: 3776.9124 - accuracy: 0.5607 - val_loss: 2183.0969 - val_accuracy: 0.6140 - 2s/epoch - 2ms/step\n",
      "Epoch 14/10000\n",
      "1126/1126 - 2s - loss: 3311.9231 - accuracy: 0.5606 - val_loss: 2142.7305 - val_accuracy: 0.5418 - 2s/epoch - 2ms/step\n",
      "Epoch 15/10000\n",
      "1126/1126 - 2s - loss: 3483.4321 - accuracy: 0.5601 - val_loss: 6893.5020 - val_accuracy: 0.6059 - 2s/epoch - 2ms/step\n",
      "Epoch 16/10000\n",
      "1126/1126 - 2s - loss: 3143.6992 - accuracy: 0.5624 - val_loss: 1552.4740 - val_accuracy: 0.5803 - 2s/epoch - 2ms/step\n",
      "Epoch 17/10000\n",
      "1126/1126 - 2s - loss: 3339.0498 - accuracy: 0.5624 - val_loss: 2997.5212 - val_accuracy: 0.6095 - 2s/epoch - 2ms/step\n",
      "Epoch 18/10000\n",
      "1126/1126 - 2s - loss: 3026.5151 - accuracy: 0.5650 - val_loss: 1812.7979 - val_accuracy: 0.5398 - 2s/epoch - 2ms/step\n",
      "Epoch 19/10000\n",
      "1126/1126 - 2s - loss: 2928.8318 - accuracy: 0.5640 - val_loss: 3116.3088 - val_accuracy: 0.6051 - 2s/epoch - 2ms/step\n",
      "Epoch 20/10000\n",
      "1126/1126 - 2s - loss: 2985.0320 - accuracy: 0.5652 - val_loss: 1874.7549 - val_accuracy: 0.6209 - 2s/epoch - 2ms/step\n",
      "Epoch 21/10000\n",
      "1126/1126 - 2s - loss: 2768.1086 - accuracy: 0.5678 - val_loss: 2272.4028 - val_accuracy: 0.6103 - 2s/epoch - 2ms/step\n",
      "Epoch 22/10000\n",
      "1126/1126 - 2s - loss: 2778.5037 - accuracy: 0.5663 - val_loss: 3044.8108 - val_accuracy: 0.6084 - 2s/epoch - 2ms/step\n",
      "Epoch 23/10000\n",
      "1126/1126 - 2s - loss: 2504.2249 - accuracy: 0.5695 - val_loss: 2397.2300 - val_accuracy: 0.4971 - 2s/epoch - 2ms/step\n",
      "Epoch 24/10000\n",
      "1126/1126 - 2s - loss: 2660.0051 - accuracy: 0.5702 - val_loss: 1207.9612 - val_accuracy: 0.6219 - 2s/epoch - 2ms/step\n",
      "Epoch 25/10000\n",
      "1126/1126 - 2s - loss: 2582.3540 - accuracy: 0.5703 - val_loss: 5761.3481 - val_accuracy: 0.6103 - 2s/epoch - 2ms/step\n",
      "Epoch 26/10000\n",
      "1126/1126 - 2s - loss: 2653.8501 - accuracy: 0.5691 - val_loss: 1359.3225 - val_accuracy: 0.6163 - 2s/epoch - 2ms/step\n",
      "Epoch 27/10000\n",
      "1126/1126 - 2s - loss: 2414.4077 - accuracy: 0.5703 - val_loss: 1687.9545 - val_accuracy: 0.6197 - 2s/epoch - 2ms/step\n",
      "Epoch 28/10000\n",
      "1126/1126 - 2s - loss: 2639.8367 - accuracy: 0.5679 - val_loss: 1262.3986 - val_accuracy: 0.5493 - 2s/epoch - 2ms/step\n",
      "Epoch 29/10000\n",
      "1126/1126 - 2s - loss: 2542.0908 - accuracy: 0.5696 - val_loss: 4462.0137 - val_accuracy: 0.4753 - 2s/epoch - 2ms/step\n",
      "Epoch 30/10000\n",
      "1126/1126 - 3s - loss: 2288.5691 - accuracy: 0.5730 - val_loss: 2018.9735 - val_accuracy: 0.6104 - 3s/epoch - 3ms/step\n",
      "Epoch 31/10000\n",
      "1126/1126 - 2s - loss: 2419.1924 - accuracy: 0.5711 - val_loss: 5043.6401 - val_accuracy: 0.4514 - 2s/epoch - 2ms/step\n",
      "Epoch 32/10000\n",
      "1126/1126 - 3s - loss: 2340.6824 - accuracy: 0.5722 - val_loss: 4378.9829 - val_accuracy: 0.6127 - 3s/epoch - 2ms/step\n",
      "Epoch 33/10000\n",
      "1126/1126 - 3s - loss: 2320.8794 - accuracy: 0.5736 - val_loss: 2335.1492 - val_accuracy: 0.4797 - 3s/epoch - 2ms/step\n",
      "Epoch 34/10000\n",
      "1126/1126 - 3s - loss: 2293.2156 - accuracy: 0.5691 - val_loss: 2519.4612 - val_accuracy: 0.4764 - 3s/epoch - 2ms/step\n",
      "Epoch 35/10000\n",
      "1126/1126 - 2s - loss: 2308.6401 - accuracy: 0.5712 - val_loss: 1253.3467 - val_accuracy: 0.6240 - 2s/epoch - 2ms/step\n",
      "Epoch 36/10000\n",
      "1126/1126 - 2s - loss: 2075.1619 - accuracy: 0.5749 - val_loss: 3655.2839 - val_accuracy: 0.4582 - 2s/epoch - 2ms/step\n",
      "Epoch 37/10000\n",
      "1126/1126 - 2s - loss: 2620.0669 - accuracy: 0.5715 - val_loss: 1430.2849 - val_accuracy: 0.6272 - 2s/epoch - 2ms/step\n",
      "Epoch 38/10000\n",
      "1126/1126 - 2s - loss: 2356.6255 - accuracy: 0.5760 - val_loss: 3133.0366 - val_accuracy: 0.6098 - 2s/epoch - 2ms/step\n",
      "Epoch 39/10000\n",
      "1126/1126 - 3s - loss: 2274.7996 - accuracy: 0.5737 - val_loss: 895.1557 - val_accuracy: 0.6318 - 3s/epoch - 3ms/step\n",
      "Epoch 40/10000\n",
      "1126/1126 - 3s - loss: 2291.9834 - accuracy: 0.5740 - val_loss: 3702.3601 - val_accuracy: 0.4744 - 3s/epoch - 3ms/step\n",
      "Epoch 41/10000\n",
      "1126/1126 - 3s - loss: 2072.0610 - accuracy: 0.5771 - val_loss: 4646.2065 - val_accuracy: 0.4652 - 3s/epoch - 2ms/step\n",
      "Epoch 42/10000\n",
      "1126/1126 - 3s - loss: 2193.6821 - accuracy: 0.5776 - val_loss: 841.8378 - val_accuracy: 0.6094 - 3s/epoch - 3ms/step\n",
      "Epoch 43/10000\n",
      "1126/1126 - 3s - loss: 1964.4523 - accuracy: 0.5766 - val_loss: 1018.1195 - val_accuracy: 0.6231 - 3s/epoch - 2ms/step\n",
      "Epoch 44/10000\n",
      "1126/1126 - 2s - loss: 2022.0383 - accuracy: 0.5766 - val_loss: 546.8027 - val_accuracy: 0.6402 - 2s/epoch - 2ms/step\n",
      "Epoch 45/10000\n",
      "1126/1126 - 3s - loss: 2005.4171 - accuracy: 0.5789 - val_loss: 2164.6660 - val_accuracy: 0.4860 - 3s/epoch - 3ms/step\n",
      "Epoch 46/10000\n",
      "1126/1126 - 5s - loss: 2124.3730 - accuracy: 0.5767 - val_loss: 3436.2241 - val_accuracy: 0.6146 - 5s/epoch - 4ms/step\n",
      "Epoch 47/10000\n",
      "1126/1126 - 8s - loss: 2021.2831 - accuracy: 0.5764 - val_loss: 1288.9438 - val_accuracy: 0.6211 - 8s/epoch - 7ms/step\n",
      "Epoch 48/10000\n",
      "1126/1126 - 7s - loss: 2121.5889 - accuracy: 0.5779 - val_loss: 2050.8057 - val_accuracy: 0.5171 - 7s/epoch - 6ms/step\n",
      "Epoch 49/10000\n",
      "1126/1126 - 10s - loss: 2056.1621 - accuracy: 0.5785 - val_loss: 557.3186 - val_accuracy: 0.6358 - 10s/epoch - 9ms/step\n",
      "Epoch 50/10000\n",
      "1126/1126 - 8s - loss: 1876.5754 - accuracy: 0.5789 - val_loss: 1185.9968 - val_accuracy: 0.5483 - 8s/epoch - 7ms/step\n",
      "Epoch 51/10000\n",
      "1126/1126 - 5s - loss: 1916.7399 - accuracy: 0.5782 - val_loss: 1459.9087 - val_accuracy: 0.5007 - 5s/epoch - 4ms/step\n",
      "Epoch 52/10000\n",
      "1126/1126 - 5s - loss: 2084.4431 - accuracy: 0.5779 - val_loss: 954.8704 - val_accuracy: 0.6261 - 5s/epoch - 4ms/step\n",
      "Epoch 53/10000\n",
      "1126/1126 - 4s - loss: 1947.6637 - accuracy: 0.5802 - val_loss: 1754.5259 - val_accuracy: 0.4984 - 4s/epoch - 4ms/step\n",
      "Epoch 54/10000\n",
      "1126/1126 - 4s - loss: 1944.3314 - accuracy: 0.5786 - val_loss: 1103.1101 - val_accuracy: 0.5815 - 4s/epoch - 3ms/step\n",
      "Epoch 55/10000\n",
      "1126/1126 - 3s - loss: 1928.7910 - accuracy: 0.5810 - val_loss: 1273.8259 - val_accuracy: 0.5473 - 3s/epoch - 3ms/step\n",
      "Epoch 56/10000\n",
      "1126/1126 - 3s - loss: 2186.2898 - accuracy: 0.5789 - val_loss: 3024.2327 - val_accuracy: 0.4757 - 3s/epoch - 3ms/step\n",
      "Epoch 57/10000\n",
      "1126/1126 - 3s - loss: 1944.9545 - accuracy: 0.5813 - val_loss: 4865.5840 - val_accuracy: 0.4618 - 3s/epoch - 2ms/step\n",
      "Epoch 58/10000\n",
      "1126/1126 - 3s - loss: 2043.0896 - accuracy: 0.5825 - val_loss: 2437.2561 - val_accuracy: 0.6168 - 3s/epoch - 2ms/step\n",
      "Epoch 59/10000\n",
      "1126/1126 - 3s - loss: 1752.2041 - accuracy: 0.5831 - val_loss: 2256.5745 - val_accuracy: 0.6171 - 3s/epoch - 2ms/step\n",
      "Epoch 60/10000\n",
      "1126/1126 - 3s - loss: 1831.0322 - accuracy: 0.5822 - val_loss: 614.4641 - val_accuracy: 0.5872 - 3s/epoch - 3ms/step\n",
      "Epoch 61/10000\n",
      "1126/1126 - 3s - loss: 1847.0753 - accuracy: 0.5815 - val_loss: 3455.4507 - val_accuracy: 0.6211 - 3s/epoch - 3ms/step\n",
      "Epoch 62/10000\n",
      "1126/1126 - 3s - loss: 1916.6938 - accuracy: 0.5842 - val_loss: 4609.0542 - val_accuracy: 0.4458 - 3s/epoch - 3ms/step\n",
      "Epoch 63/10000\n",
      "1126/1126 - 3s - loss: 2042.1627 - accuracy: 0.5832 - val_loss: 785.4485 - val_accuracy: 0.6343 - 3s/epoch - 2ms/step\n",
      "Epoch 64/10000\n",
      "1126/1126 - 3s - loss: 2009.7914 - accuracy: 0.5817 - val_loss: 1149.4456 - val_accuracy: 0.6296 - 3s/epoch - 2ms/step\n",
      "391/391 [==============================] - 1s 3ms/step\n",
      "\n",
      "Iterazione 10/128 (03-11-2023_10-25-23)\n",
      "hidden_layer_size: 20\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 2\n",
      "activation_hidden: relu\n",
      "activation_output: sigmoid\n",
      "batch_size: 100\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 20\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 64.02%\n",
      "La recall di questo modello sul validation set è: 35.78%\n",
      "La f1 di questo modello sul validation set è: 43.49%\n",
      "La balanced accuracy di questo modello sul validation set è: 58.81%\n",
      "La precision di questo modello sul validation set è: 55.44%\n",
      "La AUC di questo modello sul validation set è: 59.37%\n",
      "391/391 [==============================] - 1s 3ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.67    0.82      0.74  7,670.00\n",
      "1                  0.55    0.36      0.43  4,841.00\n",
      "accuracy           0.64    0.64      0.64      0.64\n",
      "macro avg          0.61    0.59      0.59 12,511.00\n",
      "weighted avg       0.62    0.64      0.62 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         6278         1392\n",
      "Actual 1         3109         1732\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 65.94% con il modello:\n",
      "1 - AC65.94%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 62.97% con il modello:\n",
      "1 - BA62.97%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 56.83% con il modello:\n",
      "1 - PR56.83%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "113/113 - 3s - loss: 27044498.0000 - accuracy: 0.3887 - val_loss: 25335958.0000 - val_accuracy: 0.3869 - 3s/epoch - 24ms/step\n",
      "Epoch 2/10000\n",
      "113/113 - 0s - loss: 23223684.0000 - accuracy: 0.3887 - val_loss: 21554722.0000 - val_accuracy: 0.3869 - 488ms/epoch - 4ms/step\n",
      "Epoch 3/10000\n",
      "113/113 - 0s - loss: 19517860.0000 - accuracy: 0.3887 - val_loss: 17829784.0000 - val_accuracy: 0.3869 - 412ms/epoch - 4ms/step\n",
      "Epoch 4/10000\n",
      "113/113 - 0s - loss: 15354352.0000 - accuracy: 0.3887 - val_loss: 12893854.0000 - val_accuracy: 0.3869 - 416ms/epoch - 4ms/step\n",
      "Epoch 5/10000\n",
      "113/113 - 0s - loss: 10391228.0000 - accuracy: 0.3887 - val_loss: 8181828.5000 - val_accuracy: 0.3869 - 427ms/epoch - 4ms/step\n",
      "Epoch 6/10000\n",
      "113/113 - 0s - loss: 5985547.5000 - accuracy: 0.3887 - val_loss: 3935357.0000 - val_accuracy: 0.3869 - 410ms/epoch - 4ms/step\n",
      "Epoch 7/10000\n",
      "113/113 - 0s - loss: 1918267.0000 - accuracy: 0.4148 - val_loss: 365836.9375 - val_accuracy: 0.5311 - 406ms/epoch - 4ms/step\n",
      "Epoch 8/10000\n",
      "113/113 - 0s - loss: 302275.5312 - accuracy: 0.5393 - val_loss: 283419.5938 - val_accuracy: 0.5382 - 401ms/epoch - 4ms/step\n",
      "Epoch 9/10000\n",
      "113/113 - 0s - loss: 268261.6875 - accuracy: 0.5376 - val_loss: 258584.2344 - val_accuracy: 0.5342 - 394ms/epoch - 3ms/step\n",
      "Epoch 10/10000\n",
      "113/113 - 0s - loss: 245117.2656 - accuracy: 0.5350 - val_loss: 235737.5469 - val_accuracy: 0.5305 - 410ms/epoch - 4ms/step\n",
      "Epoch 11/10000\n",
      "113/113 - 0s - loss: 222318.9688 - accuracy: 0.5308 - val_loss: 212498.0312 - val_accuracy: 0.5253 - 424ms/epoch - 4ms/step\n",
      "Epoch 12/10000\n",
      "113/113 - 0s - loss: 198714.8281 - accuracy: 0.5251 - val_loss: 188448.4375 - val_accuracy: 0.5151 - 405ms/epoch - 4ms/step\n",
      "Epoch 13/10000\n",
      "113/113 - 0s - loss: 176187.7656 - accuracy: 0.5135 - val_loss: 167859.7188 - val_accuracy: 0.5029 - 416ms/epoch - 4ms/step\n",
      "Epoch 14/10000\n",
      "113/113 - 0s - loss: 158090.3281 - accuracy: 0.4924 - val_loss: 152670.2656 - val_accuracy: 0.4767 - 422ms/epoch - 4ms/step\n",
      "Epoch 15/10000\n",
      "113/113 - 0s - loss: 145158.2344 - accuracy: 0.4644 - val_loss: 142372.1719 - val_accuracy: 0.4521 - 394ms/epoch - 3ms/step\n",
      "Epoch 16/10000\n",
      "113/113 - 0s - loss: 136328.5312 - accuracy: 0.4433 - val_loss: 134993.1250 - val_accuracy: 0.4270 - 406ms/epoch - 4ms/step\n",
      "Epoch 17/10000\n",
      "113/113 - 0s - loss: 129794.0156 - accuracy: 0.4282 - val_loss: 128883.2188 - val_accuracy: 0.4195 - 412ms/epoch - 4ms/step\n",
      "Epoch 18/10000\n",
      "113/113 - 0s - loss: 123731.0391 - accuracy: 0.4203 - val_loss: 122885.7891 - val_accuracy: 0.4117 - 405ms/epoch - 4ms/step\n",
      "391/391 [==============================] - 1s 3ms/step\n",
      "\n",
      "Iterazione 11/128 (03-11-2023_10-25-36)\n",
      "hidden_layer_size: 20\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 2\n",
      "activation_hidden: relu\n",
      "activation_output: sigmoid\n",
      "batch_size: 1000\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 10\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 53.82%\n",
      "La recall di questo modello sul validation set è: 36.27%\n",
      "La f1 di questo modello sul validation set è: 37.8%\n",
      "La balanced accuracy di questo modello sul validation set è: 50.58%\n",
      "La precision di questo modello sul validation set è: 39.47%\n",
      "La AUC di questo modello sul validation set è: 50.58%\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.62    0.65      0.63  7,670.00\n",
      "1                  0.39    0.36      0.38  4,841.00\n",
      "accuracy           0.54    0.54      0.54      0.54\n",
      "macro avg          0.51    0.51      0.51 12,511.00\n",
      "weighted avg       0.53    0.54      0.53 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         4977         2693\n",
      "Actual 1         3085         1756\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 65.94% con il modello:\n",
      "1 - AC65.94%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 62.97% con il modello:\n",
      "1 - BA62.97%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 56.83% con il modello:\n",
      "1 - PR56.83%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "113/113 - 2s - loss: 27044498.0000 - accuracy: 0.3887 - val_loss: 25335958.0000 - val_accuracy: 0.3869 - 2s/epoch - 22ms/step\n",
      "Epoch 2/10000\n",
      "113/113 - 0s - loss: 23223684.0000 - accuracy: 0.3887 - val_loss: 21554722.0000 - val_accuracy: 0.3869 - 418ms/epoch - 4ms/step\n",
      "Epoch 3/10000\n",
      "113/113 - 0s - loss: 19517860.0000 - accuracy: 0.3887 - val_loss: 17829784.0000 - val_accuracy: 0.3869 - 424ms/epoch - 4ms/step\n",
      "Epoch 4/10000\n",
      "113/113 - 0s - loss: 15354352.0000 - accuracy: 0.3887 - val_loss: 12893854.0000 - val_accuracy: 0.3869 - 398ms/epoch - 4ms/step\n",
      "Epoch 5/10000\n",
      "113/113 - 0s - loss: 10391228.0000 - accuracy: 0.3887 - val_loss: 8181828.5000 - val_accuracy: 0.3869 - 397ms/epoch - 4ms/step\n",
      "Epoch 6/10000\n",
      "113/113 - 0s - loss: 5985547.5000 - accuracy: 0.3887 - val_loss: 3935357.0000 - val_accuracy: 0.3869 - 422ms/epoch - 4ms/step\n",
      "Epoch 7/10000\n",
      "113/113 - 0s - loss: 1918267.0000 - accuracy: 0.4148 - val_loss: 365836.9375 - val_accuracy: 0.5311 - 412ms/epoch - 4ms/step\n",
      "Epoch 8/10000\n",
      "113/113 - 0s - loss: 302275.5312 - accuracy: 0.5393 - val_loss: 283419.5938 - val_accuracy: 0.5382 - 399ms/epoch - 4ms/step\n",
      "Epoch 9/10000\n",
      "113/113 - 0s - loss: 268261.6875 - accuracy: 0.5376 - val_loss: 258584.2344 - val_accuracy: 0.5342 - 399ms/epoch - 4ms/step\n",
      "Epoch 10/10000\n",
      "113/113 - 0s - loss: 245117.2656 - accuracy: 0.5350 - val_loss: 235737.5469 - val_accuracy: 0.5305 - 397ms/epoch - 4ms/step\n",
      "Epoch 11/10000\n",
      "113/113 - 0s - loss: 222318.9688 - accuracy: 0.5308 - val_loss: 212498.0312 - val_accuracy: 0.5253 - 418ms/epoch - 4ms/step\n",
      "Epoch 12/10000\n",
      "113/113 - 0s - loss: 198714.8281 - accuracy: 0.5251 - val_loss: 188448.4375 - val_accuracy: 0.5151 - 399ms/epoch - 4ms/step\n",
      "Epoch 13/10000\n",
      "113/113 - 0s - loss: 176187.7656 - accuracy: 0.5135 - val_loss: 167859.7188 - val_accuracy: 0.5029 - 397ms/epoch - 4ms/step\n",
      "Epoch 14/10000\n",
      "113/113 - 0s - loss: 158090.3281 - accuracy: 0.4924 - val_loss: 152670.2656 - val_accuracy: 0.4767 - 401ms/epoch - 4ms/step\n",
      "Epoch 15/10000\n",
      "113/113 - 0s - loss: 145158.2344 - accuracy: 0.4644 - val_loss: 142372.1719 - val_accuracy: 0.4521 - 411ms/epoch - 4ms/step\n",
      "Epoch 16/10000\n",
      "113/113 - 0s - loss: 136328.5312 - accuracy: 0.4433 - val_loss: 134993.1250 - val_accuracy: 0.4270 - 428ms/epoch - 4ms/step\n",
      "Epoch 17/10000\n",
      "113/113 - 0s - loss: 129794.0156 - accuracy: 0.4282 - val_loss: 128883.2188 - val_accuracy: 0.4195 - 403ms/epoch - 4ms/step\n",
      "Epoch 18/10000\n",
      "113/113 - 0s - loss: 123731.0391 - accuracy: 0.4203 - val_loss: 122885.7891 - val_accuracy: 0.4117 - 413ms/epoch - 4ms/step\n",
      "Epoch 19/10000\n",
      "113/113 - 0s - loss: 117592.4219 - accuracy: 0.4142 - val_loss: 116620.5859 - val_accuracy: 0.4049 - 414ms/epoch - 4ms/step\n",
      "Epoch 20/10000\n",
      "113/113 - 0s - loss: 111033.7031 - accuracy: 0.4099 - val_loss: 109894.0625 - val_accuracy: 0.4080 - 424ms/epoch - 4ms/step\n",
      "Epoch 21/10000\n",
      "113/113 - 1s - loss: 103699.3672 - accuracy: 0.4058 - val_loss: 102061.9688 - val_accuracy: 0.4018 - 555ms/epoch - 5ms/step\n",
      "Epoch 22/10000\n",
      "113/113 - 1s - loss: 95246.2031 - accuracy: 0.4020 - val_loss: 93348.6797 - val_accuracy: 0.3986 - 509ms/epoch - 5ms/step\n",
      "Epoch 23/10000\n",
      "113/113 - 0s - loss: 85766.6719 - accuracy: 0.4015 - val_loss: 83584.2188 - val_accuracy: 0.4013 - 398ms/epoch - 4ms/step\n",
      "Epoch 24/10000\n",
      "113/113 - 0s - loss: 76541.8281 - accuracy: 0.4130 - val_loss: 75592.1328 - val_accuracy: 0.4096 - 490ms/epoch - 4ms/step\n",
      "Epoch 25/10000\n",
      "113/113 - 0s - loss: 69501.5547 - accuracy: 0.4296 - val_loss: 69348.5234 - val_accuracy: 0.4301 - 418ms/epoch - 4ms/step\n",
      "Epoch 26/10000\n",
      "113/113 - 0s - loss: 64262.1055 - accuracy: 0.4405 - val_loss: 64447.8008 - val_accuracy: 0.4399 - 394ms/epoch - 3ms/step\n",
      "Epoch 27/10000\n",
      "113/113 - 0s - loss: 59883.9688 - accuracy: 0.4472 - val_loss: 60075.9258 - val_accuracy: 0.4444 - 421ms/epoch - 4ms/step\n",
      "Epoch 28/10000\n",
      "113/113 - 0s - loss: 55813.8203 - accuracy: 0.4509 - val_loss: 55861.4180 - val_accuracy: 0.4461 - 392ms/epoch - 3ms/step\n",
      "391/391 [==============================] - 1s 3ms/step\n",
      "\n",
      "Iterazione 12/128 (03-11-2023_10-25-54)\n",
      "hidden_layer_size: 20\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 2\n",
      "activation_hidden: relu\n",
      "activation_output: sigmoid\n",
      "batch_size: 1000\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 20\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 53.82%\n",
      "La recall di questo modello sul validation set è: 36.27%\n",
      "La f1 di questo modello sul validation set è: 37.8%\n",
      "La balanced accuracy di questo modello sul validation set è: 50.58%\n",
      "La precision di questo modello sul validation set è: 39.47%\n",
      "La AUC di questo modello sul validation set è: 50.58%\n",
      "391/391 [==============================] - 1s 3ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.62    0.65      0.63  7,670.00\n",
      "1                  0.39    0.36      0.38  4,841.00\n",
      "accuracy           0.54    0.54      0.54      0.54\n",
      "macro avg          0.51    0.51      0.51 12,511.00\n",
      "weighted avg       0.53    0.54      0.53 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         4977         2693\n",
      "Actual 1         3085         1756\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 65.94% con il modello:\n",
      "1 - AC65.94%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 62.97% con il modello:\n",
      "1 - BA62.97%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 56.83% con il modello:\n",
      "1 - PR56.83%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "1126/1126 - 6s - loss: 14.0334 - accuracy: 0.3887 - val_loss: 1.8630 - val_accuracy: 0.3879 - 6s/epoch - 5ms/step\n",
      "Epoch 2/10000\n",
      "1126/1126 - 6s - loss: 0.7865 - accuracy: 0.5652 - val_loss: 0.7030 - val_accuracy: 0.6155 - 6s/epoch - 5ms/step\n",
      "Epoch 3/10000\n",
      "1126/1126 - 5s - loss: 0.7000 - accuracy: 0.6122 - val_loss: 0.6916 - val_accuracy: 0.6155 - 5s/epoch - 5ms/step\n",
      "Epoch 4/10000\n",
      "1126/1126 - 3s - loss: 0.6896 - accuracy: 0.6122 - val_loss: 0.6839 - val_accuracy: 0.6156 - 3s/epoch - 3ms/step\n",
      "Epoch 5/10000\n",
      "1126/1126 - 3s - loss: 0.6827 - accuracy: 0.6122 - val_loss: 0.6797 - val_accuracy: 0.6163 - 3s/epoch - 2ms/step\n",
      "Epoch 6/10000\n",
      "1126/1126 - 3s - loss: 0.6775 - accuracy: 0.6120 - val_loss: 0.6728 - val_accuracy: 0.6163 - 3s/epoch - 3ms/step\n",
      "Epoch 7/10000\n",
      "1126/1126 - 3s - loss: 0.6719 - accuracy: 0.6120 - val_loss: 0.6698 - val_accuracy: 0.6147 - 3s/epoch - 2ms/step\n",
      "Epoch 8/10000\n",
      "1126/1126 - 3s - loss: 0.6693 - accuracy: 0.6122 - val_loss: 0.6690 - val_accuracy: 0.6146 - 3s/epoch - 2ms/step\n",
      "Epoch 9/10000\n",
      "1126/1126 - 3s - loss: 0.6677 - accuracy: 0.6117 - val_loss: 0.6667 - val_accuracy: 0.6155 - 3s/epoch - 3ms/step\n",
      "Epoch 10/10000\n",
      "1126/1126 - 3s - loss: 0.6665 - accuracy: 0.6118 - val_loss: 0.6653 - val_accuracy: 0.6145 - 3s/epoch - 2ms/step\n",
      "Epoch 11/10000\n",
      "1126/1126 - 2s - loss: 0.6659 - accuracy: 0.6115 - val_loss: 0.6637 - val_accuracy: 0.6145 - 2s/epoch - 2ms/step\n",
      "Epoch 12/10000\n",
      "1126/1126 - 2s - loss: 0.6647 - accuracy: 0.6120 - val_loss: 0.6623 - val_accuracy: 0.6159 - 2s/epoch - 2ms/step\n",
      "Epoch 13/10000\n",
      "1126/1126 - 2s - loss: 0.6640 - accuracy: 0.6118 - val_loss: 0.6622 - val_accuracy: 0.6124 - 2s/epoch - 2ms/step\n",
      "Epoch 14/10000\n",
      "1126/1126 - 2s - loss: 0.6630 - accuracy: 0.6121 - val_loss: 0.6607 - val_accuracy: 0.6159 - 2s/epoch - 2ms/step\n",
      "Epoch 15/10000\n",
      "1126/1126 - 3s - loss: 0.6599 - accuracy: 0.6118 - val_loss: 0.6587 - val_accuracy: 0.6161 - 3s/epoch - 2ms/step\n",
      "391/391 [==============================] - 2s 4ms/step\n",
      "\n",
      "Iterazione 13/128 (03-11-2023_10-26-46)\n",
      "hidden_layer_size: 20\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 2\n",
      "activation_hidden: sigmoid\n",
      "activation_output: sigmoid\n",
      "batch_size: 100\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 10\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 61.63%\n",
      "La recall di questo modello sul validation set è: 1.88%\n",
      "La f1 di questo modello sul validation set è: 3.65%\n",
      "La balanced accuracy di questo modello sul validation set è: 50.61%\n",
      "La precision di questo modello sul validation set è: 64.08%\n",
      "La AUC di questo modello sul validation set è: 56.71%\n",
      "391/391 [==============================] - 1s 3ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.62    0.99      0.76  7,670.00\n",
      "1                  0.64    0.02      0.04  4,841.00\n",
      "accuracy           0.62    0.62      0.62      0.62\n",
      "macro avg          0.63    0.51      0.40 12,511.00\n",
      "weighted avg       0.63    0.62      0.48 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         7619           51\n",
      "Actual 1         4750           91\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 65.94% con il modello:\n",
      "1 - AC65.94%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 62.97% con il modello:\n",
      "1 - BA62.97%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 64.08% con il modello:\n",
      "13 - PR64.08%, L20, mV-10, MV10, P2, AHsigmoid, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-26-48).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "1126/1126 - 4s - loss: 14.0334 - accuracy: 0.3887 - val_loss: 1.8630 - val_accuracy: 0.3879 - 4s/epoch - 4ms/step\n",
      "Epoch 2/10000\n",
      "1126/1126 - 3s - loss: 0.7865 - accuracy: 0.5652 - val_loss: 0.7030 - val_accuracy: 0.6155 - 3s/epoch - 2ms/step\n",
      "Epoch 3/10000\n",
      "1126/1126 - 2s - loss: 0.7000 - accuracy: 0.6122 - val_loss: 0.6916 - val_accuracy: 0.6155 - 2s/epoch - 2ms/step\n",
      "Epoch 4/10000\n",
      "1126/1126 - 3s - loss: 0.6896 - accuracy: 0.6122 - val_loss: 0.6839 - val_accuracy: 0.6156 - 3s/epoch - 3ms/step\n",
      "Epoch 5/10000\n",
      "1126/1126 - 2s - loss: 0.6827 - accuracy: 0.6122 - val_loss: 0.6797 - val_accuracy: 0.6163 - 2s/epoch - 2ms/step\n",
      "Epoch 6/10000\n",
      "1126/1126 - 2s - loss: 0.6775 - accuracy: 0.6120 - val_loss: 0.6728 - val_accuracy: 0.6163 - 2s/epoch - 2ms/step\n",
      "Epoch 7/10000\n",
      "1126/1126 - 2s - loss: 0.6719 - accuracy: 0.6120 - val_loss: 0.6698 - val_accuracy: 0.6147 - 2s/epoch - 2ms/step\n",
      "Epoch 8/10000\n",
      "1126/1126 - 2s - loss: 0.6693 - accuracy: 0.6122 - val_loss: 0.6690 - val_accuracy: 0.6146 - 2s/epoch - 2ms/step\n",
      "Epoch 9/10000\n",
      "1126/1126 - 3s - loss: 0.6677 - accuracy: 0.6117 - val_loss: 0.6667 - val_accuracy: 0.6155 - 3s/epoch - 2ms/step\n",
      "Epoch 10/10000\n",
      "1126/1126 - 3s - loss: 0.6665 - accuracy: 0.6118 - val_loss: 0.6653 - val_accuracy: 0.6145 - 3s/epoch - 3ms/step\n",
      "Epoch 11/10000\n",
      "1126/1126 - 4s - loss: 0.6659 - accuracy: 0.6115 - val_loss: 0.6637 - val_accuracy: 0.6145 - 4s/epoch - 4ms/step\n",
      "Epoch 12/10000\n",
      "1126/1126 - 3s - loss: 0.6647 - accuracy: 0.6120 - val_loss: 0.6623 - val_accuracy: 0.6159 - 3s/epoch - 2ms/step\n",
      "Epoch 13/10000\n",
      "1126/1126 - 2s - loss: 0.6640 - accuracy: 0.6118 - val_loss: 0.6622 - val_accuracy: 0.6124 - 2s/epoch - 2ms/step\n",
      "Epoch 14/10000\n",
      "1126/1126 - 2s - loss: 0.6630 - accuracy: 0.6121 - val_loss: 0.6607 - val_accuracy: 0.6159 - 2s/epoch - 2ms/step\n",
      "Epoch 15/10000\n",
      "1126/1126 - 2s - loss: 0.6599 - accuracy: 0.6118 - val_loss: 0.6587 - val_accuracy: 0.6161 - 2s/epoch - 2ms/step\n",
      "Epoch 16/10000\n",
      "1126/1126 - 2s - loss: 0.6591 - accuracy: 0.6121 - val_loss: 0.6594 - val_accuracy: 0.6159 - 2s/epoch - 2ms/step\n",
      "Epoch 17/10000\n",
      "1126/1126 - 2s - loss: 0.6590 - accuracy: 0.6121 - val_loss: 0.6590 - val_accuracy: 0.6141 - 2s/epoch - 2ms/step\n",
      "Epoch 18/10000\n",
      "1126/1126 - 2s - loss: 0.6585 - accuracy: 0.6121 - val_loss: 0.6577 - val_accuracy: 0.6159 - 2s/epoch - 2ms/step\n",
      "Epoch 19/10000\n",
      "1126/1126 - 2s - loss: 0.6581 - accuracy: 0.6115 - val_loss: 0.6590 - val_accuracy: 0.6138 - 2s/epoch - 2ms/step\n",
      "Epoch 20/10000\n",
      "1126/1126 - 2s - loss: 0.6575 - accuracy: 0.6115 - val_loss: 0.6578 - val_accuracy: 0.6138 - 2s/epoch - 2ms/step\n",
      "Epoch 21/10000\n",
      "1126/1126 - 2s - loss: 0.6577 - accuracy: 0.6118 - val_loss: 0.6573 - val_accuracy: 0.6153 - 2s/epoch - 2ms/step\n",
      "Epoch 22/10000\n",
      "1126/1126 - 2s - loss: 0.6574 - accuracy: 0.6124 - val_loss: 0.6572 - val_accuracy: 0.6138 - 2s/epoch - 2ms/step\n",
      "Epoch 23/10000\n",
      "1126/1126 - 2s - loss: 0.6577 - accuracy: 0.6112 - val_loss: 0.6556 - val_accuracy: 0.6142 - 2s/epoch - 2ms/step\n",
      "Epoch 24/10000\n",
      "1126/1126 - 2s - loss: 0.6573 - accuracy: 0.6118 - val_loss: 0.6567 - val_accuracy: 0.6108 - 2s/epoch - 2ms/step\n",
      "Epoch 25/10000\n",
      "1126/1126 - 2s - loss: 0.6573 - accuracy: 0.6116 - val_loss: 0.6565 - val_accuracy: 0.6155 - 2s/epoch - 2ms/step\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Iterazione 14/128 (03-11-2023_10-27-50)\n",
      "hidden_layer_size: 20\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 2\n",
      "activation_hidden: sigmoid\n",
      "activation_output: sigmoid\n",
      "batch_size: 100\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 20\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 61.63%\n",
      "La recall di questo modello sul validation set è: 1.88%\n",
      "La f1 di questo modello sul validation set è: 3.65%\n",
      "La balanced accuracy di questo modello sul validation set è: 50.61%\n",
      "La precision di questo modello sul validation set è: 64.08%\n",
      "La AUC di questo modello sul validation set è: 56.71%\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.62    0.99      0.76  7,670.00\n",
      "1                  0.64    0.02      0.04  4,841.00\n",
      "accuracy           0.62    0.62      0.62      0.62\n",
      "macro avg          0.63    0.51      0.40 12,511.00\n",
      "weighted avg       0.63    0.62      0.48 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         7619           51\n",
      "Actual 1         4750           91\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 65.94% con il modello:\n",
      "1 - AC65.94%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 62.97% con il modello:\n",
      "1 - BA62.97%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 64.08% con il modello:\n",
      "13 - PR64.08%, L20, mV-10, MV10, P2, AHsigmoid, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-26-48).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "113/113 - 2s - loss: 27.4240 - accuracy: 0.3887 - val_loss: 26.2032 - val_accuracy: 0.3869 - 2s/epoch - 16ms/step\n",
      "Epoch 2/10000\n",
      "113/113 - 0s - loss: 24.6898 - accuracy: 0.3887 - val_loss: 23.0094 - val_accuracy: 0.3869 - 384ms/epoch - 3ms/step\n",
      "Epoch 3/10000\n",
      "113/113 - 0s - loss: 21.2759 - accuracy: 0.3887 - val_loss: 19.7921 - val_accuracy: 0.3869 - 371ms/epoch - 3ms/step\n",
      "Epoch 4/10000\n",
      "113/113 - 0s - loss: 18.0762 - accuracy: 0.3887 - val_loss: 15.8636 - val_accuracy: 0.3869 - 345ms/epoch - 3ms/step\n",
      "Epoch 5/10000\n",
      "113/113 - 0s - loss: 13.7117 - accuracy: 0.3887 - val_loss: 12.2228 - val_accuracy: 0.3869 - 383ms/epoch - 3ms/step\n",
      "Epoch 6/10000\n",
      "113/113 - 0s - loss: 10.5780 - accuracy: 0.3887 - val_loss: 8.3864 - val_accuracy: 0.3869 - 380ms/epoch - 3ms/step\n",
      "Epoch 7/10000\n",
      "113/113 - 0s - loss: 6.3466 - accuracy: 0.3895 - val_loss: 4.5208 - val_accuracy: 0.3889 - 364ms/epoch - 3ms/step\n",
      "Epoch 8/10000\n",
      "113/113 - 0s - loss: 2.9007 - accuracy: 0.4448 - val_loss: 1.7579 - val_accuracy: 0.5724 - 364ms/epoch - 3ms/step\n",
      "Epoch 9/10000\n",
      "113/113 - 0s - loss: 1.5743 - accuracy: 0.5804 - val_loss: 1.4115 - val_accuracy: 0.5842 - 342ms/epoch - 3ms/step\n",
      "Epoch 10/10000\n",
      "113/113 - 0s - loss: 1.2941 - accuracy: 0.5908 - val_loss: 1.1827 - val_accuracy: 0.5934 - 339ms/epoch - 3ms/step\n",
      "Epoch 11/10000\n",
      "113/113 - 0s - loss: 1.1008 - accuracy: 0.5981 - val_loss: 0.9925 - val_accuracy: 0.6017 - 344ms/epoch - 3ms/step\n",
      "Epoch 12/10000\n",
      "113/113 - 0s - loss: 0.9314 - accuracy: 0.6057 - val_loss: 0.8729 - val_accuracy: 0.6080 - 339ms/epoch - 3ms/step\n",
      "Epoch 13/10000\n",
      "113/113 - 0s - loss: 0.8533 - accuracy: 0.6083 - val_loss: 0.8265 - val_accuracy: 0.6097 - 344ms/epoch - 3ms/step\n",
      "Epoch 14/10000\n",
      "113/113 - 0s - loss: 0.8165 - accuracy: 0.6098 - val_loss: 0.7800 - val_accuracy: 0.6140 - 403ms/epoch - 4ms/step\n",
      "Epoch 15/10000\n",
      "113/113 - 0s - loss: 0.7953 - accuracy: 0.6114 - val_loss: 0.7729 - val_accuracy: 0.6154 - 361ms/epoch - 3ms/step\n",
      "Epoch 16/10000\n",
      "113/113 - 0s - loss: 0.7920 - accuracy: 0.6122 - val_loss: 0.7761 - val_accuracy: 0.6150 - 342ms/epoch - 3ms/step\n",
      "Epoch 17/10000\n",
      "113/113 - 0s - loss: 0.7844 - accuracy: 0.6124 - val_loss: 0.7515 - val_accuracy: 0.6147 - 377ms/epoch - 3ms/step\n",
      "Epoch 18/10000\n",
      "113/113 - 0s - loss: 0.7364 - accuracy: 0.6121 - val_loss: 0.7218 - val_accuracy: 0.6146 - 388ms/epoch - 3ms/step\n",
      "Epoch 19/10000\n",
      "113/113 - 0s - loss: 0.7217 - accuracy: 0.6123 - val_loss: 0.7148 - val_accuracy: 0.6144 - 353ms/epoch - 3ms/step\n",
      "Epoch 20/10000\n",
      "113/113 - 0s - loss: 0.7166 - accuracy: 0.6125 - val_loss: 0.7030 - val_accuracy: 0.6157 - 393ms/epoch - 3ms/step\n",
      "Epoch 21/10000\n",
      "113/113 - 0s - loss: 0.7079 - accuracy: 0.6128 - val_loss: 0.6960 - val_accuracy: 0.6158 - 359ms/epoch - 3ms/step\n",
      "Epoch 22/10000\n",
      "113/113 - 0s - loss: 0.6996 - accuracy: 0.6129 - val_loss: 0.6878 - val_accuracy: 0.6177 - 383ms/epoch - 3ms/step\n",
      "Epoch 23/10000\n",
      "113/113 - 0s - loss: 0.6903 - accuracy: 0.6130 - val_loss: 0.6783 - val_accuracy: 0.6171 - 362ms/epoch - 3ms/step\n",
      "Epoch 24/10000\n",
      "113/113 - 0s - loss: 0.6815 - accuracy: 0.6131 - val_loss: 0.6758 - val_accuracy: 0.6167 - 370ms/epoch - 3ms/step\n",
      "Epoch 25/10000\n",
      "113/113 - 0s - loss: 0.6789 - accuracy: 0.6128 - val_loss: 0.6727 - val_accuracy: 0.6168 - 384ms/epoch - 3ms/step\n",
      "Epoch 26/10000\n",
      "113/113 - 0s - loss: 0.6769 - accuracy: 0.6133 - val_loss: 0.6717 - val_accuracy: 0.6170 - 341ms/epoch - 3ms/step\n",
      "Epoch 27/10000\n",
      "113/113 - 0s - loss: 0.6758 - accuracy: 0.6131 - val_loss: 0.6707 - val_accuracy: 0.6169 - 338ms/epoch - 3ms/step\n",
      "Epoch 28/10000\n",
      "113/113 - 0s - loss: 0.6752 - accuracy: 0.6129 - val_loss: 0.6708 - val_accuracy: 0.6167 - 343ms/epoch - 3ms/step\n",
      "Epoch 29/10000\n",
      "113/113 - 0s - loss: 0.6725 - accuracy: 0.6126 - val_loss: 0.6687 - val_accuracy: 0.6163 - 383ms/epoch - 3ms/step\n",
      "Epoch 30/10000\n",
      "113/113 - 0s - loss: 0.6711 - accuracy: 0.6125 - val_loss: 0.6681 - val_accuracy: 0.6161 - 349ms/epoch - 3ms/step\n",
      "Epoch 31/10000\n",
      "113/113 - 0s - loss: 0.6706 - accuracy: 0.6128 - val_loss: 0.6685 - val_accuracy: 0.6159 - 354ms/epoch - 3ms/step\n",
      "Epoch 32/10000\n",
      "113/113 - 0s - loss: 0.6695 - accuracy: 0.6125 - val_loss: 0.6679 - val_accuracy: 0.6157 - 365ms/epoch - 3ms/step\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Iterazione 15/128 (03-11-2023_10-28-06)\n",
      "hidden_layer_size: 20\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 2\n",
      "activation_hidden: sigmoid\n",
      "activation_output: sigmoid\n",
      "batch_size: 1000\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 10\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 61.77%\n",
      "La recall di questo modello sul validation set è: 3.16%\n",
      "La f1 di questo modello sul validation set è: 6.01%\n",
      "La balanced accuracy di questo modello sul validation set è: 50.96%\n",
      "La precision di questo modello sul validation set è: 61.69%\n",
      "La AUC di questo modello sul validation set è: 58.53%\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.62    0.99      0.76  7,670.00\n",
      "1                  0.62    0.03      0.06  4,841.00\n",
      "accuracy           0.62    0.62      0.62      0.62\n",
      "macro avg          0.62    0.51      0.41 12,511.00\n",
      "weighted avg       0.62    0.62      0.49 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         7575           95\n",
      "Actual 1         4688          153\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 65.94% con il modello:\n",
      "1 - AC65.94%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 62.97% con il modello:\n",
      "1 - BA62.97%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 64.08% con il modello:\n",
      "13 - PR64.08%, L20, mV-10, MV10, P2, AHsigmoid, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-26-48).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "113/113 - 2s - loss: 27.4240 - accuracy: 0.3887 - val_loss: 26.2032 - val_accuracy: 0.3869 - 2s/epoch - 18ms/step\n",
      "Epoch 2/10000\n",
      "113/113 - 0s - loss: 24.6898 - accuracy: 0.3887 - val_loss: 23.0094 - val_accuracy: 0.3869 - 346ms/epoch - 3ms/step\n",
      "Epoch 3/10000\n",
      "113/113 - 0s - loss: 21.2759 - accuracy: 0.3887 - val_loss: 19.7921 - val_accuracy: 0.3869 - 334ms/epoch - 3ms/step\n",
      "Epoch 4/10000\n",
      "113/113 - 0s - loss: 18.0762 - accuracy: 0.3887 - val_loss: 15.8636 - val_accuracy: 0.3869 - 335ms/epoch - 3ms/step\n",
      "Epoch 5/10000\n",
      "113/113 - 0s - loss: 13.7117 - accuracy: 0.3887 - val_loss: 12.2228 - val_accuracy: 0.3869 - 354ms/epoch - 3ms/step\n",
      "Epoch 6/10000\n",
      "113/113 - 0s - loss: 10.5780 - accuracy: 0.3887 - val_loss: 8.3864 - val_accuracy: 0.3869 - 359ms/epoch - 3ms/step\n",
      "Epoch 7/10000\n",
      "113/113 - 0s - loss: 6.3466 - accuracy: 0.3895 - val_loss: 4.5208 - val_accuracy: 0.3889 - 361ms/epoch - 3ms/step\n",
      "Epoch 8/10000\n",
      "113/113 - 0s - loss: 2.9007 - accuracy: 0.4448 - val_loss: 1.7579 - val_accuracy: 0.5724 - 337ms/epoch - 3ms/step\n",
      "Epoch 9/10000\n",
      "113/113 - 0s - loss: 1.5743 - accuracy: 0.5804 - val_loss: 1.4115 - val_accuracy: 0.5842 - 356ms/epoch - 3ms/step\n",
      "Epoch 10/10000\n",
      "113/113 - 0s - loss: 1.2941 - accuracy: 0.5908 - val_loss: 1.1827 - val_accuracy: 0.5934 - 347ms/epoch - 3ms/step\n",
      "Epoch 11/10000\n",
      "113/113 - 0s - loss: 1.1008 - accuracy: 0.5981 - val_loss: 0.9925 - val_accuracy: 0.6017 - 411ms/epoch - 4ms/step\n",
      "Epoch 12/10000\n",
      "113/113 - 0s - loss: 0.9314 - accuracy: 0.6057 - val_loss: 0.8729 - val_accuracy: 0.6080 - 340ms/epoch - 3ms/step\n",
      "Epoch 13/10000\n",
      "113/113 - 0s - loss: 0.8533 - accuracy: 0.6083 - val_loss: 0.8265 - val_accuracy: 0.6097 - 390ms/epoch - 3ms/step\n",
      "Epoch 14/10000\n",
      "113/113 - 0s - loss: 0.8165 - accuracy: 0.6098 - val_loss: 0.7800 - val_accuracy: 0.6140 - 370ms/epoch - 3ms/step\n",
      "Epoch 15/10000\n",
      "113/113 - 0s - loss: 0.7953 - accuracy: 0.6114 - val_loss: 0.7729 - val_accuracy: 0.6154 - 372ms/epoch - 3ms/step\n",
      "Epoch 16/10000\n",
      "113/113 - 0s - loss: 0.7920 - accuracy: 0.6122 - val_loss: 0.7761 - val_accuracy: 0.6150 - 337ms/epoch - 3ms/step\n",
      "Epoch 17/10000\n",
      "113/113 - 0s - loss: 0.7844 - accuracy: 0.6124 - val_loss: 0.7515 - val_accuracy: 0.6147 - 352ms/epoch - 3ms/step\n",
      "Epoch 18/10000\n",
      "113/113 - 0s - loss: 0.7364 - accuracy: 0.6121 - val_loss: 0.7218 - val_accuracy: 0.6146 - 338ms/epoch - 3ms/step\n",
      "Epoch 19/10000\n",
      "113/113 - 0s - loss: 0.7217 - accuracy: 0.6123 - val_loss: 0.7148 - val_accuracy: 0.6144 - 375ms/epoch - 3ms/step\n",
      "Epoch 20/10000\n",
      "113/113 - 0s - loss: 0.7166 - accuracy: 0.6125 - val_loss: 0.7030 - val_accuracy: 0.6157 - 358ms/epoch - 3ms/step\n",
      "Epoch 21/10000\n",
      "113/113 - 0s - loss: 0.7079 - accuracy: 0.6128 - val_loss: 0.6960 - val_accuracy: 0.6158 - 363ms/epoch - 3ms/step\n",
      "Epoch 22/10000\n",
      "113/113 - 0s - loss: 0.6996 - accuracy: 0.6129 - val_loss: 0.6878 - val_accuracy: 0.6177 - 359ms/epoch - 3ms/step\n",
      "Epoch 23/10000\n",
      "113/113 - 0s - loss: 0.6903 - accuracy: 0.6130 - val_loss: 0.6783 - val_accuracy: 0.6171 - 352ms/epoch - 3ms/step\n",
      "Epoch 24/10000\n",
      "113/113 - 0s - loss: 0.6815 - accuracy: 0.6131 - val_loss: 0.6758 - val_accuracy: 0.6167 - 359ms/epoch - 3ms/step\n",
      "Epoch 25/10000\n",
      "113/113 - 0s - loss: 0.6789 - accuracy: 0.6128 - val_loss: 0.6727 - val_accuracy: 0.6168 - 361ms/epoch - 3ms/step\n",
      "Epoch 26/10000\n",
      "113/113 - 0s - loss: 0.6769 - accuracy: 0.6133 - val_loss: 0.6717 - val_accuracy: 0.6170 - 380ms/epoch - 3ms/step\n",
      "Epoch 27/10000\n",
      "113/113 - 0s - loss: 0.6758 - accuracy: 0.6131 - val_loss: 0.6707 - val_accuracy: 0.6169 - 361ms/epoch - 3ms/step\n",
      "Epoch 28/10000\n",
      "113/113 - 0s - loss: 0.6752 - accuracy: 0.6129 - val_loss: 0.6708 - val_accuracy: 0.6167 - 348ms/epoch - 3ms/step\n",
      "Epoch 29/10000\n",
      "113/113 - 0s - loss: 0.6725 - accuracy: 0.6126 - val_loss: 0.6687 - val_accuracy: 0.6163 - 404ms/epoch - 4ms/step\n",
      "Epoch 30/10000\n",
      "113/113 - 0s - loss: 0.6711 - accuracy: 0.6125 - val_loss: 0.6681 - val_accuracy: 0.6161 - 384ms/epoch - 3ms/step\n",
      "Epoch 31/10000\n",
      "113/113 - 0s - loss: 0.6706 - accuracy: 0.6128 - val_loss: 0.6685 - val_accuracy: 0.6159 - 369ms/epoch - 3ms/step\n",
      "Epoch 32/10000\n",
      "113/113 - 0s - loss: 0.6695 - accuracy: 0.6125 - val_loss: 0.6679 - val_accuracy: 0.6157 - 375ms/epoch - 3ms/step\n",
      "Epoch 33/10000\n",
      "113/113 - 0s - loss: 0.6687 - accuracy: 0.6124 - val_loss: 0.6670 - val_accuracy: 0.6160 - 393ms/epoch - 3ms/step\n",
      "Epoch 34/10000\n",
      "113/113 - 0s - loss: 0.6680 - accuracy: 0.6120 - val_loss: 0.6663 - val_accuracy: 0.6149 - 359ms/epoch - 3ms/step\n",
      "Epoch 35/10000\n",
      "113/113 - 0s - loss: 0.6679 - accuracy: 0.6118 - val_loss: 0.6658 - val_accuracy: 0.6150 - 372ms/epoch - 3ms/step\n",
      "Epoch 36/10000\n",
      "113/113 - 0s - loss: 0.6674 - accuracy: 0.6118 - val_loss: 0.6657 - val_accuracy: 0.6149 - 354ms/epoch - 3ms/step\n",
      "Epoch 37/10000\n",
      "113/113 - 0s - loss: 0.6672 - accuracy: 0.6118 - val_loss: 0.6659 - val_accuracy: 0.6150 - 361ms/epoch - 3ms/step\n",
      "Epoch 38/10000\n",
      "113/113 - 0s - loss: 0.6669 - accuracy: 0.6118 - val_loss: 0.6655 - val_accuracy: 0.6162 - 359ms/epoch - 3ms/step\n",
      "Epoch 39/10000\n",
      "113/113 - 0s - loss: 0.6666 - accuracy: 0.6124 - val_loss: 0.6654 - val_accuracy: 0.6159 - 391ms/epoch - 3ms/step\n",
      "Epoch 40/10000\n",
      "113/113 - 0s - loss: 0.6661 - accuracy: 0.6123 - val_loss: 0.6651 - val_accuracy: 0.6159 - 335ms/epoch - 3ms/step\n",
      "Epoch 41/10000\n",
      "113/113 - 0s - loss: 0.6661 - accuracy: 0.6124 - val_loss: 0.6655 - val_accuracy: 0.6157 - 367ms/epoch - 3ms/step\n",
      "Epoch 42/10000\n",
      "113/113 - 0s - loss: 0.6655 - accuracy: 0.6123 - val_loss: 0.6655 - val_accuracy: 0.6149 - 368ms/epoch - 3ms/step\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Iterazione 16/128 (03-11-2023_10-28-25)\n",
      "hidden_layer_size: 20\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 2\n",
      "activation_hidden: sigmoid\n",
      "activation_output: sigmoid\n",
      "batch_size: 1000\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 20\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 61.77%\n",
      "La recall di questo modello sul validation set è: 3.16%\n",
      "La f1 di questo modello sul validation set è: 6.01%\n",
      "La balanced accuracy di questo modello sul validation set è: 50.96%\n",
      "La precision di questo modello sul validation set è: 61.69%\n",
      "La AUC di questo modello sul validation set è: 58.53%\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.62    0.99      0.76  7,670.00\n",
      "1                  0.62    0.03      0.06  4,841.00\n",
      "accuracy           0.62    0.62      0.62      0.62\n",
      "macro avg          0.62    0.51      0.41 12,511.00\n",
      "weighted avg       0.62    0.62      0.49 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         7575           95\n",
      "Actual 1         4688          153\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 65.94% con il modello:\n",
      "1 - AC65.94%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 62.97% con il modello:\n",
      "1 - BA62.97%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 64.08% con il modello:\n",
      "13 - PR64.08%, L20, mV-10, MV10, P2, AHsigmoid, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-26-48).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "1126/1126 - 4s - loss: 11118955.0000 - accuracy: 0.4505 - val_loss: 832803.3125 - val_accuracy: 0.4364 - 4s/epoch - 4ms/step\n",
      "Epoch 2/10000\n",
      "1126/1126 - 2s - loss: 461652.7188 - accuracy: 0.4891 - val_loss: 222782.8125 - val_accuracy: 0.5135 - 2s/epoch - 2ms/step\n",
      "Epoch 3/10000\n",
      "1126/1126 - 2s - loss: 199540.7188 - accuracy: 0.5563 - val_loss: 122321.3359 - val_accuracy: 0.5658 - 2s/epoch - 2ms/step\n",
      "Epoch 4/10000\n",
      "1126/1126 - 2s - loss: 166316.2812 - accuracy: 0.5615 - val_loss: 269621.1250 - val_accuracy: 0.6028 - 2s/epoch - 2ms/step\n",
      "Epoch 5/10000\n",
      "1126/1126 - 2s - loss: 152848.6406 - accuracy: 0.5649 - val_loss: 123170.4688 - val_accuracy: 0.5912 - 2s/epoch - 2ms/step\n",
      "Epoch 6/10000\n",
      "1126/1126 - 3s - loss: 142176.2031 - accuracy: 0.5668 - val_loss: 134510.4062 - val_accuracy: 0.5991 - 3s/epoch - 2ms/step\n",
      "Epoch 7/10000\n",
      "1126/1126 - 3s - loss: 118060.9375 - accuracy: 0.5695 - val_loss: 187672.3438 - val_accuracy: 0.4968 - 3s/epoch - 2ms/step\n",
      "Epoch 8/10000\n",
      "1126/1126 - 3s - loss: 121655.8984 - accuracy: 0.5692 - val_loss: 131945.0781 - val_accuracy: 0.6047 - 3s/epoch - 3ms/step\n",
      "Epoch 9/10000\n",
      "1126/1126 - 3s - loss: 118107.3750 - accuracy: 0.5713 - val_loss: 71286.6562 - val_accuracy: 0.6127 - 3s/epoch - 2ms/step\n",
      "Epoch 10/10000\n",
      "1126/1126 - 3s - loss: 120975.5000 - accuracy: 0.5715 - val_loss: 161079.2500 - val_accuracy: 0.6069 - 3s/epoch - 2ms/step\n",
      "Epoch 11/10000\n",
      "1126/1126 - 3s - loss: 117244.1406 - accuracy: 0.5726 - val_loss: 224932.5156 - val_accuracy: 0.4810 - 3s/epoch - 3ms/step\n",
      "Epoch 12/10000\n",
      "1126/1126 - 3s - loss: 103194.6172 - accuracy: 0.5733 - val_loss: 71339.9141 - val_accuracy: 0.5414 - 3s/epoch - 2ms/step\n",
      "Epoch 13/10000\n",
      "1126/1126 - 3s - loss: 109456.8594 - accuracy: 0.5742 - val_loss: 127175.4297 - val_accuracy: 0.4990 - 3s/epoch - 2ms/step\n",
      "Epoch 14/10000\n",
      "1126/1126 - 3s - loss: 104509.0469 - accuracy: 0.5711 - val_loss: 70245.0938 - val_accuracy: 0.6016 - 3s/epoch - 2ms/step\n",
      "Epoch 15/10000\n",
      "1126/1126 - 3s - loss: 111415.9219 - accuracy: 0.5705 - val_loss: 87805.3828 - val_accuracy: 0.6090 - 3s/epoch - 2ms/step\n",
      "Epoch 16/10000\n",
      "1126/1126 - 3s - loss: 106846.4219 - accuracy: 0.5738 - val_loss: 44121.0898 - val_accuracy: 0.5790 - 3s/epoch - 2ms/step\n",
      "Epoch 17/10000\n",
      "1126/1126 - 3s - loss: 105126.9531 - accuracy: 0.5738 - val_loss: 49228.9062 - val_accuracy: 0.6018 - 3s/epoch - 2ms/step\n",
      "Epoch 18/10000\n",
      "1126/1126 - 3s - loss: 92586.1875 - accuracy: 0.5733 - val_loss: 85579.9375 - val_accuracy: 0.5188 - 3s/epoch - 2ms/step\n",
      "Epoch 19/10000\n",
      "1126/1126 - 3s - loss: 96180.7656 - accuracy: 0.5762 - val_loss: 32489.2207 - val_accuracy: 0.6175 - 3s/epoch - 2ms/step\n",
      "Epoch 20/10000\n",
      "1126/1126 - 2s - loss: 101614.7969 - accuracy: 0.5730 - val_loss: 41416.0547 - val_accuracy: 0.6116 - 2s/epoch - 2ms/step\n",
      "Epoch 21/10000\n",
      "1126/1126 - 2s - loss: 100631.4766 - accuracy: 0.5721 - val_loss: 47579.5938 - val_accuracy: 0.5558 - 2s/epoch - 2ms/step\n",
      "Epoch 22/10000\n",
      "1126/1126 - 2s - loss: 98990.2266 - accuracy: 0.5757 - val_loss: 68450.8359 - val_accuracy: 0.5535 - 2s/epoch - 2ms/step\n",
      "Epoch 23/10000\n",
      "1126/1126 - 3s - loss: 90988.4688 - accuracy: 0.5760 - val_loss: 79586.0703 - val_accuracy: 0.6059 - 3s/epoch - 3ms/step\n",
      "Epoch 24/10000\n",
      "1126/1126 - 3s - loss: 94228.5859 - accuracy: 0.5741 - val_loss: 214341.2656 - val_accuracy: 0.6143 - 3s/epoch - 3ms/step\n",
      "Epoch 25/10000\n",
      "1126/1126 - 2s - loss: 85128.1172 - accuracy: 0.5764 - val_loss: 39009.8164 - val_accuracy: 0.5793 - 2s/epoch - 2ms/step\n",
      "Epoch 26/10000\n",
      "1126/1126 - 2s - loss: 97421.2656 - accuracy: 0.5752 - val_loss: 28161.4375 - val_accuracy: 0.6390 - 2s/epoch - 2ms/step\n",
      "Epoch 27/10000\n",
      "1126/1126 - 3s - loss: 107072.8906 - accuracy: 0.5750 - val_loss: 114760.1016 - val_accuracy: 0.5092 - 3s/epoch - 2ms/step\n",
      "Epoch 28/10000\n",
      "1126/1126 - 2s - loss: 97203.6016 - accuracy: 0.5722 - val_loss: 131765.1094 - val_accuracy: 0.5104 - 2s/epoch - 2ms/step\n",
      "Epoch 29/10000\n",
      "1126/1126 - 2s - loss: 97653.4766 - accuracy: 0.5743 - val_loss: 72405.1406 - val_accuracy: 0.6067 - 2s/epoch - 2ms/step\n",
      "Epoch 30/10000\n",
      "1126/1126 - 2s - loss: 91085.6719 - accuracy: 0.5761 - val_loss: 91673.4375 - val_accuracy: 0.4944 - 2s/epoch - 2ms/step\n",
      "Epoch 31/10000\n",
      "1126/1126 - 2s - loss: 90148.9922 - accuracy: 0.5755 - val_loss: 40819.0742 - val_accuracy: 0.5569 - 2s/epoch - 2ms/step\n",
      "Epoch 32/10000\n",
      "1126/1126 - 3s - loss: 89166.2188 - accuracy: 0.5767 - val_loss: 32020.1426 - val_accuracy: 0.6250 - 3s/epoch - 2ms/step\n",
      "Epoch 33/10000\n",
      "1126/1126 - 2s - loss: 87977.6016 - accuracy: 0.5763 - val_loss: 115083.7891 - val_accuracy: 0.6076 - 2s/epoch - 2ms/step\n",
      "Epoch 34/10000\n",
      "1126/1126 - 3s - loss: 97008.5781 - accuracy: 0.5754 - val_loss: 272152.5312 - val_accuracy: 0.4658 - 3s/epoch - 2ms/step\n",
      "Epoch 35/10000\n",
      "1126/1126 - 3s - loss: 85278.5312 - accuracy: 0.5787 - val_loss: 35368.3398 - val_accuracy: 0.5856 - 3s/epoch - 2ms/step\n",
      "Epoch 36/10000\n",
      "1126/1126 - 3s - loss: 101698.1328 - accuracy: 0.5751 - val_loss: 75006.9844 - val_accuracy: 0.6089 - 3s/epoch - 3ms/step\n",
      "391/391 [==============================] - 1s 3ms/step\n",
      "\n",
      "Iterazione 17/128 (03-11-2023_10-30-03)\n",
      "hidden_layer_size: 20\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 3\n",
      "activation_hidden: relu\n",
      "activation_output: sigmoid\n",
      "batch_size: 100\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 10\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 63.9%\n",
      "La recall di questo modello sul validation set è: 42.0%\n",
      "La f1 di questo modello sul validation set è: 47.37%\n",
      "La balanced accuracy di questo modello sul validation set è: 59.86%\n",
      "La precision di questo modello sul validation set è: 54.33%\n",
      "La AUC di questo modello sul validation set è: 59.85%\n",
      "391/391 [==============================] - 2s 5ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.68    0.78      0.73  7,670.00\n",
      "1                  0.54    0.42      0.47  4,841.00\n",
      "accuracy           0.64    0.64      0.64      0.64\n",
      "macro avg          0.61    0.60      0.60 12,511.00\n",
      "weighted avg       0.63    0.64      0.63 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         5961         1709\n",
      "Actual 1         2808         2033\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 65.94% con il modello:\n",
      "1 - AC65.94%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 62.97% con il modello:\n",
      "1 - BA62.97%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 64.08% con il modello:\n",
      "13 - PR64.08%, L20, mV-10, MV10, P2, AHsigmoid, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-26-48).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "1126/1126 - 10s - loss: 11118955.0000 - accuracy: 0.4505 - val_loss: 832803.3125 - val_accuracy: 0.4364 - 10s/epoch - 9ms/step\n",
      "Epoch 2/10000\n",
      "1126/1126 - 7s - loss: 461652.7188 - accuracy: 0.4891 - val_loss: 222782.8125 - val_accuracy: 0.5135 - 7s/epoch - 7ms/step\n",
      "Epoch 3/10000\n",
      "1126/1126 - 6s - loss: 199540.7188 - accuracy: 0.5563 - val_loss: 122321.3359 - val_accuracy: 0.5658 - 6s/epoch - 5ms/step\n",
      "Epoch 4/10000\n",
      "1126/1126 - 4s - loss: 166316.2812 - accuracy: 0.5615 - val_loss: 269621.1250 - val_accuracy: 0.6028 - 4s/epoch - 3ms/step\n",
      "Epoch 5/10000\n",
      "1126/1126 - 3s - loss: 152848.6406 - accuracy: 0.5649 - val_loss: 123170.4688 - val_accuracy: 0.5912 - 3s/epoch - 3ms/step\n",
      "Epoch 6/10000\n",
      "1126/1126 - 5s - loss: 142176.2031 - accuracy: 0.5668 - val_loss: 134510.4062 - val_accuracy: 0.5991 - 5s/epoch - 4ms/step\n",
      "Epoch 7/10000\n",
      "1126/1126 - 3s - loss: 118060.9375 - accuracy: 0.5695 - val_loss: 187672.3438 - val_accuracy: 0.4968 - 3s/epoch - 3ms/step\n",
      "Epoch 8/10000\n",
      "1126/1126 - 3s - loss: 121655.8984 - accuracy: 0.5692 - val_loss: 131945.0781 - val_accuracy: 0.6047 - 3s/epoch - 3ms/step\n",
      "Epoch 9/10000\n",
      "1126/1126 - 5s - loss: 118107.3750 - accuracy: 0.5713 - val_loss: 71286.6562 - val_accuracy: 0.6127 - 5s/epoch - 4ms/step\n",
      "Epoch 10/10000\n",
      "1126/1126 - 3s - loss: 120975.5000 - accuracy: 0.5715 - val_loss: 161079.2500 - val_accuracy: 0.6069 - 3s/epoch - 3ms/step\n",
      "Epoch 11/10000\n",
      "1126/1126 - 4s - loss: 117244.1406 - accuracy: 0.5726 - val_loss: 224932.5156 - val_accuracy: 0.4810 - 4s/epoch - 3ms/step\n",
      "Epoch 12/10000\n",
      "1126/1126 - 4s - loss: 103194.6172 - accuracy: 0.5733 - val_loss: 71339.9141 - val_accuracy: 0.5414 - 4s/epoch - 4ms/step\n",
      "Epoch 13/10000\n",
      "1126/1126 - 5s - loss: 109456.8594 - accuracy: 0.5742 - val_loss: 127175.4297 - val_accuracy: 0.4990 - 5s/epoch - 4ms/step\n",
      "Epoch 14/10000\n",
      "1126/1126 - 4s - loss: 104509.0469 - accuracy: 0.5711 - val_loss: 70245.0938 - val_accuracy: 0.6016 - 4s/epoch - 3ms/step\n",
      "Epoch 15/10000\n",
      "1126/1126 - 4s - loss: 111415.9219 - accuracy: 0.5705 - val_loss: 87805.3828 - val_accuracy: 0.6090 - 4s/epoch - 4ms/step\n",
      "Epoch 16/10000\n",
      "1126/1126 - 4s - loss: 106846.4219 - accuracy: 0.5738 - val_loss: 44121.0898 - val_accuracy: 0.5790 - 4s/epoch - 4ms/step\n",
      "Epoch 17/10000\n",
      "1126/1126 - 5s - loss: 105126.9531 - accuracy: 0.5738 - val_loss: 49228.9062 - val_accuracy: 0.6018 - 5s/epoch - 5ms/step\n",
      "Epoch 18/10000\n",
      "1126/1126 - 3s - loss: 92586.1875 - accuracy: 0.5733 - val_loss: 85579.9375 - val_accuracy: 0.5188 - 3s/epoch - 3ms/step\n",
      "Epoch 19/10000\n",
      "1126/1126 - 3s - loss: 96180.7656 - accuracy: 0.5762 - val_loss: 32489.2207 - val_accuracy: 0.6175 - 3s/epoch - 3ms/step\n",
      "Epoch 20/10000\n",
      "1126/1126 - 3s - loss: 101614.7969 - accuracy: 0.5730 - val_loss: 41416.0547 - val_accuracy: 0.6116 - 3s/epoch - 3ms/step\n",
      "Epoch 21/10000\n",
      "1126/1126 - 3s - loss: 100631.4766 - accuracy: 0.5721 - val_loss: 47579.5938 - val_accuracy: 0.5558 - 3s/epoch - 3ms/step\n",
      "Epoch 22/10000\n",
      "1126/1126 - 3s - loss: 98990.2266 - accuracy: 0.5757 - val_loss: 68450.8359 - val_accuracy: 0.5535 - 3s/epoch - 3ms/step\n",
      "Epoch 23/10000\n",
      "1126/1126 - 3s - loss: 90988.4688 - accuracy: 0.5760 - val_loss: 79586.0703 - val_accuracy: 0.6059 - 3s/epoch - 3ms/step\n",
      "Epoch 24/10000\n",
      "1126/1126 - 3s - loss: 94228.5859 - accuracy: 0.5741 - val_loss: 214341.2656 - val_accuracy: 0.6143 - 3s/epoch - 3ms/step\n",
      "Epoch 25/10000\n",
      "1126/1126 - 4s - loss: 85128.1172 - accuracy: 0.5764 - val_loss: 39009.8164 - val_accuracy: 0.5793 - 4s/epoch - 4ms/step\n",
      "Epoch 26/10000\n",
      "1126/1126 - 5s - loss: 97421.2656 - accuracy: 0.5752 - val_loss: 28161.4375 - val_accuracy: 0.6390 - 5s/epoch - 4ms/step\n",
      "Epoch 27/10000\n",
      "1126/1126 - 4s - loss: 107072.8906 - accuracy: 0.5750 - val_loss: 114760.1016 - val_accuracy: 0.5092 - 4s/epoch - 3ms/step\n",
      "Epoch 28/10000\n",
      "1126/1126 - 3s - loss: 97203.6016 - accuracy: 0.5722 - val_loss: 131765.1094 - val_accuracy: 0.5104 - 3s/epoch - 3ms/step\n",
      "Epoch 29/10000\n",
      "1126/1126 - 4s - loss: 97653.4766 - accuracy: 0.5743 - val_loss: 72405.1406 - val_accuracy: 0.6067 - 4s/epoch - 3ms/step\n",
      "Epoch 30/10000\n",
      "1126/1126 - 6s - loss: 91085.6719 - accuracy: 0.5761 - val_loss: 91673.4375 - val_accuracy: 0.4944 - 6s/epoch - 5ms/step\n",
      "Epoch 31/10000\n",
      "1126/1126 - 4s - loss: 90148.9922 - accuracy: 0.5755 - val_loss: 40819.0742 - val_accuracy: 0.5569 - 4s/epoch - 3ms/step\n",
      "Epoch 32/10000\n",
      "1126/1126 - 5s - loss: 89166.2188 - accuracy: 0.5767 - val_loss: 32020.1426 - val_accuracy: 0.6250 - 5s/epoch - 5ms/step\n",
      "Epoch 33/10000\n",
      "1126/1126 - 5s - loss: 87977.6016 - accuracy: 0.5763 - val_loss: 115083.7891 - val_accuracy: 0.6076 - 5s/epoch - 5ms/step\n",
      "Epoch 34/10000\n",
      "1126/1126 - 4s - loss: 97008.5781 - accuracy: 0.5754 - val_loss: 272152.5312 - val_accuracy: 0.4658 - 4s/epoch - 3ms/step\n",
      "Epoch 35/10000\n",
      "1126/1126 - 3s - loss: 85278.5312 - accuracy: 0.5787 - val_loss: 35368.3398 - val_accuracy: 0.5856 - 3s/epoch - 3ms/step\n",
      "Epoch 36/10000\n",
      "1126/1126 - 3s - loss: 101698.1328 - accuracy: 0.5751 - val_loss: 75006.9844 - val_accuracy: 0.6089 - 3s/epoch - 2ms/step\n",
      "Epoch 37/10000\n",
      "1126/1126 - 2s - loss: 102062.8203 - accuracy: 0.5745 - val_loss: 25896.5566 - val_accuracy: 0.6383 - 2s/epoch - 2ms/step\n",
      "Epoch 38/10000\n",
      "1126/1126 - 4s - loss: 92333.8672 - accuracy: 0.5750 - val_loss: 146143.8906 - val_accuracy: 0.4955 - 4s/epoch - 3ms/step\n",
      "Epoch 39/10000\n",
      "1126/1126 - 3s - loss: 90825.2500 - accuracy: 0.5769 - val_loss: 243371.4844 - val_accuracy: 0.6108 - 3s/epoch - 3ms/step\n",
      "Epoch 40/10000\n",
      "1126/1126 - 4s - loss: 95218.7891 - accuracy: 0.5750 - val_loss: 71493.1484 - val_accuracy: 0.5242 - 4s/epoch - 3ms/step\n",
      "Epoch 41/10000\n",
      "1126/1126 - 2s - loss: 91226.0156 - accuracy: 0.5776 - val_loss: 39750.3867 - val_accuracy: 0.5708 - 2s/epoch - 2ms/step\n",
      "Epoch 42/10000\n",
      "1126/1126 - 2s - loss: 87849.3047 - accuracy: 0.5778 - val_loss: 46312.1445 - val_accuracy: 0.5712 - 2s/epoch - 2ms/step\n",
      "Epoch 43/10000\n",
      "1126/1126 - 2s - loss: 91166.4766 - accuracy: 0.5779 - val_loss: 51614.7227 - val_accuracy: 0.6027 - 2s/epoch - 2ms/step\n",
      "Epoch 44/10000\n",
      "1126/1126 - 2s - loss: 99359.3750 - accuracy: 0.5763 - val_loss: 44084.3984 - val_accuracy: 0.5763 - 2s/epoch - 2ms/step\n",
      "Epoch 45/10000\n",
      "1126/1126 - 2s - loss: 92735.6406 - accuracy: 0.5797 - val_loss: 28562.3516 - val_accuracy: 0.6349 - 2s/epoch - 2ms/step\n",
      "Epoch 46/10000\n",
      "1126/1126 - 3s - loss: 84930.0078 - accuracy: 0.5782 - val_loss: 29557.7227 - val_accuracy: 0.6334 - 3s/epoch - 2ms/step\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Iterazione 18/128 (03-11-2023_10-33-08)\n",
      "hidden_layer_size: 20\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 3\n",
      "activation_hidden: relu\n",
      "activation_output: sigmoid\n",
      "batch_size: 100\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 20\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 63.9%\n",
      "La recall di questo modello sul validation set è: 42.0%\n",
      "La f1 di questo modello sul validation set è: 47.37%\n",
      "La balanced accuracy di questo modello sul validation set è: 59.86%\n",
      "La precision di questo modello sul validation set è: 54.33%\n",
      "La AUC di questo modello sul validation set è: 59.85%\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.68    0.78      0.73  7,670.00\n",
      "1                  0.54    0.42      0.47  4,841.00\n",
      "accuracy           0.64    0.64      0.64      0.64\n",
      "macro avg          0.61    0.60      0.60 12,511.00\n",
      "weighted avg       0.63    0.64      0.63 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         5961         1709\n",
      "Actual 1         2808         2033\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 65.94% con il modello:\n",
      "1 - AC65.94%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 62.97% con il modello:\n",
      "1 - BA62.97%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 64.08% con il modello:\n",
      "13 - PR64.08%, L20, mV-10, MV10, P2, AHsigmoid, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-26-48).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "113/113 - 3s - loss: 78387520.0000 - accuracy: 0.6033 - val_loss: 26535926.0000 - val_accuracy: 0.5455 - 3s/epoch - 24ms/step\n",
      "Epoch 2/10000\n",
      "113/113 - 0s - loss: 11632856.0000 - accuracy: 0.4315 - val_loss: 8192611.5000 - val_accuracy: 0.4100 - 350ms/epoch - 3ms/step\n",
      "Epoch 3/10000\n",
      "113/113 - 0s - loss: 6827047.5000 - accuracy: 0.4094 - val_loss: 5150202.0000 - val_accuracy: 0.4043 - 418ms/epoch - 4ms/step\n",
      "Epoch 4/10000\n",
      "113/113 - 0s - loss: 3507593.5000 - accuracy: 0.4103 - val_loss: 2417796.2500 - val_accuracy: 0.4110 - 361ms/epoch - 3ms/step\n",
      "Epoch 5/10000\n",
      "113/113 - 0s - loss: 1870233.5000 - accuracy: 0.4207 - val_loss: 1555619.7500 - val_accuracy: 0.4286 - 356ms/epoch - 3ms/step\n",
      "Epoch 6/10000\n",
      "113/113 - 0s - loss: 1387898.2500 - accuracy: 0.4420 - val_loss: 1296638.1250 - val_accuracy: 0.4398 - 364ms/epoch - 3ms/step\n",
      "Epoch 7/10000\n",
      "113/113 - 0s - loss: 1194066.1250 - accuracy: 0.4492 - val_loss: 1138959.0000 - val_accuracy: 0.4546 - 348ms/epoch - 3ms/step\n",
      "Epoch 8/10000\n",
      "113/113 - 0s - loss: 1047447.7500 - accuracy: 0.4521 - val_loss: 999327.2500 - val_accuracy: 0.4424 - 386ms/epoch - 3ms/step\n",
      "Epoch 9/10000\n",
      "113/113 - 0s - loss: 909541.8125 - accuracy: 0.4549 - val_loss: 854657.6875 - val_accuracy: 0.4546 - 352ms/epoch - 3ms/step\n",
      "Epoch 10/10000\n",
      "113/113 - 0s - loss: 784261.9375 - accuracy: 0.4579 - val_loss: 757438.5000 - val_accuracy: 0.4691 - 348ms/epoch - 3ms/step\n",
      "Epoch 11/10000\n",
      "113/113 - 0s - loss: 661880.4375 - accuracy: 0.4595 - val_loss: 615040.6875 - val_accuracy: 0.4641 - 362ms/epoch - 3ms/step\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Iterazione 19/128 (03-11-2023_10-33-18)\n",
      "hidden_layer_size: 20\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 3\n",
      "activation_hidden: relu\n",
      "activation_output: sigmoid\n",
      "batch_size: 1000\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 10\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 54.55%\n",
      "La recall di questo modello sul validation set è: 15.62%\n",
      "La f1 di questo modello sul validation set è: 21.01%\n",
      "La balanced accuracy di questo modello sul validation set è: 47.37%\n",
      "La precision di questo modello sul validation set è: 32.07%\n",
      "La AUC di questo modello sul validation set è: 47.37%\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.60    0.79      0.68  7,670.00\n",
      "1                  0.32    0.16      0.21  4,841.00\n",
      "accuracy           0.55    0.55      0.55      0.55\n",
      "macro avg          0.46    0.47      0.45 12,511.00\n",
      "weighted avg       0.49    0.55      0.50 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         6069         1601\n",
      "Actual 1         4085          756\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 65.94% con il modello:\n",
      "1 - AC65.94%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 62.97% con il modello:\n",
      "1 - BA62.97%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 64.08% con il modello:\n",
      "13 - PR64.08%, L20, mV-10, MV10, P2, AHsigmoid, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-26-48).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "113/113 - 3s - loss: 78387520.0000 - accuracy: 0.6033 - val_loss: 26535926.0000 - val_accuracy: 0.5455 - 3s/epoch - 30ms/step\n",
      "Epoch 2/10000\n",
      "113/113 - 0s - loss: 11632856.0000 - accuracy: 0.4315 - val_loss: 8192611.5000 - val_accuracy: 0.4100 - 435ms/epoch - 4ms/step\n",
      "Epoch 3/10000\n",
      "113/113 - 0s - loss: 6827047.5000 - accuracy: 0.4094 - val_loss: 5150202.0000 - val_accuracy: 0.4043 - 344ms/epoch - 3ms/step\n",
      "Epoch 4/10000\n",
      "113/113 - 0s - loss: 3507593.5000 - accuracy: 0.4103 - val_loss: 2417796.2500 - val_accuracy: 0.4110 - 367ms/epoch - 3ms/step\n",
      "Epoch 5/10000\n",
      "113/113 - 0s - loss: 1870233.5000 - accuracy: 0.4207 - val_loss: 1555619.7500 - val_accuracy: 0.4286 - 340ms/epoch - 3ms/step\n",
      "Epoch 6/10000\n",
      "113/113 - 0s - loss: 1387898.2500 - accuracy: 0.4420 - val_loss: 1296638.1250 - val_accuracy: 0.4398 - 358ms/epoch - 3ms/step\n",
      "Epoch 7/10000\n",
      "113/113 - 0s - loss: 1194066.1250 - accuracy: 0.4492 - val_loss: 1138959.0000 - val_accuracy: 0.4546 - 336ms/epoch - 3ms/step\n",
      "Epoch 8/10000\n",
      "113/113 - 0s - loss: 1047447.7500 - accuracy: 0.4521 - val_loss: 999327.2500 - val_accuracy: 0.4424 - 337ms/epoch - 3ms/step\n",
      "Epoch 9/10000\n",
      "113/113 - 0s - loss: 909541.8125 - accuracy: 0.4549 - val_loss: 854657.6875 - val_accuracy: 0.4546 - 317ms/epoch - 3ms/step\n",
      "Epoch 10/10000\n",
      "113/113 - 0s - loss: 784261.9375 - accuracy: 0.4579 - val_loss: 757438.5000 - val_accuracy: 0.4691 - 331ms/epoch - 3ms/step\n",
      "Epoch 11/10000\n",
      "113/113 - 0s - loss: 661880.4375 - accuracy: 0.4595 - val_loss: 615040.6875 - val_accuracy: 0.4641 - 374ms/epoch - 3ms/step\n",
      "Epoch 12/10000\n",
      "113/113 - 0s - loss: 555323.6250 - accuracy: 0.4668 - val_loss: 503254.7500 - val_accuracy: 0.4684 - 354ms/epoch - 3ms/step\n",
      "Epoch 13/10000\n",
      "113/113 - 0s - loss: 450414.0938 - accuracy: 0.4741 - val_loss: 423632.0625 - val_accuracy: 0.4916 - 333ms/epoch - 3ms/step\n",
      "Epoch 14/10000\n",
      "113/113 - 0s - loss: 363634.3125 - accuracy: 0.4877 - val_loss: 355727.4688 - val_accuracy: 0.5071 - 350ms/epoch - 3ms/step\n",
      "Epoch 15/10000\n",
      "113/113 - 0s - loss: 301272.2188 - accuracy: 0.5004 - val_loss: 265430.5000 - val_accuracy: 0.4969 - 356ms/epoch - 3ms/step\n",
      "Epoch 16/10000\n",
      "113/113 - 0s - loss: 271710.7188 - accuracy: 0.5116 - val_loss: 230922.2969 - val_accuracy: 0.5002 - 310ms/epoch - 3ms/step\n",
      "Epoch 17/10000\n",
      "113/113 - 0s - loss: 209920.8906 - accuracy: 0.5276 - val_loss: 174791.2969 - val_accuracy: 0.5342 - 328ms/epoch - 3ms/step\n",
      "Epoch 18/10000\n",
      "113/113 - 0s - loss: 191652.5938 - accuracy: 0.5408 - val_loss: 157927.3438 - val_accuracy: 0.5478 - 332ms/epoch - 3ms/step\n",
      "Epoch 19/10000\n",
      "113/113 - 0s - loss: 206383.5938 - accuracy: 0.5489 - val_loss: 160765.8438 - val_accuracy: 0.5682 - 383ms/epoch - 3ms/step\n",
      "Epoch 20/10000\n",
      "113/113 - 0s - loss: 188923.7500 - accuracy: 0.5559 - val_loss: 180872.0469 - val_accuracy: 0.5279 - 356ms/epoch - 3ms/step\n",
      "Epoch 21/10000\n",
      "113/113 - 0s - loss: 162032.1875 - accuracy: 0.5628 - val_loss: 137673.4219 - val_accuracy: 0.5704 - 363ms/epoch - 3ms/step\n",
      "Epoch 22/10000\n",
      "113/113 - 0s - loss: 160387.7500 - accuracy: 0.5623 - val_loss: 254621.9062 - val_accuracy: 0.5864 - 356ms/epoch - 3ms/step\n",
      "Epoch 23/10000\n",
      "113/113 - 0s - loss: 227377.1562 - accuracy: 0.5509 - val_loss: 187546.2500 - val_accuracy: 0.5234 - 373ms/epoch - 3ms/step\n",
      "Epoch 24/10000\n",
      "113/113 - 0s - loss: 159028.0469 - accuracy: 0.5658 - val_loss: 126646.2344 - val_accuracy: 0.5773 - 367ms/epoch - 3ms/step\n",
      "Epoch 25/10000\n",
      "113/113 - 0s - loss: 162760.8750 - accuracy: 0.5616 - val_loss: 170653.1250 - val_accuracy: 0.5269 - 387ms/epoch - 3ms/step\n",
      "Epoch 26/10000\n",
      "113/113 - 0s - loss: 183756.9219 - accuracy: 0.5564 - val_loss: 155118.6719 - val_accuracy: 0.5853 - 356ms/epoch - 3ms/step\n",
      "Epoch 27/10000\n",
      "113/113 - 0s - loss: 147226.7031 - accuracy: 0.5689 - val_loss: 120872.5391 - val_accuracy: 0.5632 - 360ms/epoch - 3ms/step\n",
      "Epoch 28/10000\n",
      "113/113 - 0s - loss: 137358.8750 - accuracy: 0.5666 - val_loss: 140174.6875 - val_accuracy: 0.5856 - 359ms/epoch - 3ms/step\n",
      "Epoch 29/10000\n",
      "113/113 - 0s - loss: 144079.2344 - accuracy: 0.5594 - val_loss: 184111.1406 - val_accuracy: 0.5135 - 356ms/epoch - 3ms/step\n",
      "Epoch 30/10000\n",
      "113/113 - 0s - loss: 134124.8750 - accuracy: 0.5693 - val_loss: 150384.2031 - val_accuracy: 0.5915 - 338ms/epoch - 3ms/step\n",
      "Epoch 31/10000\n",
      "113/113 - 0s - loss: 144172.1406 - accuracy: 0.5653 - val_loss: 95387.6016 - val_accuracy: 0.5828 - 416ms/epoch - 4ms/step\n",
      "Epoch 32/10000\n",
      "113/113 - 0s - loss: 117582.4688 - accuracy: 0.5717 - val_loss: 101922.7969 - val_accuracy: 0.5617 - 357ms/epoch - 3ms/step\n",
      "Epoch 33/10000\n",
      "113/113 - 0s - loss: 142393.9219 - accuracy: 0.5623 - val_loss: 182147.6094 - val_accuracy: 0.5074 - 362ms/epoch - 3ms/step\n",
      "Epoch 34/10000\n",
      "113/113 - 0s - loss: 125294.5469 - accuracy: 0.5690 - val_loss: 96146.5703 - val_accuracy: 0.5600 - 386ms/epoch - 3ms/step\n",
      "Epoch 35/10000\n",
      "113/113 - 0s - loss: 139842.5781 - accuracy: 0.5659 - val_loss: 98985.9766 - val_accuracy: 0.5604 - 371ms/epoch - 3ms/step\n",
      "Epoch 36/10000\n",
      "113/113 - 0s - loss: 137362.2344 - accuracy: 0.5617 - val_loss: 131742.0625 - val_accuracy: 0.5967 - 373ms/epoch - 3ms/step\n",
      "Epoch 37/10000\n",
      "113/113 - 0s - loss: 155183.6406 - accuracy: 0.5643 - val_loss: 140741.4219 - val_accuracy: 0.5273 - 378ms/epoch - 3ms/step\n",
      "Epoch 38/10000\n",
      "113/113 - 0s - loss: 97354.1172 - accuracy: 0.5823 - val_loss: 80702.5859 - val_accuracy: 0.5971 - 334ms/epoch - 3ms/step\n",
      "Epoch 39/10000\n",
      "113/113 - 0s - loss: 99066.0938 - accuracy: 0.5766 - val_loss: 103819.6406 - val_accuracy: 0.5948 - 346ms/epoch - 3ms/step\n",
      "Epoch 40/10000\n",
      "113/113 - 0s - loss: 100270.2656 - accuracy: 0.5717 - val_loss: 97935.6328 - val_accuracy: 0.5976 - 347ms/epoch - 3ms/step\n",
      "Epoch 41/10000\n",
      "113/113 - 0s - loss: 113081.5625 - accuracy: 0.5717 - val_loss: 163783.8750 - val_accuracy: 0.5072 - 375ms/epoch - 3ms/step\n",
      "Epoch 42/10000\n",
      "113/113 - 0s - loss: 154959.0156 - accuracy: 0.5635 - val_loss: 131576.0156 - val_accuracy: 0.6027 - 345ms/epoch - 3ms/step\n",
      "Epoch 43/10000\n",
      "113/113 - 0s - loss: 113002.5625 - accuracy: 0.5707 - val_loss: 142887.3438 - val_accuracy: 0.5175 - 379ms/epoch - 3ms/step\n",
      "Epoch 44/10000\n",
      "113/113 - 0s - loss: 99134.4453 - accuracy: 0.5722 - val_loss: 83026.4141 - val_accuracy: 0.6016 - 359ms/epoch - 3ms/step\n",
      "Epoch 45/10000\n",
      "113/113 - 0s - loss: 92017.8906 - accuracy: 0.5760 - val_loss: 77133.2656 - val_accuracy: 0.5708 - 361ms/epoch - 3ms/step\n",
      "Epoch 46/10000\n",
      "113/113 - 0s - loss: 94953.5703 - accuracy: 0.5782 - val_loss: 59979.3750 - val_accuracy: 0.6053 - 381ms/epoch - 3ms/step\n",
      "Epoch 47/10000\n",
      "113/113 - 0s - loss: 129762.7266 - accuracy: 0.5641 - val_loss: 60245.5117 - val_accuracy: 0.6141 - 351ms/epoch - 3ms/step\n",
      "Epoch 48/10000\n",
      "113/113 - 0s - loss: 95345.6172 - accuracy: 0.5737 - val_loss: 73927.9453 - val_accuracy: 0.5616 - 344ms/epoch - 3ms/step\n",
      "Epoch 49/10000\n",
      "113/113 - 0s - loss: 114505.1406 - accuracy: 0.5697 - val_loss: 133992.3125 - val_accuracy: 0.6026 - 339ms/epoch - 3ms/step\n",
      "Epoch 50/10000\n",
      "113/113 - 0s - loss: 107169.0547 - accuracy: 0.5714 - val_loss: 76781.6953 - val_accuracy: 0.5570 - 345ms/epoch - 3ms/step\n",
      "Epoch 51/10000\n",
      "113/113 - 0s - loss: 129144.6641 - accuracy: 0.5679 - val_loss: 126253.7500 - val_accuracy: 0.5219 - 372ms/epoch - 3ms/step\n",
      "Epoch 52/10000\n",
      "113/113 - 0s - loss: 94449.4062 - accuracy: 0.5783 - val_loss: 107895.4766 - val_accuracy: 0.5235 - 330ms/epoch - 3ms/step\n",
      "Epoch 53/10000\n",
      "113/113 - 0s - loss: 81351.8125 - accuracy: 0.5783 - val_loss: 86392.8672 - val_accuracy: 0.6014 - 350ms/epoch - 3ms/step\n",
      "Epoch 54/10000\n",
      "113/113 - 0s - loss: 119404.5312 - accuracy: 0.5675 - val_loss: 157650.4844 - val_accuracy: 0.5040 - 375ms/epoch - 3ms/step\n",
      "Epoch 55/10000\n",
      "113/113 - 0s - loss: 116398.2891 - accuracy: 0.5667 - val_loss: 88720.9844 - val_accuracy: 0.6014 - 341ms/epoch - 3ms/step\n",
      "Epoch 56/10000\n",
      "113/113 - 0s - loss: 96633.6328 - accuracy: 0.5727 - val_loss: 181665.4688 - val_accuracy: 0.6077 - 345ms/epoch - 3ms/step\n",
      "Epoch 57/10000\n",
      "113/113 - 0s - loss: 99115.6328 - accuracy: 0.5677 - val_loss: 104897.8594 - val_accuracy: 0.6034 - 364ms/epoch - 3ms/step\n",
      "Epoch 58/10000\n",
      "113/113 - 0s - loss: 77440.2031 - accuracy: 0.5804 - val_loss: 74018.0625 - val_accuracy: 0.5451 - 347ms/epoch - 3ms/step\n",
      "Epoch 59/10000\n",
      "113/113 - 0s - loss: 92630.8906 - accuracy: 0.5750 - val_loss: 72408.3047 - val_accuracy: 0.5933 - 335ms/epoch - 3ms/step\n",
      "Epoch 60/10000\n",
      "113/113 - 0s - loss: 101881.5000 - accuracy: 0.5733 - val_loss: 326591.8438 - val_accuracy: 0.4747 - 370ms/epoch - 3ms/step\n",
      "Epoch 61/10000\n",
      "113/113 - 0s - loss: 167704.1094 - accuracy: 0.5621 - val_loss: 107734.0156 - val_accuracy: 0.5271 - 344ms/epoch - 3ms/step\n",
      "Epoch 62/10000\n",
      "113/113 - 0s - loss: 77999.8125 - accuracy: 0.5800 - val_loss: 55877.5312 - val_accuracy: 0.6115 - 404ms/epoch - 4ms/step\n",
      "Epoch 63/10000\n",
      "113/113 - 0s - loss: 111340.6328 - accuracy: 0.5692 - val_loss: 163138.6719 - val_accuracy: 0.6070 - 339ms/epoch - 3ms/step\n",
      "Epoch 64/10000\n",
      "113/113 - 0s - loss: 164625.4062 - accuracy: 0.5645 - val_loss: 182287.5781 - val_accuracy: 0.4960 - 341ms/epoch - 3ms/step\n",
      "Epoch 65/10000\n",
      "113/113 - 1s - loss: 147160.3125 - accuracy: 0.5649 - val_loss: 163523.0781 - val_accuracy: 0.6082 - 588ms/epoch - 5ms/step\n",
      "Epoch 66/10000\n",
      "113/113 - 0s - loss: 95778.0000 - accuracy: 0.5764 - val_loss: 103392.4766 - val_accuracy: 0.5296 - 463ms/epoch - 4ms/step\n",
      "Epoch 67/10000\n",
      "113/113 - 0s - loss: 97845.2031 - accuracy: 0.5714 - val_loss: 116281.7109 - val_accuracy: 0.5276 - 368ms/epoch - 3ms/step\n",
      "391/391 [==============================] - 1s 3ms/step\n",
      "\n",
      "Iterazione 20/128 (03-11-2023_10-33-49)\n",
      "hidden_layer_size: 20\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 3\n",
      "activation_hidden: relu\n",
      "activation_output: sigmoid\n",
      "batch_size: 1000\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 20\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 61.41%\n",
      "La recall di questo modello sul validation set è: 35.34%\n",
      "La f1 di questo modello sul validation set è: 41.48%\n",
      "La balanced accuracy di questo modello sul validation set è: 56.6%\n",
      "La precision di questo modello sul validation set è: 50.19%\n",
      "La AUC di questo modello sul validation set è: 56.62%\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.66    0.78      0.71  7,670.00\n",
      "1                  0.50    0.35      0.41  4,841.00\n",
      "accuracy           0.61    0.61      0.61      0.61\n",
      "macro avg          0.58    0.57      0.56 12,511.00\n",
      "weighted avg       0.60    0.61      0.60 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         5972         1698\n",
      "Actual 1         3130         1711\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 65.94% con il modello:\n",
      "1 - AC65.94%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 62.97% con il modello:\n",
      "1 - BA62.97%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 64.08% con il modello:\n",
      "13 - PR64.08%, L20, mV-10, MV10, P2, AHsigmoid, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-26-48).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "1126/1126 - 6s - loss: 1.0037 - accuracy: 0.6118 - val_loss: 0.6632 - val_accuracy: 0.6121 - 6s/epoch - 5ms/step\n",
      "Epoch 2/10000\n",
      "1126/1126 - 3s - loss: 0.6630 - accuracy: 0.6115 - val_loss: 0.6603 - val_accuracy: 0.6147 - 3s/epoch - 3ms/step\n",
      "Epoch 3/10000\n",
      "1126/1126 - 3s - loss: 0.6616 - accuracy: 0.6122 - val_loss: 0.6619 - val_accuracy: 0.6143 - 3s/epoch - 3ms/step\n",
      "Epoch 4/10000\n",
      "1126/1126 - 2s - loss: 0.6614 - accuracy: 0.6125 - val_loss: 0.6608 - val_accuracy: 0.6147 - 2s/epoch - 2ms/step\n",
      "Epoch 5/10000\n",
      "1126/1126 - 2s - loss: 0.6614 - accuracy: 0.6125 - val_loss: 0.6601 - val_accuracy: 0.6161 - 2s/epoch - 2ms/step\n",
      "Epoch 6/10000\n",
      "1126/1126 - 3s - loss: 0.6620 - accuracy: 0.6125 - val_loss: 0.6600 - val_accuracy: 0.6158 - 3s/epoch - 2ms/step\n",
      "Epoch 7/10000\n",
      "1126/1126 - 3s - loss: 0.6625 - accuracy: 0.6127 - val_loss: 0.6613 - val_accuracy: 0.6141 - 3s/epoch - 3ms/step\n",
      "Epoch 8/10000\n",
      "1126/1126 - 3s - loss: 0.6621 - accuracy: 0.6123 - val_loss: 0.6620 - val_accuracy: 0.6146 - 3s/epoch - 3ms/step\n",
      "Epoch 9/10000\n",
      "1126/1126 - 3s - loss: 0.6616 - accuracy: 0.6120 - val_loss: 0.6615 - val_accuracy: 0.6158 - 3s/epoch - 3ms/step\n",
      "Epoch 10/10000\n",
      "1126/1126 - 3s - loss: 0.6622 - accuracy: 0.6124 - val_loss: 0.6613 - val_accuracy: 0.6161 - 3s/epoch - 3ms/step\n",
      "Epoch 11/10000\n",
      "1126/1126 - 2s - loss: 0.6628 - accuracy: 0.6124 - val_loss: 0.6626 - val_accuracy: 0.6151 - 2s/epoch - 2ms/step\n",
      "Epoch 12/10000\n",
      "1126/1126 - 2s - loss: 0.6629 - accuracy: 0.6124 - val_loss: 0.6615 - val_accuracy: 0.6153 - 2s/epoch - 2ms/step\n",
      "Epoch 13/10000\n",
      "1126/1126 - 2s - loss: 0.6628 - accuracy: 0.6123 - val_loss: 0.6611 - val_accuracy: 0.6159 - 2s/epoch - 2ms/step\n",
      "Epoch 14/10000\n",
      "1126/1126 - 2s - loss: 0.6624 - accuracy: 0.6127 - val_loss: 0.6610 - val_accuracy: 0.6153 - 2s/epoch - 2ms/step\n",
      "Epoch 15/10000\n",
      "1126/1126 - 2s - loss: 0.6625 - accuracy: 0.6126 - val_loss: 0.6611 - val_accuracy: 0.6141 - 2s/epoch - 2ms/step\n",
      "391/391 [==============================] - 1s 3ms/step\n",
      "\n",
      "Iterazione 21/128 (03-11-2023_10-34-34)\n",
      "hidden_layer_size: 20\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 3\n",
      "activation_hidden: sigmoid\n",
      "activation_output: sigmoid\n",
      "batch_size: 100\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 10\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 61.61%\n",
      "La recall di questo modello sul validation set è: 1.8%\n",
      "La f1 di questo modello sul validation set è: 3.5%\n",
      "La balanced accuracy di questo modello sul validation set è: 50.58%\n",
      "La precision di questo modello sul validation set è: 63.97%\n",
      "La AUC di questo modello sul validation set è: 56.65%\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.62    0.99      0.76  7,670.00\n",
      "1                  0.64    0.02      0.03  4,841.00\n",
      "accuracy           0.62    0.62      0.62      0.62\n",
      "macro avg          0.63    0.51      0.40 12,511.00\n",
      "weighted avg       0.63    0.62      0.48 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         7621           49\n",
      "Actual 1         4754           87\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 65.94% con il modello:\n",
      "1 - AC65.94%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 62.97% con il modello:\n",
      "1 - BA62.97%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 64.08% con il modello:\n",
      "13 - PR64.08%, L20, mV-10, MV10, P2, AHsigmoid, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-26-48).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "1126/1126 - 6s - loss: 1.0037 - accuracy: 0.6118 - val_loss: 0.6632 - val_accuracy: 0.6121 - 6s/epoch - 5ms/step\n",
      "Epoch 2/10000\n",
      "1126/1126 - 3s - loss: 0.6630 - accuracy: 0.6115 - val_loss: 0.6603 - val_accuracy: 0.6147 - 3s/epoch - 3ms/step\n",
      "Epoch 3/10000\n",
      "1126/1126 - 3s - loss: 0.6616 - accuracy: 0.6122 - val_loss: 0.6619 - val_accuracy: 0.6143 - 3s/epoch - 2ms/step\n",
      "Epoch 4/10000\n",
      "1126/1126 - 3s - loss: 0.6614 - accuracy: 0.6125 - val_loss: 0.6608 - val_accuracy: 0.6147 - 3s/epoch - 3ms/step\n",
      "Epoch 5/10000\n",
      "1126/1126 - 5s - loss: 0.6614 - accuracy: 0.6125 - val_loss: 0.6601 - val_accuracy: 0.6161 - 5s/epoch - 4ms/step\n",
      "Epoch 6/10000\n",
      "1126/1126 - 3s - loss: 0.6620 - accuracy: 0.6125 - val_loss: 0.6600 - val_accuracy: 0.6158 - 3s/epoch - 2ms/step\n",
      "Epoch 7/10000\n",
      "1126/1126 - 3s - loss: 0.6625 - accuracy: 0.6127 - val_loss: 0.6613 - val_accuracy: 0.6141 - 3s/epoch - 3ms/step\n",
      "Epoch 8/10000\n",
      "1126/1126 - 3s - loss: 0.6621 - accuracy: 0.6123 - val_loss: 0.6620 - val_accuracy: 0.6146 - 3s/epoch - 2ms/step\n",
      "Epoch 9/10000\n",
      "1126/1126 - 2s - loss: 0.6616 - accuracy: 0.6120 - val_loss: 0.6615 - val_accuracy: 0.6158 - 2s/epoch - 2ms/step\n",
      "Epoch 10/10000\n",
      "1126/1126 - 2s - loss: 0.6622 - accuracy: 0.6124 - val_loss: 0.6613 - val_accuracy: 0.6161 - 2s/epoch - 2ms/step\n",
      "Epoch 11/10000\n",
      "1126/1126 - 2s - loss: 0.6628 - accuracy: 0.6124 - val_loss: 0.6626 - val_accuracy: 0.6151 - 2s/epoch - 2ms/step\n",
      "Epoch 12/10000\n",
      "1126/1126 - 2s - loss: 0.6629 - accuracy: 0.6124 - val_loss: 0.6615 - val_accuracy: 0.6153 - 2s/epoch - 2ms/step\n",
      "Epoch 13/10000\n",
      "1126/1126 - 2s - loss: 0.6628 - accuracy: 0.6123 - val_loss: 0.6611 - val_accuracy: 0.6159 - 2s/epoch - 2ms/step\n",
      "Epoch 14/10000\n",
      "1126/1126 - 2s - loss: 0.6624 - accuracy: 0.6127 - val_loss: 0.6610 - val_accuracy: 0.6153 - 2s/epoch - 2ms/step\n",
      "Epoch 15/10000\n",
      "1126/1126 - 2s - loss: 0.6625 - accuracy: 0.6126 - val_loss: 0.6611 - val_accuracy: 0.6141 - 2s/epoch - 2ms/step\n",
      "Epoch 16/10000\n",
      "1126/1126 - 2s - loss: 0.6622 - accuracy: 0.6126 - val_loss: 0.6607 - val_accuracy: 0.6159 - 2s/epoch - 2ms/step\n",
      "Epoch 17/10000\n",
      "1126/1126 - 2s - loss: 0.6625 - accuracy: 0.6129 - val_loss: 0.6618 - val_accuracy: 0.6158 - 2s/epoch - 2ms/step\n",
      "Epoch 18/10000\n",
      "1126/1126 - 2s - loss: 0.6627 - accuracy: 0.6126 - val_loss: 0.6612 - val_accuracy: 0.6156 - 2s/epoch - 2ms/step\n",
      "Epoch 19/10000\n",
      "1126/1126 - 2s - loss: 0.6627 - accuracy: 0.6123 - val_loss: 0.6623 - val_accuracy: 0.6161 - 2s/epoch - 2ms/step\n",
      "Epoch 20/10000\n",
      "1126/1126 - 2s - loss: 0.6628 - accuracy: 0.6129 - val_loss: 0.6627 - val_accuracy: 0.6138 - 2s/epoch - 2ms/step\n",
      "Epoch 21/10000\n",
      "1126/1126 - 2s - loss: 0.6631 - accuracy: 0.6122 - val_loss: 0.6611 - val_accuracy: 0.6157 - 2s/epoch - 2ms/step\n",
      "Epoch 22/10000\n",
      "1126/1126 - 2s - loss: 0.6620 - accuracy: 0.6125 - val_loss: 0.6619 - val_accuracy: 0.6140 - 2s/epoch - 2ms/step\n",
      "Epoch 23/10000\n",
      "1126/1126 - 2s - loss: 0.6625 - accuracy: 0.6125 - val_loss: 0.6611 - val_accuracy: 0.6138 - 2s/epoch - 2ms/step\n",
      "Epoch 24/10000\n",
      "1126/1126 - 2s - loss: 0.6625 - accuracy: 0.6127 - val_loss: 0.6621 - val_accuracy: 0.6139 - 2s/epoch - 2ms/step\n",
      "Epoch 25/10000\n",
      "1126/1126 - 2s - loss: 0.6625 - accuracy: 0.6124 - val_loss: 0.6604 - val_accuracy: 0.6159 - 2s/epoch - 2ms/step\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Iterazione 22/128 (03-11-2023_10-35-42)\n",
      "hidden_layer_size: 20\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 3\n",
      "activation_hidden: sigmoid\n",
      "activation_output: sigmoid\n",
      "batch_size: 100\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 20\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 61.61%\n",
      "La recall di questo modello sul validation set è: 1.8%\n",
      "La f1 di questo modello sul validation set è: 3.5%\n",
      "La balanced accuracy di questo modello sul validation set è: 50.58%\n",
      "La precision di questo modello sul validation set è: 63.97%\n",
      "La AUC di questo modello sul validation set è: 56.65%\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.62    0.99      0.76  7,670.00\n",
      "1                  0.64    0.02      0.03  4,841.00\n",
      "accuracy           0.62    0.62      0.62      0.62\n",
      "macro avg          0.63    0.51      0.40 12,511.00\n",
      "weighted avg       0.63    0.62      0.48 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         7621           49\n",
      "Actual 1         4754           87\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 65.94% con il modello:\n",
      "1 - AC65.94%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 62.97% con il modello:\n",
      "1 - BA62.97%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 64.08% con il modello:\n",
      "13 - PR64.08%, L20, mV-10, MV10, P2, AHsigmoid, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-26-48).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "113/113 - 3s - loss: 2.7060 - accuracy: 0.6113 - val_loss: 2.2302 - val_accuracy: 0.6130 - 3s/epoch - 24ms/step\n",
      "Epoch 2/10000\n",
      "113/113 - 0s - loss: 1.6958 - accuracy: 0.6111 - val_loss: 1.2294 - val_accuracy: 0.6110 - 377ms/epoch - 3ms/step\n",
      "Epoch 3/10000\n",
      "113/113 - 0s - loss: 0.8736 - accuracy: 0.6116 - val_loss: 0.6671 - val_accuracy: 0.6121 - 393ms/epoch - 3ms/step\n",
      "Epoch 4/10000\n",
      "113/113 - 0s - loss: 0.6663 - accuracy: 0.6118 - val_loss: 0.6644 - val_accuracy: 0.6121 - 361ms/epoch - 3ms/step\n",
      "Epoch 5/10000\n",
      "113/113 - 0s - loss: 0.6648 - accuracy: 0.6117 - val_loss: 0.6627 - val_accuracy: 0.6142 - 370ms/epoch - 3ms/step\n",
      "Epoch 6/10000\n",
      "113/113 - 0s - loss: 0.6630 - accuracy: 0.6116 - val_loss: 0.6617 - val_accuracy: 0.6123 - 359ms/epoch - 3ms/step\n",
      "Epoch 7/10000\n",
      "113/113 - 0s - loss: 0.6616 - accuracy: 0.6118 - val_loss: 0.6598 - val_accuracy: 0.6123 - 448ms/epoch - 4ms/step\n",
      "Epoch 8/10000\n",
      "113/113 - 0s - loss: 0.6601 - accuracy: 0.6119 - val_loss: 0.6600 - val_accuracy: 0.6142 - 376ms/epoch - 3ms/step\n",
      "Epoch 9/10000\n",
      "113/113 - 0s - loss: 0.6601 - accuracy: 0.6114 - val_loss: 0.6602 - val_accuracy: 0.6123 - 473ms/epoch - 4ms/step\n",
      "Epoch 10/10000\n",
      "113/113 - 0s - loss: 0.6599 - accuracy: 0.6116 - val_loss: 0.6602 - val_accuracy: 0.6127 - 441ms/epoch - 4ms/step\n",
      "Epoch 11/10000\n",
      "113/113 - 0s - loss: 0.6602 - accuracy: 0.6117 - val_loss: 0.6590 - val_accuracy: 0.6147 - 434ms/epoch - 4ms/step\n",
      "Epoch 12/10000\n",
      "113/113 - 1s - loss: 0.6601 - accuracy: 0.6123 - val_loss: 0.6591 - val_accuracy: 0.6158 - 539ms/epoch - 5ms/step\n",
      "Epoch 13/10000\n",
      "113/113 - 1s - loss: 0.6598 - accuracy: 0.6123 - val_loss: 0.6590 - val_accuracy: 0.6159 - 513ms/epoch - 5ms/step\n",
      "Epoch 14/10000\n",
      "113/113 - 1s - loss: 0.6600 - accuracy: 0.6125 - val_loss: 0.6594 - val_accuracy: 0.6139 - 506ms/epoch - 4ms/step\n",
      "Epoch 15/10000\n",
      "113/113 - 0s - loss: 0.6600 - accuracy: 0.6121 - val_loss: 0.6589 - val_accuracy: 0.6139 - 460ms/epoch - 4ms/step\n",
      "Epoch 16/10000\n",
      "113/113 - 0s - loss: 0.6600 - accuracy: 0.6128 - val_loss: 0.6593 - val_accuracy: 0.6159 - 432ms/epoch - 4ms/step\n",
      "Epoch 17/10000\n",
      "113/113 - 0s - loss: 0.6602 - accuracy: 0.6124 - val_loss: 0.6596 - val_accuracy: 0.6159 - 427ms/epoch - 4ms/step\n",
      "Epoch 18/10000\n",
      "113/113 - 0s - loss: 0.6602 - accuracy: 0.6123 - val_loss: 0.6590 - val_accuracy: 0.6142 - 443ms/epoch - 4ms/step\n",
      "Epoch 19/10000\n",
      "113/113 - 1s - loss: 0.6607 - accuracy: 0.6126 - val_loss: 0.6596 - val_accuracy: 0.6141 - 533ms/epoch - 5ms/step\n",
      "Epoch 20/10000\n",
      "113/113 - 0s - loss: 0.6602 - accuracy: 0.6123 - val_loss: 0.6593 - val_accuracy: 0.6141 - 484ms/epoch - 4ms/step\n",
      "Epoch 21/10000\n",
      "113/113 - 0s - loss: 0.6603 - accuracy: 0.6124 - val_loss: 0.6603 - val_accuracy: 0.6141 - 387ms/epoch - 3ms/step\n",
      "Epoch 22/10000\n",
      "113/113 - 0s - loss: 0.6603 - accuracy: 0.6123 - val_loss: 0.6597 - val_accuracy: 0.6159 - 371ms/epoch - 3ms/step\n",
      "Epoch 23/10000\n",
      "113/113 - 0s - loss: 0.6604 - accuracy: 0.6121 - val_loss: 0.6600 - val_accuracy: 0.6160 - 410ms/epoch - 4ms/step\n",
      "Epoch 24/10000\n",
      "113/113 - 0s - loss: 0.6603 - accuracy: 0.6126 - val_loss: 0.6593 - val_accuracy: 0.6141 - 405ms/epoch - 4ms/step\n",
      "Epoch 25/10000\n",
      "113/113 - 0s - loss: 0.6604 - accuracy: 0.6122 - val_loss: 0.6605 - val_accuracy: 0.6158 - 399ms/epoch - 4ms/step\n",
      "Epoch 26/10000\n",
      "113/113 - 0s - loss: 0.6601 - accuracy: 0.6121 - val_loss: 0.6594 - val_accuracy: 0.6141 - 401ms/epoch - 4ms/step\n",
      "Epoch 27/10000\n",
      "113/113 - 0s - loss: 0.6603 - accuracy: 0.6126 - val_loss: 0.6593 - val_accuracy: 0.6158 - 396ms/epoch - 4ms/step\n",
      "Epoch 28/10000\n",
      "113/113 - 0s - loss: 0.6603 - accuracy: 0.6124 - val_loss: 0.6592 - val_accuracy: 0.6159 - 380ms/epoch - 3ms/step\n",
      "Epoch 29/10000\n",
      "113/113 - 0s - loss: 0.6604 - accuracy: 0.6125 - val_loss: 0.6599 - val_accuracy: 0.6159 - 361ms/epoch - 3ms/step\n",
      "Epoch 30/10000\n",
      "113/113 - 0s - loss: 0.6602 - accuracy: 0.6124 - val_loss: 0.6606 - val_accuracy: 0.6160 - 371ms/epoch - 3ms/step\n",
      "Epoch 31/10000\n",
      "113/113 - 0s - loss: 0.6600 - accuracy: 0.6120 - val_loss: 0.6593 - val_accuracy: 0.6143 - 391ms/epoch - 3ms/step\n",
      "Epoch 32/10000\n",
      "113/113 - 0s - loss: 0.6603 - accuracy: 0.6121 - val_loss: 0.6592 - val_accuracy: 0.6159 - 392ms/epoch - 3ms/step\n",
      "Epoch 33/10000\n",
      "113/113 - 0s - loss: 0.6597 - accuracy: 0.6123 - val_loss: 0.6595 - val_accuracy: 0.6158 - 423ms/epoch - 4ms/step\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Iterazione 23/128 (03-11-2023_10-36-01)\n",
      "hidden_layer_size: 20\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 3\n",
      "activation_hidden: sigmoid\n",
      "activation_output: sigmoid\n",
      "batch_size: 1000\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 10\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 61.6%\n",
      "La recall di questo modello sul validation set è: 1.96%\n",
      "La f1 di questo modello sul validation set è: 3.8%\n",
      "La balanced accuracy di questo modello sul validation set è: 50.6%\n",
      "La precision di questo modello sul validation set è: 62.09%\n",
      "La AUC di questo modello sul validation set è: 56.61%\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.62    0.99      0.76  7,670.00\n",
      "1                  0.62    0.02      0.04  4,841.00\n",
      "accuracy           0.62    0.62      0.62      0.62\n",
      "macro avg          0.62    0.51      0.40 12,511.00\n",
      "weighted avg       0.62    0.62      0.48 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         7612           58\n",
      "Actual 1         4746           95\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 65.94% con il modello:\n",
      "1 - AC65.94%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 62.97% con il modello:\n",
      "1 - BA62.97%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 64.08% con il modello:\n",
      "13 - PR64.08%, L20, mV-10, MV10, P2, AHsigmoid, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-26-48).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "113/113 - 3s - loss: 2.7060 - accuracy: 0.6113 - val_loss: 2.2302 - val_accuracy: 0.6130 - 3s/epoch - 23ms/step\n",
      "Epoch 2/10000\n",
      "113/113 - 0s - loss: 1.6958 - accuracy: 0.6111 - val_loss: 1.2294 - val_accuracy: 0.6110 - 490ms/epoch - 4ms/step\n",
      "Epoch 3/10000\n",
      "113/113 - 1s - loss: 0.8736 - accuracy: 0.6116 - val_loss: 0.6671 - val_accuracy: 0.6121 - 625ms/epoch - 6ms/step\n",
      "Epoch 4/10000\n",
      "113/113 - 0s - loss: 0.6663 - accuracy: 0.6118 - val_loss: 0.6644 - val_accuracy: 0.6121 - 487ms/epoch - 4ms/step\n",
      "Epoch 5/10000\n",
      "113/113 - 0s - loss: 0.6648 - accuracy: 0.6117 - val_loss: 0.6627 - val_accuracy: 0.6142 - 363ms/epoch - 3ms/step\n",
      "Epoch 6/10000\n",
      "113/113 - 1s - loss: 0.6630 - accuracy: 0.6116 - val_loss: 0.6617 - val_accuracy: 0.6123 - 566ms/epoch - 5ms/step\n",
      "Epoch 7/10000\n",
      "113/113 - 0s - loss: 0.6616 - accuracy: 0.6118 - val_loss: 0.6598 - val_accuracy: 0.6123 - 366ms/epoch - 3ms/step\n",
      "Epoch 8/10000\n",
      "113/113 - 1s - loss: 0.6601 - accuracy: 0.6119 - val_loss: 0.6600 - val_accuracy: 0.6142 - 619ms/epoch - 5ms/step\n",
      "Epoch 9/10000\n",
      "113/113 - 0s - loss: 0.6601 - accuracy: 0.6114 - val_loss: 0.6602 - val_accuracy: 0.6123 - 403ms/epoch - 4ms/step\n",
      "Epoch 10/10000\n",
      "113/113 - 0s - loss: 0.6599 - accuracy: 0.6116 - val_loss: 0.6602 - val_accuracy: 0.6127 - 385ms/epoch - 3ms/step\n",
      "Epoch 11/10000\n",
      "113/113 - 0s - loss: 0.6602 - accuracy: 0.6117 - val_loss: 0.6590 - val_accuracy: 0.6147 - 368ms/epoch - 3ms/step\n",
      "Epoch 12/10000\n",
      "113/113 - 0s - loss: 0.6601 - accuracy: 0.6123 - val_loss: 0.6591 - val_accuracy: 0.6158 - 385ms/epoch - 3ms/step\n",
      "Epoch 13/10000\n",
      "113/113 - 0s - loss: 0.6598 - accuracy: 0.6123 - val_loss: 0.6590 - val_accuracy: 0.6159 - 371ms/epoch - 3ms/step\n",
      "Epoch 14/10000\n",
      "113/113 - 0s - loss: 0.6600 - accuracy: 0.6125 - val_loss: 0.6594 - val_accuracy: 0.6139 - 345ms/epoch - 3ms/step\n",
      "Epoch 15/10000\n",
      "113/113 - 0s - loss: 0.6600 - accuracy: 0.6121 - val_loss: 0.6589 - val_accuracy: 0.6139 - 342ms/epoch - 3ms/step\n",
      "Epoch 16/10000\n",
      "113/113 - 0s - loss: 0.6600 - accuracy: 0.6128 - val_loss: 0.6593 - val_accuracy: 0.6159 - 352ms/epoch - 3ms/step\n",
      "Epoch 17/10000\n",
      "113/113 - 0s - loss: 0.6602 - accuracy: 0.6124 - val_loss: 0.6596 - val_accuracy: 0.6159 - 355ms/epoch - 3ms/step\n",
      "Epoch 18/10000\n",
      "113/113 - 0s - loss: 0.6602 - accuracy: 0.6123 - val_loss: 0.6590 - val_accuracy: 0.6142 - 357ms/epoch - 3ms/step\n",
      "Epoch 19/10000\n",
      "113/113 - 0s - loss: 0.6607 - accuracy: 0.6126 - val_loss: 0.6596 - val_accuracy: 0.6141 - 342ms/epoch - 3ms/step\n",
      "Epoch 20/10000\n",
      "113/113 - 0s - loss: 0.6602 - accuracy: 0.6123 - val_loss: 0.6593 - val_accuracy: 0.6141 - 354ms/epoch - 3ms/step\n",
      "Epoch 21/10000\n",
      "113/113 - 0s - loss: 0.6603 - accuracy: 0.6124 - val_loss: 0.6603 - val_accuracy: 0.6141 - 367ms/epoch - 3ms/step\n",
      "Epoch 22/10000\n",
      "113/113 - 0s - loss: 0.6603 - accuracy: 0.6123 - val_loss: 0.6597 - val_accuracy: 0.6159 - 442ms/epoch - 4ms/step\n",
      "Epoch 23/10000\n",
      "113/113 - 0s - loss: 0.6604 - accuracy: 0.6121 - val_loss: 0.6600 - val_accuracy: 0.6160 - 349ms/epoch - 3ms/step\n",
      "Epoch 24/10000\n",
      "113/113 - 0s - loss: 0.6603 - accuracy: 0.6126 - val_loss: 0.6593 - val_accuracy: 0.6141 - 373ms/epoch - 3ms/step\n",
      "Epoch 25/10000\n",
      "113/113 - 0s - loss: 0.6604 - accuracy: 0.6122 - val_loss: 0.6605 - val_accuracy: 0.6158 - 414ms/epoch - 4ms/step\n",
      "Epoch 26/10000\n",
      "113/113 - 0s - loss: 0.6601 - accuracy: 0.6121 - val_loss: 0.6594 - val_accuracy: 0.6141 - 376ms/epoch - 3ms/step\n",
      "Epoch 27/10000\n",
      "113/113 - 0s - loss: 0.6603 - accuracy: 0.6126 - val_loss: 0.6593 - val_accuracy: 0.6158 - 356ms/epoch - 3ms/step\n",
      "Epoch 28/10000\n",
      "113/113 - 0s - loss: 0.6603 - accuracy: 0.6124 - val_loss: 0.6592 - val_accuracy: 0.6159 - 353ms/epoch - 3ms/step\n",
      "Epoch 29/10000\n",
      "113/113 - 0s - loss: 0.6604 - accuracy: 0.6125 - val_loss: 0.6599 - val_accuracy: 0.6159 - 381ms/epoch - 3ms/step\n",
      "Epoch 30/10000\n",
      "113/113 - 0s - loss: 0.6602 - accuracy: 0.6124 - val_loss: 0.6606 - val_accuracy: 0.6160 - 350ms/epoch - 3ms/step\n",
      "Epoch 31/10000\n",
      "113/113 - 0s - loss: 0.6600 - accuracy: 0.6120 - val_loss: 0.6593 - val_accuracy: 0.6143 - 356ms/epoch - 3ms/step\n",
      "Epoch 32/10000\n",
      "113/113 - 0s - loss: 0.6603 - accuracy: 0.6121 - val_loss: 0.6592 - val_accuracy: 0.6159 - 361ms/epoch - 3ms/step\n",
      "Epoch 33/10000\n",
      "113/113 - 0s - loss: 0.6597 - accuracy: 0.6123 - val_loss: 0.6595 - val_accuracy: 0.6158 - 359ms/epoch - 3ms/step\n",
      "Epoch 34/10000\n",
      "113/113 - 0s - loss: 0.6597 - accuracy: 0.6123 - val_loss: 0.6593 - val_accuracy: 0.6142 - 356ms/epoch - 3ms/step\n",
      "Epoch 35/10000\n",
      "113/113 - 0s - loss: 0.6599 - accuracy: 0.6119 - val_loss: 0.6591 - val_accuracy: 0.6159 - 349ms/epoch - 3ms/step\n",
      "Epoch 36/10000\n",
      "113/113 - 0s - loss: 0.6598 - accuracy: 0.6123 - val_loss: 0.6591 - val_accuracy: 0.6159 - 354ms/epoch - 3ms/step\n",
      "Epoch 37/10000\n",
      "113/113 - 0s - loss: 0.6597 - accuracy: 0.6125 - val_loss: 0.6596 - val_accuracy: 0.6141 - 361ms/epoch - 3ms/step\n",
      "Epoch 38/10000\n",
      "113/113 - 0s - loss: 0.6597 - accuracy: 0.6125 - val_loss: 0.6602 - val_accuracy: 0.6142 - 346ms/epoch - 3ms/step\n",
      "Epoch 39/10000\n",
      "113/113 - 0s - loss: 0.6598 - accuracy: 0.6127 - val_loss: 0.6594 - val_accuracy: 0.6141 - 363ms/epoch - 3ms/step\n",
      "Epoch 40/10000\n",
      "113/113 - 0s - loss: 0.6602 - accuracy: 0.6127 - val_loss: 0.6601 - val_accuracy: 0.6141 - 368ms/epoch - 3ms/step\n",
      "Epoch 41/10000\n",
      "113/113 - 0s - loss: 0.6607 - accuracy: 0.6122 - val_loss: 0.6601 - val_accuracy: 0.6159 - 352ms/epoch - 3ms/step\n",
      "Epoch 42/10000\n",
      "113/113 - 0s - loss: 0.6606 - accuracy: 0.6125 - val_loss: 0.6599 - val_accuracy: 0.6139 - 367ms/epoch - 3ms/step\n",
      "Epoch 43/10000\n",
      "113/113 - 0s - loss: 0.6606 - accuracy: 0.6122 - val_loss: 0.6592 - val_accuracy: 0.6156 - 346ms/epoch - 3ms/step\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Iterazione 24/128 (03-11-2023_10-36-22)\n",
      "hidden_layer_size: 20\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 3\n",
      "activation_hidden: sigmoid\n",
      "activation_output: sigmoid\n",
      "batch_size: 1000\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 20\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 61.6%\n",
      "La recall di questo modello sul validation set è: 1.96%\n",
      "La f1 di questo modello sul validation set è: 3.8%\n",
      "La balanced accuracy di questo modello sul validation set è: 50.6%\n",
      "La precision di questo modello sul validation set è: 62.09%\n",
      "La AUC di questo modello sul validation set è: 56.61%\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.62    0.99      0.76  7,670.00\n",
      "1                  0.62    0.02      0.04  4,841.00\n",
      "accuracy           0.62    0.62      0.62      0.62\n",
      "macro avg          0.62    0.51      0.40 12,511.00\n",
      "weighted avg       0.62    0.62      0.48 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         7612           58\n",
      "Actual 1         4746           95\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 65.94% con il modello:\n",
      "1 - AC65.94%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 62.97% con il modello:\n",
      "1 - BA62.97%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 64.08% con il modello:\n",
      "13 - PR64.08%, L20, mV-10, MV10, P2, AHsigmoid, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-26-48).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "1126/1126 - 4s - loss: 259405952.0000 - accuracy: 0.5195 - val_loss: 10970513.0000 - val_accuracy: 0.5773 - 4s/epoch - 4ms/step\n",
      "Epoch 2/10000\n",
      "1126/1126 - 2s - loss: 9438418.0000 - accuracy: 0.5635 - val_loss: 6814420.0000 - val_accuracy: 0.5610 - 2s/epoch - 2ms/step\n",
      "Epoch 3/10000\n",
      "1126/1126 - 2s - loss: 6255961.0000 - accuracy: 0.5548 - val_loss: 4522745.0000 - val_accuracy: 0.5408 - 2s/epoch - 2ms/step\n",
      "Epoch 4/10000\n",
      "1126/1126 - 2s - loss: 4806344.0000 - accuracy: 0.5548 - val_loss: 3635527.2500 - val_accuracy: 0.5916 - 2s/epoch - 2ms/step\n",
      "Epoch 5/10000\n",
      "1126/1126 - 2s - loss: 3793591.5000 - accuracy: 0.5601 - val_loss: 3222178.0000 - val_accuracy: 0.6093 - 2s/epoch - 2ms/step\n",
      "Epoch 6/10000\n",
      "1126/1126 - 2s - loss: 3406631.5000 - accuracy: 0.5671 - val_loss: 4031250.0000 - val_accuracy: 0.4813 - 2s/epoch - 2ms/step\n",
      "Epoch 7/10000\n",
      "1126/1126 - 2s - loss: 3394466.2500 - accuracy: 0.5679 - val_loss: 5399004.5000 - val_accuracy: 0.6123 - 2s/epoch - 2ms/step\n",
      "Epoch 8/10000\n",
      "1126/1126 - 3s - loss: 3248773.7500 - accuracy: 0.5682 - val_loss: 2404393.0000 - val_accuracy: 0.5370 - 3s/epoch - 2ms/step\n",
      "Epoch 9/10000\n",
      "1126/1126 - 3s - loss: 2614837.0000 - accuracy: 0.5688 - val_loss: 1519118.8750 - val_accuracy: 0.6227 - 3s/epoch - 3ms/step\n",
      "Epoch 10/10000\n",
      "1126/1126 - 4s - loss: 2884527.2500 - accuracy: 0.5700 - val_loss: 1485276.2500 - val_accuracy: 0.5339 - 4s/epoch - 3ms/step\n",
      "Epoch 11/10000\n",
      "1126/1126 - 3s - loss: 2637112.5000 - accuracy: 0.5691 - val_loss: 2377146.7500 - val_accuracy: 0.5126 - 3s/epoch - 3ms/step\n",
      "Epoch 12/10000\n",
      "1126/1126 - 2s - loss: 2867860.2500 - accuracy: 0.5723 - val_loss: 4138282.0000 - val_accuracy: 0.6140 - 2s/epoch - 2ms/step\n",
      "Epoch 13/10000\n",
      "1126/1126 - 2s - loss: 2801945.0000 - accuracy: 0.5739 - val_loss: 897157.6875 - val_accuracy: 0.6370 - 2s/epoch - 2ms/step\n",
      "Epoch 14/10000\n",
      "1126/1126 - 3s - loss: 2425641.5000 - accuracy: 0.5741 - val_loss: 3205260.2500 - val_accuracy: 0.4695 - 3s/epoch - 2ms/step\n",
      "Epoch 15/10000\n",
      "1126/1126 - 3s - loss: 2462686.0000 - accuracy: 0.5740 - val_loss: 1593124.1250 - val_accuracy: 0.6187 - 3s/epoch - 2ms/step\n",
      "Epoch 16/10000\n",
      "1126/1126 - 3s - loss: 2397540.7500 - accuracy: 0.5718 - val_loss: 2215585.2500 - val_accuracy: 0.4953 - 3s/epoch - 2ms/step\n",
      "Epoch 17/10000\n",
      "1126/1126 - 3s - loss: 2420576.5000 - accuracy: 0.5742 - val_loss: 1318668.8750 - val_accuracy: 0.5309 - 3s/epoch - 3ms/step\n",
      "Epoch 18/10000\n",
      "1126/1126 - 2s - loss: 2506088.2500 - accuracy: 0.5719 - val_loss: 2428906.0000 - val_accuracy: 0.4994 - 2s/epoch - 2ms/step\n",
      "Epoch 19/10000\n",
      "1126/1126 - 2s - loss: 2521933.7500 - accuracy: 0.5719 - val_loss: 2976302.2500 - val_accuracy: 0.4925 - 2s/epoch - 2ms/step\n",
      "Epoch 20/10000\n",
      "1126/1126 - 2s - loss: 2515988.0000 - accuracy: 0.5715 - val_loss: 679757.8125 - val_accuracy: 0.6290 - 2s/epoch - 2ms/step\n",
      "Epoch 21/10000\n",
      "1126/1126 - 2s - loss: 2093027.8750 - accuracy: 0.5731 - val_loss: 1250164.1250 - val_accuracy: 0.6188 - 2s/epoch - 2ms/step\n",
      "Epoch 22/10000\n",
      "1126/1126 - 2s - loss: 2501881.0000 - accuracy: 0.5713 - val_loss: 772596.6875 - val_accuracy: 0.6359 - 2s/epoch - 2ms/step\n",
      "Epoch 23/10000\n",
      "1126/1126 - 3s - loss: 2335754.5000 - accuracy: 0.5721 - val_loss: 1051529.1250 - val_accuracy: 0.6224 - 3s/epoch - 2ms/step\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Iterazione 25/128 (03-11-2023_10-37-26)\n",
      "hidden_layer_size: 20\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 4\n",
      "activation_hidden: relu\n",
      "activation_output: sigmoid\n",
      "batch_size: 100\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 10\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 63.7%\n",
      "La recall di questo modello sul validation set è: 27.14%\n",
      "La f1 di questo modello sul validation set è: 36.66%\n",
      "La balanced accuracy di questo modello sul validation set è: 56.96%\n",
      "La precision di questo modello sul validation set è: 56.44%\n",
      "La AUC di questo modello sul validation set è: 56.98%\n",
      "391/391 [==============================] - 1s 3ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.65    0.87      0.75  7,670.00\n",
      "1                  0.56    0.27      0.37  4,841.00\n",
      "accuracy           0.64    0.64      0.64      0.64\n",
      "macro avg          0.61    0.57      0.56 12,511.00\n",
      "weighted avg       0.62    0.64      0.60 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         6656         1014\n",
      "Actual 1         3527         1314\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 65.94% con il modello:\n",
      "1 - AC65.94%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 62.97% con il modello:\n",
      "1 - BA62.97%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 64.08% con il modello:\n",
      "13 - PR64.08%, L20, mV-10, MV10, P2, AHsigmoid, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-26-48).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "1126/1126 - 5s - loss: 259405952.0000 - accuracy: 0.5195 - val_loss: 10970513.0000 - val_accuracy: 0.5773 - 5s/epoch - 4ms/step\n",
      "Epoch 2/10000\n",
      "1126/1126 - 3s - loss: 9438418.0000 - accuracy: 0.5635 - val_loss: 6814420.0000 - val_accuracy: 0.5610 - 3s/epoch - 2ms/step\n",
      "Epoch 3/10000\n",
      "1126/1126 - 3s - loss: 6255961.0000 - accuracy: 0.5548 - val_loss: 4522745.0000 - val_accuracy: 0.5408 - 3s/epoch - 3ms/step\n",
      "Epoch 4/10000\n",
      "1126/1126 - 3s - loss: 4806344.0000 - accuracy: 0.5548 - val_loss: 3635527.2500 - val_accuracy: 0.5916 - 3s/epoch - 2ms/step\n",
      "Epoch 5/10000\n",
      "1126/1126 - 3s - loss: 3793591.5000 - accuracy: 0.5601 - val_loss: 3222178.0000 - val_accuracy: 0.6093 - 3s/epoch - 2ms/step\n",
      "Epoch 6/10000\n",
      "1126/1126 - 3s - loss: 3406631.5000 - accuracy: 0.5671 - val_loss: 4031250.0000 - val_accuracy: 0.4813 - 3s/epoch - 2ms/step\n",
      "Epoch 7/10000\n",
      "1126/1126 - 3s - loss: 3394466.2500 - accuracy: 0.5679 - val_loss: 5399004.5000 - val_accuracy: 0.6123 - 3s/epoch - 3ms/step\n",
      "Epoch 8/10000\n",
      "1126/1126 - 3s - loss: 3248773.7500 - accuracy: 0.5682 - val_loss: 2404393.0000 - val_accuracy: 0.5370 - 3s/epoch - 3ms/step\n",
      "Epoch 9/10000\n",
      "1126/1126 - 4s - loss: 2614837.0000 - accuracy: 0.5688 - val_loss: 1519118.8750 - val_accuracy: 0.6227 - 4s/epoch - 3ms/step\n",
      "Epoch 10/10000\n",
      "1126/1126 - 3s - loss: 2884527.2500 - accuracy: 0.5700 - val_loss: 1485276.2500 - val_accuracy: 0.5339 - 3s/epoch - 3ms/step\n",
      "Epoch 11/10000\n",
      "1126/1126 - 4s - loss: 2637112.5000 - accuracy: 0.5691 - val_loss: 2377146.7500 - val_accuracy: 0.5126 - 4s/epoch - 3ms/step\n",
      "Epoch 12/10000\n",
      "1126/1126 - 3s - loss: 2867860.2500 - accuracy: 0.5723 - val_loss: 4138282.0000 - val_accuracy: 0.6140 - 3s/epoch - 3ms/step\n",
      "Epoch 13/10000\n",
      "1126/1126 - 3s - loss: 2801945.0000 - accuracy: 0.5739 - val_loss: 897157.6875 - val_accuracy: 0.6370 - 3s/epoch - 3ms/step\n",
      "Epoch 14/10000\n",
      "1126/1126 - 3s - loss: 2425641.5000 - accuracy: 0.5741 - val_loss: 3205260.2500 - val_accuracy: 0.4695 - 3s/epoch - 2ms/step\n",
      "Epoch 15/10000\n",
      "1126/1126 - 3s - loss: 2462686.0000 - accuracy: 0.5740 - val_loss: 1593124.1250 - val_accuracy: 0.6187 - 3s/epoch - 3ms/step\n",
      "Epoch 16/10000\n",
      "1126/1126 - 3s - loss: 2397540.7500 - accuracy: 0.5718 - val_loss: 2215585.2500 - val_accuracy: 0.4953 - 3s/epoch - 3ms/step\n",
      "Epoch 17/10000\n",
      "1126/1126 - 4s - loss: 2420576.5000 - accuracy: 0.5742 - val_loss: 1318668.8750 - val_accuracy: 0.5309 - 4s/epoch - 3ms/step\n",
      "Epoch 18/10000\n",
      "1126/1126 - 3s - loss: 2506088.2500 - accuracy: 0.5719 - val_loss: 2428906.0000 - val_accuracy: 0.4994 - 3s/epoch - 3ms/step\n",
      "Epoch 19/10000\n",
      "1126/1126 - 4s - loss: 2521933.7500 - accuracy: 0.5719 - val_loss: 2976302.2500 - val_accuracy: 0.4925 - 4s/epoch - 3ms/step\n",
      "Epoch 20/10000\n",
      "1126/1126 - 3s - loss: 2515988.0000 - accuracy: 0.5715 - val_loss: 679757.8125 - val_accuracy: 0.6290 - 3s/epoch - 3ms/step\n",
      "Epoch 21/10000\n",
      "1126/1126 - 3s - loss: 2093027.8750 - accuracy: 0.5731 - val_loss: 1250164.1250 - val_accuracy: 0.6188 - 3s/epoch - 3ms/step\n",
      "Epoch 22/10000\n",
      "1126/1126 - 3s - loss: 2501881.0000 - accuracy: 0.5713 - val_loss: 772596.6875 - val_accuracy: 0.6359 - 3s/epoch - 3ms/step\n",
      "Epoch 23/10000\n",
      "1126/1126 - 3s - loss: 2335754.5000 - accuracy: 0.5721 - val_loss: 1051529.1250 - val_accuracy: 0.6224 - 3s/epoch - 3ms/step\n",
      "Epoch 24/10000\n",
      "1126/1126 - 3s - loss: 2403448.0000 - accuracy: 0.5732 - val_loss: 1969627.0000 - val_accuracy: 0.6110 - 3s/epoch - 2ms/step\n",
      "Epoch 25/10000\n",
      "1126/1126 - 4s - loss: 2426730.2500 - accuracy: 0.5724 - val_loss: 4806230.5000 - val_accuracy: 0.4563 - 4s/epoch - 3ms/step\n",
      "Epoch 26/10000\n",
      "1126/1126 - 3s - loss: 2294533.0000 - accuracy: 0.5717 - val_loss: 948209.2500 - val_accuracy: 0.5697 - 3s/epoch - 3ms/step\n",
      "Epoch 27/10000\n",
      "1126/1126 - 3s - loss: 2250095.2500 - accuracy: 0.5732 - val_loss: 926076.5000 - val_accuracy: 0.6325 - 3s/epoch - 2ms/step\n",
      "Epoch 28/10000\n",
      "1126/1126 - 2s - loss: 2209712.5000 - accuracy: 0.5744 - val_loss: 3563681.5000 - val_accuracy: 0.4490 - 2s/epoch - 2ms/step\n",
      "Epoch 29/10000\n",
      "1126/1126 - 3s - loss: 2590218.2500 - accuracy: 0.5719 - val_loss: 3429638.2500 - val_accuracy: 0.4668 - 3s/epoch - 2ms/step\n",
      "Epoch 30/10000\n",
      "1126/1126 - 3s - loss: 2137114.0000 - accuracy: 0.5754 - val_loss: 539347.8125 - val_accuracy: 0.6420 - 3s/epoch - 2ms/step\n",
      "Epoch 31/10000\n",
      "1126/1126 - 3s - loss: 2242279.7500 - accuracy: 0.5723 - val_loss: 2433753.7500 - val_accuracy: 0.4647 - 3s/epoch - 3ms/step\n",
      "Epoch 32/10000\n",
      "1126/1126 - 3s - loss: 2255238.7500 - accuracy: 0.5737 - val_loss: 3171425.2500 - val_accuracy: 0.6147 - 3s/epoch - 2ms/step\n",
      "Epoch 33/10000\n",
      "1126/1126 - 2s - loss: 2133114.0000 - accuracy: 0.5751 - val_loss: 878847.6250 - val_accuracy: 0.5947 - 2s/epoch - 2ms/step\n",
      "Epoch 34/10000\n",
      "1126/1126 - 3s - loss: 2279965.7500 - accuracy: 0.5718 - val_loss: 2100012.0000 - val_accuracy: 0.4834 - 3s/epoch - 3ms/step\n",
      "Epoch 35/10000\n",
      "1126/1126 - 2s - loss: 2174800.5000 - accuracy: 0.5731 - val_loss: 1188935.8750 - val_accuracy: 0.5271 - 2s/epoch - 2ms/step\n",
      "Epoch 36/10000\n",
      "1126/1126 - 2s - loss: 2088438.7500 - accuracy: 0.5743 - val_loss: 993893.9375 - val_accuracy: 0.5679 - 2s/epoch - 2ms/step\n",
      "Epoch 37/10000\n",
      "1126/1126 - 2s - loss: 2129214.2500 - accuracy: 0.5738 - val_loss: 2431636.0000 - val_accuracy: 0.6176 - 2s/epoch - 2ms/step\n",
      "Epoch 38/10000\n",
      "1126/1126 - 3s - loss: 2003451.5000 - accuracy: 0.5745 - val_loss: 583894.4375 - val_accuracy: 0.6401 - 3s/epoch - 2ms/step\n",
      "Epoch 39/10000\n",
      "1126/1126 - 3s - loss: 2102856.5000 - accuracy: 0.5733 - val_loss: 1969063.6250 - val_accuracy: 0.6171 - 3s/epoch - 2ms/step\n",
      "Epoch 40/10000\n",
      "1126/1126 - 3s - loss: 1987879.2500 - accuracy: 0.5745 - val_loss: 2900836.2500 - val_accuracy: 0.6154 - 3s/epoch - 2ms/step\n",
      "Epoch 41/10000\n",
      "1126/1126 - 3s - loss: 2103229.5000 - accuracy: 0.5729 - val_loss: 1753522.2500 - val_accuracy: 0.6113 - 3s/epoch - 2ms/step\n",
      "Epoch 42/10000\n",
      "1126/1126 - 3s - loss: 1909175.2500 - accuracy: 0.5761 - val_loss: 1971434.5000 - val_accuracy: 0.4993 - 3s/epoch - 2ms/step\n",
      "Epoch 43/10000\n",
      "1126/1126 - 3s - loss: 2147658.7500 - accuracy: 0.5743 - val_loss: 1541994.3750 - val_accuracy: 0.6038 - 3s/epoch - 2ms/step\n",
      "Epoch 44/10000\n",
      "1126/1126 - 5s - loss: 2502621.0000 - accuracy: 0.5727 - val_loss: 571049.0000 - val_accuracy: 0.6483 - 5s/epoch - 4ms/step\n",
      "Epoch 45/10000\n",
      "1126/1126 - 3s - loss: 1951568.5000 - accuracy: 0.5768 - val_loss: 2476713.2500 - val_accuracy: 0.6139 - 3s/epoch - 3ms/step\n",
      "Epoch 46/10000\n",
      "1126/1126 - 3s - loss: 2194310.2500 - accuracy: 0.5741 - val_loss: 741026.4375 - val_accuracy: 0.6370 - 3s/epoch - 2ms/step\n",
      "Epoch 47/10000\n",
      "1126/1126 - 3s - loss: 2072342.7500 - accuracy: 0.5740 - val_loss: 2143747.2500 - val_accuracy: 0.4953 - 3s/epoch - 2ms/step\n",
      "Epoch 48/10000\n",
      "1126/1126 - 3s - loss: 2115884.5000 - accuracy: 0.5759 - val_loss: 735346.2500 - val_accuracy: 0.6203 - 3s/epoch - 2ms/step\n",
      "Epoch 49/10000\n",
      "1126/1126 - 3s - loss: 1847408.3750 - accuracy: 0.5763 - val_loss: 2701634.2500 - val_accuracy: 0.6143 - 3s/epoch - 2ms/step\n",
      "Epoch 50/10000\n",
      "1126/1126 - 2s - loss: 2181566.5000 - accuracy: 0.5724 - val_loss: 2261565.0000 - val_accuracy: 0.5028 - 2s/epoch - 2ms/step\n",
      "Epoch 51/10000\n",
      "1126/1126 - 3s - loss: 1765882.5000 - accuracy: 0.5794 - val_loss: 801013.9375 - val_accuracy: 0.6326 - 3s/epoch - 2ms/step\n",
      "Epoch 52/10000\n",
      "1126/1126 - 3s - loss: 2011242.8750 - accuracy: 0.5742 - val_loss: 5712091.5000 - val_accuracy: 0.6135 - 3s/epoch - 2ms/step\n",
      "Epoch 53/10000\n",
      "1126/1126 - 3s - loss: 2178420.5000 - accuracy: 0.5768 - val_loss: 1642421.8750 - val_accuracy: 0.6211 - 3s/epoch - 2ms/step\n",
      "Epoch 54/10000\n",
      "1126/1126 - 3s - loss: 2025295.3750 - accuracy: 0.5767 - val_loss: 2871184.7500 - val_accuracy: 0.6159 - 3s/epoch - 3ms/step\n",
      "Epoch 55/10000\n",
      "1126/1126 - 3s - loss: 2115932.5000 - accuracy: 0.5732 - val_loss: 974978.5625 - val_accuracy: 0.5708 - 3s/epoch - 2ms/step\n",
      "Epoch 56/10000\n",
      "1126/1126 - 3s - loss: 2086927.6250 - accuracy: 0.5777 - val_loss: 562574.0000 - val_accuracy: 0.6390 - 3s/epoch - 2ms/step\n",
      "Epoch 57/10000\n",
      "1126/1126 - 2s - loss: 1943953.7500 - accuracy: 0.5771 - val_loss: 2184002.0000 - val_accuracy: 0.6151 - 2s/epoch - 2ms/step\n",
      "Epoch 58/10000\n",
      "1126/1126 - 2s - loss: 1900153.0000 - accuracy: 0.5796 - val_loss: 2931045.2500 - val_accuracy: 0.4563 - 2s/epoch - 2ms/step\n",
      "Epoch 59/10000\n",
      "1126/1126 - 3s - loss: 2164142.7500 - accuracy: 0.5735 - val_loss: 507931.0938 - val_accuracy: 0.6429 - 3s/epoch - 2ms/step\n",
      "Epoch 60/10000\n",
      "1126/1126 - 3s - loss: 1885034.6250 - accuracy: 0.5788 - val_loss: 7869524.0000 - val_accuracy: 0.6116 - 3s/epoch - 2ms/step\n",
      "Epoch 61/10000\n",
      "1126/1126 - 3s - loss: 2046388.5000 - accuracy: 0.5775 - val_loss: 973585.0625 - val_accuracy: 0.6286 - 3s/epoch - 2ms/step\n",
      "Epoch 62/10000\n",
      "1126/1126 - 2s - loss: 1960886.8750 - accuracy: 0.5765 - val_loss: 583807.1250 - val_accuracy: 0.5809 - 2s/epoch - 2ms/step\n",
      "Epoch 63/10000\n",
      "1126/1126 - 2s - loss: 1970579.7500 - accuracy: 0.5762 - val_loss: 634672.8125 - val_accuracy: 0.6202 - 2s/epoch - 2ms/step\n",
      "Epoch 64/10000\n",
      "1126/1126 - 2s - loss: 1902717.3750 - accuracy: 0.5781 - val_loss: 2266630.2500 - val_accuracy: 0.6148 - 2s/epoch - 2ms/step\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Iterazione 26/128 (03-11-2023_10-40-32)\n",
      "hidden_layer_size: 20\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 4\n",
      "activation_hidden: relu\n",
      "activation_output: sigmoid\n",
      "batch_size: 100\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 20\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 64.83%\n",
      "La recall di questo modello sul validation set è: 29.81%\n",
      "La f1 di questo modello sul validation set è: 39.61%\n",
      "La balanced accuracy di questo modello sul validation set è: 58.37%\n",
      "La precision di questo modello sul validation set è: 59.02%\n",
      "La AUC di questo modello sul validation set è: 58.37%\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.66    0.87      0.75  7,670.00\n",
      "1                  0.59    0.30      0.40  4,841.00\n",
      "accuracy           0.65    0.65      0.65      0.65\n",
      "macro avg          0.63    0.58      0.57 12,511.00\n",
      "weighted avg       0.63    0.65      0.61 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         6668         1002\n",
      "Actual 1         3398         1443\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 65.94% con il modello:\n",
      "1 - AC65.94%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 62.97% con il modello:\n",
      "1 - BA62.97%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 64.08% con il modello:\n",
      "13 - PR64.08%, L20, mV-10, MV10, P2, AHsigmoid, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-26-48).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "113/113 - 2s - loss: 2220395264.0000 - accuracy: 0.3888 - val_loss: 479143584.0000 - val_accuracy: 0.3881 - 2s/epoch - 21ms/step\n",
      "Epoch 2/10000\n",
      "113/113 - 0s - loss: 173745408.0000 - accuracy: 0.4410 - val_loss: 56666552.0000 - val_accuracy: 0.4990 - 490ms/epoch - 4ms/step\n",
      "Epoch 3/10000\n",
      "113/113 - 0s - loss: 44131452.0000 - accuracy: 0.5261 - val_loss: 33316920.0000 - val_accuracy: 0.5474 - 486ms/epoch - 4ms/step\n",
      "Epoch 4/10000\n",
      "113/113 - 0s - loss: 26736940.0000 - accuracy: 0.5457 - val_loss: 20532776.0000 - val_accuracy: 0.5546 - 426ms/epoch - 4ms/step\n",
      "Epoch 5/10000\n",
      "113/113 - 0s - loss: 17887642.0000 - accuracy: 0.5459 - val_loss: 15226329.0000 - val_accuracy: 0.5437 - 367ms/epoch - 3ms/step\n",
      "Epoch 6/10000\n",
      "113/113 - 0s - loss: 14361038.0000 - accuracy: 0.5364 - val_loss: 13129471.0000 - val_accuracy: 0.5386 - 372ms/epoch - 3ms/step\n",
      "Epoch 7/10000\n",
      "113/113 - 0s - loss: 13087232.0000 - accuracy: 0.5338 - val_loss: 12358577.0000 - val_accuracy: 0.5404 - 354ms/epoch - 3ms/step\n",
      "Epoch 8/10000\n",
      "113/113 - 0s - loss: 12349699.0000 - accuracy: 0.5362 - val_loss: 12231710.0000 - val_accuracy: 0.5331 - 367ms/epoch - 3ms/step\n",
      "Epoch 9/10000\n",
      "113/113 - 0s - loss: 11479173.0000 - accuracy: 0.5448 - val_loss: 10660823.0000 - val_accuracy: 0.5521 - 348ms/epoch - 3ms/step\n",
      "Epoch 10/10000\n",
      "113/113 - 0s - loss: 10483481.0000 - accuracy: 0.5507 - val_loss: 9936654.0000 - val_accuracy: 0.5594 - 380ms/epoch - 3ms/step\n",
      "Epoch 11/10000\n",
      "113/113 - 0s - loss: 9849647.0000 - accuracy: 0.5546 - val_loss: 9351851.0000 - val_accuracy: 0.5627 - 387ms/epoch - 3ms/step\n",
      "Epoch 12/10000\n",
      "113/113 - 0s - loss: 9325724.0000 - accuracy: 0.5584 - val_loss: 8970412.0000 - val_accuracy: 0.5635 - 357ms/epoch - 3ms/step\n",
      "Epoch 13/10000\n",
      "113/113 - 0s - loss: 8768977.0000 - accuracy: 0.5640 - val_loss: 8417253.0000 - val_accuracy: 0.5751 - 359ms/epoch - 3ms/step\n",
      "Epoch 14/10000\n",
      "113/113 - 0s - loss: 8258019.0000 - accuracy: 0.5685 - val_loss: 7750029.5000 - val_accuracy: 0.5709 - 364ms/epoch - 3ms/step\n",
      "Epoch 15/10000\n",
      "113/113 - 0s - loss: 7615784.0000 - accuracy: 0.5622 - val_loss: 7183894.0000 - val_accuracy: 0.5623 - 370ms/epoch - 3ms/step\n",
      "Epoch 16/10000\n",
      "113/113 - 0s - loss: 7036678.5000 - accuracy: 0.5532 - val_loss: 6572580.0000 - val_accuracy: 0.5557 - 399ms/epoch - 4ms/step\n",
      "Epoch 17/10000\n",
      "113/113 - 0s - loss: 6578206.5000 - accuracy: 0.5509 - val_loss: 6166657.5000 - val_accuracy: 0.5487 - 384ms/epoch - 3ms/step\n",
      "Epoch 18/10000\n",
      "113/113 - 0s - loss: 6060502.0000 - accuracy: 0.5468 - val_loss: 5919628.5000 - val_accuracy: 0.5616 - 390ms/epoch - 3ms/step\n",
      "Epoch 19/10000\n",
      "113/113 - 0s - loss: 6033649.5000 - accuracy: 0.5474 - val_loss: 5273978.5000 - val_accuracy: 0.5454 - 374ms/epoch - 3ms/step\n",
      "Epoch 20/10000\n",
      "113/113 - 0s - loss: 5281642.0000 - accuracy: 0.5458 - val_loss: 4961429.5000 - val_accuracy: 0.5546 - 404ms/epoch - 4ms/step\n",
      "Epoch 21/10000\n",
      "113/113 - 0s - loss: 5272679.0000 - accuracy: 0.5502 - val_loss: 5702268.0000 - val_accuracy: 0.5326 - 370ms/epoch - 3ms/step\n",
      "Epoch 22/10000\n",
      "113/113 - 0s - loss: 4918851.5000 - accuracy: 0.5467 - val_loss: 4858595.5000 - val_accuracy: 0.5356 - 384ms/epoch - 3ms/step\n",
      "Epoch 23/10000\n",
      "113/113 - 0s - loss: 4789044.5000 - accuracy: 0.5461 - val_loss: 4906835.0000 - val_accuracy: 0.5362 - 397ms/epoch - 4ms/step\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Iterazione 27/128 (03-11-2023_10-40-45)\n",
      "hidden_layer_size: 20\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 4\n",
      "activation_hidden: relu\n",
      "activation_output: sigmoid\n",
      "batch_size: 1000\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 10\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 57.51%\n",
      "La recall di questo modello sul validation set è: 44.95%\n",
      "La f1 di questo modello sul validation set è: 45.01%\n",
      "La balanced accuracy di questo modello sul validation set è: 55.19%\n",
      "La precision di questo modello sul validation set è: 45.08%\n",
      "La AUC di questo modello sul validation set è: 55.19%\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.65    0.65      0.65  7,670.00\n",
      "1                  0.45    0.45      0.45  4,841.00\n",
      "accuracy           0.58    0.58      0.58      0.58\n",
      "macro avg          0.55    0.55      0.55 12,511.00\n",
      "weighted avg       0.57    0.58      0.57 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         5019         2651\n",
      "Actual 1         2665         2176\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 65.94% con il modello:\n",
      "1 - AC65.94%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 62.97% con il modello:\n",
      "1 - BA62.97%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 64.08% con il modello:\n",
      "13 - PR64.08%, L20, mV-10, MV10, P2, AHsigmoid, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-26-48).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "113/113 - 2s - loss: 2220395264.0000 - accuracy: 0.3888 - val_loss: 479143584.0000 - val_accuracy: 0.3881 - 2s/epoch - 21ms/step\n",
      "Epoch 2/10000\n",
      "113/113 - 0s - loss: 173745408.0000 - accuracy: 0.4410 - val_loss: 56666552.0000 - val_accuracy: 0.4990 - 487ms/epoch - 4ms/step\n",
      "Epoch 3/10000\n",
      "113/113 - 0s - loss: 44131452.0000 - accuracy: 0.5261 - val_loss: 33316920.0000 - val_accuracy: 0.5474 - 459ms/epoch - 4ms/step\n",
      "Epoch 4/10000\n",
      "113/113 - 0s - loss: 26736940.0000 - accuracy: 0.5457 - val_loss: 20532776.0000 - val_accuracy: 0.5546 - 485ms/epoch - 4ms/step\n",
      "Epoch 5/10000\n",
      "113/113 - 0s - loss: 17887642.0000 - accuracy: 0.5459 - val_loss: 15226329.0000 - val_accuracy: 0.5437 - 359ms/epoch - 3ms/step\n",
      "Epoch 6/10000\n",
      "113/113 - 0s - loss: 14361038.0000 - accuracy: 0.5364 - val_loss: 13129471.0000 - val_accuracy: 0.5386 - 349ms/epoch - 3ms/step\n",
      "Epoch 7/10000\n",
      "113/113 - 0s - loss: 13087232.0000 - accuracy: 0.5338 - val_loss: 12358577.0000 - val_accuracy: 0.5404 - 361ms/epoch - 3ms/step\n",
      "Epoch 8/10000\n",
      "113/113 - 0s - loss: 12349699.0000 - accuracy: 0.5362 - val_loss: 12231710.0000 - val_accuracy: 0.5331 - 358ms/epoch - 3ms/step\n",
      "Epoch 9/10000\n",
      "113/113 - 0s - loss: 11479173.0000 - accuracy: 0.5448 - val_loss: 10660823.0000 - val_accuracy: 0.5521 - 364ms/epoch - 3ms/step\n",
      "Epoch 10/10000\n",
      "113/113 - 0s - loss: 10483481.0000 - accuracy: 0.5507 - val_loss: 9936654.0000 - val_accuracy: 0.5594 - 352ms/epoch - 3ms/step\n",
      "Epoch 11/10000\n",
      "113/113 - 0s - loss: 9849647.0000 - accuracy: 0.5546 - val_loss: 9351851.0000 - val_accuracy: 0.5627 - 359ms/epoch - 3ms/step\n",
      "Epoch 12/10000\n",
      "113/113 - 0s - loss: 9325724.0000 - accuracy: 0.5584 - val_loss: 8970412.0000 - val_accuracy: 0.5635 - 358ms/epoch - 3ms/step\n",
      "Epoch 13/10000\n",
      "113/113 - 0s - loss: 8768977.0000 - accuracy: 0.5640 - val_loss: 8417253.0000 - val_accuracy: 0.5751 - 365ms/epoch - 3ms/step\n",
      "Epoch 14/10000\n",
      "113/113 - 0s - loss: 8258019.0000 - accuracy: 0.5685 - val_loss: 7750029.5000 - val_accuracy: 0.5709 - 378ms/epoch - 3ms/step\n",
      "Epoch 15/10000\n",
      "113/113 - 0s - loss: 7615784.0000 - accuracy: 0.5622 - val_loss: 7183894.0000 - val_accuracy: 0.5623 - 375ms/epoch - 3ms/step\n",
      "Epoch 16/10000\n",
      "113/113 - 0s - loss: 7036678.5000 - accuracy: 0.5532 - val_loss: 6572580.0000 - val_accuracy: 0.5557 - 410ms/epoch - 4ms/step\n",
      "Epoch 17/10000\n",
      "113/113 - 0s - loss: 6578206.5000 - accuracy: 0.5509 - val_loss: 6166657.5000 - val_accuracy: 0.5487 - 363ms/epoch - 3ms/step\n",
      "Epoch 18/10000\n",
      "113/113 - 0s - loss: 6060502.0000 - accuracy: 0.5468 - val_loss: 5919628.5000 - val_accuracy: 0.5616 - 368ms/epoch - 3ms/step\n",
      "Epoch 19/10000\n",
      "113/113 - 0s - loss: 6033649.5000 - accuracy: 0.5474 - val_loss: 5273978.5000 - val_accuracy: 0.5454 - 345ms/epoch - 3ms/step\n",
      "Epoch 20/10000\n",
      "113/113 - 0s - loss: 5281642.0000 - accuracy: 0.5458 - val_loss: 4961429.5000 - val_accuracy: 0.5546 - 381ms/epoch - 3ms/step\n",
      "Epoch 21/10000\n",
      "113/113 - 0s - loss: 5272679.0000 - accuracy: 0.5502 - val_loss: 5702268.0000 - val_accuracy: 0.5326 - 490ms/epoch - 4ms/step\n",
      "Epoch 22/10000\n",
      "113/113 - 0s - loss: 4918851.5000 - accuracy: 0.5467 - val_loss: 4858595.5000 - val_accuracy: 0.5356 - 363ms/epoch - 3ms/step\n",
      "Epoch 23/10000\n",
      "113/113 - 0s - loss: 4789044.5000 - accuracy: 0.5461 - val_loss: 4906835.0000 - val_accuracy: 0.5362 - 438ms/epoch - 4ms/step\n",
      "Epoch 24/10000\n",
      "113/113 - 0s - loss: 4767635.0000 - accuracy: 0.5502 - val_loss: 4601110.5000 - val_accuracy: 0.5368 - 365ms/epoch - 3ms/step\n",
      "Epoch 25/10000\n",
      "113/113 - 0s - loss: 4340171.0000 - accuracy: 0.5466 - val_loss: 3852279.7500 - val_accuracy: 0.5437 - 371ms/epoch - 3ms/step\n",
      "Epoch 26/10000\n",
      "113/113 - 1s - loss: 4437046.0000 - accuracy: 0.5479 - val_loss: 3996132.0000 - val_accuracy: 0.5650 - 633ms/epoch - 6ms/step\n",
      "Epoch 27/10000\n",
      "113/113 - 1s - loss: 3771036.0000 - accuracy: 0.5452 - val_loss: 3764376.5000 - val_accuracy: 0.5327 - 710ms/epoch - 6ms/step\n",
      "Epoch 28/10000\n",
      "113/113 - 1s - loss: 4170863.0000 - accuracy: 0.5503 - val_loss: 5107432.0000 - val_accuracy: 0.5944 - 796ms/epoch - 7ms/step\n",
      "Epoch 29/10000\n",
      "113/113 - 1s - loss: 3865503.5000 - accuracy: 0.5511 - val_loss: 3215804.0000 - val_accuracy: 0.5534 - 788ms/epoch - 7ms/step\n",
      "Epoch 30/10000\n",
      "113/113 - 1s - loss: 3484728.2500 - accuracy: 0.5481 - val_loss: 5569714.5000 - val_accuracy: 0.6003 - 717ms/epoch - 6ms/step\n",
      "Epoch 31/10000\n",
      "113/113 - 0s - loss: 3761240.5000 - accuracy: 0.5534 - val_loss: 3387968.0000 - val_accuracy: 0.5835 - 381ms/epoch - 3ms/step\n",
      "Epoch 32/10000\n",
      "113/113 - 0s - loss: 3315934.5000 - accuracy: 0.5517 - val_loss: 3727244.5000 - val_accuracy: 0.5922 - 364ms/epoch - 3ms/step\n",
      "Epoch 33/10000\n",
      "113/113 - 0s - loss: 4084655.5000 - accuracy: 0.5573 - val_loss: 2894402.2500 - val_accuracy: 0.5407 - 373ms/epoch - 3ms/step\n",
      "Epoch 34/10000\n",
      "113/113 - 1s - loss: 3335607.5000 - accuracy: 0.5557 - val_loss: 2523756.5000 - val_accuracy: 0.5520 - 814ms/epoch - 7ms/step\n",
      "Epoch 35/10000\n",
      "113/113 - 1s - loss: 3017961.7500 - accuracy: 0.5574 - val_loss: 2478183.5000 - val_accuracy: 0.5761 - 502ms/epoch - 4ms/step\n",
      "Epoch 36/10000\n",
      "113/113 - 0s - loss: 3721895.7500 - accuracy: 0.5585 - val_loss: 2243206.5000 - val_accuracy: 0.5831 - 415ms/epoch - 4ms/step\n",
      "Epoch 37/10000\n",
      "113/113 - 0s - loss: 3834254.7500 - accuracy: 0.5601 - val_loss: 2760918.0000 - val_accuracy: 0.5367 - 455ms/epoch - 4ms/step\n",
      "Epoch 38/10000\n",
      "113/113 - 1s - loss: 3235405.0000 - accuracy: 0.5642 - val_loss: 2701137.2500 - val_accuracy: 0.6050 - 527ms/epoch - 5ms/step\n",
      "Epoch 39/10000\n",
      "113/113 - 0s - loss: 2954646.7500 - accuracy: 0.5665 - val_loss: 2085317.2500 - val_accuracy: 0.5848 - 424ms/epoch - 4ms/step\n",
      "Epoch 40/10000\n",
      "113/113 - 0s - loss: 3605149.7500 - accuracy: 0.5618 - val_loss: 2108767.7500 - val_accuracy: 0.5481 - 421ms/epoch - 4ms/step\n",
      "Epoch 41/10000\n",
      "113/113 - 0s - loss: 2784562.0000 - accuracy: 0.5669 - val_loss: 2143576.7500 - val_accuracy: 0.5421 - 354ms/epoch - 3ms/step\n",
      "Epoch 42/10000\n",
      "113/113 - 0s - loss: 2892901.5000 - accuracy: 0.5675 - val_loss: 4100161.7500 - val_accuracy: 0.4833 - 343ms/epoch - 3ms/step\n",
      "Epoch 43/10000\n",
      "113/113 - 0s - loss: 3347962.5000 - accuracy: 0.5630 - val_loss: 2723998.5000 - val_accuracy: 0.5055 - 363ms/epoch - 3ms/step\n",
      "Epoch 44/10000\n",
      "113/113 - 0s - loss: 2331514.5000 - accuracy: 0.5703 - val_loss: 2478532.2500 - val_accuracy: 0.6047 - 375ms/epoch - 3ms/step\n",
      "Epoch 45/10000\n",
      "113/113 - 0s - loss: 4166635.0000 - accuracy: 0.5572 - val_loss: 5746496.5000 - val_accuracy: 0.6099 - 339ms/epoch - 3ms/step\n",
      "Epoch 46/10000\n",
      "113/113 - 0s - loss: 2929174.7500 - accuracy: 0.5701 - val_loss: 1805703.5000 - val_accuracy: 0.5456 - 347ms/epoch - 3ms/step\n",
      "Epoch 47/10000\n",
      "113/113 - 0s - loss: 2603600.2500 - accuracy: 0.5690 - val_loss: 1572593.3750 - val_accuracy: 0.5721 - 379ms/epoch - 3ms/step\n",
      "Epoch 48/10000\n",
      "113/113 - 0s - loss: 3393045.7500 - accuracy: 0.5646 - val_loss: 4402050.0000 - val_accuracy: 0.6117 - 383ms/epoch - 3ms/step\n",
      "Epoch 49/10000\n",
      "113/113 - 0s - loss: 3014316.0000 - accuracy: 0.5682 - val_loss: 2066797.0000 - val_accuracy: 0.6163 - 396ms/epoch - 4ms/step\n",
      "Epoch 50/10000\n",
      "113/113 - 0s - loss: 2521721.2500 - accuracy: 0.5689 - val_loss: 4299711.0000 - val_accuracy: 0.6104 - 429ms/epoch - 4ms/step\n",
      "Epoch 51/10000\n",
      "113/113 - 0s - loss: 4220560.5000 - accuracy: 0.5655 - val_loss: 1604550.6250 - val_accuracy: 0.6054 - 427ms/epoch - 4ms/step\n",
      "Epoch 52/10000\n",
      "113/113 - 0s - loss: 2462037.7500 - accuracy: 0.5698 - val_loss: 2031642.2500 - val_accuracy: 0.5165 - 407ms/epoch - 4ms/step\n",
      "Epoch 53/10000\n",
      "113/113 - 0s - loss: 3321549.5000 - accuracy: 0.5656 - val_loss: 6138650.0000 - val_accuracy: 0.6128 - 344ms/epoch - 3ms/step\n",
      "Epoch 54/10000\n",
      "113/113 - 0s - loss: 3823256.0000 - accuracy: 0.5703 - val_loss: 2199303.2500 - val_accuracy: 0.6128 - 386ms/epoch - 3ms/step\n",
      "Epoch 55/10000\n",
      "113/113 - 0s - loss: 2763254.0000 - accuracy: 0.5687 - val_loss: 9631680.0000 - val_accuracy: 0.6113 - 367ms/epoch - 3ms/step\n",
      "Epoch 56/10000\n",
      "113/113 - 0s - loss: 3064409.7500 - accuracy: 0.5739 - val_loss: 11015347.0000 - val_accuracy: 0.4538 - 387ms/epoch - 3ms/step\n",
      "Epoch 57/10000\n",
      "113/113 - 1s - loss: 2885277.5000 - accuracy: 0.5667 - val_loss: 5272860.5000 - val_accuracy: 0.6119 - 515ms/epoch - 5ms/step\n",
      "Epoch 58/10000\n",
      "113/113 - 0s - loss: 3552051.7500 - accuracy: 0.5654 - val_loss: 4348610.5000 - val_accuracy: 0.4700 - 378ms/epoch - 3ms/step\n",
      "Epoch 59/10000\n",
      "113/113 - 0s - loss: 2732834.2500 - accuracy: 0.5720 - val_loss: 1313344.7500 - val_accuracy: 0.6119 - 399ms/epoch - 4ms/step\n",
      "Epoch 60/10000\n",
      "113/113 - 0s - loss: 4280181.5000 - accuracy: 0.5603 - val_loss: 1617232.3750 - val_accuracy: 0.6235 - 394ms/epoch - 3ms/step\n",
      "Epoch 61/10000\n",
      "113/113 - 1s - loss: 3393639.7500 - accuracy: 0.5705 - val_loss: 2143101.2500 - val_accuracy: 0.5281 - 737ms/epoch - 7ms/step\n",
      "Epoch 62/10000\n",
      "113/113 - 1s - loss: 3722589.0000 - accuracy: 0.5666 - val_loss: 1625528.1250 - val_accuracy: 0.5765 - 739ms/epoch - 7ms/step\n",
      "Epoch 63/10000\n",
      "113/113 - 1s - loss: 3274113.5000 - accuracy: 0.5658 - val_loss: 4168847.2500 - val_accuracy: 0.4765 - 622ms/epoch - 6ms/step\n",
      "Epoch 64/10000\n",
      "113/113 - 1s - loss: 2967659.2500 - accuracy: 0.5751 - val_loss: 2479818.5000 - val_accuracy: 0.6130 - 621ms/epoch - 5ms/step\n",
      "Epoch 65/10000\n",
      "113/113 - 1s - loss: 2830621.0000 - accuracy: 0.5722 - val_loss: 2269746.5000 - val_accuracy: 0.6140 - 903ms/epoch - 8ms/step\n",
      "Epoch 66/10000\n",
      "113/113 - 0s - loss: 3238661.0000 - accuracy: 0.5717 - val_loss: 1315300.1250 - val_accuracy: 0.6185 - 396ms/epoch - 4ms/step\n",
      "Epoch 67/10000\n",
      "113/113 - 0s - loss: 1988056.6250 - accuracy: 0.5830 - val_loss: 1171722.6250 - val_accuracy: 0.6177 - 353ms/epoch - 3ms/step\n",
      "Epoch 68/10000\n",
      "113/113 - 0s - loss: 3160356.0000 - accuracy: 0.5669 - val_loss: 1284639.7500 - val_accuracy: 0.5990 - 353ms/epoch - 3ms/step\n",
      "Epoch 69/10000\n",
      "113/113 - 0s - loss: 3441167.2500 - accuracy: 0.5687 - val_loss: 3607015.7500 - val_accuracy: 0.6122 - 365ms/epoch - 3ms/step\n",
      "Epoch 70/10000\n",
      "113/113 - 0s - loss: 3090977.5000 - accuracy: 0.5708 - val_loss: 3651511.7500 - val_accuracy: 0.6120 - 452ms/epoch - 4ms/step\n",
      "Epoch 71/10000\n",
      "113/113 - 0s - loss: 2093172.2500 - accuracy: 0.5781 - val_loss: 1909916.8750 - val_accuracy: 0.6134 - 474ms/epoch - 4ms/step\n",
      "Epoch 72/10000\n",
      "113/113 - 1s - loss: 2692277.2500 - accuracy: 0.5697 - val_loss: 5877768.0000 - val_accuracy: 0.4630 - 548ms/epoch - 5ms/step\n",
      "Epoch 73/10000\n",
      "113/113 - 0s - loss: 2485353.0000 - accuracy: 0.5764 - val_loss: 2203735.5000 - val_accuracy: 0.6144 - 386ms/epoch - 3ms/step\n",
      "Epoch 74/10000\n",
      "113/113 - 0s - loss: 2579984.5000 - accuracy: 0.5703 - val_loss: 2832726.2500 - val_accuracy: 0.4854 - 407ms/epoch - 4ms/step\n",
      "Epoch 75/10000\n",
      "113/113 - 0s - loss: 2209849.2500 - accuracy: 0.5729 - val_loss: 1490239.0000 - val_accuracy: 0.6210 - 471ms/epoch - 4ms/step\n",
      "Epoch 76/10000\n",
      "113/113 - 0s - loss: 2881682.5000 - accuracy: 0.5692 - val_loss: 1437964.5000 - val_accuracy: 0.5357 - 440ms/epoch - 4ms/step\n",
      "Epoch 77/10000\n",
      "113/113 - 0s - loss: 2825089.2500 - accuracy: 0.5690 - val_loss: 2722553.7500 - val_accuracy: 0.6113 - 394ms/epoch - 3ms/step\n",
      "Epoch 78/10000\n",
      "113/113 - 0s - loss: 2049392.5000 - accuracy: 0.5745 - val_loss: 1929405.7500 - val_accuracy: 0.6135 - 351ms/epoch - 3ms/step\n",
      "Epoch 79/10000\n",
      "113/113 - 0s - loss: 2733410.0000 - accuracy: 0.5743 - val_loss: 1554587.5000 - val_accuracy: 0.5285 - 360ms/epoch - 3ms/step\n",
      "Epoch 80/10000\n",
      "113/113 - 0s - loss: 2674669.5000 - accuracy: 0.5746 - val_loss: 6341775.5000 - val_accuracy: 0.6111 - 461ms/epoch - 4ms/step\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Iterazione 28/128 (03-11-2023_10-41-26)\n",
      "hidden_layer_size: 20\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 4\n",
      "activation_hidden: relu\n",
      "activation_output: sigmoid\n",
      "batch_size: 1000\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 20\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 62.35%\n",
      "La recall di questo modello sul validation set è: 21.34%\n",
      "La f1 di questo modello sul validation set è: 30.49%\n",
      "La balanced accuracy di questo modello sul validation set è: 54.79%\n",
      "La precision di questo modello sul validation set è: 53.39%\n",
      "La AUC di questo modello sul validation set è: 54.79%\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.64    0.88      0.74  7,670.00\n",
      "1                  0.53    0.21      0.30  4,841.00\n",
      "accuracy           0.62    0.62      0.62      0.62\n",
      "macro avg          0.59    0.55      0.52 12,511.00\n",
      "weighted avg       0.60    0.62      0.57 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         6768          902\n",
      "Actual 1         3808         1033\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 65.94% con il modello:\n",
      "1 - AC65.94%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 62.97% con il modello:\n",
      "1 - BA62.97%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 64.08% con il modello:\n",
      "13 - PR64.08%, L20, mV-10, MV10, P2, AHsigmoid, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-26-48).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "1126/1126 - 6s - loss: 1.0670 - accuracy: 0.5731 - val_loss: 0.6660 - val_accuracy: 0.6132 - 6s/epoch - 5ms/step\n",
      "Epoch 2/10000\n",
      "1126/1126 - 4s - loss: 0.6658 - accuracy: 0.6082 - val_loss: 0.6630 - val_accuracy: 0.6145 - 4s/epoch - 4ms/step\n",
      "Epoch 3/10000\n",
      "1126/1126 - 3s - loss: 0.6658 - accuracy: 0.6065 - val_loss: 0.6642 - val_accuracy: 0.6121 - 3s/epoch - 3ms/step\n",
      "Epoch 4/10000\n",
      "1126/1126 - 3s - loss: 0.6647 - accuracy: 0.6090 - val_loss: 0.6642 - val_accuracy: 0.6131 - 3s/epoch - 3ms/step\n",
      "Epoch 5/10000\n",
      "1126/1126 - 3s - loss: 0.6634 - accuracy: 0.6108 - val_loss: 0.6628 - val_accuracy: 0.6137 - 3s/epoch - 2ms/step\n",
      "Epoch 6/10000\n",
      "1126/1126 - 4s - loss: 0.6618 - accuracy: 0.6114 - val_loss: 0.6576 - val_accuracy: 0.6151 - 4s/epoch - 3ms/step\n",
      "Epoch 7/10000\n",
      "1126/1126 - 3s - loss: 0.6603 - accuracy: 0.6122 - val_loss: 0.6593 - val_accuracy: 0.6148 - 3s/epoch - 3ms/step\n",
      "Epoch 8/10000\n",
      "1126/1126 - 3s - loss: 0.6599 - accuracy: 0.6116 - val_loss: 0.6593 - val_accuracy: 0.6140 - 3s/epoch - 2ms/step\n",
      "Epoch 9/10000\n",
      "1126/1126 - 3s - loss: 0.6591 - accuracy: 0.6118 - val_loss: 0.6564 - val_accuracy: 0.6160 - 3s/epoch - 3ms/step\n",
      "Epoch 10/10000\n",
      "1126/1126 - 5s - loss: 0.6590 - accuracy: 0.6120 - val_loss: 0.6599 - val_accuracy: 0.6150 - 5s/epoch - 5ms/step\n",
      "Epoch 11/10000\n",
      "1126/1126 - 3s - loss: 0.6587 - accuracy: 0.6121 - val_loss: 0.6589 - val_accuracy: 0.6147 - 3s/epoch - 2ms/step\n",
      "Epoch 12/10000\n",
      "1126/1126 - 3s - loss: 0.6585 - accuracy: 0.6121 - val_loss: 0.6564 - val_accuracy: 0.6150 - 3s/epoch - 3ms/step\n",
      "Epoch 13/10000\n",
      "1126/1126 - 2s - loss: 0.6585 - accuracy: 0.6120 - val_loss: 0.6570 - val_accuracy: 0.6140 - 2s/epoch - 2ms/step\n",
      "Epoch 14/10000\n",
      "1126/1126 - 3s - loss: 0.6585 - accuracy: 0.6115 - val_loss: 0.6570 - val_accuracy: 0.6151 - 3s/epoch - 3ms/step\n",
      "Epoch 15/10000\n",
      "1126/1126 - 2s - loss: 0.6585 - accuracy: 0.6117 - val_loss: 0.6566 - val_accuracy: 0.6160 - 2s/epoch - 2ms/step\n",
      "Epoch 16/10000\n",
      "1126/1126 - 2s - loss: 0.6581 - accuracy: 0.6120 - val_loss: 0.6564 - val_accuracy: 0.6159 - 2s/epoch - 2ms/step\n",
      "Epoch 17/10000\n",
      "1126/1126 - 3s - loss: 0.6582 - accuracy: 0.6118 - val_loss: 0.6570 - val_accuracy: 0.6118 - 3s/epoch - 2ms/step\n",
      "Epoch 18/10000\n",
      "1126/1126 - 4s - loss: 0.6584 - accuracy: 0.6122 - val_loss: 0.6572 - val_accuracy: 0.6159 - 4s/epoch - 4ms/step\n",
      "Epoch 19/10000\n",
      "1126/1126 - 3s - loss: 0.6583 - accuracy: 0.6121 - val_loss: 0.6578 - val_accuracy: 0.6157 - 3s/epoch - 2ms/step\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Iterazione 29/128 (03-11-2023_10-42-32)\n",
      "hidden_layer_size: 20\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 4\n",
      "activation_hidden: sigmoid\n",
      "activation_output: sigmoid\n",
      "batch_size: 100\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 10\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 61.6%\n",
      "La recall di questo modello sul validation set è: 2.0%\n",
      "La f1 di questo modello sul validation set è: 3.88%\n",
      "La balanced accuracy di questo modello sul validation set è: 50.61%\n",
      "La precision di questo modello sul validation set è: 61.78%\n",
      "La AUC di questo modello sul validation set è: 58.19%\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.62    0.99      0.76  7,670.00\n",
      "1                  0.62    0.02      0.04  4,841.00\n",
      "accuracy           0.62    0.62      0.62      0.62\n",
      "macro avg          0.62    0.51      0.40 12,511.00\n",
      "weighted avg       0.62    0.62      0.48 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         7610           60\n",
      "Actual 1         4744           97\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 65.94% con il modello:\n",
      "1 - AC65.94%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 62.97% con il modello:\n",
      "1 - BA62.97%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 64.08% con il modello:\n",
      "13 - PR64.08%, L20, mV-10, MV10, P2, AHsigmoid, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-26-48).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "1126/1126 - 6s - loss: 1.0670 - accuracy: 0.5731 - val_loss: 0.6660 - val_accuracy: 0.6132 - 6s/epoch - 5ms/step\n",
      "Epoch 2/10000\n",
      "1126/1126 - 3s - loss: 0.6658 - accuracy: 0.6082 - val_loss: 0.6630 - val_accuracy: 0.6145 - 3s/epoch - 3ms/step\n",
      "Epoch 3/10000\n",
      "1126/1126 - 3s - loss: 0.6658 - accuracy: 0.6065 - val_loss: 0.6642 - val_accuracy: 0.6121 - 3s/epoch - 3ms/step\n",
      "Epoch 4/10000\n",
      "1126/1126 - 2s - loss: 0.6647 - accuracy: 0.6090 - val_loss: 0.6642 - val_accuracy: 0.6131 - 2s/epoch - 2ms/step\n",
      "Epoch 5/10000\n",
      "1126/1126 - 3s - loss: 0.6634 - accuracy: 0.6108 - val_loss: 0.6628 - val_accuracy: 0.6137 - 3s/epoch - 3ms/step\n",
      "Epoch 6/10000\n",
      "1126/1126 - 4s - loss: 0.6618 - accuracy: 0.6114 - val_loss: 0.6576 - val_accuracy: 0.6151 - 4s/epoch - 4ms/step\n",
      "Epoch 7/10000\n",
      "1126/1126 - 3s - loss: 0.6603 - accuracy: 0.6122 - val_loss: 0.6593 - val_accuracy: 0.6148 - 3s/epoch - 3ms/step\n",
      "Epoch 8/10000\n",
      "1126/1126 - 3s - loss: 0.6599 - accuracy: 0.6116 - val_loss: 0.6593 - val_accuracy: 0.6140 - 3s/epoch - 2ms/step\n",
      "Epoch 9/10000\n",
      "1126/1126 - 3s - loss: 0.6591 - accuracy: 0.6118 - val_loss: 0.6564 - val_accuracy: 0.6160 - 3s/epoch - 3ms/step\n",
      "Epoch 10/10000\n",
      "1126/1126 - 2s - loss: 0.6590 - accuracy: 0.6120 - val_loss: 0.6599 - val_accuracy: 0.6150 - 2s/epoch - 2ms/step\n",
      "Epoch 11/10000\n",
      "1126/1126 - 3s - loss: 0.6587 - accuracy: 0.6121 - val_loss: 0.6589 - val_accuracy: 0.6147 - 3s/epoch - 2ms/step\n",
      "Epoch 12/10000\n",
      "1126/1126 - 3s - loss: 0.6585 - accuracy: 0.6121 - val_loss: 0.6564 - val_accuracy: 0.6150 - 3s/epoch - 3ms/step\n",
      "Epoch 13/10000\n",
      "1126/1126 - 4s - loss: 0.6585 - accuracy: 0.6120 - val_loss: 0.6570 - val_accuracy: 0.6140 - 4s/epoch - 3ms/step\n",
      "Epoch 14/10000\n",
      "1126/1126 - 3s - loss: 0.6585 - accuracy: 0.6115 - val_loss: 0.6570 - val_accuracy: 0.6151 - 3s/epoch - 2ms/step\n",
      "Epoch 15/10000\n",
      "1126/1126 - 3s - loss: 0.6585 - accuracy: 0.6117 - val_loss: 0.6566 - val_accuracy: 0.6160 - 3s/epoch - 2ms/step\n",
      "Epoch 16/10000\n",
      "1126/1126 - 3s - loss: 0.6581 - accuracy: 0.6120 - val_loss: 0.6564 - val_accuracy: 0.6159 - 3s/epoch - 2ms/step\n",
      "Epoch 17/10000\n",
      "1126/1126 - 2s - loss: 0.6582 - accuracy: 0.6118 - val_loss: 0.6570 - val_accuracy: 0.6118 - 2s/epoch - 2ms/step\n",
      "Epoch 18/10000\n",
      "1126/1126 - 3s - loss: 0.6584 - accuracy: 0.6122 - val_loss: 0.6572 - val_accuracy: 0.6159 - 3s/epoch - 3ms/step\n",
      "Epoch 19/10000\n",
      "1126/1126 - 3s - loss: 0.6583 - accuracy: 0.6121 - val_loss: 0.6578 - val_accuracy: 0.6157 - 3s/epoch - 3ms/step\n",
      "Epoch 20/10000\n",
      "1126/1126 - 3s - loss: 0.6583 - accuracy: 0.6125 - val_loss: 0.6576 - val_accuracy: 0.6134 - 3s/epoch - 3ms/step\n",
      "Epoch 21/10000\n",
      "1126/1126 - 3s - loss: 0.6585 - accuracy: 0.6118 - val_loss: 0.6571 - val_accuracy: 0.6157 - 3s/epoch - 2ms/step\n",
      "Epoch 22/10000\n",
      "1126/1126 - 3s - loss: 0.6586 - accuracy: 0.6119 - val_loss: 0.6574 - val_accuracy: 0.6161 - 3s/epoch - 2ms/step\n",
      "Epoch 23/10000\n",
      "1126/1126 - 2s - loss: 0.6582 - accuracy: 0.6122 - val_loss: 0.6571 - val_accuracy: 0.6159 - 2s/epoch - 2ms/step\n",
      "Epoch 24/10000\n",
      "1126/1126 - 2s - loss: 0.6580 - accuracy: 0.6129 - val_loss: 0.6583 - val_accuracy: 0.6141 - 2s/epoch - 2ms/step\n",
      "Epoch 25/10000\n",
      "1126/1126 - 3s - loss: 0.6585 - accuracy: 0.6121 - val_loss: 0.6569 - val_accuracy: 0.6163 - 3s/epoch - 2ms/step\n",
      "Epoch 26/10000\n",
      "1126/1126 - 3s - loss: 0.6585 - accuracy: 0.6124 - val_loss: 0.6579 - val_accuracy: 0.6127 - 3s/epoch - 3ms/step\n",
      "Epoch 27/10000\n",
      "1126/1126 - 2s - loss: 0.6587 - accuracy: 0.6116 - val_loss: 0.6573 - val_accuracy: 0.6163 - 2s/epoch - 2ms/step\n",
      "Epoch 28/10000\n",
      "1126/1126 - 3s - loss: 0.6583 - accuracy: 0.6127 - val_loss: 0.6577 - val_accuracy: 0.6163 - 3s/epoch - 2ms/step\n",
      "Epoch 29/10000\n",
      "1126/1126 - 5s - loss: 0.6586 - accuracy: 0.6128 - val_loss: 0.6603 - val_accuracy: 0.6159 - 5s/epoch - 5ms/step\n",
      "Epoch 30/10000\n",
      "1126/1126 - 3s - loss: 0.6590 - accuracy: 0.6127 - val_loss: 0.6588 - val_accuracy: 0.6151 - 3s/epoch - 3ms/step\n",
      "Epoch 31/10000\n",
      "1126/1126 - 3s - loss: 0.6588 - accuracy: 0.6125 - val_loss: 0.6573 - val_accuracy: 0.6159 - 3s/epoch - 3ms/step\n",
      "Epoch 32/10000\n",
      "1126/1126 - 3s - loss: 0.6585 - accuracy: 0.6128 - val_loss: 0.6572 - val_accuracy: 0.6158 - 3s/epoch - 3ms/step\n",
      "Epoch 33/10000\n",
      "1126/1126 - 3s - loss: 0.6584 - accuracy: 0.6126 - val_loss: 0.6575 - val_accuracy: 0.6142 - 3s/epoch - 3ms/step\n",
      "Epoch 34/10000\n",
      "1126/1126 - 3s - loss: 0.6584 - accuracy: 0.6128 - val_loss: 0.6574 - val_accuracy: 0.6145 - 3s/epoch - 3ms/step\n",
      "Epoch 35/10000\n",
      "1126/1126 - 3s - loss: 0.6590 - accuracy: 0.6127 - val_loss: 0.6574 - val_accuracy: 0.6162 - 3s/epoch - 3ms/step\n",
      "Epoch 36/10000\n",
      "1126/1126 - 3s - loss: 0.6592 - accuracy: 0.6131 - val_loss: 0.6581 - val_accuracy: 0.6147 - 3s/epoch - 3ms/step\n",
      "Epoch 37/10000\n",
      "1126/1126 - 2s - loss: 0.6591 - accuracy: 0.6127 - val_loss: 0.6570 - val_accuracy: 0.6162 - 2s/epoch - 2ms/step\n",
      "Epoch 38/10000\n",
      "1126/1126 - 3s - loss: 0.6591 - accuracy: 0.6126 - val_loss: 0.6580 - val_accuracy: 0.6161 - 3s/epoch - 2ms/step\n",
      "Epoch 39/10000\n",
      "1126/1126 - 3s - loss: 0.6592 - accuracy: 0.6129 - val_loss: 0.6577 - val_accuracy: 0.6130 - 3s/epoch - 2ms/step\n",
      "Epoch 40/10000\n",
      "1126/1126 - 3s - loss: 0.6594 - accuracy: 0.6126 - val_loss: 0.6579 - val_accuracy: 0.6163 - 3s/epoch - 3ms/step\n",
      "Epoch 41/10000\n",
      "1126/1126 - 3s - loss: 0.6596 - accuracy: 0.6126 - val_loss: 0.6586 - val_accuracy: 0.6160 - 3s/epoch - 3ms/step\n",
      "Epoch 42/10000\n",
      "1126/1126 - 3s - loss: 0.6601 - accuracy: 0.6128 - val_loss: 0.6595 - val_accuracy: 0.6159 - 3s/epoch - 3ms/step\n",
      "Epoch 43/10000\n",
      "1126/1126 - 3s - loss: 0.6604 - accuracy: 0.6130 - val_loss: 0.6589 - val_accuracy: 0.6151 - 3s/epoch - 3ms/step\n",
      "Epoch 44/10000\n",
      "1126/1126 - 4s - loss: 0.6602 - accuracy: 0.6125 - val_loss: 0.6584 - val_accuracy: 0.6147 - 4s/epoch - 3ms/step\n",
      "Epoch 45/10000\n",
      "1126/1126 - 3s - loss: 0.6596 - accuracy: 0.6130 - val_loss: 0.6581 - val_accuracy: 0.6158 - 3s/epoch - 2ms/step\n",
      "391/391 [==============================] - 2s 4ms/step\n",
      "\n",
      "Iterazione 30/128 (03-11-2023_10-44-50)\n",
      "hidden_layer_size: 20\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 4\n",
      "activation_hidden: sigmoid\n",
      "activation_output: sigmoid\n",
      "batch_size: 100\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 20\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 61.63%\n",
      "La recall di questo modello sul validation set è: 2.13%\n",
      "La f1 di questo modello sul validation set è: 4.12%\n",
      "La balanced accuracy di questo modello sul validation set è: 50.66%\n",
      "La precision di questo modello sul validation set è: 62.42%\n",
      "La AUC di questo modello sul validation set è: 57.53%\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.62    0.99      0.76  7,670.00\n",
      "1                  0.62    0.02      0.04  4,841.00\n",
      "accuracy           0.62    0.62      0.62      0.62\n",
      "macro avg          0.62    0.51      0.40 12,511.00\n",
      "weighted avg       0.62    0.62      0.48 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         7608           62\n",
      "Actual 1         4738          103\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 65.94% con il modello:\n",
      "1 - AC65.94%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 62.97% con il modello:\n",
      "1 - BA62.97%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 64.08% con il modello:\n",
      "13 - PR64.08%, L20, mV-10, MV10, P2, AHsigmoid, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-26-48).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "113/113 - 3s - loss: 4.4408 - accuracy: 0.3887 - val_loss: 1.3934 - val_accuracy: 0.3870 - 3s/epoch - 25ms/step\n",
      "Epoch 2/10000\n",
      "113/113 - 1s - loss: 0.7793 - accuracy: 0.4928 - val_loss: 0.6939 - val_accuracy: 0.5230 - 657ms/epoch - 6ms/step\n",
      "Epoch 3/10000\n",
      "113/113 - 1s - loss: 0.6854 - accuracy: 0.6083 - val_loss: 0.6798 - val_accuracy: 0.6143 - 527ms/epoch - 5ms/step\n",
      "Epoch 4/10000\n",
      "113/113 - 1s - loss: 0.6726 - accuracy: 0.6116 - val_loss: 0.6666 - val_accuracy: 0.6144 - 696ms/epoch - 6ms/step\n",
      "Epoch 5/10000\n",
      "113/113 - 1s - loss: 0.6661 - accuracy: 0.6116 - val_loss: 0.6696 - val_accuracy: 0.6136 - 684ms/epoch - 6ms/step\n",
      "Epoch 6/10000\n",
      "113/113 - 1s - loss: 0.6661 - accuracy: 0.6114 - val_loss: 0.6680 - val_accuracy: 0.6146 - 563ms/epoch - 5ms/step\n",
      "Epoch 7/10000\n",
      "113/113 - 1s - loss: 0.6652 - accuracy: 0.6115 - val_loss: 0.6661 - val_accuracy: 0.6142 - 553ms/epoch - 5ms/step\n",
      "Epoch 8/10000\n",
      "113/113 - 0s - loss: 0.6649 - accuracy: 0.6116 - val_loss: 0.6667 - val_accuracy: 0.6148 - 496ms/epoch - 4ms/step\n",
      "Epoch 9/10000\n",
      "113/113 - 0s - loss: 0.6655 - accuracy: 0.6116 - val_loss: 0.6658 - val_accuracy: 0.6153 - 417ms/epoch - 4ms/step\n",
      "Epoch 10/10000\n",
      "113/113 - 1s - loss: 0.6649 - accuracy: 0.6118 - val_loss: 0.6648 - val_accuracy: 0.6143 - 503ms/epoch - 4ms/step\n",
      "Epoch 11/10000\n",
      "113/113 - 1s - loss: 0.6654 - accuracy: 0.6118 - val_loss: 0.6648 - val_accuracy: 0.6149 - 564ms/epoch - 5ms/step\n",
      "Epoch 12/10000\n",
      "113/113 - 0s - loss: 0.6653 - accuracy: 0.6117 - val_loss: 0.6675 - val_accuracy: 0.6146 - 491ms/epoch - 4ms/step\n",
      "Epoch 13/10000\n",
      "113/113 - 0s - loss: 0.6643 - accuracy: 0.6114 - val_loss: 0.6673 - val_accuracy: 0.6147 - 443ms/epoch - 4ms/step\n",
      "Epoch 14/10000\n",
      "113/113 - 0s - loss: 0.6650 - accuracy: 0.6117 - val_loss: 0.6671 - val_accuracy: 0.6144 - 482ms/epoch - 4ms/step\n",
      "Epoch 15/10000\n",
      "113/113 - 0s - loss: 0.6647 - accuracy: 0.6116 - val_loss: 0.6658 - val_accuracy: 0.6136 - 497ms/epoch - 4ms/step\n",
      "Epoch 16/10000\n",
      "113/113 - 0s - loss: 0.6642 - accuracy: 0.6122 - val_loss: 0.6665 - val_accuracy: 0.6147 - 372ms/epoch - 3ms/step\n",
      "Epoch 17/10000\n",
      "113/113 - 1s - loss: 0.6648 - accuracy: 0.6118 - val_loss: 0.6659 - val_accuracy: 0.6145 - 609ms/epoch - 5ms/step\n",
      "Epoch 18/10000\n",
      "113/113 - 0s - loss: 0.6647 - accuracy: 0.6115 - val_loss: 0.6658 - val_accuracy: 0.6147 - 382ms/epoch - 3ms/step\n",
      "Epoch 19/10000\n",
      "113/113 - 0s - loss: 0.6647 - accuracy: 0.6115 - val_loss: 0.6675 - val_accuracy: 0.6146 - 417ms/epoch - 4ms/step\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Iterazione 31/128 (03-11-2023_10-45-05)\n",
      "hidden_layer_size: 20\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 4\n",
      "activation_hidden: sigmoid\n",
      "activation_output: sigmoid\n",
      "batch_size: 1000\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 10\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 61.53%\n",
      "La recall di questo modello sul validation set è: 1.92%\n",
      "La f1 di questo modello sul validation set è: 3.72%\n",
      "La balanced accuracy di questo modello sul validation set è: 50.54%\n",
      "La precision di questo modello sul validation set è: 58.86%\n",
      "La AUC di questo modello sul validation set è: 56.77%\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.62    0.99      0.76  7,670.00\n",
      "1                  0.59    0.02      0.04  4,841.00\n",
      "accuracy           0.62    0.62      0.62      0.62\n",
      "macro avg          0.60    0.51      0.40 12,511.00\n",
      "weighted avg       0.61    0.62      0.48 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         7605           65\n",
      "Actual 1         4748           93\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 65.94% con il modello:\n",
      "1 - AC65.94%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 62.97% con il modello:\n",
      "1 - BA62.97%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 64.08% con il modello:\n",
      "13 - PR64.08%, L20, mV-10, MV10, P2, AHsigmoid, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-26-48).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "113/113 - 2s - loss: 4.4408 - accuracy: 0.3887 - val_loss: 1.3934 - val_accuracy: 0.3870 - 2s/epoch - 17ms/step\n",
      "Epoch 2/10000\n",
      "113/113 - 0s - loss: 0.7793 - accuracy: 0.4928 - val_loss: 0.6939 - val_accuracy: 0.5230 - 417ms/epoch - 4ms/step\n",
      "Epoch 3/10000\n",
      "113/113 - 0s - loss: 0.6854 - accuracy: 0.6083 - val_loss: 0.6798 - val_accuracy: 0.6143 - 489ms/epoch - 4ms/step\n",
      "Epoch 4/10000\n",
      "113/113 - 0s - loss: 0.6726 - accuracy: 0.6116 - val_loss: 0.6666 - val_accuracy: 0.6144 - 468ms/epoch - 4ms/step\n",
      "Epoch 5/10000\n",
      "113/113 - 0s - loss: 0.6661 - accuracy: 0.6116 - val_loss: 0.6696 - val_accuracy: 0.6136 - 402ms/epoch - 4ms/step\n",
      "Epoch 6/10000\n",
      "113/113 - 0s - loss: 0.6661 - accuracy: 0.6114 - val_loss: 0.6680 - val_accuracy: 0.6146 - 439ms/epoch - 4ms/step\n",
      "Epoch 7/10000\n",
      "113/113 - 0s - loss: 0.6652 - accuracy: 0.6115 - val_loss: 0.6661 - val_accuracy: 0.6142 - 429ms/epoch - 4ms/step\n",
      "Epoch 8/10000\n",
      "113/113 - 0s - loss: 0.6649 - accuracy: 0.6116 - val_loss: 0.6667 - val_accuracy: 0.6148 - 372ms/epoch - 3ms/step\n",
      "Epoch 9/10000\n",
      "113/113 - 0s - loss: 0.6655 - accuracy: 0.6116 - val_loss: 0.6658 - val_accuracy: 0.6153 - 387ms/epoch - 3ms/step\n",
      "Epoch 10/10000\n",
      "113/113 - 0s - loss: 0.6649 - accuracy: 0.6118 - val_loss: 0.6648 - val_accuracy: 0.6143 - 379ms/epoch - 3ms/step\n",
      "Epoch 11/10000\n",
      "113/113 - 0s - loss: 0.6654 - accuracy: 0.6118 - val_loss: 0.6648 - val_accuracy: 0.6149 - 376ms/epoch - 3ms/step\n",
      "Epoch 12/10000\n",
      "113/113 - 0s - loss: 0.6653 - accuracy: 0.6117 - val_loss: 0.6675 - val_accuracy: 0.6146 - 367ms/epoch - 3ms/step\n",
      "Epoch 13/10000\n",
      "113/113 - 0s - loss: 0.6643 - accuracy: 0.6114 - val_loss: 0.6673 - val_accuracy: 0.6147 - 373ms/epoch - 3ms/step\n",
      "Epoch 14/10000\n",
      "113/113 - 0s - loss: 0.6650 - accuracy: 0.6117 - val_loss: 0.6671 - val_accuracy: 0.6144 - 396ms/epoch - 4ms/step\n",
      "Epoch 15/10000\n",
      "113/113 - 0s - loss: 0.6647 - accuracy: 0.6116 - val_loss: 0.6658 - val_accuracy: 0.6136 - 377ms/epoch - 3ms/step\n",
      "Epoch 16/10000\n",
      "113/113 - 0s - loss: 0.6642 - accuracy: 0.6122 - val_loss: 0.6665 - val_accuracy: 0.6147 - 371ms/epoch - 3ms/step\n",
      "Epoch 17/10000\n",
      "113/113 - 0s - loss: 0.6648 - accuracy: 0.6118 - val_loss: 0.6659 - val_accuracy: 0.6145 - 406ms/epoch - 4ms/step\n",
      "Epoch 18/10000\n",
      "113/113 - 0s - loss: 0.6647 - accuracy: 0.6115 - val_loss: 0.6658 - val_accuracy: 0.6147 - 392ms/epoch - 3ms/step\n",
      "Epoch 19/10000\n",
      "113/113 - 0s - loss: 0.6647 - accuracy: 0.6115 - val_loss: 0.6675 - val_accuracy: 0.6146 - 398ms/epoch - 4ms/step\n",
      "Epoch 20/10000\n",
      "113/113 - 0s - loss: 0.6643 - accuracy: 0.6118 - val_loss: 0.6656 - val_accuracy: 0.6131 - 403ms/epoch - 4ms/step\n",
      "Epoch 21/10000\n",
      "113/113 - 0s - loss: 0.6644 - accuracy: 0.6118 - val_loss: 0.6644 - val_accuracy: 0.6132 - 410ms/epoch - 4ms/step\n",
      "Epoch 22/10000\n",
      "113/113 - 0s - loss: 0.6640 - accuracy: 0.6118 - val_loss: 0.6655 - val_accuracy: 0.6131 - 474ms/epoch - 4ms/step\n",
      "Epoch 23/10000\n",
      "113/113 - 1s - loss: 0.6646 - accuracy: 0.6117 - val_loss: 0.6651 - val_accuracy: 0.6132 - 518ms/epoch - 5ms/step\n",
      "Epoch 24/10000\n",
      "113/113 - 1s - loss: 0.6644 - accuracy: 0.6121 - val_loss: 0.6649 - val_accuracy: 0.6131 - 545ms/epoch - 5ms/step\n",
      "Epoch 25/10000\n",
      "113/113 - 0s - loss: 0.6639 - accuracy: 0.6119 - val_loss: 0.6654 - val_accuracy: 0.6147 - 477ms/epoch - 4ms/step\n",
      "Epoch 26/10000\n",
      "113/113 - 0s - loss: 0.6647 - accuracy: 0.6117 - val_loss: 0.6657 - val_accuracy: 0.6133 - 416ms/epoch - 4ms/step\n",
      "Epoch 27/10000\n",
      "113/113 - 1s - loss: 0.6640 - accuracy: 0.6118 - val_loss: 0.6651 - val_accuracy: 0.6131 - 618ms/epoch - 5ms/step\n",
      "Epoch 28/10000\n",
      "113/113 - 0s - loss: 0.6638 - accuracy: 0.6119 - val_loss: 0.6653 - val_accuracy: 0.6148 - 475ms/epoch - 4ms/step\n",
      "Epoch 29/10000\n",
      "113/113 - 1s - loss: 0.6643 - accuracy: 0.6119 - val_loss: 0.6640 - val_accuracy: 0.6148 - 523ms/epoch - 5ms/step\n",
      "391/391 [==============================] - 1s 3ms/step\n",
      "\n",
      "Iterazione 32/128 (03-11-2023_10-45-22)\n",
      "hidden_layer_size: 20\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 4\n",
      "activation_hidden: sigmoid\n",
      "activation_output: sigmoid\n",
      "batch_size: 1000\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 20\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 61.53%\n",
      "La recall di questo modello sul validation set è: 1.92%\n",
      "La f1 di questo modello sul validation set è: 3.72%\n",
      "La balanced accuracy di questo modello sul validation set è: 50.54%\n",
      "La precision di questo modello sul validation set è: 58.86%\n",
      "La AUC di questo modello sul validation set è: 56.77%\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.62    0.99      0.76  7,670.00\n",
      "1                  0.59    0.02      0.04  4,841.00\n",
      "accuracy           0.62    0.62      0.62      0.62\n",
      "macro avg          0.60    0.51      0.40 12,511.00\n",
      "weighted avg       0.61    0.62      0.48 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         7605           65\n",
      "Actual 1         4748           93\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 65.94% con il modello:\n",
      "1 - AC65.94%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 62.97% con il modello:\n",
      "1 - BA62.97%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 64.08% con il modello:\n",
      "13 - PR64.08%, L20, mV-10, MV10, P2, AHsigmoid, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-26-48).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "1126/1126 - 4s - loss: 84714.7578 - accuracy: 0.4284 - val_loss: 8369.0400 - val_accuracy: 0.4701 - 4s/epoch - 4ms/step\n",
      "Epoch 2/10000\n",
      "1126/1126 - 3s - loss: 3238.0464 - accuracy: 0.5729 - val_loss: 1373.8621 - val_accuracy: 0.6080 - 3s/epoch - 2ms/step\n",
      "Epoch 3/10000\n",
      "1126/1126 - 3s - loss: 865.2729 - accuracy: 0.5851 - val_loss: 542.9740 - val_accuracy: 0.5542 - 3s/epoch - 2ms/step\n",
      "Epoch 4/10000\n",
      "1126/1126 - 2s - loss: 571.7576 - accuracy: 0.5702 - val_loss: 760.2421 - val_accuracy: 0.5087 - 2s/epoch - 2ms/step\n",
      "Epoch 5/10000\n",
      "1126/1126 - 2s - loss: 498.1020 - accuracy: 0.5727 - val_loss: 546.0151 - val_accuracy: 0.6124 - 2s/epoch - 2ms/step\n",
      "Epoch 6/10000\n",
      "1126/1126 - 2s - loss: 422.3287 - accuracy: 0.5695 - val_loss: 451.0757 - val_accuracy: 0.5086 - 2s/epoch - 2ms/step\n",
      "Epoch 7/10000\n",
      "1126/1126 - 2s - loss: 402.6197 - accuracy: 0.5711 - val_loss: 324.8324 - val_accuracy: 0.6032 - 2s/epoch - 2ms/step\n",
      "Epoch 8/10000\n",
      "1126/1126 - 2s - loss: 390.2561 - accuracy: 0.5718 - val_loss: 773.6419 - val_accuracy: 0.6098 - 2s/epoch - 2ms/step\n",
      "Epoch 9/10000\n",
      "1126/1126 - 2s - loss: 370.8912 - accuracy: 0.5722 - val_loss: 336.4809 - val_accuracy: 0.6097 - 2s/epoch - 2ms/step\n",
      "Epoch 10/10000\n",
      "1126/1126 - 2s - loss: 334.8464 - accuracy: 0.5741 - val_loss: 386.6794 - val_accuracy: 0.6087 - 2s/epoch - 2ms/step\n",
      "Epoch 11/10000\n",
      "1126/1126 - 3s - loss: 315.2604 - accuracy: 0.5747 - val_loss: 248.2462 - val_accuracy: 0.6075 - 3s/epoch - 2ms/step\n",
      "Epoch 12/10000\n",
      "1126/1126 - 3s - loss: 314.1728 - accuracy: 0.5768 - val_loss: 275.7261 - val_accuracy: 0.6099 - 3s/epoch - 2ms/step\n",
      "Epoch 13/10000\n",
      "1126/1126 - 3s - loss: 321.0891 - accuracy: 0.5738 - val_loss: 282.8702 - val_accuracy: 0.5462 - 3s/epoch - 2ms/step\n",
      "Epoch 14/10000\n",
      "1126/1126 - 2s - loss: 291.0792 - accuracy: 0.5765 - val_loss: 180.4349 - val_accuracy: 0.6077 - 2s/epoch - 2ms/step\n",
      "Epoch 15/10000\n",
      "1126/1126 - 3s - loss: 287.1214 - accuracy: 0.5759 - val_loss: 220.4818 - val_accuracy: 0.6082 - 3s/epoch - 2ms/step\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Iterazione 33/128 (03-11-2023_10-46-03)\n",
      "hidden_layer_size: 100\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 1\n",
      "activation_hidden: relu\n",
      "activation_output: sigmoid\n",
      "batch_size: 100\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 10\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 61.24%\n",
      "La recall di questo modello sul validation set è: 13.16%\n",
      "La f1 di questo modello sul validation set è: 20.81%\n",
      "La balanced accuracy di questo modello sul validation set è: 52.37%\n",
      "La precision di questo modello sul validation set è: 49.69%\n",
      "La AUC di questo modello sul validation set è: 52.58%\n",
      "391/391 [==============================] - 1s 3ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.63    0.92      0.74  7,670.00\n",
      "1                  0.50    0.13      0.21  4,841.00\n",
      "accuracy           0.61    0.61      0.61      0.61\n",
      "macro avg          0.56    0.52      0.48 12,511.00\n",
      "weighted avg       0.58    0.61      0.54 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         7025          645\n",
      "Actual 1         4204          637\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 65.94% con il modello:\n",
      "1 - AC65.94%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 62.97% con il modello:\n",
      "1 - BA62.97%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 64.08% con il modello:\n",
      "13 - PR64.08%, L20, mV-10, MV10, P2, AHsigmoid, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-26-48).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "1126/1126 - 4s - loss: 84714.7578 - accuracy: 0.4284 - val_loss: 8369.0400 - val_accuracy: 0.4701 - 4s/epoch - 4ms/step\n",
      "Epoch 2/10000\n",
      "1126/1126 - 2s - loss: 3238.0464 - accuracy: 0.5729 - val_loss: 1373.8621 - val_accuracy: 0.6080 - 2s/epoch - 2ms/step\n",
      "Epoch 3/10000\n",
      "1126/1126 - 2s - loss: 865.2729 - accuracy: 0.5851 - val_loss: 542.9740 - val_accuracy: 0.5542 - 2s/epoch - 2ms/step\n",
      "Epoch 4/10000\n",
      "1126/1126 - 3s - loss: 571.7576 - accuracy: 0.5702 - val_loss: 760.2421 - val_accuracy: 0.5087 - 3s/epoch - 2ms/step\n",
      "Epoch 5/10000\n",
      "1126/1126 - 2s - loss: 498.1020 - accuracy: 0.5727 - val_loss: 546.0151 - val_accuracy: 0.6124 - 2s/epoch - 2ms/step\n",
      "Epoch 6/10000\n",
      "1126/1126 - 2s - loss: 422.3287 - accuracy: 0.5695 - val_loss: 451.0757 - val_accuracy: 0.5086 - 2s/epoch - 2ms/step\n",
      "Epoch 7/10000\n",
      "1126/1126 - 2s - loss: 402.6197 - accuracy: 0.5711 - val_loss: 324.8324 - val_accuracy: 0.6032 - 2s/epoch - 2ms/step\n",
      "Epoch 8/10000\n",
      "1126/1126 - 2s - loss: 390.2561 - accuracy: 0.5718 - val_loss: 773.6419 - val_accuracy: 0.6098 - 2s/epoch - 2ms/step\n",
      "Epoch 9/10000\n",
      "1126/1126 - 2s - loss: 370.8912 - accuracy: 0.5722 - val_loss: 336.4809 - val_accuracy: 0.6097 - 2s/epoch - 2ms/step\n",
      "Epoch 10/10000\n",
      "1126/1126 - 2s - loss: 334.8464 - accuracy: 0.5741 - val_loss: 386.6794 - val_accuracy: 0.6087 - 2s/epoch - 2ms/step\n",
      "Epoch 11/10000\n",
      "1126/1126 - 2s - loss: 315.2604 - accuracy: 0.5747 - val_loss: 248.2462 - val_accuracy: 0.6075 - 2s/epoch - 2ms/step\n",
      "Epoch 12/10000\n",
      "1126/1126 - 2s - loss: 314.1728 - accuracy: 0.5768 - val_loss: 275.7261 - val_accuracy: 0.6099 - 2s/epoch - 2ms/step\n",
      "Epoch 13/10000\n",
      "1126/1126 - 2s - loss: 321.0891 - accuracy: 0.5738 - val_loss: 282.8702 - val_accuracy: 0.5462 - 2s/epoch - 2ms/step\n",
      "Epoch 14/10000\n",
      "1126/1126 - 2s - loss: 291.0792 - accuracy: 0.5765 - val_loss: 180.4349 - val_accuracy: 0.6077 - 2s/epoch - 2ms/step\n",
      "Epoch 15/10000\n",
      "1126/1126 - 2s - loss: 287.1214 - accuracy: 0.5759 - val_loss: 220.4818 - val_accuracy: 0.6082 - 2s/epoch - 2ms/step\n",
      "Epoch 16/10000\n",
      "1126/1126 - 2s - loss: 295.7518 - accuracy: 0.5759 - val_loss: 247.8971 - val_accuracy: 0.5415 - 2s/epoch - 2ms/step\n",
      "Epoch 17/10000\n",
      "1126/1126 - 2s - loss: 291.0644 - accuracy: 0.5783 - val_loss: 228.5746 - val_accuracy: 0.5355 - 2s/epoch - 2ms/step\n",
      "Epoch 18/10000\n",
      "1126/1126 - 2s - loss: 278.6688 - accuracy: 0.5785 - val_loss: 424.7258 - val_accuracy: 0.4994 - 2s/epoch - 2ms/step\n",
      "Epoch 19/10000\n",
      "1126/1126 - 2s - loss: 270.3464 - accuracy: 0.5785 - val_loss: 163.7294 - val_accuracy: 0.5841 - 2s/epoch - 2ms/step\n",
      "Epoch 20/10000\n",
      "1126/1126 - 2s - loss: 275.7842 - accuracy: 0.5789 - val_loss: 148.5570 - val_accuracy: 0.6226 - 2s/epoch - 2ms/step\n",
      "Epoch 21/10000\n",
      "1126/1126 - 2s - loss: 246.6916 - accuracy: 0.5809 - val_loss: 248.8167 - val_accuracy: 0.6128 - 2s/epoch - 2ms/step\n",
      "Epoch 22/10000\n",
      "1126/1126 - 2s - loss: 261.1881 - accuracy: 0.5805 - val_loss: 149.0426 - val_accuracy: 0.6310 - 2s/epoch - 2ms/step\n",
      "Epoch 23/10000\n",
      "1126/1126 - 2s - loss: 225.7574 - accuracy: 0.5826 - val_loss: 136.5011 - val_accuracy: 0.6012 - 2s/epoch - 2ms/step\n",
      "Epoch 24/10000\n",
      "1126/1126 - 2s - loss: 246.7413 - accuracy: 0.5812 - val_loss: 166.7894 - val_accuracy: 0.6202 - 2s/epoch - 2ms/step\n",
      "Epoch 25/10000\n",
      "1126/1126 - 2s - loss: 245.5392 - accuracy: 0.5786 - val_loss: 543.5159 - val_accuracy: 0.6103 - 2s/epoch - 2ms/step\n",
      "Epoch 26/10000\n",
      "1126/1126 - 3s - loss: 250.3360 - accuracy: 0.5814 - val_loss: 202.6308 - val_accuracy: 0.5263 - 3s/epoch - 2ms/step\n",
      "Epoch 27/10000\n",
      "1126/1126 - 3s - loss: 231.7528 - accuracy: 0.5809 - val_loss: 236.7751 - val_accuracy: 0.5143 - 3s/epoch - 3ms/step\n",
      "Epoch 28/10000\n",
      "1126/1126 - 3s - loss: 234.5640 - accuracy: 0.5806 - val_loss: 291.9632 - val_accuracy: 0.6109 - 3s/epoch - 2ms/step\n",
      "Epoch 29/10000\n",
      "1126/1126 - 2s - loss: 230.3449 - accuracy: 0.5809 - val_loss: 209.8043 - val_accuracy: 0.5210 - 2s/epoch - 2ms/step\n",
      "Epoch 30/10000\n",
      "1126/1126 - 3s - loss: 225.6023 - accuracy: 0.5810 - val_loss: 138.2012 - val_accuracy: 0.6211 - 3s/epoch - 2ms/step\n",
      "Epoch 31/10000\n",
      "1126/1126 - 2s - loss: 226.0533 - accuracy: 0.5799 - val_loss: 472.6334 - val_accuracy: 0.4777 - 2s/epoch - 2ms/step\n",
      "Epoch 32/10000\n",
      "1126/1126 - 2s - loss: 239.7677 - accuracy: 0.5822 - val_loss: 243.2833 - val_accuracy: 0.6123 - 2s/epoch - 2ms/step\n",
      "Epoch 33/10000\n",
      "1126/1126 - 2s - loss: 231.3153 - accuracy: 0.5831 - val_loss: 124.3936 - val_accuracy: 0.6374 - 2s/epoch - 2ms/step\n",
      "Epoch 34/10000\n",
      "1126/1126 - 2s - loss: 210.3409 - accuracy: 0.5859 - val_loss: 220.1056 - val_accuracy: 0.6139 - 2s/epoch - 2ms/step\n",
      "Epoch 35/10000\n",
      "1126/1126 - 2s - loss: 223.1118 - accuracy: 0.5832 - val_loss: 121.8270 - val_accuracy: 0.5553 - 2s/epoch - 2ms/step\n",
      "Epoch 36/10000\n",
      "1126/1126 - 2s - loss: 223.2514 - accuracy: 0.5837 - val_loss: 119.4892 - val_accuracy: 0.6318 - 2s/epoch - 2ms/step\n",
      "Epoch 37/10000\n",
      "1126/1126 - 2s - loss: 215.7007 - accuracy: 0.5841 - val_loss: 101.7550 - val_accuracy: 0.6403 - 2s/epoch - 2ms/step\n",
      "Epoch 38/10000\n",
      "1126/1126 - 2s - loss: 224.8951 - accuracy: 0.5839 - val_loss: 134.8202 - val_accuracy: 0.5497 - 2s/epoch - 2ms/step\n",
      "Epoch 39/10000\n",
      "1126/1126 - 2s - loss: 216.2875 - accuracy: 0.5839 - val_loss: 93.1898 - val_accuracy: 0.6421 - 2s/epoch - 2ms/step\n",
      "Epoch 40/10000\n",
      "1126/1126 - 3s - loss: 212.5416 - accuracy: 0.5847 - val_loss: 140.6202 - val_accuracy: 0.5775 - 3s/epoch - 2ms/step\n",
      "Epoch 41/10000\n",
      "1126/1126 - 3s - loss: 208.4863 - accuracy: 0.5852 - val_loss: 85.4363 - val_accuracy: 0.6456 - 3s/epoch - 2ms/step\n",
      "Epoch 42/10000\n",
      "1126/1126 - 2s - loss: 220.4756 - accuracy: 0.5849 - val_loss: 420.6847 - val_accuracy: 0.4753 - 2s/epoch - 2ms/step\n",
      "Epoch 43/10000\n",
      "1126/1126 - 3s - loss: 204.3206 - accuracy: 0.5856 - val_loss: 119.8626 - val_accuracy: 0.6251 - 3s/epoch - 2ms/step\n",
      "Epoch 44/10000\n",
      "1126/1126 - 2s - loss: 217.9388 - accuracy: 0.5861 - val_loss: 267.2007 - val_accuracy: 0.6125 - 2s/epoch - 2ms/step\n",
      "Epoch 45/10000\n",
      "1126/1126 - 2s - loss: 202.4000 - accuracy: 0.5874 - val_loss: 198.7806 - val_accuracy: 0.5341 - 2s/epoch - 2ms/step\n",
      "Epoch 46/10000\n",
      "1126/1126 - 2s - loss: 232.0076 - accuracy: 0.5827 - val_loss: 91.4441 - val_accuracy: 0.6508 - 2s/epoch - 2ms/step\n",
      "Epoch 47/10000\n",
      "1126/1126 - 2s - loss: 221.0975 - accuracy: 0.5841 - val_loss: 534.5103 - val_accuracy: 0.6152 - 2s/epoch - 2ms/step\n",
      "Epoch 48/10000\n",
      "1126/1126 - 2s - loss: 207.4267 - accuracy: 0.5866 - val_loss: 181.4191 - val_accuracy: 0.6218 - 2s/epoch - 2ms/step\n",
      "Epoch 49/10000\n",
      "1126/1126 - 2s - loss: 219.2379 - accuracy: 0.5876 - val_loss: 509.2498 - val_accuracy: 0.4549 - 2s/epoch - 2ms/step\n",
      "Epoch 50/10000\n",
      "1126/1126 - 2s - loss: 212.9882 - accuracy: 0.5836 - val_loss: 234.5664 - val_accuracy: 0.5072 - 2s/epoch - 2ms/step\n",
      "Epoch 51/10000\n",
      "1126/1126 - 2s - loss: 191.0475 - accuracy: 0.5887 - val_loss: 274.0733 - val_accuracy: 0.4877 - 2s/epoch - 2ms/step\n",
      "Epoch 52/10000\n",
      "1126/1126 - 3s - loss: 213.5335 - accuracy: 0.5864 - val_loss: 304.2834 - val_accuracy: 0.6171 - 3s/epoch - 2ms/step\n",
      "Epoch 53/10000\n",
      "1126/1126 - 3s - loss: 211.5424 - accuracy: 0.5866 - val_loss: 87.0168 - val_accuracy: 0.6295 - 3s/epoch - 2ms/step\n",
      "Epoch 54/10000\n",
      "1126/1126 - 2s - loss: 190.7700 - accuracy: 0.5876 - val_loss: 91.6659 - val_accuracy: 0.6422 - 2s/epoch - 2ms/step\n",
      "Epoch 55/10000\n",
      "1126/1126 - 2s - loss: 196.1133 - accuracy: 0.5885 - val_loss: 130.5341 - val_accuracy: 0.6235 - 2s/epoch - 2ms/step\n",
      "Epoch 56/10000\n",
      "1126/1126 - 2s - loss: 204.8491 - accuracy: 0.5877 - val_loss: 117.8623 - val_accuracy: 0.6294 - 2s/epoch - 2ms/step\n",
      "Epoch 57/10000\n",
      "1126/1126 - 2s - loss: 202.0782 - accuracy: 0.5883 - val_loss: 283.6154 - val_accuracy: 0.5054 - 2s/epoch - 2ms/step\n",
      "Epoch 58/10000\n",
      "1126/1126 - 2s - loss: 199.9781 - accuracy: 0.5880 - val_loss: 162.6213 - val_accuracy: 0.6274 - 2s/epoch - 2ms/step\n",
      "Epoch 59/10000\n",
      "1126/1126 - 2s - loss: 205.6221 - accuracy: 0.5883 - val_loss: 234.6554 - val_accuracy: 0.5045 - 2s/epoch - 2ms/step\n",
      "Epoch 60/10000\n",
      "1126/1126 - 2s - loss: 203.9048 - accuracy: 0.5910 - val_loss: 73.6326 - val_accuracy: 0.6406 - 2s/epoch - 2ms/step\n",
      "Epoch 61/10000\n",
      "1126/1126 - 2s - loss: 192.2534 - accuracy: 0.5931 - val_loss: 305.0077 - val_accuracy: 0.6200 - 2s/epoch - 2ms/step\n",
      "Epoch 62/10000\n",
      "1126/1126 - 2s - loss: 191.0983 - accuracy: 0.5902 - val_loss: 152.2684 - val_accuracy: 0.5514 - 2s/epoch - 2ms/step\n",
      "Epoch 63/10000\n",
      "1126/1126 - 2s - loss: 206.1663 - accuracy: 0.5862 - val_loss: 82.8386 - val_accuracy: 0.6266 - 2s/epoch - 2ms/step\n",
      "Epoch 64/10000\n",
      "1126/1126 - 2s - loss: 192.9854 - accuracy: 0.5899 - val_loss: 107.1703 - val_accuracy: 0.5862 - 2s/epoch - 2ms/step\n",
      "Epoch 65/10000\n",
      "1126/1126 - 2s - loss: 202.6023 - accuracy: 0.5889 - val_loss: 258.5570 - val_accuracy: 0.6233 - 2s/epoch - 2ms/step\n",
      "Epoch 66/10000\n",
      "1126/1126 - 2s - loss: 181.5505 - accuracy: 0.5909 - val_loss: 87.5023 - val_accuracy: 0.6361 - 2s/epoch - 2ms/step\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Iterazione 34/128 (03-11-2023_10-48-30)\n",
      "hidden_layer_size: 100\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 1\n",
      "activation_hidden: relu\n",
      "activation_output: sigmoid\n",
      "batch_size: 100\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 20\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 65.08%\n",
      "La recall di questo modello sul validation set è: 45.4%\n",
      "La f1 di questo modello sul validation set è: 50.15%\n",
      "La balanced accuracy di questo modello sul validation set è: 61.45%\n",
      "La precision di questo modello sul validation set è: 56.01%\n",
      "La AUC di questo modello sul validation set è: 64.57%\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.69    0.77      0.73  7,670.00\n",
      "1                  0.56    0.45      0.50  4,841.00\n",
      "accuracy           0.65    0.65      0.65      0.65\n",
      "macro avg          0.63    0.61      0.62 12,511.00\n",
      "weighted avg       0.64    0.65      0.64 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         5944         1726\n",
      "Actual 1         2643         2198\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 65.94% con il modello:\n",
      "1 - AC65.94%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 62.97% con il modello:\n",
      "1 - BA62.97%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 64.08% con il modello:\n",
      "13 - PR64.08%, L20, mV-10, MV10, P2, AHsigmoid, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-26-48).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "113/113 - 2s - loss: 260975.5469 - accuracy: 0.5233 - val_loss: 171619.2969 - val_accuracy: 0.4668 - 2s/epoch - 15ms/step\n",
      "Epoch 2/10000\n",
      "113/113 - 0s - loss: 138964.4688 - accuracy: 0.4270 - val_loss: 120836.8359 - val_accuracy: 0.4120 - 399ms/epoch - 4ms/step\n",
      "Epoch 3/10000\n",
      "113/113 - 0s - loss: 110889.4766 - accuracy: 0.4119 - val_loss: 100208.9844 - val_accuracy: 0.4094 - 383ms/epoch - 3ms/step\n",
      "Epoch 4/10000\n",
      "113/113 - 0s - loss: 91479.2109 - accuracy: 0.4087 - val_loss: 81852.7500 - val_accuracy: 0.4080 - 371ms/epoch - 3ms/step\n",
      "Epoch 5/10000\n",
      "113/113 - 1s - loss: 73389.3984 - accuracy: 0.4059 - val_loss: 64242.3125 - val_accuracy: 0.4055 - 785ms/epoch - 7ms/step\n",
      "Epoch 6/10000\n",
      "113/113 - 1s - loss: 55379.6055 - accuracy: 0.4046 - val_loss: 46033.9922 - val_accuracy: 0.4036 - 671ms/epoch - 6ms/step\n",
      "Epoch 7/10000\n",
      "113/113 - 0s - loss: 36219.0977 - accuracy: 0.4037 - val_loss: 26437.8496 - val_accuracy: 0.4044 - 437ms/epoch - 4ms/step\n",
      "Epoch 8/10000\n",
      "113/113 - 0s - loss: 17725.6699 - accuracy: 0.4059 - val_loss: 11010.0664 - val_accuracy: 0.4184 - 448ms/epoch - 4ms/step\n",
      "Epoch 9/10000\n",
      "113/113 - 0s - loss: 7652.5093 - accuracy: 0.4634 - val_loss: 5677.9121 - val_accuracy: 0.4992 - 452ms/epoch - 4ms/step\n",
      "Epoch 10/10000\n",
      "113/113 - 0s - loss: 4747.7695 - accuracy: 0.5262 - val_loss: 4122.0034 - val_accuracy: 0.5468 - 383ms/epoch - 3ms/step\n",
      "Epoch 11/10000\n",
      "113/113 - 0s - loss: 3517.5042 - accuracy: 0.5542 - val_loss: 3062.9248 - val_accuracy: 0.5592 - 357ms/epoch - 3ms/step\n",
      "Epoch 12/10000\n",
      "113/113 - 0s - loss: 2664.1843 - accuracy: 0.5714 - val_loss: 2253.8687 - val_accuracy: 0.5780 - 382ms/epoch - 3ms/step\n",
      "Epoch 13/10000\n",
      "113/113 - 0s - loss: 1635.5604 - accuracy: 0.6030 - val_loss: 1263.0197 - val_accuracy: 0.6145 - 383ms/epoch - 3ms/step\n",
      "Epoch 14/10000\n",
      "113/113 - 0s - loss: 1155.0320 - accuracy: 0.6032 - val_loss: 1053.8951 - val_accuracy: 0.6055 - 380ms/epoch - 3ms/step\n",
      "Epoch 15/10000\n",
      "113/113 - 0s - loss: 1013.4922 - accuracy: 0.5853 - val_loss: 940.9068 - val_accuracy: 0.5884 - 490ms/epoch - 4ms/step\n",
      "Epoch 16/10000\n",
      "113/113 - 0s - loss: 942.5886 - accuracy: 0.5833 - val_loss: 873.3279 - val_accuracy: 0.6025 - 482ms/epoch - 4ms/step\n",
      "Epoch 17/10000\n",
      "113/113 - 0s - loss: 873.9789 - accuracy: 0.5809 - val_loss: 849.7325 - val_accuracy: 0.5698 - 332ms/epoch - 3ms/step\n",
      "Epoch 18/10000\n",
      "113/113 - 0s - loss: 839.7272 - accuracy: 0.5797 - val_loss: 853.8524 - val_accuracy: 0.5992 - 370ms/epoch - 3ms/step\n",
      "Epoch 19/10000\n",
      "113/113 - 0s - loss: 797.2340 - accuracy: 0.5807 - val_loss: 817.5735 - val_accuracy: 0.6016 - 352ms/epoch - 3ms/step\n",
      "Epoch 20/10000\n",
      "113/113 - 0s - loss: 767.5755 - accuracy: 0.5791 - val_loss: 717.3948 - val_accuracy: 0.5793 - 348ms/epoch - 3ms/step\n",
      "Epoch 21/10000\n",
      "113/113 - 0s - loss: 753.5961 - accuracy: 0.5783 - val_loss: 682.6575 - val_accuracy: 0.5856 - 358ms/epoch - 3ms/step\n",
      "Epoch 22/10000\n",
      "113/113 - 0s - loss: 736.9008 - accuracy: 0.5785 - val_loss: 718.6052 - val_accuracy: 0.5540 - 339ms/epoch - 3ms/step\n",
      "Epoch 23/10000\n",
      "113/113 - 0s - loss: 707.6810 - accuracy: 0.5792 - val_loss: 649.8267 - val_accuracy: 0.5976 - 354ms/epoch - 3ms/step\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Iterazione 35/128 (03-11-2023_10-48-44)\n",
      "hidden_layer_size: 100\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 1\n",
      "activation_hidden: relu\n",
      "activation_output: sigmoid\n",
      "batch_size: 1000\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 10\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 61.45%\n",
      "La recall di questo modello sul validation set è: 41.46%\n",
      "La f1 di questo modello sul validation set è: 45.42%\n",
      "La balanced accuracy di questo modello sul validation set è: 57.76%\n",
      "La precision di questo modello sul validation set è: 50.23%\n",
      "La AUC di questo modello sul validation set è: 57.86%\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.67    0.74      0.70  7,670.00\n",
      "1                  0.50    0.41      0.45  4,841.00\n",
      "accuracy           0.61    0.61      0.61      0.61\n",
      "macro avg          0.58    0.58      0.58 12,511.00\n",
      "weighted avg       0.60    0.61      0.61 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         5681         1989\n",
      "Actual 1         2834         2007\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 65.94% con il modello:\n",
      "1 - AC65.94%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 62.97% con il modello:\n",
      "1 - BA62.97%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 64.08% con il modello:\n",
      "13 - PR64.08%, L20, mV-10, MV10, P2, AHsigmoid, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-26-48).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "113/113 - 1s - loss: 260975.5469 - accuracy: 0.5233 - val_loss: 171619.2969 - val_accuracy: 0.4668 - 1s/epoch - 12ms/step\n",
      "Epoch 2/10000\n",
      "113/113 - 0s - loss: 138964.4688 - accuracy: 0.4270 - val_loss: 120836.8359 - val_accuracy: 0.4120 - 453ms/epoch - 4ms/step\n",
      "Epoch 3/10000\n",
      "113/113 - 0s - loss: 110889.4766 - accuracy: 0.4119 - val_loss: 100208.9844 - val_accuracy: 0.4094 - 386ms/epoch - 3ms/step\n",
      "Epoch 4/10000\n",
      "113/113 - 0s - loss: 91479.2109 - accuracy: 0.4087 - val_loss: 81852.7500 - val_accuracy: 0.4080 - 359ms/epoch - 3ms/step\n",
      "Epoch 5/10000\n",
      "113/113 - 0s - loss: 73389.3984 - accuracy: 0.4059 - val_loss: 64242.3125 - val_accuracy: 0.4055 - 429ms/epoch - 4ms/step\n",
      "Epoch 6/10000\n",
      "113/113 - 0s - loss: 55379.6055 - accuracy: 0.4046 - val_loss: 46033.9922 - val_accuracy: 0.4036 - 355ms/epoch - 3ms/step\n",
      "Epoch 7/10000\n",
      "113/113 - 0s - loss: 36219.0977 - accuracy: 0.4037 - val_loss: 26437.8496 - val_accuracy: 0.4044 - 364ms/epoch - 3ms/step\n",
      "Epoch 8/10000\n",
      "113/113 - 0s - loss: 17725.6699 - accuracy: 0.4059 - val_loss: 11010.0664 - val_accuracy: 0.4184 - 371ms/epoch - 3ms/step\n",
      "Epoch 9/10000\n",
      "113/113 - 0s - loss: 7652.5093 - accuracy: 0.4634 - val_loss: 5677.9121 - val_accuracy: 0.4992 - 462ms/epoch - 4ms/step\n",
      "Epoch 10/10000\n",
      "113/113 - 0s - loss: 4747.7695 - accuracy: 0.5262 - val_loss: 4122.0034 - val_accuracy: 0.5468 - 459ms/epoch - 4ms/step\n",
      "Epoch 11/10000\n",
      "113/113 - 0s - loss: 3517.5042 - accuracy: 0.5542 - val_loss: 3062.9248 - val_accuracy: 0.5592 - 326ms/epoch - 3ms/step\n",
      "Epoch 12/10000\n",
      "113/113 - 0s - loss: 2664.1843 - accuracy: 0.5714 - val_loss: 2253.8687 - val_accuracy: 0.5780 - 360ms/epoch - 3ms/step\n",
      "Epoch 13/10000\n",
      "113/113 - 0s - loss: 1635.5604 - accuracy: 0.6030 - val_loss: 1263.0197 - val_accuracy: 0.6145 - 344ms/epoch - 3ms/step\n",
      "Epoch 14/10000\n",
      "113/113 - 0s - loss: 1155.0320 - accuracy: 0.6032 - val_loss: 1053.8951 - val_accuracy: 0.6055 - 341ms/epoch - 3ms/step\n",
      "Epoch 15/10000\n",
      "113/113 - 0s - loss: 1013.4922 - accuracy: 0.5853 - val_loss: 940.9068 - val_accuracy: 0.5884 - 370ms/epoch - 3ms/step\n",
      "Epoch 16/10000\n",
      "113/113 - 0s - loss: 942.5886 - accuracy: 0.5833 - val_loss: 873.3279 - val_accuracy: 0.6025 - 370ms/epoch - 3ms/step\n",
      "Epoch 17/10000\n",
      "113/113 - 0s - loss: 873.9789 - accuracy: 0.5809 - val_loss: 849.7325 - val_accuracy: 0.5698 - 353ms/epoch - 3ms/step\n",
      "Epoch 18/10000\n",
      "113/113 - 0s - loss: 839.7272 - accuracy: 0.5797 - val_loss: 853.8524 - val_accuracy: 0.5992 - 350ms/epoch - 3ms/step\n",
      "Epoch 19/10000\n",
      "113/113 - 0s - loss: 797.2340 - accuracy: 0.5807 - val_loss: 817.5735 - val_accuracy: 0.6016 - 360ms/epoch - 3ms/step\n",
      "Epoch 20/10000\n",
      "113/113 - 0s - loss: 767.5755 - accuracy: 0.5791 - val_loss: 717.3948 - val_accuracy: 0.5793 - 358ms/epoch - 3ms/step\n",
      "Epoch 21/10000\n",
      "113/113 - 0s - loss: 753.5961 - accuracy: 0.5783 - val_loss: 682.6575 - val_accuracy: 0.5856 - 356ms/epoch - 3ms/step\n",
      "Epoch 22/10000\n",
      "113/113 - 0s - loss: 736.9008 - accuracy: 0.5785 - val_loss: 718.6052 - val_accuracy: 0.5540 - 360ms/epoch - 3ms/step\n",
      "Epoch 23/10000\n",
      "113/113 - 0s - loss: 707.6810 - accuracy: 0.5792 - val_loss: 649.8267 - val_accuracy: 0.5976 - 388ms/epoch - 3ms/step\n",
      "Epoch 24/10000\n",
      "113/113 - 0s - loss: 692.2856 - accuracy: 0.5779 - val_loss: 632.2316 - val_accuracy: 0.5897 - 362ms/epoch - 3ms/step\n",
      "Epoch 25/10000\n",
      "113/113 - 0s - loss: 665.1602 - accuracy: 0.5776 - val_loss: 652.2598 - val_accuracy: 0.6013 - 344ms/epoch - 3ms/step\n",
      "Epoch 26/10000\n",
      "113/113 - 0s - loss: 639.2847 - accuracy: 0.5792 - val_loss: 606.7170 - val_accuracy: 0.5563 - 368ms/epoch - 3ms/step\n",
      "Epoch 27/10000\n",
      "113/113 - 0s - loss: 611.9583 - accuracy: 0.5807 - val_loss: 568.8884 - val_accuracy: 0.5920 - 385ms/epoch - 3ms/step\n",
      "Epoch 28/10000\n",
      "113/113 - 0s - loss: 635.6688 - accuracy: 0.5774 - val_loss: 626.7838 - val_accuracy: 0.6033 - 326ms/epoch - 3ms/step\n",
      "Epoch 29/10000\n",
      "113/113 - 0s - loss: 624.9030 - accuracy: 0.5793 - val_loss: 568.0796 - val_accuracy: 0.5663 - 345ms/epoch - 3ms/step\n",
      "Epoch 30/10000\n",
      "113/113 - 0s - loss: 584.3428 - accuracy: 0.5751 - val_loss: 672.6967 - val_accuracy: 0.6033 - 329ms/epoch - 3ms/step\n",
      "Epoch 31/10000\n",
      "113/113 - 0s - loss: 621.4923 - accuracy: 0.5764 - val_loss: 596.3857 - val_accuracy: 0.5501 - 357ms/epoch - 3ms/step\n",
      "Epoch 32/10000\n",
      "113/113 - 0s - loss: 598.4557 - accuracy: 0.5773 - val_loss: 585.0043 - val_accuracy: 0.6008 - 359ms/epoch - 3ms/step\n",
      "Epoch 33/10000\n",
      "113/113 - 0s - loss: 547.9603 - accuracy: 0.5785 - val_loss: 492.6740 - val_accuracy: 0.5916 - 362ms/epoch - 3ms/step\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Iterazione 36/128 (03-11-2023_10-48-59)\n",
      "hidden_layer_size: 100\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 1\n",
      "activation_hidden: relu\n",
      "activation_output: sigmoid\n",
      "batch_size: 1000\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 20\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 61.45%\n",
      "La recall di questo modello sul validation set è: 41.46%\n",
      "La f1 di questo modello sul validation set è: 45.42%\n",
      "La balanced accuracy di questo modello sul validation set è: 57.76%\n",
      "La precision di questo modello sul validation set è: 50.23%\n",
      "La AUC di questo modello sul validation set è: 57.86%\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.67    0.74      0.70  7,670.00\n",
      "1                  0.50    0.41      0.45  4,841.00\n",
      "accuracy           0.61    0.61      0.61      0.61\n",
      "macro avg          0.58    0.58      0.58 12,511.00\n",
      "weighted avg       0.60    0.61      0.61 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         5681         1989\n",
      "Actual 1         2834         2007\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 65.94% con il modello:\n",
      "1 - AC65.94%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 62.97% con il modello:\n",
      "1 - BA62.97%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 64.08% con il modello:\n",
      "13 - PR64.08%, L20, mV-10, MV10, P2, AHsigmoid, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-26-48).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "1126/1126 - 4s - loss: 3.2454 - accuracy: 0.5000 - val_loss: 1.1500 - val_accuracy: 0.5493 - 4s/epoch - 3ms/step\n",
      "Epoch 2/10000\n",
      "1126/1126 - 3s - loss: 1.1017 - accuracy: 0.5499 - val_loss: 1.0405 - val_accuracy: 0.5874 - 3s/epoch - 3ms/step\n",
      "Epoch 3/10000\n",
      "1126/1126 - 2s - loss: 1.0248 - accuracy: 0.5790 - val_loss: 0.9797 - val_accuracy: 0.5904 - 2s/epoch - 2ms/step\n",
      "Epoch 4/10000\n",
      "1126/1126 - 2s - loss: 0.9666 - accuracy: 0.5899 - val_loss: 0.9150 - val_accuracy: 0.6004 - 2s/epoch - 2ms/step\n",
      "Epoch 5/10000\n",
      "1126/1126 - 2s - loss: 0.9092 - accuracy: 0.5961 - val_loss: 0.8520 - val_accuracy: 0.6049 - 2s/epoch - 2ms/step\n",
      "Epoch 6/10000\n",
      "1126/1126 - 2s - loss: 0.8583 - accuracy: 0.5978 - val_loss: 0.8140 - val_accuracy: 0.6025 - 2s/epoch - 2ms/step\n",
      "Epoch 7/10000\n",
      "1126/1126 - 3s - loss: 0.8341 - accuracy: 0.5955 - val_loss: 0.8018 - val_accuracy: 0.6017 - 3s/epoch - 3ms/step\n",
      "Epoch 8/10000\n",
      "1126/1126 - 3s - loss: 0.8137 - accuracy: 0.5963 - val_loss: 0.7936 - val_accuracy: 0.6064 - 3s/epoch - 2ms/step\n",
      "Epoch 9/10000\n",
      "1126/1126 - 3s - loss: 0.8003 - accuracy: 0.5982 - val_loss: 0.7706 - val_accuracy: 0.6070 - 3s/epoch - 3ms/step\n",
      "Epoch 10/10000\n",
      "1126/1126 - 2s - loss: 0.7838 - accuracy: 0.6026 - val_loss: 0.7640 - val_accuracy: 0.6126 - 2s/epoch - 2ms/step\n",
      "Epoch 11/10000\n",
      "1126/1126 - 3s - loss: 0.7706 - accuracy: 0.6057 - val_loss: 0.7500 - val_accuracy: 0.6123 - 3s/epoch - 3ms/step\n",
      "Epoch 12/10000\n",
      "1126/1126 - 3s - loss: 0.7582 - accuracy: 0.6050 - val_loss: 0.7329 - val_accuracy: 0.6026 - 3s/epoch - 2ms/step\n",
      "Epoch 13/10000\n",
      "1126/1126 - 2s - loss: 0.7492 - accuracy: 0.6072 - val_loss: 0.7286 - val_accuracy: 0.6149 - 2s/epoch - 2ms/step\n",
      "Epoch 14/10000\n",
      "1126/1126 - 3s - loss: 0.7391 - accuracy: 0.6074 - val_loss: 0.7170 - val_accuracy: 0.6155 - 3s/epoch - 3ms/step\n",
      "Epoch 15/10000\n",
      "1126/1126 - 3s - loss: 0.7320 - accuracy: 0.6060 - val_loss: 0.7092 - val_accuracy: 0.6187 - 3s/epoch - 2ms/step\n",
      "Epoch 16/10000\n",
      "1126/1126 - 2s - loss: 0.7234 - accuracy: 0.6066 - val_loss: 0.7069 - val_accuracy: 0.6049 - 2s/epoch - 2ms/step\n",
      "Epoch 17/10000\n",
      "1126/1126 - 2s - loss: 0.7175 - accuracy: 0.6034 - val_loss: 0.7003 - val_accuracy: 0.6131 - 2s/epoch - 2ms/step\n",
      "Epoch 18/10000\n",
      "1126/1126 - 2s - loss: 0.7089 - accuracy: 0.6039 - val_loss: 0.6984 - val_accuracy: 0.6033 - 2s/epoch - 2ms/step\n",
      "Epoch 19/10000\n",
      "1126/1126 - 2s - loss: 0.7046 - accuracy: 0.6037 - val_loss: 0.6924 - val_accuracy: 0.6114 - 2s/epoch - 2ms/step\n",
      "Epoch 20/10000\n",
      "1126/1126 - 2s - loss: 0.6983 - accuracy: 0.6040 - val_loss: 0.6867 - val_accuracy: 0.6123 - 2s/epoch - 2ms/step\n",
      "Epoch 21/10000\n",
      "1126/1126 - 2s - loss: 0.6955 - accuracy: 0.6076 - val_loss: 0.6835 - val_accuracy: 0.6144 - 2s/epoch - 2ms/step\n",
      "Epoch 22/10000\n",
      "1126/1126 - 2s - loss: 0.6913 - accuracy: 0.6082 - val_loss: 0.6791 - val_accuracy: 0.6188 - 2s/epoch - 2ms/step\n",
      "Epoch 23/10000\n",
      "1126/1126 - 2s - loss: 0.6857 - accuracy: 0.6092 - val_loss: 0.6788 - val_accuracy: 0.6200 - 2s/epoch - 2ms/step\n",
      "Epoch 24/10000\n",
      "1126/1126 - 2s - loss: 0.6829 - accuracy: 0.6114 - val_loss: 0.6696 - val_accuracy: 0.6195 - 2s/epoch - 2ms/step\n",
      "Epoch 25/10000\n",
      "1126/1126 - 2s - loss: 0.6787 - accuracy: 0.6105 - val_loss: 0.6737 - val_accuracy: 0.6210 - 2s/epoch - 2ms/step\n",
      "Epoch 26/10000\n",
      "1126/1126 - 2s - loss: 0.6792 - accuracy: 0.6124 - val_loss: 0.6669 - val_accuracy: 0.6139 - 2s/epoch - 2ms/step\n",
      "Epoch 27/10000\n",
      "1126/1126 - 2s - loss: 0.6765 - accuracy: 0.6153 - val_loss: 0.6662 - val_accuracy: 0.6141 - 2s/epoch - 2ms/step\n",
      "Epoch 28/10000\n",
      "1126/1126 - 2s - loss: 0.6748 - accuracy: 0.6194 - val_loss: 0.6682 - val_accuracy: 0.6252 - 2s/epoch - 2ms/step\n",
      "Epoch 29/10000\n",
      "1126/1126 - 3s - loss: 0.6732 - accuracy: 0.6175 - val_loss: 0.6695 - val_accuracy: 0.6195 - 3s/epoch - 2ms/step\n",
      "Epoch 30/10000\n",
      "1126/1126 - 3s - loss: 0.6719 - accuracy: 0.6165 - val_loss: 0.6606 - val_accuracy: 0.6258 - 3s/epoch - 2ms/step\n",
      "Epoch 31/10000\n",
      "1126/1126 - 2s - loss: 0.6713 - accuracy: 0.6184 - val_loss: 0.6621 - val_accuracy: 0.6235 - 2s/epoch - 2ms/step\n",
      "Epoch 32/10000\n",
      "1126/1126 - 3s - loss: 0.6687 - accuracy: 0.6194 - val_loss: 0.6582 - val_accuracy: 0.6243 - 3s/epoch - 2ms/step\n",
      "Epoch 33/10000\n",
      "1126/1126 - 3s - loss: 0.6678 - accuracy: 0.6189 - val_loss: 0.6583 - val_accuracy: 0.6223 - 3s/epoch - 2ms/step\n",
      "Epoch 34/10000\n",
      "1126/1126 - 3s - loss: 0.6685 - accuracy: 0.6218 - val_loss: 0.6565 - val_accuracy: 0.6265 - 3s/epoch - 3ms/step\n",
      "Epoch 35/10000\n",
      "1126/1126 - 2s - loss: 0.6670 - accuracy: 0.6184 - val_loss: 0.6595 - val_accuracy: 0.6208 - 2s/epoch - 2ms/step\n",
      "Epoch 36/10000\n",
      "1126/1126 - 3s - loss: 0.6663 - accuracy: 0.6152 - val_loss: 0.6576 - val_accuracy: 0.6224 - 3s/epoch - 2ms/step\n",
      "Epoch 37/10000\n",
      "1126/1126 - 3s - loss: 0.6635 - accuracy: 0.6182 - val_loss: 0.6535 - val_accuracy: 0.6270 - 3s/epoch - 2ms/step\n",
      "Epoch 38/10000\n",
      "1126/1126 - 2s - loss: 0.6621 - accuracy: 0.6190 - val_loss: 0.6597 - val_accuracy: 0.6270 - 2s/epoch - 2ms/step\n",
      "Epoch 39/10000\n",
      "1126/1126 - 3s - loss: 0.6623 - accuracy: 0.6220 - val_loss: 0.6562 - val_accuracy: 0.6221 - 3s/epoch - 2ms/step\n",
      "Epoch 40/10000\n",
      "1126/1126 - 2s - loss: 0.6618 - accuracy: 0.6175 - val_loss: 0.6539 - val_accuracy: 0.6250 - 2s/epoch - 2ms/step\n",
      "Epoch 41/10000\n",
      "1126/1126 - 2s - loss: 0.6611 - accuracy: 0.6211 - val_loss: 0.6524 - val_accuracy: 0.6271 - 2s/epoch - 2ms/step\n",
      "Epoch 42/10000\n",
      "1126/1126 - 2s - loss: 0.6612 - accuracy: 0.6203 - val_loss: 0.6579 - val_accuracy: 0.6162 - 2s/epoch - 2ms/step\n",
      "Epoch 43/10000\n",
      "1126/1126 - 2s - loss: 0.6620 - accuracy: 0.6214 - val_loss: 0.6512 - val_accuracy: 0.6164 - 2s/epoch - 2ms/step\n",
      "Epoch 44/10000\n",
      "1126/1126 - 2s - loss: 0.6591 - accuracy: 0.6185 - val_loss: 0.6559 - val_accuracy: 0.6155 - 2s/epoch - 2ms/step\n",
      "Epoch 45/10000\n",
      "1126/1126 - 2s - loss: 0.6599 - accuracy: 0.6167 - val_loss: 0.6545 - val_accuracy: 0.6277 - 2s/epoch - 2ms/step\n",
      "Epoch 46/10000\n",
      "1126/1126 - 2s - loss: 0.6580 - accuracy: 0.6215 - val_loss: 0.6505 - val_accuracy: 0.6278 - 2s/epoch - 2ms/step\n",
      "Epoch 47/10000\n",
      "1126/1126 - 2s - loss: 0.6567 - accuracy: 0.6187 - val_loss: 0.6500 - val_accuracy: 0.6258 - 2s/epoch - 2ms/step\n",
      "Epoch 48/10000\n",
      "1126/1126 - 2s - loss: 0.6568 - accuracy: 0.6197 - val_loss: 0.6490 - val_accuracy: 0.6304 - 2s/epoch - 2ms/step\n",
      "Epoch 49/10000\n",
      "1126/1126 - 3s - loss: 0.6577 - accuracy: 0.6271 - val_loss: 0.6468 - val_accuracy: 0.6396 - 3s/epoch - 3ms/step\n",
      "Epoch 50/10000\n",
      "1126/1126 - 3s - loss: 0.6553 - accuracy: 0.6284 - val_loss: 0.6477 - val_accuracy: 0.6338 - 3s/epoch - 3ms/step\n",
      "Epoch 51/10000\n",
      "1126/1126 - 3s - loss: 0.6546 - accuracy: 0.6259 - val_loss: 0.6479 - val_accuracy: 0.6360 - 3s/epoch - 3ms/step\n",
      "Epoch 52/10000\n",
      "1126/1126 - 3s - loss: 0.6536 - accuracy: 0.6282 - val_loss: 0.6475 - val_accuracy: 0.6333 - 3s/epoch - 2ms/step\n",
      "Epoch 53/10000\n",
      "1126/1126 - 3s - loss: 0.6535 - accuracy: 0.6273 - val_loss: 0.6474 - val_accuracy: 0.6326 - 3s/epoch - 2ms/step\n",
      "Epoch 54/10000\n",
      "1126/1126 - 3s - loss: 0.6545 - accuracy: 0.6274 - val_loss: 0.6507 - val_accuracy: 0.6347 - 3s/epoch - 2ms/step\n",
      "Epoch 55/10000\n",
      "1126/1126 - 2s - loss: 0.6540 - accuracy: 0.6273 - val_loss: 0.6471 - val_accuracy: 0.6302 - 2s/epoch - 2ms/step\n",
      "Epoch 56/10000\n",
      "1126/1126 - 2s - loss: 0.6539 - accuracy: 0.6223 - val_loss: 0.6480 - val_accuracy: 0.6301 - 2s/epoch - 2ms/step\n",
      "Epoch 57/10000\n",
      "1126/1126 - 3s - loss: 0.6529 - accuracy: 0.6248 - val_loss: 0.6452 - val_accuracy: 0.6309 - 3s/epoch - 2ms/step\n",
      "Epoch 58/10000\n",
      "1126/1126 - 2s - loss: 0.6524 - accuracy: 0.6219 - val_loss: 0.6453 - val_accuracy: 0.6271 - 2s/epoch - 2ms/step\n",
      "Epoch 59/10000\n",
      "1126/1126 - 2s - loss: 0.6518 - accuracy: 0.6222 - val_loss: 0.6488 - val_accuracy: 0.6259 - 2s/epoch - 2ms/step\n",
      "391/391 [==============================] - 1s 3ms/step\n",
      "\n",
      "Iterazione 37/128 (03-11-2023_10-51-28)\n",
      "hidden_layer_size: 100\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 1\n",
      "activation_hidden: sigmoid\n",
      "activation_output: sigmoid\n",
      "batch_size: 100\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 10\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 63.96%\n",
      "La recall di questo modello sul validation set è: 37.45%\n",
      "La f1 di questo modello sul validation set è: 44.57%\n",
      "La balanced accuracy di questo modello sul validation set è: 59.07%\n",
      "La precision di questo modello sul validation set è: 55.04%\n",
      "La AUC di questo modello sul validation set è: 63.78%\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.67    0.81      0.73  7,670.00\n",
      "1                  0.55    0.37      0.45  4,841.00\n",
      "accuracy           0.64    0.64      0.64      0.64\n",
      "macro avg          0.61    0.59      0.59 12,511.00\n",
      "weighted avg       0.62    0.64      0.62 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         6189         1481\n",
      "Actual 1         3028         1813\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 65.94% con il modello:\n",
      "1 - AC65.94%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 62.97% con il modello:\n",
      "1 - BA62.97%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 64.08% con il modello:\n",
      "13 - PR64.08%, L20, mV-10, MV10, P2, AHsigmoid, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-26-48).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "1126/1126 - 3s - loss: 3.2454 - accuracy: 0.5000 - val_loss: 1.1500 - val_accuracy: 0.5493 - 3s/epoch - 3ms/step\n",
      "Epoch 2/10000\n",
      "1126/1126 - 4s - loss: 1.1017 - accuracy: 0.5499 - val_loss: 1.0405 - val_accuracy: 0.5874 - 4s/epoch - 3ms/step\n",
      "Epoch 3/10000\n",
      "1126/1126 - 2s - loss: 1.0248 - accuracy: 0.5790 - val_loss: 0.9797 - val_accuracy: 0.5904 - 2s/epoch - 2ms/step\n",
      "Epoch 4/10000\n",
      "1126/1126 - 2s - loss: 0.9666 - accuracy: 0.5899 - val_loss: 0.9150 - val_accuracy: 0.6004 - 2s/epoch - 2ms/step\n",
      "Epoch 5/10000\n",
      "1126/1126 - 2s - loss: 0.9092 - accuracy: 0.5961 - val_loss: 0.8520 - val_accuracy: 0.6049 - 2s/epoch - 2ms/step\n",
      "Epoch 6/10000\n",
      "1126/1126 - 2s - loss: 0.8583 - accuracy: 0.5978 - val_loss: 0.8140 - val_accuracy: 0.6025 - 2s/epoch - 2ms/step\n",
      "Epoch 7/10000\n",
      "1126/1126 - 2s - loss: 0.8341 - accuracy: 0.5955 - val_loss: 0.8018 - val_accuracy: 0.6017 - 2s/epoch - 2ms/step\n",
      "Epoch 8/10000\n",
      "1126/1126 - 2s - loss: 0.8137 - accuracy: 0.5963 - val_loss: 0.7936 - val_accuracy: 0.6064 - 2s/epoch - 2ms/step\n",
      "Epoch 9/10000\n",
      "1126/1126 - 2s - loss: 0.8003 - accuracy: 0.5982 - val_loss: 0.7706 - val_accuracy: 0.6070 - 2s/epoch - 2ms/step\n",
      "Epoch 10/10000\n",
      "1126/1126 - 2s - loss: 0.7838 - accuracy: 0.6026 - val_loss: 0.7640 - val_accuracy: 0.6126 - 2s/epoch - 2ms/step\n",
      "Epoch 11/10000\n",
      "1126/1126 - 2s - loss: 0.7706 - accuracy: 0.6057 - val_loss: 0.7500 - val_accuracy: 0.6123 - 2s/epoch - 2ms/step\n",
      "Epoch 12/10000\n",
      "1126/1126 - 3s - loss: 0.7582 - accuracy: 0.6050 - val_loss: 0.7329 - val_accuracy: 0.6026 - 3s/epoch - 3ms/step\n",
      "Epoch 13/10000\n",
      "1126/1126 - 3s - loss: 0.7492 - accuracy: 0.6072 - val_loss: 0.7286 - val_accuracy: 0.6149 - 3s/epoch - 3ms/step\n",
      "Epoch 14/10000\n",
      "1126/1126 - 4s - loss: 0.7391 - accuracy: 0.6074 - val_loss: 0.7170 - val_accuracy: 0.6155 - 4s/epoch - 4ms/step\n",
      "Epoch 15/10000\n",
      "1126/1126 - 3s - loss: 0.7320 - accuracy: 0.6060 - val_loss: 0.7092 - val_accuracy: 0.6187 - 3s/epoch - 2ms/step\n",
      "Epoch 16/10000\n",
      "1126/1126 - 2s - loss: 0.7234 - accuracy: 0.6066 - val_loss: 0.7069 - val_accuracy: 0.6049 - 2s/epoch - 2ms/step\n",
      "Epoch 17/10000\n",
      "1126/1126 - 3s - loss: 0.7175 - accuracy: 0.6034 - val_loss: 0.7003 - val_accuracy: 0.6131 - 3s/epoch - 3ms/step\n",
      "Epoch 18/10000\n",
      "1126/1126 - 3s - loss: 0.7089 - accuracy: 0.6039 - val_loss: 0.6984 - val_accuracy: 0.6033 - 3s/epoch - 3ms/step\n",
      "Epoch 19/10000\n",
      "1126/1126 - 2s - loss: 0.7046 - accuracy: 0.6037 - val_loss: 0.6924 - val_accuracy: 0.6114 - 2s/epoch - 2ms/step\n",
      "Epoch 20/10000\n",
      "1126/1126 - 3s - loss: 0.6983 - accuracy: 0.6040 - val_loss: 0.6867 - val_accuracy: 0.6123 - 3s/epoch - 3ms/step\n",
      "Epoch 21/10000\n",
      "1126/1126 - 2s - loss: 0.6955 - accuracy: 0.6076 - val_loss: 0.6835 - val_accuracy: 0.6144 - 2s/epoch - 2ms/step\n",
      "Epoch 22/10000\n",
      "1126/1126 - 4s - loss: 0.6913 - accuracy: 0.6082 - val_loss: 0.6791 - val_accuracy: 0.6188 - 4s/epoch - 3ms/step\n",
      "Epoch 23/10000\n",
      "1126/1126 - 4s - loss: 0.6857 - accuracy: 0.6092 - val_loss: 0.6788 - val_accuracy: 0.6200 - 4s/epoch - 3ms/step\n",
      "Epoch 24/10000\n",
      "1126/1126 - 2s - loss: 0.6829 - accuracy: 0.6114 - val_loss: 0.6696 - val_accuracy: 0.6195 - 2s/epoch - 2ms/step\n",
      "Epoch 25/10000\n",
      "1126/1126 - 2s - loss: 0.6787 - accuracy: 0.6105 - val_loss: 0.6737 - val_accuracy: 0.6210 - 2s/epoch - 2ms/step\n",
      "Epoch 26/10000\n",
      "1126/1126 - 2s - loss: 0.6792 - accuracy: 0.6124 - val_loss: 0.6669 - val_accuracy: 0.6139 - 2s/epoch - 2ms/step\n",
      "Epoch 27/10000\n",
      "1126/1126 - 2s - loss: 0.6765 - accuracy: 0.6153 - val_loss: 0.6662 - val_accuracy: 0.6141 - 2s/epoch - 2ms/step\n",
      "Epoch 28/10000\n",
      "1126/1126 - 3s - loss: 0.6748 - accuracy: 0.6194 - val_loss: 0.6682 - val_accuracy: 0.6252 - 3s/epoch - 3ms/step\n",
      "Epoch 29/10000\n",
      "1126/1126 - 2s - loss: 0.6732 - accuracy: 0.6175 - val_loss: 0.6695 - val_accuracy: 0.6195 - 2s/epoch - 2ms/step\n",
      "Epoch 30/10000\n",
      "1126/1126 - 2s - loss: 0.6719 - accuracy: 0.6165 - val_loss: 0.6606 - val_accuracy: 0.6258 - 2s/epoch - 2ms/step\n",
      "Epoch 31/10000\n",
      "1126/1126 - 3s - loss: 0.6713 - accuracy: 0.6184 - val_loss: 0.6621 - val_accuracy: 0.6235 - 3s/epoch - 3ms/step\n",
      "Epoch 32/10000\n",
      "1126/1126 - 2s - loss: 0.6687 - accuracy: 0.6194 - val_loss: 0.6582 - val_accuracy: 0.6243 - 2s/epoch - 2ms/step\n",
      "Epoch 33/10000\n",
      "1126/1126 - 2s - loss: 0.6678 - accuracy: 0.6189 - val_loss: 0.6583 - val_accuracy: 0.6223 - 2s/epoch - 2ms/step\n",
      "Epoch 34/10000\n",
      "1126/1126 - 2s - loss: 0.6685 - accuracy: 0.6218 - val_loss: 0.6565 - val_accuracy: 0.6265 - 2s/epoch - 2ms/step\n",
      "Epoch 35/10000\n",
      "1126/1126 - 2s - loss: 0.6670 - accuracy: 0.6184 - val_loss: 0.6595 - val_accuracy: 0.6208 - 2s/epoch - 2ms/step\n",
      "Epoch 36/10000\n",
      "1126/1126 - 2s - loss: 0.6663 - accuracy: 0.6152 - val_loss: 0.6576 - val_accuracy: 0.6224 - 2s/epoch - 2ms/step\n",
      "Epoch 37/10000\n",
      "1126/1126 - 2s - loss: 0.6635 - accuracy: 0.6182 - val_loss: 0.6535 - val_accuracy: 0.6270 - 2s/epoch - 2ms/step\n",
      "Epoch 38/10000\n",
      "1126/1126 - 2s - loss: 0.6621 - accuracy: 0.6190 - val_loss: 0.6597 - val_accuracy: 0.6270 - 2s/epoch - 2ms/step\n",
      "Epoch 39/10000\n",
      "1126/1126 - 3s - loss: 0.6623 - accuracy: 0.6220 - val_loss: 0.6562 - val_accuracy: 0.6221 - 3s/epoch - 2ms/step\n",
      "Epoch 40/10000\n",
      "1126/1126 - 3s - loss: 0.6618 - accuracy: 0.6175 - val_loss: 0.6539 - val_accuracy: 0.6250 - 3s/epoch - 2ms/step\n",
      "Epoch 41/10000\n",
      "1126/1126 - 3s - loss: 0.6611 - accuracy: 0.6211 - val_loss: 0.6524 - val_accuracy: 0.6271 - 3s/epoch - 2ms/step\n",
      "Epoch 42/10000\n",
      "1126/1126 - 3s - loss: 0.6612 - accuracy: 0.6203 - val_loss: 0.6579 - val_accuracy: 0.6162 - 3s/epoch - 2ms/step\n",
      "Epoch 43/10000\n",
      "1126/1126 - 2s - loss: 0.6620 - accuracy: 0.6214 - val_loss: 0.6512 - val_accuracy: 0.6164 - 2s/epoch - 2ms/step\n",
      "Epoch 44/10000\n",
      "1126/1126 - 2s - loss: 0.6591 - accuracy: 0.6185 - val_loss: 0.6559 - val_accuracy: 0.6155 - 2s/epoch - 2ms/step\n",
      "Epoch 45/10000\n",
      "1126/1126 - 2s - loss: 0.6599 - accuracy: 0.6167 - val_loss: 0.6545 - val_accuracy: 0.6277 - 2s/epoch - 2ms/step\n",
      "Epoch 46/10000\n",
      "1126/1126 - 2s - loss: 0.6580 - accuracy: 0.6215 - val_loss: 0.6505 - val_accuracy: 0.6278 - 2s/epoch - 2ms/step\n",
      "Epoch 47/10000\n",
      "1126/1126 - 2s - loss: 0.6567 - accuracy: 0.6187 - val_loss: 0.6500 - val_accuracy: 0.6258 - 2s/epoch - 2ms/step\n",
      "Epoch 48/10000\n",
      "1126/1126 - 2s - loss: 0.6568 - accuracy: 0.6197 - val_loss: 0.6490 - val_accuracy: 0.6304 - 2s/epoch - 2ms/step\n",
      "Epoch 49/10000\n",
      "1126/1126 - 2s - loss: 0.6577 - accuracy: 0.6271 - val_loss: 0.6468 - val_accuracy: 0.6396 - 2s/epoch - 2ms/step\n",
      "Epoch 50/10000\n",
      "1126/1126 - 2s - loss: 0.6553 - accuracy: 0.6284 - val_loss: 0.6477 - val_accuracy: 0.6338 - 2s/epoch - 2ms/step\n",
      "Epoch 51/10000\n",
      "1126/1126 - 2s - loss: 0.6546 - accuracy: 0.6259 - val_loss: 0.6479 - val_accuracy: 0.6360 - 2s/epoch - 2ms/step\n",
      "Epoch 52/10000\n",
      "1126/1126 - 2s - loss: 0.6536 - accuracy: 0.6282 - val_loss: 0.6475 - val_accuracy: 0.6333 - 2s/epoch - 2ms/step\n",
      "Epoch 53/10000\n",
      "1126/1126 - 2s - loss: 0.6535 - accuracy: 0.6273 - val_loss: 0.6474 - val_accuracy: 0.6326 - 2s/epoch - 2ms/step\n",
      "Epoch 54/10000\n",
      "1126/1126 - 2s - loss: 0.6545 - accuracy: 0.6274 - val_loss: 0.6507 - val_accuracy: 0.6347 - 2s/epoch - 2ms/step\n",
      "Epoch 55/10000\n",
      "1126/1126 - 4s - loss: 0.6540 - accuracy: 0.6273 - val_loss: 0.6471 - val_accuracy: 0.6302 - 4s/epoch - 3ms/step\n",
      "Epoch 56/10000\n",
      "1126/1126 - 3s - loss: 0.6539 - accuracy: 0.6223 - val_loss: 0.6480 - val_accuracy: 0.6301 - 3s/epoch - 2ms/step\n",
      "Epoch 57/10000\n",
      "1126/1126 - 2s - loss: 0.6529 - accuracy: 0.6248 - val_loss: 0.6452 - val_accuracy: 0.6309 - 2s/epoch - 2ms/step\n",
      "Epoch 58/10000\n",
      "1126/1126 - 3s - loss: 0.6524 - accuracy: 0.6219 - val_loss: 0.6453 - val_accuracy: 0.6271 - 3s/epoch - 3ms/step\n",
      "Epoch 59/10000\n",
      "1126/1126 - 4s - loss: 0.6518 - accuracy: 0.6222 - val_loss: 0.6488 - val_accuracy: 0.6259 - 4s/epoch - 3ms/step\n",
      "Epoch 60/10000\n",
      "1126/1126 - 3s - loss: 0.6514 - accuracy: 0.6229 - val_loss: 0.6449 - val_accuracy: 0.6282 - 3s/epoch - 3ms/step\n",
      "Epoch 61/10000\n",
      "1126/1126 - 3s - loss: 0.6521 - accuracy: 0.6213 - val_loss: 0.6477 - val_accuracy: 0.6226 - 3s/epoch - 2ms/step\n",
      "Epoch 62/10000\n",
      "1126/1126 - 2s - loss: 0.6515 - accuracy: 0.6204 - val_loss: 0.6448 - val_accuracy: 0.6289 - 2s/epoch - 2ms/step\n",
      "Epoch 63/10000\n",
      "1126/1126 - 3s - loss: 0.6498 - accuracy: 0.6238 - val_loss: 0.6437 - val_accuracy: 0.6334 - 3s/epoch - 2ms/step\n",
      "Epoch 64/10000\n",
      "1126/1126 - 3s - loss: 0.6498 - accuracy: 0.6261 - val_loss: 0.6491 - val_accuracy: 0.6241 - 3s/epoch - 3ms/step\n",
      "Epoch 65/10000\n",
      "1126/1126 - 3s - loss: 0.6495 - accuracy: 0.6266 - val_loss: 0.6452 - val_accuracy: 0.6331 - 3s/epoch - 3ms/step\n",
      "Epoch 66/10000\n",
      "1126/1126 - 2s - loss: 0.6491 - accuracy: 0.6267 - val_loss: 0.6432 - val_accuracy: 0.6254 - 2s/epoch - 2ms/step\n",
      "Epoch 67/10000\n",
      "1126/1126 - 2s - loss: 0.6508 - accuracy: 0.6240 - val_loss: 0.6455 - val_accuracy: 0.6276 - 2s/epoch - 2ms/step\n",
      "Epoch 68/10000\n",
      "1126/1126 - 2s - loss: 0.6506 - accuracy: 0.6214 - val_loss: 0.6458 - val_accuracy: 0.6177 - 2s/epoch - 2ms/step\n",
      "Epoch 69/10000\n",
      "1126/1126 - 2s - loss: 0.6502 - accuracy: 0.6175 - val_loss: 0.6438 - val_accuracy: 0.6213 - 2s/epoch - 2ms/step\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Iterazione 38/128 (03-11-2023_10-54-23)\n",
      "hidden_layer_size: 100\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 1\n",
      "activation_hidden: sigmoid\n",
      "activation_output: sigmoid\n",
      "batch_size: 100\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 20\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 63.96%\n",
      "La recall di questo modello sul validation set è: 37.45%\n",
      "La f1 di questo modello sul validation set è: 44.57%\n",
      "La balanced accuracy di questo modello sul validation set è: 59.07%\n",
      "La precision di questo modello sul validation set è: 55.04%\n",
      "La AUC di questo modello sul validation set è: 63.78%\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.67    0.81      0.73  7,670.00\n",
      "1                  0.55    0.37      0.45  4,841.00\n",
      "accuracy           0.64    0.64      0.64      0.64\n",
      "macro avg          0.61    0.59      0.59 12,511.00\n",
      "weighted avg       0.62    0.64      0.62 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         6189         1481\n",
      "Actual 1         3028         1813\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 65.94% con il modello:\n",
      "1 - AC65.94%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 62.97% con il modello:\n",
      "1 - BA62.97%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 64.08% con il modello:\n",
      "13 - PR64.08%, L20, mV-10, MV10, P2, AHsigmoid, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-26-48).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "113/113 - 2s - loss: 10.7092 - accuracy: 0.3887 - val_loss: 8.7979 - val_accuracy: 0.3870 - 2s/epoch - 13ms/step\n",
      "Epoch 2/10000\n",
      "113/113 - 0s - loss: 6.8722 - accuracy: 0.3903 - val_loss: 4.9977 - val_accuracy: 0.3909 - 395ms/epoch - 3ms/step\n",
      "Epoch 3/10000\n",
      "113/113 - 0s - loss: 3.4488 - accuracy: 0.4384 - val_loss: 2.3127 - val_accuracy: 0.5125 - 452ms/epoch - 4ms/step\n",
      "Epoch 4/10000\n",
      "113/113 - 0s - loss: 1.7980 - accuracy: 0.5300 - val_loss: 1.5077 - val_accuracy: 0.5283 - 415ms/epoch - 4ms/step\n",
      "Epoch 5/10000\n",
      "113/113 - 0s - loss: 1.4335 - accuracy: 0.5399 - val_loss: 1.3907 - val_accuracy: 0.5396 - 415ms/epoch - 4ms/step\n",
      "Epoch 6/10000\n",
      "113/113 - 0s - loss: 1.3759 - accuracy: 0.5476 - val_loss: 1.3552 - val_accuracy: 0.5424 - 405ms/epoch - 4ms/step\n",
      "Epoch 7/10000\n",
      "113/113 - 0s - loss: 1.3430 - accuracy: 0.5487 - val_loss: 1.3238 - val_accuracy: 0.5435 - 393ms/epoch - 3ms/step\n",
      "Epoch 8/10000\n",
      "113/113 - 0s - loss: 1.3118 - accuracy: 0.5485 - val_loss: 1.2900 - val_accuracy: 0.5414 - 417ms/epoch - 4ms/step\n",
      "Epoch 9/10000\n",
      "113/113 - 0s - loss: 1.2862 - accuracy: 0.5484 - val_loss: 1.2645 - val_accuracy: 0.5472 - 399ms/epoch - 4ms/step\n",
      "Epoch 10/10000\n",
      "113/113 - 0s - loss: 1.2633 - accuracy: 0.5510 - val_loss: 1.2282 - val_accuracy: 0.5475 - 364ms/epoch - 3ms/step\n",
      "Epoch 11/10000\n",
      "113/113 - 0s - loss: 1.2323 - accuracy: 0.5538 - val_loss: 1.1963 - val_accuracy: 0.5556 - 374ms/epoch - 3ms/step\n",
      "Epoch 12/10000\n",
      "113/113 - 0s - loss: 1.2008 - accuracy: 0.5584 - val_loss: 1.1734 - val_accuracy: 0.5584 - 421ms/epoch - 4ms/step\n",
      "Epoch 13/10000\n",
      "113/113 - 0s - loss: 1.1704 - accuracy: 0.5657 - val_loss: 1.1507 - val_accuracy: 0.5667 - 453ms/epoch - 4ms/step\n",
      "Epoch 14/10000\n",
      "113/113 - 0s - loss: 1.1529 - accuracy: 0.5686 - val_loss: 1.1365 - val_accuracy: 0.5703 - 449ms/epoch - 4ms/step\n",
      "Epoch 15/10000\n",
      "113/113 - 0s - loss: 1.1449 - accuracy: 0.5698 - val_loss: 1.1191 - val_accuracy: 0.5692 - 397ms/epoch - 4ms/step\n",
      "Epoch 16/10000\n",
      "113/113 - 0s - loss: 1.1246 - accuracy: 0.5705 - val_loss: 1.1055 - val_accuracy: 0.5706 - 457ms/epoch - 4ms/step\n",
      "Epoch 17/10000\n",
      "113/113 - 0s - loss: 1.1097 - accuracy: 0.5719 - val_loss: 1.0982 - val_accuracy: 0.5695 - 434ms/epoch - 4ms/step\n",
      "Epoch 18/10000\n",
      "113/113 - 0s - loss: 1.0975 - accuracy: 0.5763 - val_loss: 1.0872 - val_accuracy: 0.5741 - 390ms/epoch - 3ms/step\n",
      "Epoch 19/10000\n",
      "113/113 - 0s - loss: 1.0808 - accuracy: 0.5779 - val_loss: 1.0728 - val_accuracy: 0.5744 - 426ms/epoch - 4ms/step\n",
      "Epoch 20/10000\n",
      "113/113 - 0s - loss: 1.0684 - accuracy: 0.5769 - val_loss: 1.0567 - val_accuracy: 0.5745 - 441ms/epoch - 4ms/step\n",
      "Epoch 21/10000\n",
      "113/113 - 0s - loss: 1.0532 - accuracy: 0.5772 - val_loss: 1.0402 - val_accuracy: 0.5726 - 437ms/epoch - 4ms/step\n",
      "Epoch 22/10000\n",
      "113/113 - 1s - loss: 1.0348 - accuracy: 0.5795 - val_loss: 1.0146 - val_accuracy: 0.5785 - 596ms/epoch - 5ms/step\n",
      "Epoch 23/10000\n",
      "113/113 - 0s - loss: 1.0115 - accuracy: 0.5798 - val_loss: 0.9901 - val_accuracy: 0.5786 - 404ms/epoch - 4ms/step\n",
      "Epoch 24/10000\n",
      "113/113 - 0s - loss: 0.9963 - accuracy: 0.5793 - val_loss: 0.9798 - val_accuracy: 0.5777 - 387ms/epoch - 3ms/step\n",
      "Epoch 25/10000\n",
      "113/113 - 1s - loss: 0.9837 - accuracy: 0.5778 - val_loss: 0.9737 - val_accuracy: 0.5689 - 509ms/epoch - 5ms/step\n",
      "Epoch 26/10000\n",
      "113/113 - 0s - loss: 0.9617 - accuracy: 0.5755 - val_loss: 0.9472 - val_accuracy: 0.5671 - 363ms/epoch - 3ms/step\n",
      "Epoch 27/10000\n",
      "113/113 - 0s - loss: 0.9435 - accuracy: 0.5713 - val_loss: 0.9332 - val_accuracy: 0.5661 - 391ms/epoch - 3ms/step\n",
      "Epoch 28/10000\n",
      "113/113 - 0s - loss: 0.9357 - accuracy: 0.5728 - val_loss: 0.9372 - val_accuracy: 0.5670 - 354ms/epoch - 3ms/step\n",
      "Epoch 29/10000\n",
      "113/113 - 0s - loss: 0.9365 - accuracy: 0.5765 - val_loss: 0.9229 - val_accuracy: 0.5731 - 347ms/epoch - 3ms/step\n",
      "Epoch 30/10000\n",
      "113/113 - 0s - loss: 0.9242 - accuracy: 0.5776 - val_loss: 0.9063 - val_accuracy: 0.5713 - 357ms/epoch - 3ms/step\n",
      "Epoch 31/10000\n",
      "113/113 - 0s - loss: 0.9152 - accuracy: 0.5752 - val_loss: 0.8983 - val_accuracy: 0.5711 - 367ms/epoch - 3ms/step\n",
      "Epoch 32/10000\n",
      "113/113 - 0s - loss: 0.9073 - accuracy: 0.5739 - val_loss: 0.8955 - val_accuracy: 0.5710 - 364ms/epoch - 3ms/step\n",
      "Epoch 33/10000\n",
      "113/113 - 0s - loss: 0.8991 - accuracy: 0.5755 - val_loss: 0.8913 - val_accuracy: 0.5715 - 359ms/epoch - 3ms/step\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Iterazione 39/128 (03-11-2023_10-54-40)\n",
      "hidden_layer_size: 100\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 1\n",
      "activation_hidden: sigmoid\n",
      "activation_output: sigmoid\n",
      "batch_size: 1000\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 10\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 57.86%\n",
      "La recall di questo modello sul validation set è: 56.56%\n",
      "La f1 di questo modello sul validation set è: 50.95%\n",
      "La balanced accuracy di questo modello sul validation set è: 57.62%\n",
      "La precision di questo modello sul validation set è: 46.35%\n",
      "La AUC di questo modello sul validation set è: 56.45%\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.68    0.59      0.63  7,670.00\n",
      "1                  0.46    0.57      0.51  4,841.00\n",
      "accuracy           0.58    0.58      0.58      0.58\n",
      "macro avg          0.57    0.58      0.57 12,511.00\n",
      "weighted avg       0.60    0.58      0.58 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         4501         3169\n",
      "Actual 1         2103         2738\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 65.94% con il modello:\n",
      "1 - AC65.94%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 62.97% con il modello:\n",
      "1 - BA62.97%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 64.08% con il modello:\n",
      "13 - PR64.08%, L20, mV-10, MV10, P2, AHsigmoid, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-26-48).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "113/113 - 3s - loss: 10.7092 - accuracy: 0.3887 - val_loss: 8.7979 - val_accuracy: 0.3870 - 3s/epoch - 23ms/step\n",
      "Epoch 2/10000\n",
      "113/113 - 0s - loss: 6.8722 - accuracy: 0.3903 - val_loss: 4.9977 - val_accuracy: 0.3909 - 480ms/epoch - 4ms/step\n",
      "Epoch 3/10000\n",
      "113/113 - 0s - loss: 3.4488 - accuracy: 0.4384 - val_loss: 2.3127 - val_accuracy: 0.5125 - 493ms/epoch - 4ms/step\n",
      "Epoch 4/10000\n",
      "113/113 - 1s - loss: 1.7980 - accuracy: 0.5300 - val_loss: 1.5077 - val_accuracy: 0.5283 - 590ms/epoch - 5ms/step\n",
      "Epoch 5/10000\n",
      "113/113 - 1s - loss: 1.4335 - accuracy: 0.5399 - val_loss: 1.3907 - val_accuracy: 0.5396 - 572ms/epoch - 5ms/step\n",
      "Epoch 6/10000\n",
      "113/113 - 1s - loss: 1.3759 - accuracy: 0.5476 - val_loss: 1.3552 - val_accuracy: 0.5424 - 505ms/epoch - 4ms/step\n",
      "Epoch 7/10000\n",
      "113/113 - 1s - loss: 1.3430 - accuracy: 0.5487 - val_loss: 1.3238 - val_accuracy: 0.5435 - 577ms/epoch - 5ms/step\n",
      "Epoch 8/10000\n",
      "113/113 - 1s - loss: 1.3118 - accuracy: 0.5485 - val_loss: 1.2900 - val_accuracy: 0.5414 - 508ms/epoch - 4ms/step\n",
      "Epoch 9/10000\n",
      "113/113 - 0s - loss: 1.2862 - accuracy: 0.5484 - val_loss: 1.2645 - val_accuracy: 0.5472 - 471ms/epoch - 4ms/step\n",
      "Epoch 10/10000\n",
      "113/113 - 1s - loss: 1.2633 - accuracy: 0.5510 - val_loss: 1.2282 - val_accuracy: 0.5475 - 504ms/epoch - 4ms/step\n",
      "Epoch 11/10000\n",
      "113/113 - 1s - loss: 1.2323 - accuracy: 0.5538 - val_loss: 1.1963 - val_accuracy: 0.5556 - 532ms/epoch - 5ms/step\n",
      "Epoch 12/10000\n",
      "113/113 - 1s - loss: 1.2008 - accuracy: 0.5584 - val_loss: 1.1734 - val_accuracy: 0.5584 - 604ms/epoch - 5ms/step\n",
      "Epoch 13/10000\n",
      "113/113 - 1s - loss: 1.1704 - accuracy: 0.5657 - val_loss: 1.1507 - val_accuracy: 0.5667 - 524ms/epoch - 5ms/step\n",
      "Epoch 14/10000\n",
      "113/113 - 1s - loss: 1.1529 - accuracy: 0.5686 - val_loss: 1.1365 - val_accuracy: 0.5703 - 521ms/epoch - 5ms/step\n",
      "Epoch 15/10000\n",
      "113/113 - 0s - loss: 1.1449 - accuracy: 0.5698 - val_loss: 1.1191 - val_accuracy: 0.5692 - 399ms/epoch - 4ms/step\n",
      "Epoch 16/10000\n",
      "113/113 - 0s - loss: 1.1246 - accuracy: 0.5705 - val_loss: 1.1055 - val_accuracy: 0.5706 - 412ms/epoch - 4ms/step\n",
      "Epoch 17/10000\n",
      "113/113 - 0s - loss: 1.1097 - accuracy: 0.5719 - val_loss: 1.0982 - val_accuracy: 0.5695 - 483ms/epoch - 4ms/step\n",
      "Epoch 18/10000\n",
      "113/113 - 0s - loss: 1.0975 - accuracy: 0.5763 - val_loss: 1.0872 - val_accuracy: 0.5741 - 468ms/epoch - 4ms/step\n",
      "Epoch 19/10000\n",
      "113/113 - 0s - loss: 1.0808 - accuracy: 0.5779 - val_loss: 1.0728 - val_accuracy: 0.5744 - 441ms/epoch - 4ms/step\n",
      "Epoch 20/10000\n",
      "113/113 - 0s - loss: 1.0684 - accuracy: 0.5769 - val_loss: 1.0567 - val_accuracy: 0.5745 - 443ms/epoch - 4ms/step\n",
      "Epoch 21/10000\n",
      "113/113 - 1s - loss: 1.0532 - accuracy: 0.5772 - val_loss: 1.0402 - val_accuracy: 0.5726 - 528ms/epoch - 5ms/step\n",
      "Epoch 22/10000\n",
      "113/113 - 0s - loss: 1.0348 - accuracy: 0.5795 - val_loss: 1.0146 - val_accuracy: 0.5785 - 465ms/epoch - 4ms/step\n",
      "Epoch 23/10000\n",
      "113/113 - 0s - loss: 1.0115 - accuracy: 0.5798 - val_loss: 0.9901 - val_accuracy: 0.5786 - 452ms/epoch - 4ms/step\n",
      "Epoch 24/10000\n",
      "113/113 - 0s - loss: 0.9963 - accuracy: 0.5793 - val_loss: 0.9798 - val_accuracy: 0.5777 - 433ms/epoch - 4ms/step\n",
      "Epoch 25/10000\n",
      "113/113 - 0s - loss: 0.9837 - accuracy: 0.5778 - val_loss: 0.9737 - val_accuracy: 0.5689 - 446ms/epoch - 4ms/step\n",
      "Epoch 26/10000\n",
      "113/113 - 1s - loss: 0.9617 - accuracy: 0.5755 - val_loss: 0.9472 - val_accuracy: 0.5671 - 543ms/epoch - 5ms/step\n",
      "Epoch 27/10000\n",
      "113/113 - 0s - loss: 0.9435 - accuracy: 0.5713 - val_loss: 0.9332 - val_accuracy: 0.5661 - 443ms/epoch - 4ms/step\n",
      "Epoch 28/10000\n",
      "113/113 - 0s - loss: 0.9357 - accuracy: 0.5728 - val_loss: 0.9372 - val_accuracy: 0.5670 - 471ms/epoch - 4ms/step\n",
      "Epoch 29/10000\n",
      "113/113 - 0s - loss: 0.9365 - accuracy: 0.5765 - val_loss: 0.9229 - val_accuracy: 0.5731 - 378ms/epoch - 3ms/step\n",
      "Epoch 30/10000\n",
      "113/113 - 0s - loss: 0.9242 - accuracy: 0.5776 - val_loss: 0.9063 - val_accuracy: 0.5713 - 335ms/epoch - 3ms/step\n",
      "Epoch 31/10000\n",
      "113/113 - 0s - loss: 0.9152 - accuracy: 0.5752 - val_loss: 0.8983 - val_accuracy: 0.5711 - 327ms/epoch - 3ms/step\n",
      "Epoch 32/10000\n",
      "113/113 - 0s - loss: 0.9073 - accuracy: 0.5739 - val_loss: 0.8955 - val_accuracy: 0.5710 - 348ms/epoch - 3ms/step\n",
      "Epoch 33/10000\n",
      "113/113 - 0s - loss: 0.8991 - accuracy: 0.5755 - val_loss: 0.8913 - val_accuracy: 0.5715 - 431ms/epoch - 4ms/step\n",
      "Epoch 34/10000\n",
      "113/113 - 0s - loss: 0.8924 - accuracy: 0.5749 - val_loss: 0.8865 - val_accuracy: 0.5706 - 346ms/epoch - 3ms/step\n",
      "Epoch 35/10000\n",
      "113/113 - 0s - loss: 0.8928 - accuracy: 0.5739 - val_loss: 0.8828 - val_accuracy: 0.5693 - 375ms/epoch - 3ms/step\n",
      "Epoch 36/10000\n",
      "113/113 - 0s - loss: 0.8831 - accuracy: 0.5729 - val_loss: 0.8726 - val_accuracy: 0.5697 - 360ms/epoch - 3ms/step\n",
      "Epoch 37/10000\n",
      "113/113 - 0s - loss: 0.8772 - accuracy: 0.5726 - val_loss: 0.8641 - val_accuracy: 0.5688 - 370ms/epoch - 3ms/step\n",
      "Epoch 38/10000\n",
      "113/113 - 0s - loss: 0.8725 - accuracy: 0.5725 - val_loss: 0.8588 - val_accuracy: 0.5689 - 360ms/epoch - 3ms/step\n",
      "Epoch 39/10000\n",
      "113/113 - 0s - loss: 0.8698 - accuracy: 0.5714 - val_loss: 0.8527 - val_accuracy: 0.5913 - 418ms/epoch - 4ms/step\n",
      "Epoch 40/10000\n",
      "113/113 - 0s - loss: 0.8615 - accuracy: 0.5740 - val_loss: 0.8524 - val_accuracy: 0.5695 - 383ms/epoch - 3ms/step\n",
      "Epoch 41/10000\n",
      "113/113 - 0s - loss: 0.8598 - accuracy: 0.5774 - val_loss: 0.8489 - val_accuracy: 0.5907 - 338ms/epoch - 3ms/step\n",
      "Epoch 42/10000\n",
      "113/113 - 0s - loss: 0.8549 - accuracy: 0.5779 - val_loss: 0.8487 - val_accuracy: 0.5690 - 396ms/epoch - 4ms/step\n",
      "Epoch 43/10000\n",
      "113/113 - 0s - loss: 0.8539 - accuracy: 0.5861 - val_loss: 0.8406 - val_accuracy: 0.5951 - 351ms/epoch - 3ms/step\n",
      "Epoch 44/10000\n",
      "113/113 - 0s - loss: 0.8500 - accuracy: 0.5824 - val_loss: 0.8477 - val_accuracy: 0.5916 - 377ms/epoch - 3ms/step\n",
      "Epoch 45/10000\n",
      "113/113 - 0s - loss: 0.8465 - accuracy: 0.5865 - val_loss: 0.8335 - val_accuracy: 0.5917 - 366ms/epoch - 3ms/step\n",
      "Epoch 46/10000\n",
      "113/113 - 0s - loss: 0.8404 - accuracy: 0.5901 - val_loss: 0.8311 - val_accuracy: 0.5935 - 369ms/epoch - 3ms/step\n",
      "Epoch 47/10000\n",
      "113/113 - 0s - loss: 0.8363 - accuracy: 0.5899 - val_loss: 0.8222 - val_accuracy: 0.5920 - 424ms/epoch - 4ms/step\n",
      "Epoch 48/10000\n",
      "113/113 - 1s - loss: 0.8350 - accuracy: 0.5890 - val_loss: 0.8235 - val_accuracy: 0.5937 - 507ms/epoch - 4ms/step\n",
      "Epoch 49/10000\n",
      "113/113 - 0s - loss: 0.8283 - accuracy: 0.5912 - val_loss: 0.8199 - val_accuracy: 0.5939 - 435ms/epoch - 4ms/step\n",
      "Epoch 50/10000\n",
      "113/113 - 0s - loss: 0.8272 - accuracy: 0.5905 - val_loss: 0.8165 - val_accuracy: 0.5942 - 419ms/epoch - 4ms/step\n",
      "Epoch 51/10000\n",
      "113/113 - 0s - loss: 0.8244 - accuracy: 0.5895 - val_loss: 0.8092 - val_accuracy: 0.5934 - 439ms/epoch - 4ms/step\n",
      "Epoch 52/10000\n",
      "113/113 - 0s - loss: 0.8170 - accuracy: 0.5904 - val_loss: 0.8090 - val_accuracy: 0.5943 - 477ms/epoch - 4ms/step\n",
      "Epoch 53/10000\n",
      "113/113 - 0s - loss: 0.8169 - accuracy: 0.5914 - val_loss: 0.8085 - val_accuracy: 0.5930 - 437ms/epoch - 4ms/step\n",
      "Epoch 54/10000\n",
      "113/113 - 0s - loss: 0.8154 - accuracy: 0.5900 - val_loss: 0.8031 - val_accuracy: 0.5925 - 451ms/epoch - 4ms/step\n",
      "Epoch 55/10000\n",
      "113/113 - 0s - loss: 0.8157 - accuracy: 0.5922 - val_loss: 0.8038 - val_accuracy: 0.5931 - 452ms/epoch - 4ms/step\n",
      "Epoch 56/10000\n",
      "113/113 - 1s - loss: 0.8068 - accuracy: 0.5904 - val_loss: 0.7943 - val_accuracy: 0.5936 - 568ms/epoch - 5ms/step\n",
      "Epoch 57/10000\n",
      "113/113 - 0s - loss: 0.8065 - accuracy: 0.5944 - val_loss: 0.7862 - val_accuracy: 0.6000 - 449ms/epoch - 4ms/step\n",
      "Epoch 58/10000\n",
      "113/113 - 0s - loss: 0.7968 - accuracy: 0.5949 - val_loss: 0.7861 - val_accuracy: 0.5995 - 338ms/epoch - 3ms/step\n",
      "Epoch 59/10000\n",
      "113/113 - 0s - loss: 0.7938 - accuracy: 0.5965 - val_loss: 0.7825 - val_accuracy: 0.6011 - 360ms/epoch - 3ms/step\n",
      "Epoch 60/10000\n",
      "113/113 - 0s - loss: 0.7915 - accuracy: 0.5982 - val_loss: 0.7789 - val_accuracy: 0.6008 - 429ms/epoch - 4ms/step\n",
      "Epoch 61/10000\n",
      "113/113 - 0s - loss: 0.7889 - accuracy: 0.5989 - val_loss: 0.7894 - val_accuracy: 0.6032 - 356ms/epoch - 3ms/step\n",
      "Epoch 62/10000\n",
      "113/113 - 0s - loss: 0.7857 - accuracy: 0.6006 - val_loss: 0.7763 - val_accuracy: 0.6041 - 411ms/epoch - 4ms/step\n",
      "Epoch 63/10000\n",
      "113/113 - 1s - loss: 0.7826 - accuracy: 0.6011 - val_loss: 0.7712 - val_accuracy: 0.6035 - 557ms/epoch - 5ms/step\n",
      "Epoch 64/10000\n",
      "113/113 - 0s - loss: 0.7783 - accuracy: 0.5982 - val_loss: 0.7667 - val_accuracy: 0.6018 - 460ms/epoch - 4ms/step\n",
      "Epoch 65/10000\n",
      "113/113 - 0s - loss: 0.7756 - accuracy: 0.5983 - val_loss: 0.7721 - val_accuracy: 0.6035 - 353ms/epoch - 3ms/step\n",
      "Epoch 66/10000\n",
      "113/113 - 0s - loss: 0.7742 - accuracy: 0.5994 - val_loss: 0.7689 - val_accuracy: 0.6004 - 375ms/epoch - 3ms/step\n",
      "Epoch 67/10000\n",
      "113/113 - 0s - loss: 0.7705 - accuracy: 0.5985 - val_loss: 0.7641 - val_accuracy: 0.6021 - 423ms/epoch - 4ms/step\n",
      "Epoch 68/10000\n",
      "113/113 - 0s - loss: 0.7703 - accuracy: 0.5956 - val_loss: 0.7614 - val_accuracy: 0.5993 - 361ms/epoch - 3ms/step\n",
      "Epoch 69/10000\n",
      "113/113 - 0s - loss: 0.7663 - accuracy: 0.5980 - val_loss: 0.7546 - val_accuracy: 0.6010 - 414ms/epoch - 4ms/step\n",
      "Epoch 70/10000\n",
      "113/113 - 0s - loss: 0.7633 - accuracy: 0.5963 - val_loss: 0.7585 - val_accuracy: 0.5994 - 399ms/epoch - 4ms/step\n",
      "Epoch 71/10000\n",
      "113/113 - 0s - loss: 0.7595 - accuracy: 0.5965 - val_loss: 0.7534 - val_accuracy: 0.6021 - 339ms/epoch - 3ms/step\n",
      "Epoch 72/10000\n",
      "113/113 - 0s - loss: 0.7602 - accuracy: 0.5974 - val_loss: 0.7509 - val_accuracy: 0.6004 - 352ms/epoch - 3ms/step\n",
      "Epoch 73/10000\n",
      "113/113 - 0s - loss: 0.7595 - accuracy: 0.5959 - val_loss: 0.7500 - val_accuracy: 0.5999 - 397ms/epoch - 4ms/step\n",
      "Epoch 74/10000\n",
      "113/113 - 0s - loss: 0.7558 - accuracy: 0.5980 - val_loss: 0.7457 - val_accuracy: 0.6033 - 434ms/epoch - 4ms/step\n",
      "Epoch 75/10000\n",
      "113/113 - 0s - loss: 0.7507 - accuracy: 0.5994 - val_loss: 0.7457 - val_accuracy: 0.6002 - 339ms/epoch - 3ms/step\n",
      "Epoch 76/10000\n",
      "113/113 - 0s - loss: 0.7527 - accuracy: 0.6002 - val_loss: 0.7519 - val_accuracy: 0.6019 - 461ms/epoch - 4ms/step\n",
      "Epoch 77/10000\n",
      "113/113 - 0s - loss: 0.7540 - accuracy: 0.6010 - val_loss: 0.7397 - val_accuracy: 0.6052 - 443ms/epoch - 4ms/step\n",
      "Epoch 78/10000\n",
      "113/113 - 0s - loss: 0.7464 - accuracy: 0.5997 - val_loss: 0.7385 - val_accuracy: 0.6031 - 436ms/epoch - 4ms/step\n",
      "Epoch 79/10000\n",
      "113/113 - 1s - loss: 0.7486 - accuracy: 0.5984 - val_loss: 0.7363 - val_accuracy: 0.6045 - 500ms/epoch - 4ms/step\n",
      "Epoch 80/10000\n",
      "113/113 - 0s - loss: 0.7421 - accuracy: 0.5994 - val_loss: 0.7388 - val_accuracy: 0.6022 - 408ms/epoch - 4ms/step\n",
      "Epoch 81/10000\n",
      "113/113 - 0s - loss: 0.7419 - accuracy: 0.5993 - val_loss: 0.7309 - val_accuracy: 0.6048 - 413ms/epoch - 4ms/step\n",
      "Epoch 82/10000\n",
      "113/113 - 0s - loss: 0.7398 - accuracy: 0.5953 - val_loss: 0.7294 - val_accuracy: 0.6000 - 362ms/epoch - 3ms/step\n",
      "Epoch 83/10000\n",
      "113/113 - 0s - loss: 0.7378 - accuracy: 0.5966 - val_loss: 0.7292 - val_accuracy: 0.5996 - 402ms/epoch - 4ms/step\n",
      "Epoch 84/10000\n",
      "113/113 - 1s - loss: 0.7363 - accuracy: 0.5964 - val_loss: 0.7231 - val_accuracy: 0.6000 - 682ms/epoch - 6ms/step\n",
      "Epoch 85/10000\n",
      "113/113 - 0s - loss: 0.7337 - accuracy: 0.5977 - val_loss: 0.7256 - val_accuracy: 0.6032 - 342ms/epoch - 3ms/step\n",
      "Epoch 86/10000\n",
      "113/113 - 1s - loss: 0.7312 - accuracy: 0.5970 - val_loss: 0.7224 - val_accuracy: 0.6028 - 517ms/epoch - 5ms/step\n",
      "Epoch 87/10000\n",
      "113/113 - 0s - loss: 0.7308 - accuracy: 0.5961 - val_loss: 0.7189 - val_accuracy: 0.6048 - 373ms/epoch - 3ms/step\n",
      "Epoch 88/10000\n",
      "113/113 - 0s - loss: 0.7282 - accuracy: 0.5982 - val_loss: 0.7212 - val_accuracy: 0.6058 - 356ms/epoch - 3ms/step\n",
      "Epoch 89/10000\n",
      "113/113 - 0s - loss: 0.7266 - accuracy: 0.6004 - val_loss: 0.7158 - val_accuracy: 0.6065 - 342ms/epoch - 3ms/step\n",
      "Epoch 90/10000\n",
      "113/113 - 0s - loss: 0.7250 - accuracy: 0.6022 - val_loss: 0.7165 - val_accuracy: 0.6078 - 383ms/epoch - 3ms/step\n",
      "Epoch 91/10000\n",
      "113/113 - 0s - loss: 0.7241 - accuracy: 0.6041 - val_loss: 0.7159 - val_accuracy: 0.6073 - 389ms/epoch - 3ms/step\n",
      "Epoch 92/10000\n",
      "113/113 - 0s - loss: 0.7230 - accuracy: 0.6060 - val_loss: 0.7174 - val_accuracy: 0.6075 - 360ms/epoch - 3ms/step\n",
      "Epoch 93/10000\n",
      "113/113 - 0s - loss: 0.7216 - accuracy: 0.6092 - val_loss: 0.7089 - val_accuracy: 0.6196 - 359ms/epoch - 3ms/step\n",
      "Epoch 94/10000\n",
      "113/113 - 0s - loss: 0.7185 - accuracy: 0.6114 - val_loss: 0.7097 - val_accuracy: 0.6087 - 341ms/epoch - 3ms/step\n",
      "Epoch 95/10000\n",
      "113/113 - 0s - loss: 0.7165 - accuracy: 0.6088 - val_loss: 0.7054 - val_accuracy: 0.6091 - 394ms/epoch - 3ms/step\n",
      "Epoch 96/10000\n",
      "113/113 - 0s - loss: 0.7152 - accuracy: 0.6115 - val_loss: 0.7218 - val_accuracy: 0.6159 - 333ms/epoch - 3ms/step\n",
      "Epoch 97/10000\n",
      "113/113 - 0s - loss: 0.7165 - accuracy: 0.6108 - val_loss: 0.7053 - val_accuracy: 0.6172 - 360ms/epoch - 3ms/step\n",
      "Epoch 98/10000\n",
      "113/113 - 0s - loss: 0.7139 - accuracy: 0.6106 - val_loss: 0.7042 - val_accuracy: 0.6160 - 399ms/epoch - 4ms/step\n",
      "Epoch 99/10000\n",
      "113/113 - 0s - loss: 0.7124 - accuracy: 0.6106 - val_loss: 0.7071 - val_accuracy: 0.6158 - 411ms/epoch - 4ms/step\n",
      "Epoch 100/10000\n",
      "113/113 - 1s - loss: 0.7107 - accuracy: 0.6093 - val_loss: 0.6993 - val_accuracy: 0.6177 - 563ms/epoch - 5ms/step\n",
      "Epoch 101/10000\n",
      "113/113 - 0s - loss: 0.7075 - accuracy: 0.6115 - val_loss: 0.7013 - val_accuracy: 0.6143 - 459ms/epoch - 4ms/step\n",
      "Epoch 102/10000\n",
      "113/113 - 0s - loss: 0.7084 - accuracy: 0.6109 - val_loss: 0.7012 - val_accuracy: 0.6147 - 367ms/epoch - 3ms/step\n",
      "Epoch 103/10000\n",
      "113/113 - 0s - loss: 0.7080 - accuracy: 0.6113 - val_loss: 0.6992 - val_accuracy: 0.6206 - 337ms/epoch - 3ms/step\n",
      "Epoch 104/10000\n",
      "113/113 - 0s - loss: 0.7063 - accuracy: 0.6143 - val_loss: 0.6953 - val_accuracy: 0.6227 - 367ms/epoch - 3ms/step\n",
      "Epoch 105/10000\n",
      "113/113 - 0s - loss: 0.7053 - accuracy: 0.6131 - val_loss: 0.6970 - val_accuracy: 0.6190 - 404ms/epoch - 4ms/step\n",
      "Epoch 106/10000\n",
      "113/113 - 0s - loss: 0.7041 - accuracy: 0.6136 - val_loss: 0.7031 - val_accuracy: 0.6199 - 406ms/epoch - 4ms/step\n",
      "Epoch 107/10000\n",
      "113/113 - 0s - loss: 0.7058 - accuracy: 0.6139 - val_loss: 0.6919 - val_accuracy: 0.6197 - 337ms/epoch - 3ms/step\n",
      "Epoch 108/10000\n",
      "113/113 - 0s - loss: 0.6998 - accuracy: 0.6141 - val_loss: 0.6909 - val_accuracy: 0.6189 - 418ms/epoch - 4ms/step\n",
      "Epoch 109/10000\n",
      "113/113 - 1s - loss: 0.7019 - accuracy: 0.6131 - val_loss: 0.6938 - val_accuracy: 0.6177 - 504ms/epoch - 4ms/step\n",
      "Epoch 110/10000\n",
      "113/113 - 0s - loss: 0.7000 - accuracy: 0.6125 - val_loss: 0.6920 - val_accuracy: 0.6173 - 375ms/epoch - 3ms/step\n",
      "Epoch 111/10000\n",
      "113/113 - 0s - loss: 0.6979 - accuracy: 0.6133 - val_loss: 0.6879 - val_accuracy: 0.6195 - 367ms/epoch - 3ms/step\n",
      "Epoch 112/10000\n",
      "113/113 - 1s - loss: 0.6989 - accuracy: 0.6147 - val_loss: 0.6912 - val_accuracy: 0.6195 - 528ms/epoch - 5ms/step\n",
      "Epoch 113/10000\n",
      "113/113 - 0s - loss: 0.6959 - accuracy: 0.6149 - val_loss: 0.6929 - val_accuracy: 0.6200 - 333ms/epoch - 3ms/step\n",
      "Epoch 114/10000\n",
      "113/113 - 0s - loss: 0.6946 - accuracy: 0.6148 - val_loss: 0.6878 - val_accuracy: 0.6185 - 331ms/epoch - 3ms/step\n",
      "Epoch 115/10000\n",
      "113/113 - 0s - loss: 0.6922 - accuracy: 0.6128 - val_loss: 0.6847 - val_accuracy: 0.6171 - 348ms/epoch - 3ms/step\n",
      "Epoch 116/10000\n",
      "113/113 - 0s - loss: 0.6921 - accuracy: 0.6127 - val_loss: 0.6862 - val_accuracy: 0.6171 - 366ms/epoch - 3ms/step\n",
      "Epoch 117/10000\n",
      "113/113 - 0s - loss: 0.6907 - accuracy: 0.6139 - val_loss: 0.6865 - val_accuracy: 0.6172 - 382ms/epoch - 3ms/step\n",
      "Epoch 118/10000\n",
      "113/113 - 0s - loss: 0.6884 - accuracy: 0.6139 - val_loss: 0.6809 - val_accuracy: 0.6176 - 363ms/epoch - 3ms/step\n",
      "Epoch 119/10000\n",
      "113/113 - 0s - loss: 0.6886 - accuracy: 0.6138 - val_loss: 0.6817 - val_accuracy: 0.6186 - 482ms/epoch - 4ms/step\n",
      "Epoch 120/10000\n",
      "113/113 - 0s - loss: 0.6868 - accuracy: 0.6135 - val_loss: 0.6842 - val_accuracy: 0.6166 - 438ms/epoch - 4ms/step\n",
      "Epoch 121/10000\n",
      "113/113 - 0s - loss: 0.6870 - accuracy: 0.6132 - val_loss: 0.6816 - val_accuracy: 0.6167 - 374ms/epoch - 3ms/step\n",
      "Epoch 122/10000\n",
      "113/113 - 0s - loss: 0.6842 - accuracy: 0.6136 - val_loss: 0.6801 - val_accuracy: 0.6183 - 409ms/epoch - 4ms/step\n",
      "Epoch 123/10000\n",
      "113/113 - 0s - loss: 0.6847 - accuracy: 0.6137 - val_loss: 0.6779 - val_accuracy: 0.6183 - 408ms/epoch - 4ms/step\n",
      "Epoch 124/10000\n",
      "113/113 - 1s - loss: 0.6850 - accuracy: 0.6138 - val_loss: 0.6800 - val_accuracy: 0.6175 - 587ms/epoch - 5ms/step\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Iterazione 40/128 (03-11-2023_10-55-37)\n",
      "hidden_layer_size: 100\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 1\n",
      "activation_hidden: sigmoid\n",
      "activation_output: sigmoid\n",
      "batch_size: 1000\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 20\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 62.27%\n",
      "La recall di questo modello sul validation set è: 25.16%\n",
      "La f1 di questo modello sul validation set è: 34.04%\n",
      "La balanced accuracy di questo modello sul validation set è: 55.43%\n",
      "La precision di questo modello sul validation set è: 52.61%\n",
      "La AUC di questo modello sul validation set è: 61.29%\n",
      "391/391 [==============================] - 1s 3ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.64    0.86      0.74  7,670.00\n",
      "1                  0.53    0.25      0.34  4,841.00\n",
      "accuracy           0.62    0.62      0.62      0.62\n",
      "macro avg          0.59    0.55      0.54 12,511.00\n",
      "weighted avg       0.60    0.62      0.58 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         6573         1097\n",
      "Actual 1         3623         1218\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 65.94% con il modello:\n",
      "1 - AC65.94%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 62.97% con il modello:\n",
      "1 - BA62.97%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 64.08% con il modello:\n",
      "13 - PR64.08%, L20, mV-10, MV10, P2, AHsigmoid, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-26-48).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "1126/1126 - 6s - loss: 2274994.0000 - accuracy: 0.4561 - val_loss: 197459.0156 - val_accuracy: 0.4549 - 6s/epoch - 5ms/step\n",
      "Epoch 2/10000\n",
      "1126/1126 - 3s - loss: 76200.4688 - accuracy: 0.5324 - val_loss: 36429.2305 - val_accuracy: 0.5242 - 3s/epoch - 3ms/step\n",
      "Epoch 3/10000\n",
      "1126/1126 - 3s - loss: 40644.7930 - accuracy: 0.5664 - val_loss: 28843.0469 - val_accuracy: 0.6018 - 3s/epoch - 3ms/step\n",
      "Epoch 4/10000\n",
      "1126/1126 - 3s - loss: 37952.3906 - accuracy: 0.5691 - val_loss: 37777.9023 - val_accuracy: 0.6103 - 3s/epoch - 3ms/step\n",
      "Epoch 5/10000\n",
      "1126/1126 - 3s - loss: 31566.0547 - accuracy: 0.5743 - val_loss: 28842.0234 - val_accuracy: 0.5526 - 3s/epoch - 2ms/step\n",
      "Epoch 6/10000\n",
      "1126/1126 - 3s - loss: 33346.2812 - accuracy: 0.5752 - val_loss: 41034.4336 - val_accuracy: 0.6111 - 3s/epoch - 2ms/step\n",
      "Epoch 7/10000\n",
      "1126/1126 - 3s - loss: 29559.8477 - accuracy: 0.5775 - val_loss: 38232.2422 - val_accuracy: 0.6093 - 3s/epoch - 3ms/step\n",
      "Epoch 8/10000\n",
      "1126/1126 - 3s - loss: 28745.1094 - accuracy: 0.5781 - val_loss: 45552.4141 - val_accuracy: 0.6130 - 3s/epoch - 2ms/step\n",
      "Epoch 9/10000\n",
      "1126/1126 - 3s - loss: 32488.1211 - accuracy: 0.5781 - val_loss: 10843.5439 - val_accuracy: 0.6249 - 3s/epoch - 2ms/step\n",
      "Epoch 10/10000\n",
      "1126/1126 - 2s - loss: 29804.9980 - accuracy: 0.5777 - val_loss: 14809.0918 - val_accuracy: 0.5883 - 2s/epoch - 2ms/step\n",
      "Epoch 11/10000\n",
      "1126/1126 - 2s - loss: 27824.4961 - accuracy: 0.5804 - val_loss: 18263.5156 - val_accuracy: 0.6204 - 2s/epoch - 2ms/step\n",
      "Epoch 12/10000\n",
      "1126/1126 - 2s - loss: 31007.1133 - accuracy: 0.5813 - val_loss: 39429.5820 - val_accuracy: 0.5117 - 2s/epoch - 2ms/step\n",
      "Epoch 13/10000\n",
      "1126/1126 - 2s - loss: 27117.3047 - accuracy: 0.5821 - val_loss: 17007.3262 - val_accuracy: 0.6116 - 2s/epoch - 2ms/step\n",
      "Epoch 14/10000\n",
      "1126/1126 - 2s - loss: 29963.8633 - accuracy: 0.5807 - val_loss: 14087.0850 - val_accuracy: 0.6307 - 2s/epoch - 2ms/step\n",
      "Epoch 15/10000\n",
      "1126/1126 - 3s - loss: 27667.4883 - accuracy: 0.5836 - val_loss: 24174.5996 - val_accuracy: 0.5228 - 3s/epoch - 2ms/step\n",
      "Epoch 16/10000\n",
      "1126/1126 - 2s - loss: 27957.5586 - accuracy: 0.5824 - val_loss: 21650.3359 - val_accuracy: 0.6236 - 2s/epoch - 2ms/step\n",
      "Epoch 17/10000\n",
      "1126/1126 - 3s - loss: 28286.8691 - accuracy: 0.5814 - val_loss: 51904.9883 - val_accuracy: 0.6133 - 3s/epoch - 3ms/step\n",
      "Epoch 18/10000\n",
      "1126/1126 - 3s - loss: 26190.1719 - accuracy: 0.5856 - val_loss: 38449.1953 - val_accuracy: 0.4832 - 3s/epoch - 3ms/step\n",
      "Epoch 19/10000\n",
      "1126/1126 - 2s - loss: 29738.7656 - accuracy: 0.5841 - val_loss: 30226.8223 - val_accuracy: 0.5492 - 2s/epoch - 2ms/step\n",
      "Epoch 20/10000\n",
      "1126/1126 - 2s - loss: 27899.5117 - accuracy: 0.5850 - val_loss: 28085.8359 - val_accuracy: 0.6199 - 2s/epoch - 2ms/step\n",
      "Epoch 21/10000\n",
      "1126/1126 - 2s - loss: 25987.9180 - accuracy: 0.5850 - val_loss: 8985.8447 - val_accuracy: 0.6325 - 2s/epoch - 2ms/step\n",
      "Epoch 22/10000\n",
      "1126/1126 - 2s - loss: 25479.2012 - accuracy: 0.5860 - val_loss: 14161.1846 - val_accuracy: 0.5535 - 2s/epoch - 2ms/step\n",
      "Epoch 23/10000\n",
      "1126/1126 - 2s - loss: 28426.0137 - accuracy: 0.5852 - val_loss: 12813.7158 - val_accuracy: 0.6366 - 2s/epoch - 2ms/step\n",
      "Epoch 24/10000\n",
      "1126/1126 - 3s - loss: 26667.5273 - accuracy: 0.5837 - val_loss: 33267.4883 - val_accuracy: 0.4771 - 3s/epoch - 2ms/step\n",
      "Epoch 25/10000\n",
      "1126/1126 - 2s - loss: 25570.2852 - accuracy: 0.5851 - val_loss: 10371.2910 - val_accuracy: 0.6022 - 2s/epoch - 2ms/step\n",
      "Epoch 26/10000\n",
      "1126/1126 - 3s - loss: 28144.1875 - accuracy: 0.5858 - val_loss: 8913.8027 - val_accuracy: 0.6476 - 3s/epoch - 2ms/step\n",
      "Epoch 27/10000\n",
      "1126/1126 - 2s - loss: 29037.2910 - accuracy: 0.5852 - val_loss: 41543.4492 - val_accuracy: 0.6167 - 2s/epoch - 2ms/step\n",
      "Epoch 28/10000\n",
      "1126/1126 - 2s - loss: 25845.8438 - accuracy: 0.5862 - val_loss: 8473.8789 - val_accuracy: 0.6354 - 2s/epoch - 2ms/step\n",
      "Epoch 29/10000\n",
      "1126/1126 - 3s - loss: 28643.6582 - accuracy: 0.5837 - val_loss: 27606.6133 - val_accuracy: 0.6156 - 3s/epoch - 2ms/step\n",
      "Epoch 30/10000\n",
      "1126/1126 - 3s - loss: 30750.6895 - accuracy: 0.5834 - val_loss: 51310.9648 - val_accuracy: 0.6175 - 3s/epoch - 2ms/step\n",
      "Epoch 31/10000\n",
      "1126/1126 - 3s - loss: 22954.2383 - accuracy: 0.5884 - val_loss: 24799.5312 - val_accuracy: 0.5526 - 3s/epoch - 2ms/step\n",
      "Epoch 32/10000\n",
      "1126/1126 - 3s - loss: 29042.1465 - accuracy: 0.5841 - val_loss: 8418.4014 - val_accuracy: 0.6265 - 3s/epoch - 2ms/step\n",
      "Epoch 33/10000\n",
      "1126/1126 - 3s - loss: 29513.1680 - accuracy: 0.5857 - val_loss: 56423.5469 - val_accuracy: 0.4692 - 3s/epoch - 2ms/step\n",
      "Epoch 34/10000\n",
      "1126/1126 - 3s - loss: 25598.9434 - accuracy: 0.5863 - val_loss: 26531.9316 - val_accuracy: 0.6209 - 3s/epoch - 2ms/step\n",
      "Epoch 35/10000\n",
      "1126/1126 - 3s - loss: 26877.6465 - accuracy: 0.5865 - val_loss: 48174.6992 - val_accuracy: 0.6167 - 3s/epoch - 2ms/step\n",
      "Epoch 36/10000\n",
      "1126/1126 - 3s - loss: 28621.3477 - accuracy: 0.5865 - val_loss: 13453.5254 - val_accuracy: 0.6403 - 3s/epoch - 2ms/step\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Iterazione 41/128 (03-11-2023_10-57-17)\n",
      "hidden_layer_size: 100\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 2\n",
      "activation_hidden: relu\n",
      "activation_output: sigmoid\n",
      "batch_size: 100\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 10\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 64.76%\n",
      "La recall di questo modello sul validation set è: 46.42%\n",
      "La f1 di questo modello sul validation set è: 50.48%\n",
      "La balanced accuracy di questo modello sul validation set è: 61.38%\n",
      "La precision di questo modello sul validation set è: 55.32%\n",
      "La AUC di questo modello sul validation set è: 61.39%\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.69    0.76      0.73  7,670.00\n",
      "1                  0.55    0.46      0.50  4,841.00\n",
      "accuracy           0.65    0.65      0.65      0.65\n",
      "macro avg          0.62    0.61      0.62 12,511.00\n",
      "weighted avg       0.64    0.65      0.64 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         5855         1815\n",
      "Actual 1         2594         2247\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 65.94% con il modello:\n",
      "1 - AC65.94%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 62.97% con il modello:\n",
      "1 - BA62.97%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 64.08% con il modello:\n",
      "13 - PR64.08%, L20, mV-10, MV10, P2, AHsigmoid, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-26-48).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "1126/1126 - 4s - loss: 2274994.0000 - accuracy: 0.4561 - val_loss: 197459.0156 - val_accuracy: 0.4549 - 4s/epoch - 4ms/step\n",
      "Epoch 2/10000\n",
      "1126/1126 - 3s - loss: 76200.4688 - accuracy: 0.5324 - val_loss: 36429.2305 - val_accuracy: 0.5242 - 3s/epoch - 2ms/step\n",
      "Epoch 3/10000\n",
      "1126/1126 - 3s - loss: 40644.7930 - accuracy: 0.5664 - val_loss: 28843.0469 - val_accuracy: 0.6018 - 3s/epoch - 3ms/step\n",
      "Epoch 4/10000\n",
      "1126/1126 - 4s - loss: 37952.3906 - accuracy: 0.5691 - val_loss: 37777.9023 - val_accuracy: 0.6103 - 4s/epoch - 3ms/step\n",
      "Epoch 5/10000\n",
      "1126/1126 - 3s - loss: 31566.0547 - accuracy: 0.5743 - val_loss: 28842.0234 - val_accuracy: 0.5526 - 3s/epoch - 2ms/step\n",
      "Epoch 6/10000\n",
      "1126/1126 - 3s - loss: 33346.2812 - accuracy: 0.5752 - val_loss: 41034.4336 - val_accuracy: 0.6111 - 3s/epoch - 3ms/step\n",
      "Epoch 7/10000\n",
      "1126/1126 - 3s - loss: 29559.8477 - accuracy: 0.5775 - val_loss: 38232.2422 - val_accuracy: 0.6093 - 3s/epoch - 3ms/step\n",
      "Epoch 8/10000\n",
      "1126/1126 - 3s - loss: 28745.1094 - accuracy: 0.5781 - val_loss: 45552.4141 - val_accuracy: 0.6130 - 3s/epoch - 2ms/step\n",
      "Epoch 9/10000\n",
      "1126/1126 - 3s - loss: 32488.1211 - accuracy: 0.5781 - val_loss: 10843.5439 - val_accuracy: 0.6249 - 3s/epoch - 2ms/step\n",
      "Epoch 10/10000\n",
      "1126/1126 - 2s - loss: 29804.9980 - accuracy: 0.5777 - val_loss: 14809.0918 - val_accuracy: 0.5883 - 2s/epoch - 2ms/step\n",
      "Epoch 11/10000\n",
      "1126/1126 - 3s - loss: 27824.4961 - accuracy: 0.5804 - val_loss: 18263.5156 - val_accuracy: 0.6204 - 3s/epoch - 2ms/step\n",
      "Epoch 12/10000\n",
      "1126/1126 - 3s - loss: 31007.1133 - accuracy: 0.5813 - val_loss: 39429.5820 - val_accuracy: 0.5117 - 3s/epoch - 2ms/step\n",
      "Epoch 13/10000\n",
      "1126/1126 - 3s - loss: 27117.3047 - accuracy: 0.5821 - val_loss: 17007.3262 - val_accuracy: 0.6116 - 3s/epoch - 2ms/step\n",
      "Epoch 14/10000\n",
      "1126/1126 - 3s - loss: 29963.8633 - accuracy: 0.5807 - val_loss: 14087.0850 - val_accuracy: 0.6307 - 3s/epoch - 2ms/step\n",
      "Epoch 15/10000\n",
      "1126/1126 - 2s - loss: 27667.4883 - accuracy: 0.5836 - val_loss: 24174.5996 - val_accuracy: 0.5228 - 2s/epoch - 2ms/step\n",
      "Epoch 16/10000\n",
      "1126/1126 - 2s - loss: 27957.5586 - accuracy: 0.5824 - val_loss: 21650.3359 - val_accuracy: 0.6236 - 2s/epoch - 2ms/step\n",
      "Epoch 17/10000\n",
      "1126/1126 - 2s - loss: 28286.8691 - accuracy: 0.5814 - val_loss: 51904.9883 - val_accuracy: 0.6133 - 2s/epoch - 2ms/step\n",
      "Epoch 18/10000\n",
      "1126/1126 - 2s - loss: 26190.1719 - accuracy: 0.5856 - val_loss: 38449.1953 - val_accuracy: 0.4832 - 2s/epoch - 2ms/step\n",
      "Epoch 19/10000\n",
      "1126/1126 - 2s - loss: 29738.7656 - accuracy: 0.5841 - val_loss: 30226.8223 - val_accuracy: 0.5492 - 2s/epoch - 2ms/step\n",
      "Epoch 20/10000\n",
      "1126/1126 - 2s - loss: 27899.5117 - accuracy: 0.5850 - val_loss: 28085.8359 - val_accuracy: 0.6199 - 2s/epoch - 2ms/step\n",
      "Epoch 21/10000\n",
      "1126/1126 - 2s - loss: 25987.9180 - accuracy: 0.5850 - val_loss: 8985.8447 - val_accuracy: 0.6325 - 2s/epoch - 2ms/step\n",
      "Epoch 22/10000\n",
      "1126/1126 - 2s - loss: 25479.2012 - accuracy: 0.5860 - val_loss: 14161.1846 - val_accuracy: 0.5535 - 2s/epoch - 2ms/step\n",
      "Epoch 23/10000\n",
      "1126/1126 - 2s - loss: 28426.0137 - accuracy: 0.5852 - val_loss: 12813.7158 - val_accuracy: 0.6366 - 2s/epoch - 2ms/step\n",
      "Epoch 24/10000\n",
      "1126/1126 - 3s - loss: 26667.5273 - accuracy: 0.5837 - val_loss: 33267.4883 - val_accuracy: 0.4771 - 3s/epoch - 3ms/step\n",
      "Epoch 25/10000\n",
      "1126/1126 - 3s - loss: 25570.2852 - accuracy: 0.5851 - val_loss: 10371.2910 - val_accuracy: 0.6022 - 3s/epoch - 2ms/step\n",
      "Epoch 26/10000\n",
      "1126/1126 - 3s - loss: 28144.1875 - accuracy: 0.5858 - val_loss: 8913.8027 - val_accuracy: 0.6476 - 3s/epoch - 3ms/step\n",
      "Epoch 27/10000\n",
      "1126/1126 - 3s - loss: 29037.2910 - accuracy: 0.5852 - val_loss: 41543.4492 - val_accuracy: 0.6167 - 3s/epoch - 2ms/step\n",
      "Epoch 28/10000\n",
      "1126/1126 - 3s - loss: 25845.8438 - accuracy: 0.5862 - val_loss: 8473.8789 - val_accuracy: 0.6354 - 3s/epoch - 2ms/step\n",
      "Epoch 29/10000\n",
      "1126/1126 - 3s - loss: 28643.6582 - accuracy: 0.5837 - val_loss: 27606.6133 - val_accuracy: 0.6156 - 3s/epoch - 2ms/step\n",
      "Epoch 30/10000\n",
      "1126/1126 - 3s - loss: 30750.6895 - accuracy: 0.5834 - val_loss: 51310.9648 - val_accuracy: 0.6175 - 3s/epoch - 2ms/step\n",
      "Epoch 31/10000\n",
      "1126/1126 - 3s - loss: 22954.2383 - accuracy: 0.5884 - val_loss: 24799.5312 - val_accuracy: 0.5526 - 3s/epoch - 2ms/step\n",
      "Epoch 32/10000\n",
      "1126/1126 - 3s - loss: 29042.1465 - accuracy: 0.5841 - val_loss: 8418.4014 - val_accuracy: 0.6265 - 3s/epoch - 2ms/step\n",
      "Epoch 33/10000\n",
      "1126/1126 - 2s - loss: 29513.1680 - accuracy: 0.5857 - val_loss: 56423.5469 - val_accuracy: 0.4692 - 2s/epoch - 2ms/step\n",
      "Epoch 34/10000\n",
      "1126/1126 - 2s - loss: 25598.9434 - accuracy: 0.5863 - val_loss: 26531.9316 - val_accuracy: 0.6209 - 2s/epoch - 2ms/step\n",
      "Epoch 35/10000\n",
      "1126/1126 - 2s - loss: 26877.6465 - accuracy: 0.5865 - val_loss: 48174.6992 - val_accuracy: 0.6167 - 2s/epoch - 2ms/step\n",
      "Epoch 36/10000\n",
      "1126/1126 - 2s - loss: 28621.3477 - accuracy: 0.5865 - val_loss: 13453.5254 - val_accuracy: 0.6403 - 2s/epoch - 2ms/step\n",
      "Epoch 37/10000\n",
      "1126/1126 - 2s - loss: 27789.8594 - accuracy: 0.5887 - val_loss: 11917.6406 - val_accuracy: 0.6344 - 2s/epoch - 2ms/step\n",
      "Epoch 38/10000\n",
      "1126/1126 - 3s - loss: 28154.4668 - accuracy: 0.5858 - val_loss: 20342.5137 - val_accuracy: 0.5495 - 3s/epoch - 2ms/step\n",
      "Epoch 39/10000\n",
      "1126/1126 - 3s - loss: 25946.2754 - accuracy: 0.5898 - val_loss: 12473.3525 - val_accuracy: 0.5995 - 3s/epoch - 2ms/step\n",
      "Epoch 40/10000\n",
      "1126/1126 - 3s - loss: 28009.9902 - accuracy: 0.5870 - val_loss: 26918.9551 - val_accuracy: 0.5689 - 3s/epoch - 2ms/step\n",
      "Epoch 41/10000\n",
      "1126/1126 - 3s - loss: 26878.7383 - accuracy: 0.5890 - val_loss: 30268.9512 - val_accuracy: 0.6192 - 3s/epoch - 3ms/step\n",
      "Epoch 42/10000\n",
      "1126/1126 - 3s - loss: 26435.8730 - accuracy: 0.5876 - val_loss: 18256.3574 - val_accuracy: 0.5860 - 3s/epoch - 2ms/step\n",
      "Epoch 43/10000\n",
      "1126/1126 - 3s - loss: 26798.0762 - accuracy: 0.5883 - val_loss: 21623.4980 - val_accuracy: 0.5305 - 3s/epoch - 3ms/step\n",
      "Epoch 44/10000\n",
      "1126/1126 - 3s - loss: 26251.6934 - accuracy: 0.5899 - val_loss: 9889.6836 - val_accuracy: 0.6329 - 3s/epoch - 3ms/step\n",
      "Epoch 45/10000\n",
      "1126/1126 - 3s - loss: 24957.8633 - accuracy: 0.5905 - val_loss: 25302.1289 - val_accuracy: 0.5131 - 3s/epoch - 3ms/step\n",
      "Epoch 46/10000\n",
      "1126/1126 - 3s - loss: 26596.9590 - accuracy: 0.5872 - val_loss: 22795.7598 - val_accuracy: 0.6256 - 3s/epoch - 3ms/step\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Iterazione 42/128 (03-11-2023_10-59-24)\n",
      "hidden_layer_size: 100\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 2\n",
      "activation_hidden: relu\n",
      "activation_output: sigmoid\n",
      "batch_size: 100\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 20\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 64.76%\n",
      "La recall di questo modello sul validation set è: 46.42%\n",
      "La f1 di questo modello sul validation set è: 50.48%\n",
      "La balanced accuracy di questo modello sul validation set è: 61.38%\n",
      "La precision di questo modello sul validation set è: 55.32%\n",
      "La AUC di questo modello sul validation set è: 61.39%\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.69    0.76      0.73  7,670.00\n",
      "1                  0.55    0.46      0.50  4,841.00\n",
      "accuracy           0.65    0.65      0.65      0.65\n",
      "macro avg          0.62    0.61      0.62 12,511.00\n",
      "weighted avg       0.64    0.65      0.64 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         5855         1815\n",
      "Actual 1         2594         2247\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 65.94% con il modello:\n",
      "1 - AC65.94%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 62.97% con il modello:\n",
      "1 - BA62.97%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 64.08% con il modello:\n",
      "13 - PR64.08%, L20, mV-10, MV10, P2, AHsigmoid, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-26-48).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "113/113 - 2s - loss: 16591868.0000 - accuracy: 0.5816 - val_loss: 1693879.5000 - val_accuracy: 0.3836 - 2s/epoch - 18ms/step\n",
      "Epoch 2/10000\n",
      "113/113 - 1s - loss: 1094856.8750 - accuracy: 0.4188 - val_loss: 1024325.9375 - val_accuracy: 0.4155 - 571ms/epoch - 5ms/step\n",
      "Epoch 3/10000\n",
      "113/113 - 0s - loss: 938269.5625 - accuracy: 0.4289 - val_loss: 904135.5000 - val_accuracy: 0.4249 - 460ms/epoch - 4ms/step\n",
      "Epoch 4/10000\n",
      "113/113 - 1s - loss: 827466.3125 - accuracy: 0.4356 - val_loss: 793394.3125 - val_accuracy: 0.4284 - 634ms/epoch - 6ms/step\n",
      "Epoch 5/10000\n",
      "113/113 - 1s - loss: 685787.9375 - accuracy: 0.4405 - val_loss: 591996.1250 - val_accuracy: 0.4300 - 515ms/epoch - 5ms/step\n",
      "Epoch 6/10000\n",
      "113/113 - 1s - loss: 512356.8125 - accuracy: 0.4423 - val_loss: 445713.2188 - val_accuracy: 0.4407 - 670ms/epoch - 6ms/step\n",
      "Epoch 7/10000\n",
      "113/113 - 1s - loss: 329436.2188 - accuracy: 0.4677 - val_loss: 231464.5781 - val_accuracy: 0.4806 - 595ms/epoch - 5ms/step\n",
      "Epoch 8/10000\n",
      "113/113 - 0s - loss: 184268.0312 - accuracy: 0.4904 - val_loss: 152161.3281 - val_accuracy: 0.4997 - 419ms/epoch - 4ms/step\n",
      "Epoch 9/10000\n",
      "113/113 - 0s - loss: 133369.9844 - accuracy: 0.5093 - val_loss: 114873.3828 - val_accuracy: 0.5125 - 420ms/epoch - 4ms/step\n",
      "Epoch 10/10000\n",
      "113/113 - 0s - loss: 104022.1562 - accuracy: 0.5202 - val_loss: 91097.0469 - val_accuracy: 0.5202 - 428ms/epoch - 4ms/step\n",
      "Epoch 11/10000\n",
      "113/113 - 0s - loss: 84332.6875 - accuracy: 0.5270 - val_loss: 76695.0000 - val_accuracy: 0.5300 - 434ms/epoch - 4ms/step\n",
      "Epoch 12/10000\n",
      "113/113 - 0s - loss: 71495.7266 - accuracy: 0.5354 - val_loss: 63750.0859 - val_accuracy: 0.5437 - 444ms/epoch - 4ms/step\n",
      "Epoch 13/10000\n",
      "113/113 - 0s - loss: 61587.2422 - accuracy: 0.5427 - val_loss: 76747.5156 - val_accuracy: 0.5760 - 434ms/epoch - 4ms/step\n",
      "Epoch 14/10000\n",
      "113/113 - 0s - loss: 60678.1250 - accuracy: 0.5493 - val_loss: 48766.4258 - val_accuracy: 0.5621 - 439ms/epoch - 4ms/step\n",
      "Epoch 15/10000\n",
      "113/113 - 0s - loss: 50773.2305 - accuracy: 0.5508 - val_loss: 41234.6055 - val_accuracy: 0.5635 - 438ms/epoch - 4ms/step\n",
      "Epoch 16/10000\n",
      "113/113 - 0s - loss: 45834.0078 - accuracy: 0.5549 - val_loss: 52585.4453 - val_accuracy: 0.5135 - 429ms/epoch - 4ms/step\n",
      "Epoch 17/10000\n",
      "113/113 - 0s - loss: 45306.4648 - accuracy: 0.5585 - val_loss: 36520.9258 - val_accuracy: 0.5504 - 466ms/epoch - 4ms/step\n",
      "Epoch 18/10000\n",
      "113/113 - 0s - loss: 62996.6211 - accuracy: 0.5575 - val_loss: 90363.6953 - val_accuracy: 0.5949 - 406ms/epoch - 4ms/step\n",
      "Epoch 19/10000\n",
      "113/113 - 0s - loss: 45631.6211 - accuracy: 0.5620 - val_loss: 44346.8789 - val_accuracy: 0.5159 - 411ms/epoch - 4ms/step\n",
      "Epoch 20/10000\n",
      "113/113 - 0s - loss: 37010.1250 - accuracy: 0.5632 - val_loss: 43361.5391 - val_accuracy: 0.5848 - 440ms/epoch - 4ms/step\n",
      "Epoch 21/10000\n",
      "113/113 - 1s - loss: 35257.9688 - accuracy: 0.5640 - val_loss: 32784.8633 - val_accuracy: 0.5431 - 578ms/epoch - 5ms/step\n",
      "Epoch 22/10000\n",
      "113/113 - 1s - loss: 34330.6250 - accuracy: 0.5644 - val_loss: 24959.5527 - val_accuracy: 0.5793 - 504ms/epoch - 4ms/step\n",
      "Epoch 23/10000\n",
      "113/113 - 0s - loss: 34768.8867 - accuracy: 0.5656 - val_loss: 55352.3945 - val_accuracy: 0.5962 - 467ms/epoch - 4ms/step\n",
      "Epoch 24/10000\n",
      "113/113 - 0s - loss: 32261.7383 - accuracy: 0.5674 - val_loss: 38826.1250 - val_accuracy: 0.5988 - 450ms/epoch - 4ms/step\n",
      "Epoch 25/10000\n",
      "113/113 - 0s - loss: 33745.8594 - accuracy: 0.5695 - val_loss: 21695.2715 - val_accuracy: 0.5952 - 429ms/epoch - 4ms/step\n",
      "Epoch 26/10000\n",
      "113/113 - 0s - loss: 28003.6172 - accuracy: 0.5680 - val_loss: 122987.4844 - val_accuracy: 0.6089 - 416ms/epoch - 4ms/step\n",
      "Epoch 27/10000\n",
      "113/113 - 0s - loss: 41763.7891 - accuracy: 0.5665 - val_loss: 22612.7793 - val_accuracy: 0.6065 - 434ms/epoch - 4ms/step\n",
      "Epoch 28/10000\n",
      "113/113 - 0s - loss: 38695.5156 - accuracy: 0.5686 - val_loss: 22021.1406 - val_accuracy: 0.6063 - 423ms/epoch - 4ms/step\n",
      "Epoch 29/10000\n",
      "113/113 - 0s - loss: 26850.1875 - accuracy: 0.5728 - val_loss: 19475.3242 - val_accuracy: 0.5972 - 491ms/epoch - 4ms/step\n",
      "Epoch 30/10000\n",
      "113/113 - 0s - loss: 33165.4297 - accuracy: 0.5676 - val_loss: 34017.3750 - val_accuracy: 0.5294 - 405ms/epoch - 4ms/step\n",
      "Epoch 31/10000\n",
      "113/113 - 0s - loss: 31002.4004 - accuracy: 0.5693 - val_loss: 34611.6914 - val_accuracy: 0.6000 - 397ms/epoch - 4ms/step\n",
      "Epoch 32/10000\n",
      "113/113 - 0s - loss: 28828.1309 - accuracy: 0.5720 - val_loss: 18794.9980 - val_accuracy: 0.6116 - 431ms/epoch - 4ms/step\n",
      "Epoch 33/10000\n",
      "113/113 - 0s - loss: 44904.2070 - accuracy: 0.5685 - val_loss: 17046.5449 - val_accuracy: 0.5998 - 410ms/epoch - 4ms/step\n",
      "Epoch 34/10000\n",
      "113/113 - 0s - loss: 24246.8594 - accuracy: 0.5767 - val_loss: 16658.5625 - val_accuracy: 0.5958 - 414ms/epoch - 4ms/step\n",
      "Epoch 35/10000\n",
      "113/113 - 0s - loss: 22462.2754 - accuracy: 0.5770 - val_loss: 40851.0078 - val_accuracy: 0.5004 - 398ms/epoch - 4ms/step\n",
      "Epoch 36/10000\n",
      "113/113 - 0s - loss: 30541.4434 - accuracy: 0.5671 - val_loss: 25046.5488 - val_accuracy: 0.5725 - 447ms/epoch - 4ms/step\n",
      "Epoch 37/10000\n",
      "113/113 - 0s - loss: 31199.3613 - accuracy: 0.5769 - val_loss: 17471.9922 - val_accuracy: 0.6079 - 423ms/epoch - 4ms/step\n",
      "Epoch 38/10000\n",
      "113/113 - 0s - loss: 25670.1602 - accuracy: 0.5713 - val_loss: 28521.8887 - val_accuracy: 0.5216 - 405ms/epoch - 4ms/step\n",
      "Epoch 39/10000\n",
      "113/113 - 0s - loss: 32911.4141 - accuracy: 0.5691 - val_loss: 26406.1777 - val_accuracy: 0.6041 - 412ms/epoch - 4ms/step\n",
      "Epoch 40/10000\n",
      "113/113 - 0s - loss: 25877.7109 - accuracy: 0.5763 - val_loss: 24844.3047 - val_accuracy: 0.6012 - 419ms/epoch - 4ms/step\n",
      "Epoch 41/10000\n",
      "113/113 - 0s - loss: 25364.2461 - accuracy: 0.5743 - val_loss: 20020.6348 - val_accuracy: 0.5876 - 420ms/epoch - 4ms/step\n",
      "Epoch 42/10000\n",
      "113/113 - 0s - loss: 26642.1289 - accuracy: 0.5726 - val_loss: 21423.2402 - val_accuracy: 0.5656 - 423ms/epoch - 4ms/step\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Iterazione 43/128 (03-11-2023_10-59-47)\n",
      "hidden_layer_size: 100\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 2\n",
      "activation_hidden: relu\n",
      "activation_output: sigmoid\n",
      "batch_size: 1000\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 10\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 61.16%\n",
      "La recall di questo modello sul validation set è: 50.32%\n",
      "La f1 di questo modello sul validation set è: 50.07%\n",
      "La balanced accuracy di questo modello sul validation set è: 59.16%\n",
      "La precision di questo modello sul validation set è: 49.82%\n",
      "La AUC di questo modello sul validation set è: 59.15%\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.68    0.68      0.68  7,670.00\n",
      "1                  0.50    0.50      0.50  4,841.00\n",
      "accuracy           0.61    0.61      0.61      0.61\n",
      "macro avg          0.59    0.59      0.59 12,511.00\n",
      "weighted avg       0.61    0.61      0.61 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         5216         2454\n",
      "Actual 1         2405         2436\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 65.94% con il modello:\n",
      "1 - AC65.94%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 62.97% con il modello:\n",
      "1 - BA62.97%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 64.08% con il modello:\n",
      "13 - PR64.08%, L20, mV-10, MV10, P2, AHsigmoid, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-26-48).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "113/113 - 2s - loss: 16591868.0000 - accuracy: 0.5816 - val_loss: 1693879.5000 - val_accuracy: 0.3836 - 2s/epoch - 15ms/step\n",
      "Epoch 2/10000\n",
      "113/113 - 1s - loss: 1094856.8750 - accuracy: 0.4188 - val_loss: 1024325.9375 - val_accuracy: 0.4155 - 523ms/epoch - 5ms/step\n",
      "Epoch 3/10000\n",
      "113/113 - 0s - loss: 938269.5625 - accuracy: 0.4289 - val_loss: 904135.5000 - val_accuracy: 0.4249 - 452ms/epoch - 4ms/step\n",
      "Epoch 4/10000\n",
      "113/113 - 0s - loss: 827466.3125 - accuracy: 0.4356 - val_loss: 793394.3125 - val_accuracy: 0.4284 - 467ms/epoch - 4ms/step\n",
      "Epoch 5/10000\n",
      "113/113 - 1s - loss: 685787.9375 - accuracy: 0.4405 - val_loss: 591996.1250 - val_accuracy: 0.4300 - 504ms/epoch - 4ms/step\n",
      "Epoch 6/10000\n",
      "113/113 - 0s - loss: 512356.8125 - accuracy: 0.4423 - val_loss: 445713.2188 - val_accuracy: 0.4407 - 440ms/epoch - 4ms/step\n",
      "Epoch 7/10000\n",
      "113/113 - 0s - loss: 329436.2188 - accuracy: 0.4677 - val_loss: 231464.5781 - val_accuracy: 0.4806 - 450ms/epoch - 4ms/step\n",
      "Epoch 8/10000\n",
      "113/113 - 1s - loss: 184268.0312 - accuracy: 0.4904 - val_loss: 152161.3281 - val_accuracy: 0.4997 - 572ms/epoch - 5ms/step\n",
      "Epoch 9/10000\n",
      "113/113 - 0s - loss: 133369.9844 - accuracy: 0.5093 - val_loss: 114873.3828 - val_accuracy: 0.5125 - 433ms/epoch - 4ms/step\n",
      "Epoch 10/10000\n",
      "113/113 - 0s - loss: 104022.1562 - accuracy: 0.5202 - val_loss: 91097.0469 - val_accuracy: 0.5202 - 454ms/epoch - 4ms/step\n",
      "Epoch 11/10000\n",
      "113/113 - 1s - loss: 84332.6875 - accuracy: 0.5270 - val_loss: 76695.0000 - val_accuracy: 0.5300 - 508ms/epoch - 4ms/step\n",
      "Epoch 12/10000\n",
      "113/113 - 0s - loss: 71495.7266 - accuracy: 0.5354 - val_loss: 63750.0859 - val_accuracy: 0.5437 - 489ms/epoch - 4ms/step\n",
      "Epoch 13/10000\n",
      "113/113 - 0s - loss: 61587.2422 - accuracy: 0.5427 - val_loss: 76747.5156 - val_accuracy: 0.5760 - 397ms/epoch - 4ms/step\n",
      "Epoch 14/10000\n",
      "113/113 - 0s - loss: 60678.1250 - accuracy: 0.5493 - val_loss: 48766.4258 - val_accuracy: 0.5621 - 432ms/epoch - 4ms/step\n",
      "Epoch 15/10000\n",
      "113/113 - 0s - loss: 50773.2305 - accuracy: 0.5508 - val_loss: 41234.6055 - val_accuracy: 0.5635 - 411ms/epoch - 4ms/step\n",
      "Epoch 16/10000\n",
      "113/113 - 0s - loss: 45834.0078 - accuracy: 0.5549 - val_loss: 52585.4453 - val_accuracy: 0.5135 - 434ms/epoch - 4ms/step\n",
      "Epoch 17/10000\n",
      "113/113 - 0s - loss: 45306.4648 - accuracy: 0.5585 - val_loss: 36520.9258 - val_accuracy: 0.5504 - 412ms/epoch - 4ms/step\n",
      "Epoch 18/10000\n",
      "113/113 - 0s - loss: 62996.6211 - accuracy: 0.5575 - val_loss: 90363.6953 - val_accuracy: 0.5949 - 422ms/epoch - 4ms/step\n",
      "Epoch 19/10000\n",
      "113/113 - 0s - loss: 45631.6211 - accuracy: 0.5620 - val_loss: 44346.8789 - val_accuracy: 0.5159 - 432ms/epoch - 4ms/step\n",
      "Epoch 20/10000\n",
      "113/113 - 1s - loss: 37010.1250 - accuracy: 0.5632 - val_loss: 43361.5391 - val_accuracy: 0.5848 - 511ms/epoch - 5ms/step\n",
      "Epoch 21/10000\n",
      "113/113 - 0s - loss: 35257.9688 - accuracy: 0.5640 - val_loss: 32784.8633 - val_accuracy: 0.5431 - 398ms/epoch - 4ms/step\n",
      "Epoch 22/10000\n",
      "113/113 - 1s - loss: 34330.6250 - accuracy: 0.5644 - val_loss: 24959.5527 - val_accuracy: 0.5793 - 507ms/epoch - 4ms/step\n",
      "Epoch 23/10000\n",
      "113/113 - 0s - loss: 34768.8867 - accuracy: 0.5656 - val_loss: 55352.3945 - val_accuracy: 0.5962 - 447ms/epoch - 4ms/step\n",
      "Epoch 24/10000\n",
      "113/113 - 0s - loss: 32261.7383 - accuracy: 0.5674 - val_loss: 38826.1250 - val_accuracy: 0.5988 - 478ms/epoch - 4ms/step\n",
      "Epoch 25/10000\n",
      "113/113 - 0s - loss: 33745.8594 - accuracy: 0.5695 - val_loss: 21695.2715 - val_accuracy: 0.5952 - 412ms/epoch - 4ms/step\n",
      "Epoch 26/10000\n",
      "113/113 - 0s - loss: 28003.6172 - accuracy: 0.5680 - val_loss: 122987.4844 - val_accuracy: 0.6089 - 418ms/epoch - 4ms/step\n",
      "Epoch 27/10000\n",
      "113/113 - 0s - loss: 41763.7891 - accuracy: 0.5665 - val_loss: 22612.7793 - val_accuracy: 0.6065 - 396ms/epoch - 4ms/step\n",
      "Epoch 28/10000\n",
      "113/113 - 0s - loss: 38695.5156 - accuracy: 0.5686 - val_loss: 22021.1406 - val_accuracy: 0.6063 - 423ms/epoch - 4ms/step\n",
      "Epoch 29/10000\n",
      "113/113 - 1s - loss: 26850.1875 - accuracy: 0.5728 - val_loss: 19475.3242 - val_accuracy: 0.5972 - 504ms/epoch - 4ms/step\n",
      "Epoch 30/10000\n",
      "113/113 - 0s - loss: 33165.4297 - accuracy: 0.5676 - val_loss: 34017.3750 - val_accuracy: 0.5294 - 390ms/epoch - 3ms/step\n",
      "Epoch 31/10000\n",
      "113/113 - 0s - loss: 31002.4004 - accuracy: 0.5693 - val_loss: 34611.6914 - val_accuracy: 0.6000 - 436ms/epoch - 4ms/step\n",
      "Epoch 32/10000\n",
      "113/113 - 0s - loss: 28828.1309 - accuracy: 0.5720 - val_loss: 18794.9980 - val_accuracy: 0.6116 - 411ms/epoch - 4ms/step\n",
      "Epoch 33/10000\n",
      "113/113 - 0s - loss: 44904.2070 - accuracy: 0.5685 - val_loss: 17046.5449 - val_accuracy: 0.5998 - 403ms/epoch - 4ms/step\n",
      "Epoch 34/10000\n",
      "113/113 - 0s - loss: 24246.8594 - accuracy: 0.5767 - val_loss: 16658.5625 - val_accuracy: 0.5958 - 408ms/epoch - 4ms/step\n",
      "Epoch 35/10000\n",
      "113/113 - 0s - loss: 22462.2754 - accuracy: 0.5770 - val_loss: 40851.0078 - val_accuracy: 0.5004 - 415ms/epoch - 4ms/step\n",
      "Epoch 36/10000\n",
      "113/113 - 0s - loss: 30541.4434 - accuracy: 0.5671 - val_loss: 25046.5488 - val_accuracy: 0.5725 - 414ms/epoch - 4ms/step\n",
      "Epoch 37/10000\n",
      "113/113 - 0s - loss: 31199.3613 - accuracy: 0.5769 - val_loss: 17471.9922 - val_accuracy: 0.6079 - 428ms/epoch - 4ms/step\n",
      "Epoch 38/10000\n",
      "113/113 - 0s - loss: 25670.1602 - accuracy: 0.5713 - val_loss: 28521.8887 - val_accuracy: 0.5216 - 425ms/epoch - 4ms/step\n",
      "Epoch 39/10000\n",
      "113/113 - 0s - loss: 32911.4141 - accuracy: 0.5691 - val_loss: 26406.1777 - val_accuracy: 0.6041 - 419ms/epoch - 4ms/step\n",
      "Epoch 40/10000\n",
      "113/113 - 0s - loss: 25877.7109 - accuracy: 0.5763 - val_loss: 24844.3047 - val_accuracy: 0.6012 - 409ms/epoch - 4ms/step\n",
      "Epoch 41/10000\n",
      "113/113 - 0s - loss: 25364.2461 - accuracy: 0.5743 - val_loss: 20020.6348 - val_accuracy: 0.5876 - 469ms/epoch - 4ms/step\n",
      "Epoch 42/10000\n",
      "113/113 - 0s - loss: 26642.1289 - accuracy: 0.5726 - val_loss: 21423.2402 - val_accuracy: 0.5656 - 415ms/epoch - 4ms/step\n",
      "Epoch 43/10000\n",
      "113/113 - 0s - loss: 26328.9688 - accuracy: 0.5738 - val_loss: 23319.3574 - val_accuracy: 0.5538 - 433ms/epoch - 4ms/step\n",
      "Epoch 44/10000\n",
      "113/113 - 0s - loss: 26928.6621 - accuracy: 0.5735 - val_loss: 33267.1094 - val_accuracy: 0.6038 - 414ms/epoch - 4ms/step\n",
      "Epoch 45/10000\n",
      "113/113 - 0s - loss: 20180.3027 - accuracy: 0.5805 - val_loss: 15990.7266 - val_accuracy: 0.5948 - 428ms/epoch - 4ms/step\n",
      "Epoch 46/10000\n",
      "113/113 - 1s - loss: 26554.5664 - accuracy: 0.5747 - val_loss: 23724.6465 - val_accuracy: 0.6028 - 519ms/epoch - 5ms/step\n",
      "Epoch 47/10000\n",
      "113/113 - 0s - loss: 25012.6758 - accuracy: 0.5757 - val_loss: 32080.6211 - val_accuracy: 0.5140 - 411ms/epoch - 4ms/step\n",
      "Epoch 48/10000\n",
      "113/113 - 0s - loss: 24909.9922 - accuracy: 0.5738 - val_loss: 16013.7686 - val_accuracy: 0.6235 - 465ms/epoch - 4ms/step\n",
      "Epoch 49/10000\n",
      "113/113 - 0s - loss: 26946.0957 - accuracy: 0.5776 - val_loss: 64448.1992 - val_accuracy: 0.6077 - 434ms/epoch - 4ms/step\n",
      "Epoch 50/10000\n",
      "113/113 - 0s - loss: 24135.4082 - accuracy: 0.5799 - val_loss: 22740.2344 - val_accuracy: 0.6043 - 419ms/epoch - 4ms/step\n",
      "Epoch 51/10000\n",
      "113/113 - 0s - loss: 32360.2930 - accuracy: 0.5702 - val_loss: 31631.8145 - val_accuracy: 0.6070 - 409ms/epoch - 4ms/step\n",
      "Epoch 52/10000\n",
      "113/113 - 0s - loss: 23169.5117 - accuracy: 0.5847 - val_loss: 19103.5234 - val_accuracy: 0.5493 - 417ms/epoch - 4ms/step\n",
      "Epoch 53/10000\n",
      "113/113 - 0s - loss: 25343.7031 - accuracy: 0.5713 - val_loss: 15661.6621 - val_accuracy: 0.6166 - 443ms/epoch - 4ms/step\n",
      "Epoch 54/10000\n",
      "113/113 - 0s - loss: 26112.3164 - accuracy: 0.5767 - val_loss: 49107.2539 - val_accuracy: 0.6091 - 457ms/epoch - 4ms/step\n",
      "Epoch 55/10000\n",
      "113/113 - 0s - loss: 22726.3379 - accuracy: 0.5788 - val_loss: 35110.5117 - val_accuracy: 0.6018 - 406ms/epoch - 4ms/step\n",
      "Epoch 56/10000\n",
      "113/113 - 0s - loss: 22874.7227 - accuracy: 0.5801 - val_loss: 35893.9727 - val_accuracy: 0.6048 - 439ms/epoch - 4ms/step\n",
      "Epoch 57/10000\n",
      "113/113 - 0s - loss: 25139.2168 - accuracy: 0.5755 - val_loss: 30223.5996 - val_accuracy: 0.6012 - 418ms/epoch - 4ms/step\n",
      "Epoch 58/10000\n",
      "113/113 - 0s - loss: 26222.3906 - accuracy: 0.5811 - val_loss: 12698.5596 - val_accuracy: 0.6089 - 418ms/epoch - 4ms/step\n",
      "Epoch 59/10000\n",
      "113/113 - 0s - loss: 26323.5859 - accuracy: 0.5728 - val_loss: 37726.0977 - val_accuracy: 0.6059 - 405ms/epoch - 4ms/step\n",
      "Epoch 60/10000\n",
      "113/113 - 0s - loss: 25970.5469 - accuracy: 0.5797 - val_loss: 37092.5508 - val_accuracy: 0.6061 - 463ms/epoch - 4ms/step\n",
      "Epoch 61/10000\n",
      "113/113 - 0s - loss: 23520.5957 - accuracy: 0.5799 - val_loss: 13061.9980 - val_accuracy: 0.6246 - 443ms/epoch - 4ms/step\n",
      "Epoch 62/10000\n",
      "113/113 - 0s - loss: 23411.7812 - accuracy: 0.5766 - val_loss: 16660.4980 - val_accuracy: 0.6152 - 410ms/epoch - 4ms/step\n",
      "Epoch 63/10000\n",
      "113/113 - 0s - loss: 17814.7363 - accuracy: 0.5881 - val_loss: 11844.1572 - val_accuracy: 0.6143 - 403ms/epoch - 4ms/step\n",
      "Epoch 64/10000\n",
      "113/113 - 0s - loss: 22393.4023 - accuracy: 0.5802 - val_loss: 22909.4277 - val_accuracy: 0.5210 - 415ms/epoch - 4ms/step\n",
      "Epoch 65/10000\n",
      "113/113 - 0s - loss: 23081.5137 - accuracy: 0.5762 - val_loss: 34218.0000 - val_accuracy: 0.5064 - 400ms/epoch - 4ms/step\n",
      "Epoch 66/10000\n",
      "113/113 - 1s - loss: 22634.2676 - accuracy: 0.5802 - val_loss: 35230.6367 - val_accuracy: 0.4921 - 521ms/epoch - 5ms/step\n",
      "Epoch 67/10000\n",
      "113/113 - 0s - loss: 24104.6602 - accuracy: 0.5728 - val_loss: 25140.4160 - val_accuracy: 0.6030 - 414ms/epoch - 4ms/step\n",
      "Epoch 68/10000\n",
      "113/113 - 0s - loss: 21612.0000 - accuracy: 0.5844 - val_loss: 24915.4180 - val_accuracy: 0.5341 - 408ms/epoch - 4ms/step\n",
      "Epoch 69/10000\n",
      "113/113 - 0s - loss: 22240.5156 - accuracy: 0.5807 - val_loss: 17877.1777 - val_accuracy: 0.5492 - 430ms/epoch - 4ms/step\n",
      "Epoch 70/10000\n",
      "113/113 - 0s - loss: 27696.8184 - accuracy: 0.5749 - val_loss: 53684.2852 - val_accuracy: 0.4798 - 433ms/epoch - 4ms/step\n",
      "Epoch 71/10000\n",
      "113/113 - 0s - loss: 24155.5703 - accuracy: 0.5799 - val_loss: 27403.3555 - val_accuracy: 0.5203 - 432ms/epoch - 4ms/step\n",
      "Epoch 72/10000\n",
      "113/113 - 0s - loss: 26847.2422 - accuracy: 0.5801 - val_loss: 13250.1699 - val_accuracy: 0.5998 - 468ms/epoch - 4ms/step\n",
      "Epoch 73/10000\n",
      "113/113 - 0s - loss: 26084.4629 - accuracy: 0.5792 - val_loss: 17361.7285 - val_accuracy: 0.6063 - 418ms/epoch - 4ms/step\n",
      "Epoch 74/10000\n",
      "113/113 - 0s - loss: 21254.3301 - accuracy: 0.5800 - val_loss: 30994.6504 - val_accuracy: 0.6074 - 426ms/epoch - 4ms/step\n",
      "Epoch 75/10000\n",
      "113/113 - 0s - loss: 33203.9766 - accuracy: 0.5733 - val_loss: 17743.4199 - val_accuracy: 0.6039 - 435ms/epoch - 4ms/step\n",
      "Epoch 76/10000\n",
      "113/113 - 0s - loss: 20197.6855 - accuracy: 0.5881 - val_loss: 16284.9580 - val_accuracy: 0.6076 - 442ms/epoch - 4ms/step\n",
      "Epoch 77/10000\n",
      "113/113 - 0s - loss: 28980.7383 - accuracy: 0.5752 - val_loss: 56306.1680 - val_accuracy: 0.4876 - 417ms/epoch - 4ms/step\n",
      "Epoch 78/10000\n",
      "113/113 - 0s - loss: 26939.7832 - accuracy: 0.5820 - val_loss: 22973.4473 - val_accuracy: 0.5549 - 448ms/epoch - 4ms/step\n",
      "Epoch 79/10000\n",
      "113/113 - 0s - loss: 22707.8418 - accuracy: 0.5813 - val_loss: 13344.1543 - val_accuracy: 0.6258 - 399ms/epoch - 4ms/step\n",
      "Epoch 80/10000\n",
      "113/113 - 0s - loss: 27717.9805 - accuracy: 0.5780 - val_loss: 55396.9023 - val_accuracy: 0.6123 - 393ms/epoch - 3ms/step\n",
      "Epoch 81/10000\n",
      "113/113 - 0s - loss: 29132.4824 - accuracy: 0.5842 - val_loss: 17993.0137 - val_accuracy: 0.6130 - 399ms/epoch - 4ms/step\n",
      "Epoch 82/10000\n",
      "113/113 - 0s - loss: 23046.9336 - accuracy: 0.5808 - val_loss: 14837.8955 - val_accuracy: 0.6302 - 409ms/epoch - 4ms/step\n",
      "Epoch 83/10000\n",
      "113/113 - 0s - loss: 25962.6367 - accuracy: 0.5783 - val_loss: 32145.3398 - val_accuracy: 0.5181 - 403ms/epoch - 4ms/step\n",
      "Epoch 84/10000\n",
      "113/113 - 0s - loss: 16469.4863 - accuracy: 0.5941 - val_loss: 17010.4648 - val_accuracy: 0.5542 - 452ms/epoch - 4ms/step\n",
      "Epoch 85/10000\n",
      "113/113 - 0s - loss: 29409.7090 - accuracy: 0.5762 - val_loss: 33758.3555 - val_accuracy: 0.6065 - 409ms/epoch - 4ms/step\n",
      "Epoch 86/10000\n",
      "113/113 - 0s - loss: 21554.9941 - accuracy: 0.5828 - val_loss: 28625.7832 - val_accuracy: 0.5183 - 405ms/epoch - 4ms/step\n",
      "Epoch 87/10000\n",
      "113/113 - 0s - loss: 20525.0488 - accuracy: 0.5848 - val_loss: 44059.9062 - val_accuracy: 0.4791 - 491ms/epoch - 4ms/step\n",
      "Epoch 88/10000\n",
      "113/113 - 0s - loss: 27205.1621 - accuracy: 0.5818 - val_loss: 61359.1758 - val_accuracy: 0.6085 - 419ms/epoch - 4ms/step\n",
      "Epoch 89/10000\n",
      "113/113 - 0s - loss: 20199.5859 - accuracy: 0.5869 - val_loss: 15830.1768 - val_accuracy: 0.6033 - 403ms/epoch - 4ms/step\n",
      "Epoch 90/10000\n",
      "113/113 - 0s - loss: 18844.4688 - accuracy: 0.5884 - val_loss: 20708.8281 - val_accuracy: 0.5239 - 468ms/epoch - 4ms/step\n",
      "Epoch 91/10000\n",
      "113/113 - 0s - loss: 28356.7012 - accuracy: 0.5774 - val_loss: 12988.8936 - val_accuracy: 0.6226 - 445ms/epoch - 4ms/step\n",
      "Epoch 92/10000\n",
      "113/113 - 0s - loss: 30565.6562 - accuracy: 0.5771 - val_loss: 24509.4082 - val_accuracy: 0.5529 - 401ms/epoch - 4ms/step\n",
      "Epoch 93/10000\n",
      "113/113 - 0s - loss: 22242.5371 - accuracy: 0.5817 - val_loss: 29440.8359 - val_accuracy: 0.6104 - 409ms/epoch - 4ms/step\n",
      "Epoch 94/10000\n",
      "113/113 - 0s - loss: 18957.5098 - accuracy: 0.5900 - val_loss: 10573.7383 - val_accuracy: 0.6246 - 432ms/epoch - 4ms/step\n",
      "Epoch 95/10000\n",
      "113/113 - 0s - loss: 23831.6523 - accuracy: 0.5858 - val_loss: 11628.8984 - val_accuracy: 0.5979 - 443ms/epoch - 4ms/step\n",
      "Epoch 96/10000\n",
      "113/113 - 1s - loss: 18535.1055 - accuracy: 0.5858 - val_loss: 10607.2197 - val_accuracy: 0.6262 - 538ms/epoch - 5ms/step\n",
      "Epoch 97/10000\n",
      "113/113 - 0s - loss: 20396.9668 - accuracy: 0.5826 - val_loss: 22232.1094 - val_accuracy: 0.6082 - 444ms/epoch - 4ms/step\n",
      "Epoch 98/10000\n",
      "113/113 - 0s - loss: 21573.3965 - accuracy: 0.5823 - val_loss: 26780.8496 - val_accuracy: 0.5362 - 461ms/epoch - 4ms/step\n",
      "Epoch 99/10000\n",
      "113/113 - 0s - loss: 27494.0977 - accuracy: 0.5778 - val_loss: 15634.1035 - val_accuracy: 0.5920 - 436ms/epoch - 4ms/step\n",
      "Epoch 100/10000\n",
      "113/113 - 0s - loss: 15752.5703 - accuracy: 0.5923 - val_loss: 10367.5605 - val_accuracy: 0.6187 - 419ms/epoch - 4ms/step\n",
      "Epoch 101/10000\n",
      "113/113 - 0s - loss: 20619.2148 - accuracy: 0.5813 - val_loss: 20183.5918 - val_accuracy: 0.6129 - 423ms/epoch - 4ms/step\n",
      "Epoch 102/10000\n",
      "113/113 - 0s - loss: 19061.5293 - accuracy: 0.5852 - val_loss: 21717.1758 - val_accuracy: 0.6107 - 482ms/epoch - 4ms/step\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Iterazione 44/128 (03-11-2023_11-00-36)\n",
      "hidden_layer_size: 100\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 2\n",
      "activation_hidden: relu\n",
      "activation_output: sigmoid\n",
      "batch_size: 1000\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 20\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 63.02%\n",
      "La recall di questo modello sul validation set è: 22.78%\n",
      "La f1 di questo modello sul validation set è: 32.28%\n",
      "La balanced accuracy di questo modello sul validation set è: 55.6%\n",
      "La precision di questo modello sul validation set è: 55.37%\n",
      "La AUC di questo modello sul validation set è: 55.64%\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.64    0.88      0.75  7,670.00\n",
      "1                  0.55    0.23      0.32  4,841.00\n",
      "accuracy           0.63    0.63      0.63      0.63\n",
      "macro avg          0.60    0.56      0.53 12,511.00\n",
      "weighted avg       0.61    0.63      0.58 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         6781          889\n",
      "Actual 1         3738         1103\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 65.94% con il modello:\n",
      "1 - AC65.94%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 62.97% con il modello:\n",
      "1 - BA62.97%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 64.08% con il modello:\n",
      "13 - PR64.08%, L20, mV-10, MV10, P2, AHsigmoid, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-26-48).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "1126/1126 - 4s - loss: 0.9533 - accuracy: 0.6041 - val_loss: 0.7523 - val_accuracy: 0.6159 - 4s/epoch - 4ms/step\n",
      "Epoch 2/10000\n",
      "1126/1126 - 3s - loss: 0.7307 - accuracy: 0.6124 - val_loss: 0.7106 - val_accuracy: 0.6189 - 3s/epoch - 3ms/step\n",
      "Epoch 3/10000\n",
      "1126/1126 - 2s - loss: 0.7074 - accuracy: 0.6133 - val_loss: 0.6996 - val_accuracy: 0.6135 - 2s/epoch - 2ms/step\n",
      "Epoch 4/10000\n",
      "1126/1126 - 3s - loss: 0.6915 - accuracy: 0.6139 - val_loss: 0.6851 - val_accuracy: 0.6208 - 3s/epoch - 2ms/step\n",
      "Epoch 5/10000\n",
      "1126/1126 - 3s - loss: 0.6852 - accuracy: 0.6144 - val_loss: 0.6809 - val_accuracy: 0.6195 - 3s/epoch - 2ms/step\n",
      "Epoch 6/10000\n",
      "1126/1126 - 3s - loss: 0.6811 - accuracy: 0.6167 - val_loss: 0.6828 - val_accuracy: 0.6178 - 3s/epoch - 2ms/step\n",
      "Epoch 7/10000\n",
      "1126/1126 - 3s - loss: 0.6814 - accuracy: 0.6130 - val_loss: 0.6783 - val_accuracy: 0.6157 - 3s/epoch - 2ms/step\n",
      "Epoch 8/10000\n",
      "1126/1126 - 3s - loss: 0.6784 - accuracy: 0.6116 - val_loss: 0.6739 - val_accuracy: 0.6193 - 3s/epoch - 2ms/step\n",
      "Epoch 9/10000\n",
      "1126/1126 - 3s - loss: 0.6766 - accuracy: 0.6124 - val_loss: 0.6764 - val_accuracy: 0.6182 - 3s/epoch - 2ms/step\n",
      "Epoch 10/10000\n",
      "1126/1126 - 3s - loss: 0.6735 - accuracy: 0.6115 - val_loss: 0.6720 - val_accuracy: 0.6211 - 3s/epoch - 2ms/step\n",
      "Epoch 11/10000\n",
      "1126/1126 - 2s - loss: 0.6710 - accuracy: 0.6129 - val_loss: 0.6653 - val_accuracy: 0.6191 - 2s/epoch - 2ms/step\n",
      "Epoch 12/10000\n",
      "1126/1126 - 3s - loss: 0.6706 - accuracy: 0.6149 - val_loss: 0.6685 - val_accuracy: 0.6227 - 3s/epoch - 2ms/step\n",
      "Epoch 13/10000\n",
      "1126/1126 - 2s - loss: 0.6705 - accuracy: 0.6152 - val_loss: 0.6663 - val_accuracy: 0.6239 - 2s/epoch - 2ms/step\n",
      "Epoch 14/10000\n",
      "1126/1126 - 2s - loss: 0.6678 - accuracy: 0.6169 - val_loss: 0.6641 - val_accuracy: 0.6236 - 2s/epoch - 2ms/step\n",
      "Epoch 15/10000\n",
      "1126/1126 - 3s - loss: 0.6686 - accuracy: 0.6171 - val_loss: 0.6645 - val_accuracy: 0.6175 - 3s/epoch - 2ms/step\n",
      "Epoch 16/10000\n",
      "1126/1126 - 2s - loss: 0.6680 - accuracy: 0.6163 - val_loss: 0.6648 - val_accuracy: 0.6231 - 2s/epoch - 2ms/step\n",
      "Epoch 17/10000\n",
      "1126/1126 - 2s - loss: 0.6665 - accuracy: 0.6160 - val_loss: 0.6633 - val_accuracy: 0.6199 - 2s/epoch - 2ms/step\n",
      "Epoch 18/10000\n",
      "1126/1126 - 2s - loss: 0.6662 - accuracy: 0.6134 - val_loss: 0.6681 - val_accuracy: 0.6163 - 2s/epoch - 2ms/step\n",
      "Epoch 19/10000\n",
      "1126/1126 - 2s - loss: 0.6650 - accuracy: 0.6137 - val_loss: 0.6587 - val_accuracy: 0.6183 - 2s/epoch - 2ms/step\n",
      "Epoch 20/10000\n",
      "1126/1126 - 2s - loss: 0.6645 - accuracy: 0.6147 - val_loss: 0.6589 - val_accuracy: 0.6230 - 2s/epoch - 2ms/step\n",
      "Epoch 21/10000\n",
      "1126/1126 - 3s - loss: 0.6640 - accuracy: 0.6194 - val_loss: 0.6615 - val_accuracy: 0.6272 - 3s/epoch - 2ms/step\n",
      "Epoch 22/10000\n",
      "1126/1126 - 2s - loss: 0.6629 - accuracy: 0.6205 - val_loss: 0.6620 - val_accuracy: 0.6242 - 2s/epoch - 2ms/step\n",
      "Epoch 23/10000\n",
      "1126/1126 - 3s - loss: 0.6627 - accuracy: 0.6205 - val_loss: 0.6595 - val_accuracy: 0.6270 - 3s/epoch - 2ms/step\n",
      "Epoch 24/10000\n",
      "1126/1126 - 2s - loss: 0.6627 - accuracy: 0.6209 - val_loss: 0.6567 - val_accuracy: 0.6240 - 2s/epoch - 2ms/step\n",
      "Epoch 25/10000\n",
      "1126/1126 - 3s - loss: 0.6621 - accuracy: 0.6211 - val_loss: 0.6571 - val_accuracy: 0.6272 - 3s/epoch - 2ms/step\n",
      "Epoch 26/10000\n",
      "1126/1126 - 2s - loss: 0.6604 - accuracy: 0.6228 - val_loss: 0.6549 - val_accuracy: 0.6243 - 2s/epoch - 2ms/step\n",
      "Epoch 27/10000\n",
      "1126/1126 - 2s - loss: 0.6590 - accuracy: 0.6244 - val_loss: 0.6582 - val_accuracy: 0.6189 - 2s/epoch - 2ms/step\n",
      "Epoch 28/10000\n",
      "1126/1126 - 2s - loss: 0.6580 - accuracy: 0.6245 - val_loss: 0.6578 - val_accuracy: 0.6294 - 2s/epoch - 2ms/step\n",
      "Epoch 29/10000\n",
      "1126/1126 - 3s - loss: 0.6581 - accuracy: 0.6261 - val_loss: 0.6613 - val_accuracy: 0.6198 - 3s/epoch - 2ms/step\n",
      "Epoch 30/10000\n",
      "1126/1126 - 3s - loss: 0.6565 - accuracy: 0.6245 - val_loss: 0.6514 - val_accuracy: 0.6316 - 3s/epoch - 2ms/step\n",
      "Epoch 31/10000\n",
      "1126/1126 - 2s - loss: 0.6571 - accuracy: 0.6267 - val_loss: 0.6507 - val_accuracy: 0.6313 - 2s/epoch - 2ms/step\n",
      "Epoch 32/10000\n",
      "1126/1126 - 2s - loss: 0.6564 - accuracy: 0.6255 - val_loss: 0.6529 - val_accuracy: 0.6319 - 2s/epoch - 2ms/step\n",
      "Epoch 33/10000\n",
      "1126/1126 - 3s - loss: 0.6572 - accuracy: 0.6261 - val_loss: 0.6540 - val_accuracy: 0.6306 - 3s/epoch - 2ms/step\n",
      "Epoch 34/10000\n",
      "1126/1126 - 3s - loss: 0.6584 - accuracy: 0.6246 - val_loss: 0.6599 - val_accuracy: 0.6271 - 3s/epoch - 2ms/step\n",
      "Epoch 35/10000\n",
      "1126/1126 - 3s - loss: 0.6587 - accuracy: 0.6204 - val_loss: 0.6588 - val_accuracy: 0.6239 - 3s/epoch - 3ms/step\n",
      "Epoch 36/10000\n",
      "1126/1126 - 3s - loss: 0.6562 - accuracy: 0.6201 - val_loss: 0.6560 - val_accuracy: 0.6272 - 3s/epoch - 2ms/step\n",
      "Epoch 37/10000\n",
      "1126/1126 - 2s - loss: 0.6561 - accuracy: 0.6211 - val_loss: 0.6538 - val_accuracy: 0.6253 - 2s/epoch - 2ms/step\n",
      "Epoch 38/10000\n",
      "1126/1126 - 2s - loss: 0.6551 - accuracy: 0.6202 - val_loss: 0.6670 - val_accuracy: 0.6155 - 2s/epoch - 2ms/step\n",
      "Epoch 39/10000\n",
      "1126/1126 - 2s - loss: 0.6557 - accuracy: 0.6221 - val_loss: 0.6572 - val_accuracy: 0.6243 - 2s/epoch - 2ms/step\n",
      "Epoch 40/10000\n",
      "1126/1126 - 2s - loss: 0.6547 - accuracy: 0.6177 - val_loss: 0.6508 - val_accuracy: 0.6168 - 2s/epoch - 2ms/step\n",
      "Epoch 41/10000\n",
      "1126/1126 - 2s - loss: 0.6551 - accuracy: 0.6171 - val_loss: 0.6502 - val_accuracy: 0.6067 - 2s/epoch - 2ms/step\n",
      "Epoch 42/10000\n",
      "1126/1126 - 2s - loss: 0.6541 - accuracy: 0.6169 - val_loss: 0.6521 - val_accuracy: 0.6079 - 2s/epoch - 2ms/step\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Iterazione 45/128 (03-11-2023_11-02-25)\n",
      "hidden_layer_size: 100\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 2\n",
      "activation_hidden: sigmoid\n",
      "activation_output: sigmoid\n",
      "batch_size: 100\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 10\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 63.19%\n",
      "La recall di questo modello sul validation set è: 30.45%\n",
      "La f1 di questo modello sul validation set è: 39.03%\n",
      "La balanced accuracy di questo modello sul validation set è: 57.15%\n",
      "La precision di questo modello sul validation set è: 54.35%\n",
      "La AUC di questo modello sul validation set è: 61.21%\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.66    0.84      0.74  7,670.00\n",
      "1                  0.54    0.30      0.39  4,841.00\n",
      "accuracy           0.63    0.63      0.63      0.63\n",
      "macro avg          0.60    0.57      0.56 12,511.00\n",
      "weighted avg       0.61    0.63      0.60 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         6432         1238\n",
      "Actual 1         3367         1474\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 65.94% con il modello:\n",
      "1 - AC65.94%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 62.97% con il modello:\n",
      "1 - BA62.97%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 64.08% con il modello:\n",
      "13 - PR64.08%, L20, mV-10, MV10, P2, AHsigmoid, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-26-48).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "1126/1126 - 4s - loss: 0.9533 - accuracy: 0.6041 - val_loss: 0.7523 - val_accuracy: 0.6159 - 4s/epoch - 3ms/step\n",
      "Epoch 2/10000\n",
      "1126/1126 - 3s - loss: 0.7307 - accuracy: 0.6124 - val_loss: 0.7106 - val_accuracy: 0.6189 - 3s/epoch - 2ms/step\n",
      "Epoch 3/10000\n",
      "1126/1126 - 2s - loss: 0.7074 - accuracy: 0.6133 - val_loss: 0.6996 - val_accuracy: 0.6135 - 2s/epoch - 2ms/step\n",
      "Epoch 4/10000\n",
      "1126/1126 - 2s - loss: 0.6915 - accuracy: 0.6139 - val_loss: 0.6851 - val_accuracy: 0.6208 - 2s/epoch - 2ms/step\n",
      "Epoch 5/10000\n",
      "1126/1126 - 2s - loss: 0.6852 - accuracy: 0.6144 - val_loss: 0.6809 - val_accuracy: 0.6195 - 2s/epoch - 2ms/step\n",
      "Epoch 6/10000\n",
      "1126/1126 - 2s - loss: 0.6811 - accuracy: 0.6167 - val_loss: 0.6828 - val_accuracy: 0.6178 - 2s/epoch - 2ms/step\n",
      "Epoch 7/10000\n",
      "1126/1126 - 2s - loss: 0.6814 - accuracy: 0.6130 - val_loss: 0.6783 - val_accuracy: 0.6157 - 2s/epoch - 2ms/step\n",
      "Epoch 8/10000\n",
      "1126/1126 - 2s - loss: 0.6784 - accuracy: 0.6116 - val_loss: 0.6739 - val_accuracy: 0.6193 - 2s/epoch - 2ms/step\n",
      "Epoch 9/10000\n",
      "1126/1126 - 2s - loss: 0.6766 - accuracy: 0.6124 - val_loss: 0.6764 - val_accuracy: 0.6182 - 2s/epoch - 2ms/step\n",
      "Epoch 10/10000\n",
      "1126/1126 - 3s - loss: 0.6735 - accuracy: 0.6115 - val_loss: 0.6720 - val_accuracy: 0.6211 - 3s/epoch - 3ms/step\n",
      "Epoch 11/10000\n",
      "1126/1126 - 3s - loss: 0.6710 - accuracy: 0.6129 - val_loss: 0.6653 - val_accuracy: 0.6191 - 3s/epoch - 3ms/step\n",
      "Epoch 12/10000\n",
      "1126/1126 - 3s - loss: 0.6706 - accuracy: 0.6149 - val_loss: 0.6685 - val_accuracy: 0.6227 - 3s/epoch - 3ms/step\n",
      "Epoch 13/10000\n",
      "1126/1126 - 3s - loss: 0.6705 - accuracy: 0.6152 - val_loss: 0.6663 - val_accuracy: 0.6239 - 3s/epoch - 3ms/step\n",
      "Epoch 14/10000\n",
      "1126/1126 - 2s - loss: 0.6678 - accuracy: 0.6169 - val_loss: 0.6641 - val_accuracy: 0.6236 - 2s/epoch - 2ms/step\n",
      "Epoch 15/10000\n",
      "1126/1126 - 3s - loss: 0.6686 - accuracy: 0.6171 - val_loss: 0.6645 - val_accuracy: 0.6175 - 3s/epoch - 3ms/step\n",
      "Epoch 16/10000\n",
      "1126/1126 - 3s - loss: 0.6680 - accuracy: 0.6163 - val_loss: 0.6648 - val_accuracy: 0.6231 - 3s/epoch - 2ms/step\n",
      "Epoch 17/10000\n",
      "1126/1126 - 2s - loss: 0.6665 - accuracy: 0.6160 - val_loss: 0.6633 - val_accuracy: 0.6199 - 2s/epoch - 2ms/step\n",
      "Epoch 18/10000\n",
      "1126/1126 - 2s - loss: 0.6662 - accuracy: 0.6134 - val_loss: 0.6681 - val_accuracy: 0.6163 - 2s/epoch - 2ms/step\n",
      "Epoch 19/10000\n",
      "1126/1126 - 2s - loss: 0.6650 - accuracy: 0.6137 - val_loss: 0.6587 - val_accuracy: 0.6183 - 2s/epoch - 2ms/step\n",
      "Epoch 20/10000\n",
      "1126/1126 - 2s - loss: 0.6645 - accuracy: 0.6147 - val_loss: 0.6589 - val_accuracy: 0.6230 - 2s/epoch - 2ms/step\n",
      "Epoch 21/10000\n",
      "1126/1126 - 2s - loss: 0.6640 - accuracy: 0.6194 - val_loss: 0.6615 - val_accuracy: 0.6272 - 2s/epoch - 2ms/step\n",
      "Epoch 22/10000\n",
      "1126/1126 - 2s - loss: 0.6629 - accuracy: 0.6205 - val_loss: 0.6620 - val_accuracy: 0.6242 - 2s/epoch - 2ms/step\n",
      "Epoch 23/10000\n",
      "1126/1126 - 2s - loss: 0.6627 - accuracy: 0.6205 - val_loss: 0.6595 - val_accuracy: 0.6270 - 2s/epoch - 2ms/step\n",
      "Epoch 24/10000\n",
      "1126/1126 - 3s - loss: 0.6627 - accuracy: 0.6209 - val_loss: 0.6567 - val_accuracy: 0.6240 - 3s/epoch - 2ms/step\n",
      "Epoch 25/10000\n",
      "1126/1126 - 2s - loss: 0.6621 - accuracy: 0.6211 - val_loss: 0.6571 - val_accuracy: 0.6272 - 2s/epoch - 2ms/step\n",
      "Epoch 26/10000\n",
      "1126/1126 - 2s - loss: 0.6604 - accuracy: 0.6228 - val_loss: 0.6549 - val_accuracy: 0.6243 - 2s/epoch - 2ms/step\n",
      "Epoch 27/10000\n",
      "1126/1126 - 2s - loss: 0.6590 - accuracy: 0.6244 - val_loss: 0.6582 - val_accuracy: 0.6189 - 2s/epoch - 2ms/step\n",
      "Epoch 28/10000\n",
      "1126/1126 - 2s - loss: 0.6580 - accuracy: 0.6245 - val_loss: 0.6578 - val_accuracy: 0.6294 - 2s/epoch - 2ms/step\n",
      "Epoch 29/10000\n",
      "1126/1126 - 2s - loss: 0.6581 - accuracy: 0.6261 - val_loss: 0.6613 - val_accuracy: 0.6198 - 2s/epoch - 2ms/step\n",
      "Epoch 30/10000\n",
      "1126/1126 - 2s - loss: 0.6565 - accuracy: 0.6245 - val_loss: 0.6514 - val_accuracy: 0.6316 - 2s/epoch - 2ms/step\n",
      "Epoch 31/10000\n",
      "1126/1126 - 2s - loss: 0.6571 - accuracy: 0.6267 - val_loss: 0.6507 - val_accuracy: 0.6313 - 2s/epoch - 2ms/step\n",
      "Epoch 32/10000\n",
      "1126/1126 - 2s - loss: 0.6564 - accuracy: 0.6255 - val_loss: 0.6529 - val_accuracy: 0.6319 - 2s/epoch - 2ms/step\n",
      "Epoch 33/10000\n",
      "1126/1126 - 2s - loss: 0.6572 - accuracy: 0.6261 - val_loss: 0.6540 - val_accuracy: 0.6306 - 2s/epoch - 2ms/step\n",
      "Epoch 34/10000\n",
      "1126/1126 - 2s - loss: 0.6584 - accuracy: 0.6246 - val_loss: 0.6599 - val_accuracy: 0.6271 - 2s/epoch - 2ms/step\n",
      "Epoch 35/10000\n",
      "1126/1126 - 2s - loss: 0.6587 - accuracy: 0.6204 - val_loss: 0.6588 - val_accuracy: 0.6239 - 2s/epoch - 2ms/step\n",
      "Epoch 36/10000\n",
      "1126/1126 - 2s - loss: 0.6562 - accuracy: 0.6201 - val_loss: 0.6560 - val_accuracy: 0.6272 - 2s/epoch - 2ms/step\n",
      "Epoch 37/10000\n",
      "1126/1126 - 2s - loss: 0.6561 - accuracy: 0.6211 - val_loss: 0.6538 - val_accuracy: 0.6253 - 2s/epoch - 2ms/step\n",
      "Epoch 38/10000\n",
      "1126/1126 - 2s - loss: 0.6551 - accuracy: 0.6202 - val_loss: 0.6670 - val_accuracy: 0.6155 - 2s/epoch - 2ms/step\n",
      "Epoch 39/10000\n",
      "1126/1126 - 2s - loss: 0.6557 - accuracy: 0.6221 - val_loss: 0.6572 - val_accuracy: 0.6243 - 2s/epoch - 2ms/step\n",
      "Epoch 40/10000\n",
      "1126/1126 - 2s - loss: 0.6547 - accuracy: 0.6177 - val_loss: 0.6508 - val_accuracy: 0.6168 - 2s/epoch - 2ms/step\n",
      "Epoch 41/10000\n",
      "1126/1126 - 2s - loss: 0.6551 - accuracy: 0.6171 - val_loss: 0.6502 - val_accuracy: 0.6067 - 2s/epoch - 2ms/step\n",
      "Epoch 42/10000\n",
      "1126/1126 - 2s - loss: 0.6541 - accuracy: 0.6169 - val_loss: 0.6521 - val_accuracy: 0.6079 - 2s/epoch - 2ms/step\n",
      "Epoch 43/10000\n",
      "1126/1126 - 2s - loss: 0.6553 - accuracy: 0.6148 - val_loss: 0.6598 - val_accuracy: 0.6199 - 2s/epoch - 2ms/step\n",
      "Epoch 44/10000\n",
      "1126/1126 - 2s - loss: 0.6554 - accuracy: 0.6131 - val_loss: 0.6540 - val_accuracy: 0.6155 - 2s/epoch - 2ms/step\n",
      "Epoch 45/10000\n",
      "1126/1126 - 2s - loss: 0.6546 - accuracy: 0.6141 - val_loss: 0.6525 - val_accuracy: 0.6216 - 2s/epoch - 2ms/step\n",
      "Epoch 46/10000\n",
      "1126/1126 - 2s - loss: 0.6541 - accuracy: 0.6149 - val_loss: 0.6528 - val_accuracy: 0.6219 - 2s/epoch - 2ms/step\n",
      "Epoch 47/10000\n",
      "1126/1126 - 2s - loss: 0.6549 - accuracy: 0.6151 - val_loss: 0.6537 - val_accuracy: 0.6224 - 2s/epoch - 2ms/step\n",
      "Epoch 48/10000\n",
      "1126/1126 - 2s - loss: 0.6552 - accuracy: 0.6170 - val_loss: 0.6554 - val_accuracy: 0.6262 - 2s/epoch - 2ms/step\n",
      "Epoch 49/10000\n",
      "1126/1126 - 2s - loss: 0.6553 - accuracy: 0.6163 - val_loss: 0.6549 - val_accuracy: 0.6235 - 2s/epoch - 2ms/step\n",
      "Epoch 50/10000\n",
      "1126/1126 - 2s - loss: 0.6557 - accuracy: 0.6155 - val_loss: 0.6524 - val_accuracy: 0.6199 - 2s/epoch - 2ms/step\n",
      "Epoch 51/10000\n",
      "1126/1126 - 2s - loss: 0.6540 - accuracy: 0.6150 - val_loss: 0.6523 - val_accuracy: 0.6171 - 2s/epoch - 2ms/step\n",
      "Epoch 52/10000\n",
      "1126/1126 - 2s - loss: 0.6533 - accuracy: 0.6143 - val_loss: 0.6508 - val_accuracy: 0.6083 - 2s/epoch - 2ms/step\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Iterazione 46/128 (03-11-2023_11-04-34)\n",
      "hidden_layer_size: 100\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 2\n",
      "activation_hidden: sigmoid\n",
      "activation_output: sigmoid\n",
      "batch_size: 100\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 20\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 63.19%\n",
      "La recall di questo modello sul validation set è: 30.45%\n",
      "La f1 di questo modello sul validation set è: 39.03%\n",
      "La balanced accuracy di questo modello sul validation set è: 57.15%\n",
      "La precision di questo modello sul validation set è: 54.35%\n",
      "La AUC di questo modello sul validation set è: 61.21%\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.66    0.84      0.74  7,670.00\n",
      "1                  0.54    0.30      0.39  4,841.00\n",
      "accuracy           0.63    0.63      0.63      0.63\n",
      "macro avg          0.60    0.57      0.56 12,511.00\n",
      "weighted avg       0.61    0.63      0.60 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         6432         1238\n",
      "Actual 1         3367         1474\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 65.94% con il modello:\n",
      "1 - AC65.94%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 62.97% con il modello:\n",
      "1 - BA62.97%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 64.08% con il modello:\n",
      "13 - PR64.08%, L20, mV-10, MV10, P2, AHsigmoid, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-26-48).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "113/113 - 2s - loss: 1.9236 - accuracy: 0.5824 - val_loss: 1.0295 - val_accuracy: 0.6016 - 2s/epoch - 15ms/step\n",
      "Epoch 2/10000\n",
      "113/113 - 1s - loss: 0.9324 - accuracy: 0.6001 - val_loss: 0.8548 - val_accuracy: 0.6079 - 507ms/epoch - 4ms/step\n",
      "Epoch 3/10000\n",
      "113/113 - 0s - loss: 0.8292 - accuracy: 0.6055 - val_loss: 0.7874 - val_accuracy: 0.6114 - 476ms/epoch - 4ms/step\n",
      "Epoch 4/10000\n",
      "113/113 - 0s - loss: 0.7834 - accuracy: 0.6113 - val_loss: 0.7600 - val_accuracy: 0.6128 - 452ms/epoch - 4ms/step\n",
      "Epoch 5/10000\n",
      "113/113 - 0s - loss: 0.7593 - accuracy: 0.6142 - val_loss: 0.7389 - val_accuracy: 0.6207 - 466ms/epoch - 4ms/step\n",
      "Epoch 6/10000\n",
      "113/113 - 1s - loss: 0.7466 - accuracy: 0.6148 - val_loss: 0.7269 - val_accuracy: 0.6150 - 507ms/epoch - 4ms/step\n",
      "Epoch 7/10000\n",
      "113/113 - 0s - loss: 0.7361 - accuracy: 0.6137 - val_loss: 0.7119 - val_accuracy: 0.6206 - 463ms/epoch - 4ms/step\n",
      "Epoch 8/10000\n",
      "113/113 - 0s - loss: 0.7211 - accuracy: 0.6155 - val_loss: 0.7044 - val_accuracy: 0.6161 - 474ms/epoch - 4ms/step\n",
      "Epoch 9/10000\n",
      "113/113 - 1s - loss: 0.7144 - accuracy: 0.6152 - val_loss: 0.6995 - val_accuracy: 0.6258 - 530ms/epoch - 5ms/step\n",
      "Epoch 10/10000\n",
      "113/113 - 0s - loss: 0.7090 - accuracy: 0.6167 - val_loss: 0.6926 - val_accuracy: 0.6258 - 467ms/epoch - 4ms/step\n",
      "Epoch 11/10000\n",
      "113/113 - 1s - loss: 0.7030 - accuracy: 0.6186 - val_loss: 0.6951 - val_accuracy: 0.6243 - 548ms/epoch - 5ms/step\n",
      "Epoch 12/10000\n",
      "113/113 - 0s - loss: 0.7018 - accuracy: 0.6168 - val_loss: 0.6954 - val_accuracy: 0.6208 - 456ms/epoch - 4ms/step\n",
      "Epoch 13/10000\n",
      "113/113 - 0s - loss: 0.6978 - accuracy: 0.6162 - val_loss: 0.6911 - val_accuracy: 0.6164 - 452ms/epoch - 4ms/step\n",
      "Epoch 14/10000\n",
      "113/113 - 0s - loss: 0.6960 - accuracy: 0.6159 - val_loss: 0.6909 - val_accuracy: 0.6192 - 436ms/epoch - 4ms/step\n",
      "Epoch 15/10000\n",
      "113/113 - 0s - loss: 0.6932 - accuracy: 0.6142 - val_loss: 0.6873 - val_accuracy: 0.6215 - 441ms/epoch - 4ms/step\n",
      "Epoch 16/10000\n",
      "113/113 - 0s - loss: 0.6896 - accuracy: 0.6151 - val_loss: 0.6839 - val_accuracy: 0.6194 - 411ms/epoch - 4ms/step\n",
      "Epoch 17/10000\n",
      "113/113 - 0s - loss: 0.6877 - accuracy: 0.6157 - val_loss: 0.6817 - val_accuracy: 0.6210 - 429ms/epoch - 4ms/step\n",
      "Epoch 18/10000\n",
      "113/113 - 0s - loss: 0.6858 - accuracy: 0.6165 - val_loss: 0.6807 - val_accuracy: 0.6211 - 402ms/epoch - 4ms/step\n",
      "Epoch 19/10000\n",
      "113/113 - 0s - loss: 0.6848 - accuracy: 0.6173 - val_loss: 0.6857 - val_accuracy: 0.6197 - 429ms/epoch - 4ms/step\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Iterazione 47/128 (03-11-2023_11-04-46)\n",
      "hidden_layer_size: 100\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 2\n",
      "activation_hidden: sigmoid\n",
      "activation_output: sigmoid\n",
      "batch_size: 1000\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 10\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 62.58%\n",
      "La recall di questo modello sul validation set è: 29.27%\n",
      "La f1 di questo modello sul validation set è: 37.71%\n",
      "La balanced accuracy di questo modello sul validation set è: 56.43%\n",
      "La precision di questo modello sul validation set è: 52.97%\n",
      "La AUC di questo modello sul validation set è: 59.69%\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.65    0.84      0.73  7,670.00\n",
      "1                  0.53    0.29      0.38  4,841.00\n",
      "accuracy           0.63    0.63      0.63      0.63\n",
      "macro avg          0.59    0.56      0.55 12,511.00\n",
      "weighted avg       0.60    0.63      0.59 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         6412         1258\n",
      "Actual 1         3424         1417\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 65.94% con il modello:\n",
      "1 - AC65.94%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 62.97% con il modello:\n",
      "1 - BA62.97%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 64.08% con il modello:\n",
      "13 - PR64.08%, L20, mV-10, MV10, P2, AHsigmoid, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-26-48).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "113/113 - 2s - loss: 1.9236 - accuracy: 0.5824 - val_loss: 1.0295 - val_accuracy: 0.6016 - 2s/epoch - 13ms/step\n",
      "Epoch 2/10000\n",
      "113/113 - 1s - loss: 0.9324 - accuracy: 0.6001 - val_loss: 0.8548 - val_accuracy: 0.6079 - 544ms/epoch - 5ms/step\n",
      "Epoch 3/10000\n",
      "113/113 - 0s - loss: 0.8292 - accuracy: 0.6055 - val_loss: 0.7874 - val_accuracy: 0.6114 - 433ms/epoch - 4ms/step\n",
      "Epoch 4/10000\n",
      "113/113 - 0s - loss: 0.7834 - accuracy: 0.6113 - val_loss: 0.7600 - val_accuracy: 0.6128 - 446ms/epoch - 4ms/step\n",
      "Epoch 5/10000\n",
      "113/113 - 1s - loss: 0.7593 - accuracy: 0.6142 - val_loss: 0.7389 - val_accuracy: 0.6207 - 521ms/epoch - 5ms/step\n",
      "Epoch 6/10000\n",
      "113/113 - 0s - loss: 0.7466 - accuracy: 0.6148 - val_loss: 0.7269 - val_accuracy: 0.6150 - 468ms/epoch - 4ms/step\n",
      "Epoch 7/10000\n",
      "113/113 - 0s - loss: 0.7361 - accuracy: 0.6137 - val_loss: 0.7119 - val_accuracy: 0.6206 - 468ms/epoch - 4ms/step\n",
      "Epoch 8/10000\n",
      "113/113 - 1s - loss: 0.7211 - accuracy: 0.6155 - val_loss: 0.7044 - val_accuracy: 0.6161 - 508ms/epoch - 4ms/step\n",
      "Epoch 9/10000\n",
      "113/113 - 0s - loss: 0.7144 - accuracy: 0.6152 - val_loss: 0.6995 - val_accuracy: 0.6258 - 458ms/epoch - 4ms/step\n",
      "Epoch 10/10000\n",
      "113/113 - 0s - loss: 0.7090 - accuracy: 0.6167 - val_loss: 0.6926 - val_accuracy: 0.6258 - 448ms/epoch - 4ms/step\n",
      "Epoch 11/10000\n",
      "113/113 - 0s - loss: 0.7030 - accuracy: 0.6186 - val_loss: 0.6951 - val_accuracy: 0.6243 - 492ms/epoch - 4ms/step\n",
      "Epoch 12/10000\n",
      "113/113 - 1s - loss: 0.7018 - accuracy: 0.6168 - val_loss: 0.6954 - val_accuracy: 0.6208 - 502ms/epoch - 4ms/step\n",
      "Epoch 13/10000\n",
      "113/113 - 1s - loss: 0.6978 - accuracy: 0.6162 - val_loss: 0.6911 - val_accuracy: 0.6164 - 539ms/epoch - 5ms/step\n",
      "Epoch 14/10000\n",
      "113/113 - 1s - loss: 0.6960 - accuracy: 0.6159 - val_loss: 0.6909 - val_accuracy: 0.6192 - 518ms/epoch - 5ms/step\n",
      "Epoch 15/10000\n",
      "113/113 - 0s - loss: 0.6932 - accuracy: 0.6142 - val_loss: 0.6873 - val_accuracy: 0.6215 - 417ms/epoch - 4ms/step\n",
      "Epoch 16/10000\n",
      "113/113 - 0s - loss: 0.6896 - accuracy: 0.6151 - val_loss: 0.6839 - val_accuracy: 0.6194 - 423ms/epoch - 4ms/step\n",
      "Epoch 17/10000\n",
      "113/113 - 0s - loss: 0.6877 - accuracy: 0.6157 - val_loss: 0.6817 - val_accuracy: 0.6210 - 415ms/epoch - 4ms/step\n",
      "Epoch 18/10000\n",
      "113/113 - 0s - loss: 0.6858 - accuracy: 0.6165 - val_loss: 0.6807 - val_accuracy: 0.6211 - 403ms/epoch - 4ms/step\n",
      "Epoch 19/10000\n",
      "113/113 - 0s - loss: 0.6848 - accuracy: 0.6173 - val_loss: 0.6857 - val_accuracy: 0.6197 - 422ms/epoch - 4ms/step\n",
      "Epoch 20/10000\n",
      "113/113 - 0s - loss: 0.6864 - accuracy: 0.6154 - val_loss: 0.6841 - val_accuracy: 0.6208 - 447ms/epoch - 4ms/step\n",
      "Epoch 21/10000\n",
      "113/113 - 0s - loss: 0.6857 - accuracy: 0.6175 - val_loss: 0.6814 - val_accuracy: 0.6167 - 433ms/epoch - 4ms/step\n",
      "Epoch 22/10000\n",
      "113/113 - 0s - loss: 0.6819 - accuracy: 0.6175 - val_loss: 0.6834 - val_accuracy: 0.6179 - 488ms/epoch - 4ms/step\n",
      "Epoch 23/10000\n",
      "113/113 - 0s - loss: 0.6806 - accuracy: 0.6182 - val_loss: 0.6774 - val_accuracy: 0.6230 - 422ms/epoch - 4ms/step\n",
      "Epoch 24/10000\n",
      "113/113 - 0s - loss: 0.6790 - accuracy: 0.6186 - val_loss: 0.6786 - val_accuracy: 0.6206 - 448ms/epoch - 4ms/step\n",
      "Epoch 25/10000\n",
      "113/113 - 0s - loss: 0.6801 - accuracy: 0.6178 - val_loss: 0.6780 - val_accuracy: 0.6204 - 419ms/epoch - 4ms/step\n",
      "Epoch 26/10000\n",
      "113/113 - 0s - loss: 0.6779 - accuracy: 0.6188 - val_loss: 0.6758 - val_accuracy: 0.6210 - 420ms/epoch - 4ms/step\n",
      "Epoch 27/10000\n",
      "113/113 - 0s - loss: 0.6756 - accuracy: 0.6195 - val_loss: 0.6772 - val_accuracy: 0.6203 - 444ms/epoch - 4ms/step\n",
      "Epoch 28/10000\n",
      "113/113 - 1s - loss: 0.6748 - accuracy: 0.6200 - val_loss: 0.6724 - val_accuracy: 0.6222 - 501ms/epoch - 4ms/step\n",
      "Epoch 29/10000\n",
      "113/113 - 0s - loss: 0.6744 - accuracy: 0.6181 - val_loss: 0.6706 - val_accuracy: 0.6230 - 433ms/epoch - 4ms/step\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Iterazione 48/128 (03-11-2023_11-05-03)\n",
      "hidden_layer_size: 100\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 2\n",
      "activation_hidden: sigmoid\n",
      "activation_output: sigmoid\n",
      "batch_size: 1000\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 20\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 62.58%\n",
      "La recall di questo modello sul validation set è: 29.27%\n",
      "La f1 di questo modello sul validation set è: 37.71%\n",
      "La balanced accuracy di questo modello sul validation set è: 56.43%\n",
      "La precision di questo modello sul validation set è: 52.97%\n",
      "La AUC di questo modello sul validation set è: 59.69%\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.65    0.84      0.73  7,670.00\n",
      "1                  0.53    0.29      0.38  4,841.00\n",
      "accuracy           0.63    0.63      0.63      0.63\n",
      "macro avg          0.59    0.56      0.55 12,511.00\n",
      "weighted avg       0.60    0.63      0.59 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         6412         1258\n",
      "Actual 1         3424         1417\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 65.94% con il modello:\n",
      "1 - AC65.94%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 62.97% con il modello:\n",
      "1 - BA62.97%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 64.08% con il modello:\n",
      "13 - PR64.08%, L20, mV-10, MV10, P2, AHsigmoid, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-26-48).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "1126/1126 - 4s - loss: 222186848.0000 - accuracy: 0.5576 - val_loss: 2556793.7500 - val_accuracy: 0.6122 - 4s/epoch - 4ms/step\n",
      "Epoch 2/10000\n",
      "1126/1126 - 3s - loss: 2489645.7500 - accuracy: 0.5827 - val_loss: 1786073.2500 - val_accuracy: 0.5938 - 3s/epoch - 2ms/step\n",
      "Epoch 3/10000\n",
      "1126/1126 - 2s - loss: 2066118.0000 - accuracy: 0.5763 - val_loss: 1189415.6250 - val_accuracy: 0.6041 - 2s/epoch - 2ms/step\n",
      "Epoch 4/10000\n",
      "1126/1126 - 2s - loss: 1897283.5000 - accuracy: 0.5747 - val_loss: 1123776.8750 - val_accuracy: 0.6020 - 2s/epoch - 2ms/step\n",
      "Epoch 5/10000\n",
      "1126/1126 - 3s - loss: 1705037.5000 - accuracy: 0.5755 - val_loss: 2327347.7500 - val_accuracy: 0.4969 - 3s/epoch - 2ms/step\n",
      "Epoch 6/10000\n",
      "1126/1126 - 3s - loss: 1686733.2500 - accuracy: 0.5760 - val_loss: 1699035.3750 - val_accuracy: 0.6090 - 3s/epoch - 2ms/step\n",
      "Epoch 7/10000\n",
      "1126/1126 - 2s - loss: 1654858.7500 - accuracy: 0.5753 - val_loss: 780819.5000 - val_accuracy: 0.5883 - 2s/epoch - 2ms/step\n",
      "Epoch 8/10000\n",
      "1126/1126 - 3s - loss: 1583424.1250 - accuracy: 0.5756 - val_loss: 1466956.8750 - val_accuracy: 0.5408 - 3s/epoch - 2ms/step\n",
      "Epoch 9/10000\n",
      "1126/1126 - 3s - loss: 1582743.8750 - accuracy: 0.5770 - val_loss: 746363.0000 - val_accuracy: 0.6247 - 3s/epoch - 2ms/step\n",
      "Epoch 10/10000\n",
      "1126/1126 - 3s - loss: 1510314.0000 - accuracy: 0.5764 - val_loss: 1758760.3750 - val_accuracy: 0.6076 - 3s/epoch - 2ms/step\n",
      "Epoch 11/10000\n",
      "1126/1126 - 2s - loss: 1567898.7500 - accuracy: 0.5761 - val_loss: 755960.5625 - val_accuracy: 0.6300 - 2s/epoch - 2ms/step\n",
      "Epoch 12/10000\n",
      "1126/1126 - 3s - loss: 1411696.2500 - accuracy: 0.5770 - val_loss: 1632275.1250 - val_accuracy: 0.5225 - 3s/epoch - 2ms/step\n",
      "Epoch 13/10000\n",
      "1126/1126 - 3s - loss: 1688045.0000 - accuracy: 0.5798 - val_loss: 900079.2500 - val_accuracy: 0.6262 - 3s/epoch - 2ms/step\n",
      "Epoch 14/10000\n",
      "1126/1126 - 2s - loss: 1455035.3750 - accuracy: 0.5792 - val_loss: 1700457.2500 - val_accuracy: 0.4907 - 2s/epoch - 2ms/step\n",
      "Epoch 15/10000\n",
      "1126/1126 - 2s - loss: 1419409.0000 - accuracy: 0.5784 - val_loss: 2915462.2500 - val_accuracy: 0.6156 - 2s/epoch - 2ms/step\n",
      "Epoch 16/10000\n",
      "1126/1126 - 3s - loss: 1305770.3750 - accuracy: 0.5823 - val_loss: 1073417.8750 - val_accuracy: 0.6182 - 3s/epoch - 2ms/step\n",
      "Epoch 17/10000\n",
      "1126/1126 - 2s - loss: 1333541.5000 - accuracy: 0.5787 - val_loss: 1381923.5000 - val_accuracy: 0.6118 - 2s/epoch - 2ms/step\n",
      "Epoch 18/10000\n",
      "1126/1126 - 3s - loss: 1356034.0000 - accuracy: 0.5806 - val_loss: 1375377.0000 - val_accuracy: 0.6073 - 3s/epoch - 2ms/step\n",
      "Epoch 19/10000\n",
      "1126/1126 - 3s - loss: 1223627.7500 - accuracy: 0.5812 - val_loss: 699615.6875 - val_accuracy: 0.5699 - 3s/epoch - 2ms/step\n",
      "Epoch 20/10000\n",
      "1126/1126 - 2s - loss: 1344703.7500 - accuracy: 0.5815 - val_loss: 2460493.2500 - val_accuracy: 0.6139 - 2s/epoch - 2ms/step\n",
      "Epoch 21/10000\n",
      "1126/1126 - 2s - loss: 1310972.0000 - accuracy: 0.5836 - val_loss: 1724034.7500 - val_accuracy: 0.6194 - 2s/epoch - 2ms/step\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Iterazione 49/128 (03-11-2023_11-05-59)\n",
      "hidden_layer_size: 100\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 3\n",
      "activation_hidden: relu\n",
      "activation_output: sigmoid\n",
      "batch_size: 100\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 10\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 63.0%\n",
      "La recall di questo modello sul validation set è: 23.88%\n",
      "La f1 di questo modello sul validation set è: 33.31%\n",
      "La balanced accuracy di questo modello sul validation set è: 55.79%\n",
      "La precision di questo modello sul validation set è: 55.05%\n",
      "La AUC di questo modello sul validation set è: 55.79%\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.65    0.88      0.74  7,670.00\n",
      "1                  0.55    0.24      0.33  4,841.00\n",
      "accuracy           0.63    0.63      0.63      0.63\n",
      "macro avg          0.60    0.56      0.54 12,511.00\n",
      "weighted avg       0.61    0.63      0.58 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         6726          944\n",
      "Actual 1         3685         1156\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 65.94% con il modello:\n",
      "1 - AC65.94%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 62.97% con il modello:\n",
      "1 - BA62.97%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 64.08% con il modello:\n",
      "13 - PR64.08%, L20, mV-10, MV10, P2, AHsigmoid, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-26-48).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "1126/1126 - 4s - loss: 222186848.0000 - accuracy: 0.5576 - val_loss: 2556793.7500 - val_accuracy: 0.6122 - 4s/epoch - 4ms/step\n",
      "Epoch 2/10000\n",
      "1126/1126 - 3s - loss: 2489645.7500 - accuracy: 0.5827 - val_loss: 1786073.2500 - val_accuracy: 0.5938 - 3s/epoch - 2ms/step\n",
      "Epoch 3/10000\n",
      "1126/1126 - 3s - loss: 2066118.0000 - accuracy: 0.5763 - val_loss: 1189415.6250 - val_accuracy: 0.6041 - 3s/epoch - 2ms/step\n",
      "Epoch 4/10000\n",
      "1126/1126 - 2s - loss: 1897283.5000 - accuracy: 0.5747 - val_loss: 1123776.8750 - val_accuracy: 0.6020 - 2s/epoch - 2ms/step\n",
      "Epoch 5/10000\n",
      "1126/1126 - 3s - loss: 1705037.5000 - accuracy: 0.5755 - val_loss: 2327347.7500 - val_accuracy: 0.4969 - 3s/epoch - 2ms/step\n",
      "Epoch 6/10000\n",
      "1126/1126 - 3s - loss: 1686733.2500 - accuracy: 0.5760 - val_loss: 1699035.3750 - val_accuracy: 0.6090 - 3s/epoch - 2ms/step\n",
      "Epoch 7/10000\n",
      "1126/1126 - 4s - loss: 1654858.7500 - accuracy: 0.5753 - val_loss: 780819.5000 - val_accuracy: 0.5883 - 4s/epoch - 3ms/step\n",
      "Epoch 8/10000\n",
      "1126/1126 - 3s - loss: 1583424.1250 - accuracy: 0.5756 - val_loss: 1466956.8750 - val_accuracy: 0.5408 - 3s/epoch - 3ms/step\n",
      "Epoch 9/10000\n",
      "1126/1126 - 3s - loss: 1582743.8750 - accuracy: 0.5770 - val_loss: 746363.0000 - val_accuracy: 0.6247 - 3s/epoch - 3ms/step\n",
      "Epoch 10/10000\n",
      "1126/1126 - 3s - loss: 1510314.0000 - accuracy: 0.5764 - val_loss: 1758760.3750 - val_accuracy: 0.6076 - 3s/epoch - 3ms/step\n",
      "Epoch 11/10000\n",
      "1126/1126 - 3s - loss: 1567898.7500 - accuracy: 0.5761 - val_loss: 755960.5625 - val_accuracy: 0.6300 - 3s/epoch - 3ms/step\n",
      "Epoch 12/10000\n",
      "1126/1126 - 3s - loss: 1411696.2500 - accuracy: 0.5770 - val_loss: 1632275.1250 - val_accuracy: 0.5225 - 3s/epoch - 3ms/step\n",
      "Epoch 13/10000\n",
      "1126/1126 - 3s - loss: 1688045.0000 - accuracy: 0.5798 - val_loss: 900079.2500 - val_accuracy: 0.6262 - 3s/epoch - 3ms/step\n",
      "Epoch 14/10000\n",
      "1126/1126 - 3s - loss: 1455035.3750 - accuracy: 0.5792 - val_loss: 1700457.2500 - val_accuracy: 0.4907 - 3s/epoch - 3ms/step\n",
      "Epoch 15/10000\n",
      "1126/1126 - 3s - loss: 1419409.0000 - accuracy: 0.5784 - val_loss: 2915462.2500 - val_accuracy: 0.6156 - 3s/epoch - 3ms/step\n",
      "Epoch 16/10000\n",
      "1126/1126 - 3s - loss: 1305770.3750 - accuracy: 0.5823 - val_loss: 1073417.8750 - val_accuracy: 0.6182 - 3s/epoch - 2ms/step\n",
      "Epoch 17/10000\n",
      "1126/1126 - 3s - loss: 1333541.5000 - accuracy: 0.5787 - val_loss: 1381923.5000 - val_accuracy: 0.6118 - 3s/epoch - 2ms/step\n",
      "Epoch 18/10000\n",
      "1126/1126 - 3s - loss: 1356034.0000 - accuracy: 0.5806 - val_loss: 1375377.0000 - val_accuracy: 0.6073 - 3s/epoch - 2ms/step\n",
      "Epoch 19/10000\n",
      "1126/1126 - 3s - loss: 1223627.7500 - accuracy: 0.5812 - val_loss: 699615.6875 - val_accuracy: 0.5699 - 3s/epoch - 2ms/step\n",
      "Epoch 20/10000\n",
      "1126/1126 - 3s - loss: 1344703.7500 - accuracy: 0.5815 - val_loss: 2460493.2500 - val_accuracy: 0.6139 - 3s/epoch - 3ms/step\n",
      "Epoch 21/10000\n",
      "1126/1126 - 3s - loss: 1310972.0000 - accuracy: 0.5836 - val_loss: 1724034.7500 - val_accuracy: 0.6194 - 3s/epoch - 2ms/step\n",
      "Epoch 22/10000\n",
      "1126/1126 - 3s - loss: 1402954.8750 - accuracy: 0.5818 - val_loss: 602146.0625 - val_accuracy: 0.6310 - 3s/epoch - 3ms/step\n",
      "Epoch 23/10000\n",
      "1126/1126 - 3s - loss: 1263318.0000 - accuracy: 0.5826 - val_loss: 499346.6562 - val_accuracy: 0.6276 - 3s/epoch - 2ms/step\n",
      "Epoch 24/10000\n",
      "1126/1126 - 3s - loss: 1267078.8750 - accuracy: 0.5828 - val_loss: 1452491.8750 - val_accuracy: 0.6216 - 3s/epoch - 2ms/step\n",
      "Epoch 25/10000\n",
      "1126/1126 - 3s - loss: 1305304.5000 - accuracy: 0.5839 - val_loss: 919552.2500 - val_accuracy: 0.6211 - 3s/epoch - 2ms/step\n",
      "Epoch 26/10000\n",
      "1126/1126 - 3s - loss: 1273993.8750 - accuracy: 0.5845 - val_loss: 1995178.2500 - val_accuracy: 0.6164 - 3s/epoch - 2ms/step\n",
      "Epoch 27/10000\n",
      "1126/1126 - 2s - loss: 1468803.8750 - accuracy: 0.5824 - val_loss: 644734.3125 - val_accuracy: 0.6031 - 2s/epoch - 2ms/step\n",
      "Epoch 28/10000\n",
      "1126/1126 - 2s - loss: 1458550.2500 - accuracy: 0.5836 - val_loss: 500519.9062 - val_accuracy: 0.6322 - 2s/epoch - 2ms/step\n",
      "Epoch 29/10000\n",
      "1126/1126 - 3s - loss: 1268179.0000 - accuracy: 0.5829 - val_loss: 510542.0312 - val_accuracy: 0.6403 - 3s/epoch - 2ms/step\n",
      "Epoch 30/10000\n",
      "1126/1126 - 3s - loss: 1227038.6250 - accuracy: 0.5862 - val_loss: 1381158.0000 - val_accuracy: 0.6158 - 3s/epoch - 2ms/step\n",
      "Epoch 31/10000\n",
      "1126/1126 - 3s - loss: 1408668.2500 - accuracy: 0.5829 - val_loss: 618108.1875 - val_accuracy: 0.6378 - 3s/epoch - 2ms/step\n",
      "Epoch 32/10000\n",
      "1126/1126 - 3s - loss: 1352395.7500 - accuracy: 0.5852 - val_loss: 1878966.7500 - val_accuracy: 0.6149 - 3s/epoch - 2ms/step\n",
      "Epoch 33/10000\n",
      "1126/1126 - 3s - loss: 1307324.3750 - accuracy: 0.5833 - val_loss: 938499.6250 - val_accuracy: 0.5466 - 3s/epoch - 2ms/step\n",
      "Epoch 34/10000\n",
      "1126/1126 - 3s - loss: 1300235.2500 - accuracy: 0.5863 - val_loss: 462390.4688 - val_accuracy: 0.6472 - 3s/epoch - 2ms/step\n",
      "Epoch 35/10000\n",
      "1126/1126 - 3s - loss: 1297289.8750 - accuracy: 0.5868 - val_loss: 2195339.2500 - val_accuracy: 0.4698 - 3s/epoch - 2ms/step\n",
      "Epoch 36/10000\n",
      "1126/1126 - 3s - loss: 1300669.7500 - accuracy: 0.5863 - val_loss: 636494.3750 - val_accuracy: 0.6314 - 3s/epoch - 2ms/step\n",
      "Epoch 37/10000\n",
      "1126/1126 - 3s - loss: 1283070.2500 - accuracy: 0.5863 - val_loss: 346106.0625 - val_accuracy: 0.6444 - 3s/epoch - 2ms/step\n",
      "Epoch 38/10000\n",
      "1126/1126 - 3s - loss: 1395793.3750 - accuracy: 0.5817 - val_loss: 559156.0000 - val_accuracy: 0.6438 - 3s/epoch - 2ms/step\n",
      "Epoch 39/10000\n",
      "1126/1126 - 3s - loss: 1244214.1250 - accuracy: 0.5841 - val_loss: 726055.6250 - val_accuracy: 0.6074 - 3s/epoch - 2ms/step\n",
      "Epoch 40/10000\n",
      "1126/1126 - 3s - loss: 1208582.7500 - accuracy: 0.5875 - val_loss: 490878.9688 - val_accuracy: 0.6434 - 3s/epoch - 2ms/step\n",
      "Epoch 41/10000\n",
      "1126/1126 - 3s - loss: 1250467.3750 - accuracy: 0.5881 - val_loss: 1619278.8750 - val_accuracy: 0.4945 - 3s/epoch - 2ms/step\n",
      "Epoch 42/10000\n",
      "1126/1126 - 3s - loss: 1264021.6250 - accuracy: 0.5871 - val_loss: 442345.9062 - val_accuracy: 0.6537 - 3s/epoch - 2ms/step\n",
      "Epoch 43/10000\n",
      "1126/1126 - 3s - loss: 1172961.3750 - accuracy: 0.5885 - val_loss: 1192892.2500 - val_accuracy: 0.4994 - 3s/epoch - 2ms/step\n",
      "Epoch 44/10000\n",
      "1126/1126 - 3s - loss: 1239758.0000 - accuracy: 0.5865 - val_loss: 379885.2188 - val_accuracy: 0.6475 - 3s/epoch - 2ms/step\n",
      "Epoch 45/10000\n",
      "1126/1126 - 2s - loss: 1381282.5000 - accuracy: 0.5889 - val_loss: 1853718.1250 - val_accuracy: 0.6139 - 2s/epoch - 2ms/step\n",
      "Epoch 46/10000\n",
      "1126/1126 - 3s - loss: 1301837.1250 - accuracy: 0.5855 - val_loss: 988725.3750 - val_accuracy: 0.5770 - 3s/epoch - 2ms/step\n",
      "Epoch 47/10000\n",
      "1126/1126 - 3s - loss: 1267104.1250 - accuracy: 0.5866 - val_loss: 942674.1875 - val_accuracy: 0.5458 - 3s/epoch - 2ms/step\n",
      "Epoch 48/10000\n",
      "1126/1126 - 2s - loss: 1154048.3750 - accuracy: 0.5889 - val_loss: 3704370.0000 - val_accuracy: 0.4692 - 2s/epoch - 2ms/step\n",
      "Epoch 49/10000\n",
      "1126/1126 - 3s - loss: 1320304.1250 - accuracy: 0.5877 - val_loss: 1467264.8750 - val_accuracy: 0.4936 - 3s/epoch - 2ms/step\n",
      "Epoch 50/10000\n",
      "1126/1126 - 3s - loss: 1219716.6250 - accuracy: 0.5904 - val_loss: 2246781.5000 - val_accuracy: 0.6184 - 3s/epoch - 2ms/step\n",
      "Epoch 51/10000\n",
      "1126/1126 - 3s - loss: 1088220.5000 - accuracy: 0.5919 - val_loss: 1732809.1250 - val_accuracy: 0.5080 - 3s/epoch - 2ms/step\n",
      "Epoch 52/10000\n",
      "1126/1126 - 3s - loss: 1315664.1250 - accuracy: 0.5852 - val_loss: 1056320.2500 - val_accuracy: 0.6251 - 3s/epoch - 2ms/step\n",
      "Epoch 53/10000\n",
      "1126/1126 - 3s - loss: 1086759.5000 - accuracy: 0.5911 - val_loss: 473235.8438 - val_accuracy: 0.6342 - 3s/epoch - 2ms/step\n",
      "Epoch 54/10000\n",
      "1126/1126 - 3s - loss: 1191711.8750 - accuracy: 0.5892 - val_loss: 471630.1562 - val_accuracy: 0.5796 - 3s/epoch - 2ms/step\n",
      "Epoch 55/10000\n",
      "1126/1126 - 3s - loss: 1199291.1250 - accuracy: 0.5893 - val_loss: 370584.7500 - val_accuracy: 0.6402 - 3s/epoch - 2ms/step\n",
      "Epoch 56/10000\n",
      "1126/1126 - 3s - loss: 1338076.1250 - accuracy: 0.5876 - val_loss: 1130677.7500 - val_accuracy: 0.6276 - 3s/epoch - 2ms/step\n",
      "Epoch 57/10000\n",
      "1126/1126 - 3s - loss: 1169555.2500 - accuracy: 0.5910 - val_loss: 2499602.0000 - val_accuracy: 0.5464 - 3s/epoch - 2ms/step\n",
      "Epoch 58/10000\n",
      "1126/1126 - 3s - loss: 1273908.5000 - accuracy: 0.5873 - val_loss: 918273.1875 - val_accuracy: 0.5577 - 3s/epoch - 2ms/step\n",
      "Epoch 59/10000\n",
      "1126/1126 - 2s - loss: 1335411.6250 - accuracy: 0.5886 - val_loss: 854893.8125 - val_accuracy: 0.5840 - 2s/epoch - 2ms/step\n",
      "Epoch 60/10000\n",
      "1126/1126 - 3s - loss: 1223784.2500 - accuracy: 0.5924 - val_loss: 326906.0938 - val_accuracy: 0.6366 - 3s/epoch - 2ms/step\n",
      "Epoch 61/10000\n",
      "1126/1126 - 3s - loss: 1256625.5000 - accuracy: 0.5901 - val_loss: 3294212.0000 - val_accuracy: 0.6203 - 3s/epoch - 2ms/step\n",
      "Epoch 62/10000\n",
      "1126/1126 - 3s - loss: 1217267.7500 - accuracy: 0.5916 - val_loss: 2954642.0000 - val_accuracy: 0.4556 - 3s/epoch - 2ms/step\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Iterazione 50/128 (03-11-2023_11-08-50)\n",
      "hidden_layer_size: 100\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 3\n",
      "activation_hidden: relu\n",
      "activation_output: sigmoid\n",
      "batch_size: 100\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 20\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 65.37%\n",
      "La recall di questo modello sul validation set è: 52.2%\n",
      "La f1 di questo modello sul validation set è: 53.85%\n",
      "La balanced accuracy di questo modello sul validation set è: 62.94%\n",
      "La precision di questo modello sul validation set è: 55.6%\n",
      "La AUC di questo modello sul validation set è: 62.94%\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.71    0.74      0.72  7,670.00\n",
      "1                  0.56    0.52      0.54  4,841.00\n",
      "accuracy           0.65    0.65      0.65      0.65\n",
      "macro avg          0.63    0.63      0.63 12,511.00\n",
      "weighted avg       0.65    0.65      0.65 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         5652         2018\n",
      "Actual 1         2314         2527\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 65.94% con il modello:\n",
      "1 - AC65.94%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 62.97% con il modello:\n",
      "1 - BA62.97%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 64.08% con il modello:\n",
      "13 - PR64.08%, L20, mV-10, MV10, P2, AHsigmoid, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-26-48).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "113/113 - 2s - loss: 1988906240.0000 - accuracy: 0.3887 - val_loss: 506851360.0000 - val_accuracy: 0.3871 - 2s/epoch - 16ms/step\n",
      "Epoch 2/10000\n",
      "113/113 - 1s - loss: 158271552.0000 - accuracy: 0.4569 - val_loss: 31970716.0000 - val_accuracy: 0.5470 - 625ms/epoch - 6ms/step\n",
      "Epoch 3/10000\n",
      "113/113 - 1s - loss: 21601620.0000 - accuracy: 0.5664 - val_loss: 12973387.0000 - val_accuracy: 0.5849 - 578ms/epoch - 5ms/step\n",
      "Epoch 4/10000\n",
      "113/113 - 1s - loss: 8902825.0000 - accuracy: 0.5809 - val_loss: 6046157.0000 - val_accuracy: 0.5933 - 546ms/epoch - 5ms/step\n",
      "Epoch 5/10000\n",
      "113/113 - 1s - loss: 5142498.0000 - accuracy: 0.5930 - val_loss: 4516010.5000 - val_accuracy: 0.5990 - 558ms/epoch - 5ms/step\n",
      "Epoch 6/10000\n",
      "113/113 - 1s - loss: 4303823.0000 - accuracy: 0.6018 - val_loss: 3864076.5000 - val_accuracy: 0.6061 - 562ms/epoch - 5ms/step\n",
      "Epoch 7/10000\n",
      "113/113 - 1s - loss: 3806015.5000 - accuracy: 0.6031 - val_loss: 3433917.0000 - val_accuracy: 0.6076 - 600ms/epoch - 5ms/step\n",
      "Epoch 8/10000\n",
      "113/113 - 1s - loss: 3405245.7500 - accuracy: 0.6012 - val_loss: 3012583.5000 - val_accuracy: 0.6012 - 536ms/epoch - 5ms/step\n",
      "Epoch 9/10000\n",
      "113/113 - 1s - loss: 3319943.7500 - accuracy: 0.5960 - val_loss: 3024727.5000 - val_accuracy: 0.5943 - 557ms/epoch - 5ms/step\n",
      "Epoch 10/10000\n",
      "113/113 - 1s - loss: 3105187.5000 - accuracy: 0.5944 - val_loss: 2604809.7500 - val_accuracy: 0.5948 - 559ms/epoch - 5ms/step\n",
      "Epoch 11/10000\n",
      "113/113 - 1s - loss: 3009622.0000 - accuracy: 0.5892 - val_loss: 3636670.5000 - val_accuracy: 0.6025 - 593ms/epoch - 5ms/step\n",
      "Epoch 12/10000\n",
      "113/113 - 1s - loss: 2610674.0000 - accuracy: 0.5916 - val_loss: 2300558.2500 - val_accuracy: 0.5896 - 600ms/epoch - 5ms/step\n",
      "Epoch 13/10000\n",
      "113/113 - 1s - loss: 2417287.2500 - accuracy: 0.5915 - val_loss: 3112555.2500 - val_accuracy: 0.5617 - 558ms/epoch - 5ms/step\n",
      "Epoch 14/10000\n",
      "113/113 - 0s - loss: 2673399.5000 - accuracy: 0.5868 - val_loss: 3024589.5000 - val_accuracy: 0.6064 - 488ms/epoch - 4ms/step\n",
      "Epoch 15/10000\n",
      "113/113 - 1s - loss: 2088796.2500 - accuracy: 0.5913 - val_loss: 2021652.3750 - val_accuracy: 0.5955 - 545ms/epoch - 5ms/step\n",
      "Epoch 16/10000\n",
      "113/113 - 0s - loss: 2268332.0000 - accuracy: 0.5876 - val_loss: 2422346.0000 - val_accuracy: 0.6040 - 468ms/epoch - 4ms/step\n",
      "Epoch 17/10000\n",
      "113/113 - 0s - loss: 2037711.2500 - accuracy: 0.5879 - val_loss: 3087197.0000 - val_accuracy: 0.5358 - 478ms/epoch - 4ms/step\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Iterazione 51/128 (03-11-2023_11-09-03)\n",
      "hidden_layer_size: 100\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 3\n",
      "activation_hidden: relu\n",
      "activation_output: sigmoid\n",
      "batch_size: 1000\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 10\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 60.76%\n",
      "La recall di questo modello sul validation set è: 41.42%\n",
      "La f1 di questo modello sul validation set è: 44.96%\n",
      "La balanced accuracy di questo modello sul validation set è: 57.19%\n",
      "La precision di questo modello sul validation set è: 49.17%\n",
      "La AUC di questo modello sul validation set è: 57.19%\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.66    0.73      0.70  7,670.00\n",
      "1                  0.49    0.41      0.45  4,841.00\n",
      "accuracy           0.61    0.61      0.61      0.61\n",
      "macro avg          0.58    0.57      0.57 12,511.00\n",
      "weighted avg       0.60    0.61      0.60 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         5597         2073\n",
      "Actual 1         2836         2005\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 65.94% con il modello:\n",
      "1 - AC65.94%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 62.97% con il modello:\n",
      "1 - BA62.97%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 64.08% con il modello:\n",
      "13 - PR64.08%, L20, mV-10, MV10, P2, AHsigmoid, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-26-48).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "113/113 - 2s - loss: 1988906240.0000 - accuracy: 0.3887 - val_loss: 506851360.0000 - val_accuracy: 0.3871 - 2s/epoch - 17ms/step\n",
      "Epoch 2/10000\n",
      "113/113 - 1s - loss: 158271552.0000 - accuracy: 0.4569 - val_loss: 31970716.0000 - val_accuracy: 0.5470 - 517ms/epoch - 5ms/step\n",
      "Epoch 3/10000\n",
      "113/113 - 1s - loss: 21601620.0000 - accuracy: 0.5664 - val_loss: 12973387.0000 - val_accuracy: 0.5849 - 606ms/epoch - 5ms/step\n",
      "Epoch 4/10000\n",
      "113/113 - 1s - loss: 8902825.0000 - accuracy: 0.5809 - val_loss: 6046157.0000 - val_accuracy: 0.5933 - 512ms/epoch - 5ms/step\n",
      "Epoch 5/10000\n",
      "113/113 - 1s - loss: 5142498.0000 - accuracy: 0.5930 - val_loss: 4516010.5000 - val_accuracy: 0.5990 - 566ms/epoch - 5ms/step\n",
      "Epoch 6/10000\n",
      "113/113 - 1s - loss: 4303823.0000 - accuracy: 0.6018 - val_loss: 3864076.5000 - val_accuracy: 0.6061 - 549ms/epoch - 5ms/step\n",
      "Epoch 7/10000\n",
      "113/113 - 1s - loss: 3806015.5000 - accuracy: 0.6031 - val_loss: 3433917.0000 - val_accuracy: 0.6076 - 531ms/epoch - 5ms/step\n",
      "Epoch 8/10000\n",
      "113/113 - 1s - loss: 3405245.7500 - accuracy: 0.6012 - val_loss: 3012583.5000 - val_accuracy: 0.6012 - 536ms/epoch - 5ms/step\n",
      "Epoch 9/10000\n",
      "113/113 - 1s - loss: 3319943.7500 - accuracy: 0.5960 - val_loss: 3024727.5000 - val_accuracy: 0.5943 - 596ms/epoch - 5ms/step\n",
      "Epoch 10/10000\n",
      "113/113 - 1s - loss: 3105187.5000 - accuracy: 0.5944 - val_loss: 2604809.7500 - val_accuracy: 0.5948 - 688ms/epoch - 6ms/step\n",
      "Epoch 11/10000\n",
      "113/113 - 1s - loss: 3009622.0000 - accuracy: 0.5892 - val_loss: 3636670.5000 - val_accuracy: 0.6025 - 579ms/epoch - 5ms/step\n",
      "Epoch 12/10000\n",
      "113/113 - 0s - loss: 2610674.0000 - accuracy: 0.5916 - val_loss: 2300558.2500 - val_accuracy: 0.5896 - 484ms/epoch - 4ms/step\n",
      "Epoch 13/10000\n",
      "113/113 - 0s - loss: 2417287.2500 - accuracy: 0.5915 - val_loss: 3112555.2500 - val_accuracy: 0.5617 - 475ms/epoch - 4ms/step\n",
      "Epoch 14/10000\n",
      "113/113 - 1s - loss: 2673399.5000 - accuracy: 0.5868 - val_loss: 3024589.5000 - val_accuracy: 0.6064 - 539ms/epoch - 5ms/step\n",
      "Epoch 15/10000\n",
      "113/113 - 0s - loss: 2088796.2500 - accuracy: 0.5913 - val_loss: 2021652.3750 - val_accuracy: 0.5955 - 469ms/epoch - 4ms/step\n",
      "Epoch 16/10000\n",
      "113/113 - 0s - loss: 2268332.0000 - accuracy: 0.5876 - val_loss: 2422346.0000 - val_accuracy: 0.6040 - 486ms/epoch - 4ms/step\n",
      "Epoch 17/10000\n",
      "113/113 - 0s - loss: 2037711.2500 - accuracy: 0.5879 - val_loss: 3087197.0000 - val_accuracy: 0.5358 - 463ms/epoch - 4ms/step\n",
      "Epoch 18/10000\n",
      "113/113 - 0s - loss: 2168611.7500 - accuracy: 0.5832 - val_loss: 1685041.7500 - val_accuracy: 0.5867 - 475ms/epoch - 4ms/step\n",
      "Epoch 19/10000\n",
      "113/113 - 1s - loss: 1635631.5000 - accuracy: 0.5893 - val_loss: 1371905.7500 - val_accuracy: 0.5880 - 538ms/epoch - 5ms/step\n",
      "Epoch 20/10000\n",
      "113/113 - 0s - loss: 1540011.5000 - accuracy: 0.5878 - val_loss: 2454654.5000 - val_accuracy: 0.5331 - 464ms/epoch - 4ms/step\n",
      "Epoch 21/10000\n",
      "113/113 - 0s - loss: 1928742.6250 - accuracy: 0.5818 - val_loss: 1905428.8750 - val_accuracy: 0.6050 - 477ms/epoch - 4ms/step\n",
      "Epoch 22/10000\n",
      "113/113 - 0s - loss: 1612066.1250 - accuracy: 0.5886 - val_loss: 2188189.2500 - val_accuracy: 0.5525 - 484ms/epoch - 4ms/step\n",
      "Epoch 23/10000\n",
      "113/113 - 0s - loss: 1739744.7500 - accuracy: 0.5839 - val_loss: 1437877.7500 - val_accuracy: 0.5706 - 467ms/epoch - 4ms/step\n",
      "Epoch 24/10000\n",
      "113/113 - 0s - loss: 1795755.0000 - accuracy: 0.5795 - val_loss: 1453412.6250 - val_accuracy: 0.6036 - 474ms/epoch - 4ms/step\n",
      "Epoch 25/10000\n",
      "113/113 - 1s - loss: 1359471.0000 - accuracy: 0.5913 - val_loss: 1070722.3750 - val_accuracy: 0.5920 - 531ms/epoch - 5ms/step\n",
      "Epoch 26/10000\n",
      "113/113 - 1s - loss: 1365521.2500 - accuracy: 0.5866 - val_loss: 1026161.4375 - val_accuracy: 0.5862 - 506ms/epoch - 4ms/step\n",
      "Epoch 27/10000\n",
      "113/113 - 0s - loss: 1482443.2500 - accuracy: 0.5822 - val_loss: 1469202.8750 - val_accuracy: 0.6048 - 474ms/epoch - 4ms/step\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Iterazione 52/128 (03-11-2023_11-09-20)\n",
      "hidden_layer_size: 100\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 3\n",
      "activation_hidden: relu\n",
      "activation_output: sigmoid\n",
      "batch_size: 1000\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 20\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 60.76%\n",
      "La recall di questo modello sul validation set è: 41.42%\n",
      "La f1 di questo modello sul validation set è: 44.96%\n",
      "La balanced accuracy di questo modello sul validation set è: 57.19%\n",
      "La precision di questo modello sul validation set è: 49.17%\n",
      "La AUC di questo modello sul validation set è: 57.19%\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.66    0.73      0.70  7,670.00\n",
      "1                  0.49    0.41      0.45  4,841.00\n",
      "accuracy           0.61    0.61      0.61      0.61\n",
      "macro avg          0.58    0.57      0.57 12,511.00\n",
      "weighted avg       0.60    0.61      0.60 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         5597         2073\n",
      "Actual 1         2836         2005\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 65.94% con il modello:\n",
      "1 - AC65.94%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 62.97% con il modello:\n",
      "1 - BA62.97%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 64.08% con il modello:\n",
      "13 - PR64.08%, L20, mV-10, MV10, P2, AHsigmoid, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-26-48).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "1126/1126 - 4s - loss: 2.4511 - accuracy: 0.5793 - val_loss: 0.7456 - val_accuracy: 0.6155 - 4s/epoch - 4ms/step\n",
      "Epoch 2/10000\n",
      "1126/1126 - 3s - loss: 0.7325 - accuracy: 0.6138 - val_loss: 0.7073 - val_accuracy: 0.6083 - 3s/epoch - 3ms/step\n",
      "Epoch 3/10000\n",
      "1126/1126 - 3s - loss: 0.7053 - accuracy: 0.6122 - val_loss: 0.6859 - val_accuracy: 0.6184 - 3s/epoch - 2ms/step\n",
      "Epoch 4/10000\n",
      "1126/1126 - 3s - loss: 0.6936 - accuracy: 0.6133 - val_loss: 0.6816 - val_accuracy: 0.6155 - 3s/epoch - 2ms/step\n",
      "Epoch 5/10000\n",
      "1126/1126 - 3s - loss: 0.6856 - accuracy: 0.6149 - val_loss: 0.6727 - val_accuracy: 0.6194 - 3s/epoch - 3ms/step\n",
      "Epoch 6/10000\n",
      "1126/1126 - 3s - loss: 0.6852 - accuracy: 0.6155 - val_loss: 0.6762 - val_accuracy: 0.6155 - 3s/epoch - 3ms/step\n",
      "Epoch 7/10000\n",
      "1126/1126 - 3s - loss: 0.6825 - accuracy: 0.6156 - val_loss: 0.6678 - val_accuracy: 0.6147 - 3s/epoch - 3ms/step\n",
      "Epoch 8/10000\n",
      "1126/1126 - 3s - loss: 0.6791 - accuracy: 0.6135 - val_loss: 0.6792 - val_accuracy: 0.6098 - 3s/epoch - 2ms/step\n",
      "Epoch 9/10000\n",
      "1126/1126 - 3s - loss: 0.6764 - accuracy: 0.6123 - val_loss: 0.6703 - val_accuracy: 0.6213 - 3s/epoch - 2ms/step\n",
      "Epoch 10/10000\n",
      "1126/1126 - 3s - loss: 0.6748 - accuracy: 0.6160 - val_loss: 0.6729 - val_accuracy: 0.6143 - 3s/epoch - 2ms/step\n",
      "Epoch 11/10000\n",
      "1126/1126 - 3s - loss: 0.6745 - accuracy: 0.6128 - val_loss: 0.6703 - val_accuracy: 0.6174 - 3s/epoch - 3ms/step\n",
      "Epoch 12/10000\n",
      "1126/1126 - 3s - loss: 0.6724 - accuracy: 0.6134 - val_loss: 0.6645 - val_accuracy: 0.6214 - 3s/epoch - 2ms/step\n",
      "Epoch 13/10000\n",
      "1126/1126 - 3s - loss: 0.6697 - accuracy: 0.6147 - val_loss: 0.6644 - val_accuracy: 0.6170 - 3s/epoch - 2ms/step\n",
      "Epoch 14/10000\n",
      "1126/1126 - 3s - loss: 0.6690 - accuracy: 0.6151 - val_loss: 0.6690 - val_accuracy: 0.6094 - 3s/epoch - 2ms/step\n",
      "Epoch 15/10000\n",
      "1126/1126 - 3s - loss: 0.6697 - accuracy: 0.6141 - val_loss: 0.6614 - val_accuracy: 0.6178 - 3s/epoch - 2ms/step\n",
      "Epoch 16/10000\n",
      "1126/1126 - 3s - loss: 0.6690 - accuracy: 0.6160 - val_loss: 0.6634 - val_accuracy: 0.6201 - 3s/epoch - 2ms/step\n",
      "Epoch 17/10000\n",
      "1126/1126 - 3s - loss: 0.6680 - accuracy: 0.6188 - val_loss: 0.6655 - val_accuracy: 0.6194 - 3s/epoch - 3ms/step\n",
      "Epoch 18/10000\n",
      "1126/1126 - 3s - loss: 0.6639 - accuracy: 0.6201 - val_loss: 0.6621 - val_accuracy: 0.6240 - 3s/epoch - 3ms/step\n",
      "Epoch 19/10000\n",
      "1126/1126 - 3s - loss: 0.6643 - accuracy: 0.6177 - val_loss: 0.6583 - val_accuracy: 0.6239 - 3s/epoch - 2ms/step\n",
      "Epoch 20/10000\n",
      "1126/1126 - 3s - loss: 0.6608 - accuracy: 0.6189 - val_loss: 0.6547 - val_accuracy: 0.6259 - 3s/epoch - 2ms/step\n",
      "Epoch 21/10000\n",
      "1126/1126 - 3s - loss: 0.6612 - accuracy: 0.6177 - val_loss: 0.6603 - val_accuracy: 0.5991 - 3s/epoch - 2ms/step\n",
      "Epoch 22/10000\n",
      "1126/1126 - 3s - loss: 0.6627 - accuracy: 0.6169 - val_loss: 0.6557 - val_accuracy: 0.6084 - 3s/epoch - 2ms/step\n",
      "Epoch 23/10000\n",
      "1126/1126 - 3s - loss: 0.6612 - accuracy: 0.6168 - val_loss: 0.6649 - val_accuracy: 0.6081 - 3s/epoch - 3ms/step\n",
      "Epoch 24/10000\n",
      "1126/1126 - 3s - loss: 0.6628 - accuracy: 0.6172 - val_loss: 0.6615 - val_accuracy: 0.6243 - 3s/epoch - 2ms/step\n",
      "Epoch 25/10000\n",
      "1126/1126 - 3s - loss: 0.6601 - accuracy: 0.6188 - val_loss: 0.6565 - val_accuracy: 0.6229 - 3s/epoch - 2ms/step\n",
      "Epoch 26/10000\n",
      "1126/1126 - 3s - loss: 0.6591 - accuracy: 0.6186 - val_loss: 0.6557 - val_accuracy: 0.6205 - 3s/epoch - 2ms/step\n",
      "Epoch 27/10000\n",
      "1126/1126 - 3s - loss: 0.6598 - accuracy: 0.6176 - val_loss: 0.6646 - val_accuracy: 0.6191 - 3s/epoch - 3ms/step\n",
      "Epoch 28/10000\n",
      "1126/1126 - 3s - loss: 0.6591 - accuracy: 0.6171 - val_loss: 0.6560 - val_accuracy: 0.6251 - 3s/epoch - 2ms/step\n",
      "Epoch 29/10000\n",
      "1126/1126 - 3s - loss: 0.6563 - accuracy: 0.6199 - val_loss: 0.6666 - val_accuracy: 0.5996 - 3s/epoch - 2ms/step\n",
      "Epoch 30/10000\n",
      "1126/1126 - 3s - loss: 0.6575 - accuracy: 0.6191 - val_loss: 0.6567 - val_accuracy: 0.6247 - 3s/epoch - 2ms/step\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Iterazione 53/128 (03-11-2023_11-10-49)\n",
      "hidden_layer_size: 100\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 3\n",
      "activation_hidden: sigmoid\n",
      "activation_output: sigmoid\n",
      "batch_size: 100\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 10\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 62.59%\n",
      "La recall di questo modello sul validation set è: 23.96%\n",
      "La f1 di questo modello sul validation set è: 33.14%\n",
      "La balanced accuracy di questo modello sul validation set è: 55.47%\n",
      "La precision di questo modello sul validation set è: 53.73%\n",
      "La AUC di questo modello sul validation set è: 60.43%\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.64    0.87      0.74  7,670.00\n",
      "1                  0.54    0.24      0.33  4,841.00\n",
      "accuracy           0.63    0.63      0.63      0.63\n",
      "macro avg          0.59    0.55      0.54 12,511.00\n",
      "weighted avg       0.60    0.63      0.58 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         6671          999\n",
      "Actual 1         3681         1160\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 65.94% con il modello:\n",
      "1 - AC65.94%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 62.97% con il modello:\n",
      "1 - BA62.97%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 64.08% con il modello:\n",
      "13 - PR64.08%, L20, mV-10, MV10, P2, AHsigmoid, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-26-48).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "1126/1126 - 4s - loss: 2.4511 - accuracy: 0.5793 - val_loss: 0.7456 - val_accuracy: 0.6155 - 4s/epoch - 4ms/step\n",
      "Epoch 2/10000\n",
      "1126/1126 - 3s - loss: 0.7325 - accuracy: 0.6138 - val_loss: 0.7073 - val_accuracy: 0.6083 - 3s/epoch - 3ms/step\n",
      "Epoch 3/10000\n",
      "1126/1126 - 3s - loss: 0.7053 - accuracy: 0.6122 - val_loss: 0.6859 - val_accuracy: 0.6184 - 3s/epoch - 2ms/step\n",
      "Epoch 4/10000\n",
      "1126/1126 - 3s - loss: 0.6936 - accuracy: 0.6133 - val_loss: 0.6816 - val_accuracy: 0.6155 - 3s/epoch - 2ms/step\n",
      "Epoch 5/10000\n",
      "1126/1126 - 3s - loss: 0.6856 - accuracy: 0.6149 - val_loss: 0.6727 - val_accuracy: 0.6194 - 3s/epoch - 2ms/step\n",
      "Epoch 6/10000\n",
      "1126/1126 - 3s - loss: 0.6852 - accuracy: 0.6155 - val_loss: 0.6762 - val_accuracy: 0.6155 - 3s/epoch - 2ms/step\n",
      "Epoch 7/10000\n",
      "1126/1126 - 3s - loss: 0.6825 - accuracy: 0.6156 - val_loss: 0.6678 - val_accuracy: 0.6147 - 3s/epoch - 2ms/step\n",
      "Epoch 8/10000\n",
      "1126/1126 - 3s - loss: 0.6791 - accuracy: 0.6135 - val_loss: 0.6792 - val_accuracy: 0.6098 - 3s/epoch - 2ms/step\n",
      "Epoch 9/10000\n",
      "1126/1126 - 3s - loss: 0.6764 - accuracy: 0.6123 - val_loss: 0.6703 - val_accuracy: 0.6213 - 3s/epoch - 2ms/step\n",
      "Epoch 10/10000\n",
      "1126/1126 - 3s - loss: 0.6748 - accuracy: 0.6160 - val_loss: 0.6729 - val_accuracy: 0.6143 - 3s/epoch - 2ms/step\n",
      "Epoch 11/10000\n",
      "1126/1126 - 3s - loss: 0.6745 - accuracy: 0.6128 - val_loss: 0.6703 - val_accuracy: 0.6174 - 3s/epoch - 2ms/step\n",
      "Epoch 12/10000\n",
      "1126/1126 - 3s - loss: 0.6724 - accuracy: 0.6134 - val_loss: 0.6645 - val_accuracy: 0.6214 - 3s/epoch - 2ms/step\n",
      "Epoch 13/10000\n",
      "1126/1126 - 3s - loss: 0.6697 - accuracy: 0.6147 - val_loss: 0.6644 - val_accuracy: 0.6170 - 3s/epoch - 3ms/step\n",
      "Epoch 14/10000\n",
      "1126/1126 - 3s - loss: 0.6690 - accuracy: 0.6151 - val_loss: 0.6690 - val_accuracy: 0.6094 - 3s/epoch - 2ms/step\n",
      "Epoch 15/10000\n",
      "1126/1126 - 3s - loss: 0.6697 - accuracy: 0.6141 - val_loss: 0.6614 - val_accuracy: 0.6178 - 3s/epoch - 2ms/step\n",
      "Epoch 16/10000\n",
      "1126/1126 - 3s - loss: 0.6690 - accuracy: 0.6160 - val_loss: 0.6634 - val_accuracy: 0.6201 - 3s/epoch - 2ms/step\n",
      "Epoch 17/10000\n",
      "1126/1126 - 3s - loss: 0.6680 - accuracy: 0.6188 - val_loss: 0.6655 - val_accuracy: 0.6194 - 3s/epoch - 2ms/step\n",
      "Epoch 18/10000\n",
      "1126/1126 - 3s - loss: 0.6639 - accuracy: 0.6201 - val_loss: 0.6621 - val_accuracy: 0.6240 - 3s/epoch - 2ms/step\n",
      "Epoch 19/10000\n",
      "1126/1126 - 3s - loss: 0.6643 - accuracy: 0.6177 - val_loss: 0.6583 - val_accuracy: 0.6239 - 3s/epoch - 2ms/step\n",
      "Epoch 20/10000\n",
      "1126/1126 - 3s - loss: 0.6608 - accuracy: 0.6189 - val_loss: 0.6547 - val_accuracy: 0.6259 - 3s/epoch - 2ms/step\n",
      "Epoch 21/10000\n",
      "1126/1126 - 3s - loss: 0.6612 - accuracy: 0.6177 - val_loss: 0.6603 - val_accuracy: 0.5991 - 3s/epoch - 2ms/step\n",
      "Epoch 22/10000\n",
      "1126/1126 - 3s - loss: 0.6627 - accuracy: 0.6169 - val_loss: 0.6557 - val_accuracy: 0.6084 - 3s/epoch - 2ms/step\n",
      "Epoch 23/10000\n",
      "1126/1126 - 2s - loss: 0.6612 - accuracy: 0.6168 - val_loss: 0.6649 - val_accuracy: 0.6081 - 2s/epoch - 2ms/step\n",
      "Epoch 24/10000\n",
      "1126/1126 - 3s - loss: 0.6628 - accuracy: 0.6172 - val_loss: 0.6615 - val_accuracy: 0.6243 - 3s/epoch - 2ms/step\n",
      "Epoch 25/10000\n",
      "1126/1126 - 3s - loss: 0.6601 - accuracy: 0.6188 - val_loss: 0.6565 - val_accuracy: 0.6229 - 3s/epoch - 2ms/step\n",
      "Epoch 26/10000\n",
      "1126/1126 - 3s - loss: 0.6591 - accuracy: 0.6186 - val_loss: 0.6557 - val_accuracy: 0.6205 - 3s/epoch - 2ms/step\n",
      "Epoch 27/10000\n",
      "1126/1126 - 3s - loss: 0.6598 - accuracy: 0.6176 - val_loss: 0.6646 - val_accuracy: 0.6191 - 3s/epoch - 2ms/step\n",
      "Epoch 28/10000\n",
      "1126/1126 - 3s - loss: 0.6591 - accuracy: 0.6171 - val_loss: 0.6560 - val_accuracy: 0.6251 - 3s/epoch - 3ms/step\n",
      "Epoch 29/10000\n",
      "1126/1126 - 3s - loss: 0.6563 - accuracy: 0.6199 - val_loss: 0.6666 - val_accuracy: 0.5996 - 3s/epoch - 2ms/step\n",
      "Epoch 30/10000\n",
      "1126/1126 - 3s - loss: 0.6575 - accuracy: 0.6191 - val_loss: 0.6567 - val_accuracy: 0.6247 - 3s/epoch - 2ms/step\n",
      "Epoch 31/10000\n",
      "1126/1126 - 2s - loss: 0.6578 - accuracy: 0.6202 - val_loss: 0.6563 - val_accuracy: 0.6207 - 2s/epoch - 2ms/step\n",
      "Epoch 32/10000\n",
      "1126/1126 - 3s - loss: 0.6565 - accuracy: 0.6186 - val_loss: 0.6579 - val_accuracy: 0.6226 - 3s/epoch - 2ms/step\n",
      "Epoch 33/10000\n",
      "1126/1126 - 3s - loss: 0.6578 - accuracy: 0.6188 - val_loss: 0.6575 - val_accuracy: 0.6203 - 3s/epoch - 2ms/step\n",
      "Epoch 34/10000\n",
      "1126/1126 - 3s - loss: 0.6583 - accuracy: 0.6180 - val_loss: 0.6676 - val_accuracy: 0.5987 - 3s/epoch - 2ms/step\n",
      "Epoch 35/10000\n",
      "1126/1126 - 2s - loss: 0.6604 - accuracy: 0.6171 - val_loss: 0.6594 - val_accuracy: 0.6231 - 2s/epoch - 2ms/step\n",
      "Epoch 36/10000\n",
      "1126/1126 - 2s - loss: 0.6604 - accuracy: 0.6194 - val_loss: 0.6557 - val_accuracy: 0.6234 - 2s/epoch - 2ms/step\n",
      "Epoch 37/10000\n",
      "1126/1126 - 3s - loss: 0.6572 - accuracy: 0.6203 - val_loss: 0.6558 - val_accuracy: 0.6195 - 3s/epoch - 2ms/step\n",
      "Epoch 38/10000\n",
      "1126/1126 - 3s - loss: 0.6570 - accuracy: 0.6213 - val_loss: 0.6582 - val_accuracy: 0.6241 - 3s/epoch - 2ms/step\n",
      "Epoch 39/10000\n",
      "1126/1126 - 3s - loss: 0.6564 - accuracy: 0.6214 - val_loss: 0.6572 - val_accuracy: 0.6204 - 3s/epoch - 2ms/step\n",
      "Epoch 40/10000\n",
      "1126/1126 - 2s - loss: 0.6561 - accuracy: 0.6216 - val_loss: 0.6546 - val_accuracy: 0.6260 - 2s/epoch - 2ms/step\n",
      "Epoch 41/10000\n",
      "1126/1126 - 3s - loss: 0.6569 - accuracy: 0.6227 - val_loss: 0.6543 - val_accuracy: 0.6258 - 3s/epoch - 2ms/step\n",
      "Epoch 42/10000\n",
      "1126/1126 - 3s - loss: 0.6609 - accuracy: 0.6193 - val_loss: 0.6552 - val_accuracy: 0.6235 - 3s/epoch - 2ms/step\n",
      "Epoch 43/10000\n",
      "1126/1126 - 2s - loss: 0.6601 - accuracy: 0.6177 - val_loss: 0.6531 - val_accuracy: 0.6216 - 2s/epoch - 2ms/step\n",
      "Epoch 44/10000\n",
      "1126/1126 - 3s - loss: 0.6563 - accuracy: 0.6206 - val_loss: 0.6552 - val_accuracy: 0.6250 - 3s/epoch - 2ms/step\n",
      "Epoch 45/10000\n",
      "1126/1126 - 3s - loss: 0.6564 - accuracy: 0.6190 - val_loss: 0.6534 - val_accuracy: 0.6241 - 3s/epoch - 2ms/step\n",
      "Epoch 46/10000\n",
      "1126/1126 - 3s - loss: 0.6573 - accuracy: 0.6182 - val_loss: 0.6584 - val_accuracy: 0.6220 - 3s/epoch - 2ms/step\n",
      "Epoch 47/10000\n",
      "1126/1126 - 2s - loss: 0.6570 - accuracy: 0.6191 - val_loss: 0.6577 - val_accuracy: 0.6208 - 2s/epoch - 2ms/step\n",
      "Epoch 48/10000\n",
      "1126/1126 - 3s - loss: 0.6574 - accuracy: 0.6196 - val_loss: 0.6532 - val_accuracy: 0.6246 - 3s/epoch - 2ms/step\n",
      "Epoch 49/10000\n",
      "1126/1126 - 2s - loss: 0.6582 - accuracy: 0.6191 - val_loss: 0.6581 - val_accuracy: 0.6168 - 2s/epoch - 2ms/step\n",
      "Epoch 50/10000\n",
      "1126/1126 - 3s - loss: 0.6589 - accuracy: 0.6193 - val_loss: 0.6540 - val_accuracy: 0.6237 - 3s/epoch - 2ms/step\n",
      "Epoch 51/10000\n",
      "1126/1126 - 3s - loss: 0.6587 - accuracy: 0.6191 - val_loss: 0.6577 - val_accuracy: 0.6219 - 3s/epoch - 2ms/step\n",
      "Epoch 52/10000\n",
      "1126/1126 - 2s - loss: 0.6572 - accuracy: 0.6202 - val_loss: 0.6573 - val_accuracy: 0.6219 - 2s/epoch - 2ms/step\n",
      "Epoch 53/10000\n",
      "1126/1126 - 2s - loss: 0.6558 - accuracy: 0.6210 - val_loss: 0.6587 - val_accuracy: 0.6229 - 2s/epoch - 2ms/step\n",
      "Epoch 54/10000\n",
      "1126/1126 - 2s - loss: 0.6577 - accuracy: 0.6212 - val_loss: 0.6578 - val_accuracy: 0.6255 - 2s/epoch - 2ms/step\n",
      "Epoch 55/10000\n",
      "1126/1126 - 2s - loss: 0.6578 - accuracy: 0.6216 - val_loss: 0.6558 - val_accuracy: 0.6240 - 2s/epoch - 2ms/step\n",
      "Epoch 56/10000\n",
      "1126/1126 - 2s - loss: 0.6582 - accuracy: 0.6214 - val_loss: 0.6510 - val_accuracy: 0.6290 - 2s/epoch - 2ms/step\n",
      "Epoch 57/10000\n",
      "1126/1126 - 2s - loss: 0.6567 - accuracy: 0.6220 - val_loss: 0.6577 - val_accuracy: 0.6262 - 2s/epoch - 2ms/step\n",
      "Epoch 58/10000\n",
      "1126/1126 - 2s - loss: 0.6563 - accuracy: 0.6212 - val_loss: 0.6539 - val_accuracy: 0.6294 - 2s/epoch - 2ms/step\n",
      "Epoch 59/10000\n",
      "1126/1126 - 2s - loss: 0.6564 - accuracy: 0.6215 - val_loss: 0.6609 - val_accuracy: 0.6226 - 2s/epoch - 2ms/step\n",
      "Epoch 60/10000\n",
      "1126/1126 - 3s - loss: 0.6559 - accuracy: 0.6214 - val_loss: 0.6559 - val_accuracy: 0.6286 - 3s/epoch - 2ms/step\n",
      "Epoch 61/10000\n",
      "1126/1126 - 3s - loss: 0.6577 - accuracy: 0.6208 - val_loss: 0.6577 - val_accuracy: 0.6174 - 3s/epoch - 2ms/step\n",
      "Epoch 62/10000\n",
      "1126/1126 - 3s - loss: 0.6565 - accuracy: 0.6215 - val_loss: 0.6555 - val_accuracy: 0.6280 - 3s/epoch - 2ms/step\n",
      "Epoch 63/10000\n",
      "1126/1126 - 2s - loss: 0.6555 - accuracy: 0.6221 - val_loss: 0.6557 - val_accuracy: 0.6269 - 2s/epoch - 2ms/step\n",
      "Epoch 64/10000\n",
      "1126/1126 - 2s - loss: 0.6554 - accuracy: 0.6232 - val_loss: 0.6535 - val_accuracy: 0.6273 - 2s/epoch - 2ms/step\n",
      "Epoch 65/10000\n",
      "1126/1126 - 3s - loss: 0.6569 - accuracy: 0.6226 - val_loss: 0.6533 - val_accuracy: 0.6278 - 3s/epoch - 2ms/step\n",
      "Epoch 66/10000\n",
      "1126/1126 - 3s - loss: 0.6590 - accuracy: 0.6209 - val_loss: 0.6599 - val_accuracy: 0.6276 - 3s/epoch - 2ms/step\n",
      "Epoch 67/10000\n",
      "1126/1126 - 3s - loss: 0.6588 - accuracy: 0.6217 - val_loss: 0.6529 - val_accuracy: 0.6286 - 3s/epoch - 2ms/step\n",
      "Epoch 68/10000\n",
      "1126/1126 - 3s - loss: 0.6559 - accuracy: 0.6227 - val_loss: 0.6519 - val_accuracy: 0.6147 - 3s/epoch - 2ms/step\n",
      "Epoch 69/10000\n",
      "1126/1126 - 3s - loss: 0.6548 - accuracy: 0.6172 - val_loss: 0.6515 - val_accuracy: 0.6268 - 3s/epoch - 2ms/step\n",
      "Epoch 70/10000\n",
      "1126/1126 - 3s - loss: 0.6542 - accuracy: 0.6206 - val_loss: 0.6521 - val_accuracy: 0.6294 - 3s/epoch - 2ms/step\n",
      "Epoch 71/10000\n",
      "1126/1126 - 3s - loss: 0.6547 - accuracy: 0.6222 - val_loss: 0.6510 - val_accuracy: 0.6290 - 3s/epoch - 2ms/step\n",
      "Epoch 72/10000\n",
      "1126/1126 - 2s - loss: 0.6553 - accuracy: 0.6226 - val_loss: 0.6512 - val_accuracy: 0.6302 - 2s/epoch - 2ms/step\n",
      "Epoch 73/10000\n",
      "1126/1126 - 3s - loss: 0.6549 - accuracy: 0.6224 - val_loss: 0.6518 - val_accuracy: 0.6292 - 3s/epoch - 2ms/step\n",
      "Epoch 74/10000\n",
      "1126/1126 - 3s - loss: 0.6551 - accuracy: 0.6202 - val_loss: 0.6507 - val_accuracy: 0.6183 - 3s/epoch - 2ms/step\n",
      "Epoch 75/10000\n",
      "1126/1126 - 3s - loss: 0.6558 - accuracy: 0.6191 - val_loss: 0.6503 - val_accuracy: 0.6220 - 3s/epoch - 2ms/step\n",
      "Epoch 76/10000\n",
      "1126/1126 - 3s - loss: 0.6571 - accuracy: 0.6195 - val_loss: 0.6527 - val_accuracy: 0.6206 - 3s/epoch - 2ms/step\n",
      "Epoch 77/10000\n",
      "1126/1126 - 3s - loss: 0.6584 - accuracy: 0.6163 - val_loss: 0.6507 - val_accuracy: 0.6251 - 3s/epoch - 2ms/step\n",
      "Epoch 78/10000\n",
      "1126/1126 - 3s - loss: 0.6571 - accuracy: 0.6198 - val_loss: 0.6491 - val_accuracy: 0.6269 - 3s/epoch - 2ms/step\n",
      "Epoch 79/10000\n",
      "1126/1126 - 3s - loss: 0.6546 - accuracy: 0.6211 - val_loss: 0.6507 - val_accuracy: 0.6295 - 3s/epoch - 2ms/step\n",
      "Epoch 80/10000\n",
      "1126/1126 - 2s - loss: 0.6554 - accuracy: 0.6223 - val_loss: 0.6559 - val_accuracy: 0.6231 - 2s/epoch - 2ms/step\n",
      "Epoch 81/10000\n",
      "1126/1126 - 3s - loss: 0.6550 - accuracy: 0.6245 - val_loss: 0.6507 - val_accuracy: 0.6290 - 3s/epoch - 2ms/step\n",
      "Epoch 82/10000\n",
      "1126/1126 - 3s - loss: 0.6532 - accuracy: 0.6239 - val_loss: 0.6537 - val_accuracy: 0.6258 - 3s/epoch - 2ms/step\n",
      "Epoch 83/10000\n",
      "1126/1126 - 3s - loss: 0.6553 - accuracy: 0.6214 - val_loss: 0.6538 - val_accuracy: 0.6216 - 3s/epoch - 2ms/step\n",
      "Epoch 84/10000\n",
      "1126/1126 - 3s - loss: 0.6552 - accuracy: 0.6195 - val_loss: 0.6560 - val_accuracy: 0.6243 - 3s/epoch - 2ms/step\n",
      "Epoch 85/10000\n",
      "1126/1126 - 3s - loss: 0.6544 - accuracy: 0.6212 - val_loss: 0.6546 - val_accuracy: 0.6247 - 3s/epoch - 2ms/step\n",
      "Epoch 86/10000\n",
      "1126/1126 - 3s - loss: 0.6546 - accuracy: 0.6195 - val_loss: 0.6531 - val_accuracy: 0.6271 - 3s/epoch - 2ms/step\n",
      "Epoch 87/10000\n",
      "1126/1126 - 3s - loss: 0.6537 - accuracy: 0.6213 - val_loss: 0.6511 - val_accuracy: 0.6266 - 3s/epoch - 2ms/step\n",
      "Epoch 88/10000\n",
      "1126/1126 - 3s - loss: 0.6548 - accuracy: 0.6221 - val_loss: 0.6515 - val_accuracy: 0.6247 - 3s/epoch - 2ms/step\n",
      "Epoch 89/10000\n",
      "1126/1126 - 3s - loss: 0.6540 - accuracy: 0.6224 - val_loss: 0.6521 - val_accuracy: 0.6219 - 3s/epoch - 2ms/step\n",
      "Epoch 90/10000\n",
      "1126/1126 - 3s - loss: 0.6538 - accuracy: 0.6235 - val_loss: 0.6513 - val_accuracy: 0.6242 - 3s/epoch - 2ms/step\n",
      "Epoch 91/10000\n",
      "1126/1126 - 3s - loss: 0.6551 - accuracy: 0.6215 - val_loss: 0.6531 - val_accuracy: 0.6286 - 3s/epoch - 2ms/step\n",
      "Epoch 92/10000\n",
      "1126/1126 - 3s - loss: 0.6534 - accuracy: 0.6232 - val_loss: 0.6495 - val_accuracy: 0.6280 - 3s/epoch - 2ms/step\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Iterazione 54/128 (03-11-2023_11-14-52)\n",
      "hidden_layer_size: 100\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 3\n",
      "activation_hidden: sigmoid\n",
      "activation_output: sigmoid\n",
      "batch_size: 100\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 20\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 63.02%\n",
      "La recall di questo modello sul validation set è: 29.52%\n",
      "La f1 di questo modello sul validation set è: 38.18%\n",
      "La balanced accuracy di questo modello sul validation set è: 56.84%\n",
      "La precision di questo modello sul validation set è: 54.05%\n",
      "La AUC di questo modello sul validation set è: 61.55%\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.65    0.84      0.74  7,670.00\n",
      "1                  0.54    0.30      0.38  4,841.00\n",
      "accuracy           0.63    0.63      0.63      0.63\n",
      "macro avg          0.60    0.57      0.56 12,511.00\n",
      "weighted avg       0.61    0.63      0.60 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         6455         1215\n",
      "Actual 1         3412         1429\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 65.94% con il modello:\n",
      "1 - AC65.94%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 62.97% con il modello:\n",
      "1 - BA62.97%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 64.08% con il modello:\n",
      "13 - PR64.08%, L20, mV-10, MV10, P2, AHsigmoid, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-26-48).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "113/113 - 2s - loss: 13.4946 - accuracy: 0.4052 - val_loss: 3.9219 - val_accuracy: 0.4329 - 2s/epoch - 16ms/step\n",
      "Epoch 2/10000\n",
      "113/113 - 1s - loss: 2.1322 - accuracy: 0.5640 - val_loss: 1.1002 - val_accuracy: 0.6131 - 559ms/epoch - 5ms/step\n",
      "Epoch 3/10000\n",
      "113/113 - 1s - loss: 1.0078 - accuracy: 0.6115 - val_loss: 0.9064 - val_accuracy: 0.6154 - 657ms/epoch - 6ms/step\n",
      "Epoch 4/10000\n",
      "113/113 - 1s - loss: 0.8690 - accuracy: 0.6135 - val_loss: 0.7770 - val_accuracy: 0.6171 - 582ms/epoch - 5ms/step\n",
      "Epoch 5/10000\n",
      "113/113 - 1s - loss: 0.7907 - accuracy: 0.6143 - val_loss: 0.7605 - val_accuracy: 0.6157 - 571ms/epoch - 5ms/step\n",
      "Epoch 6/10000\n",
      "113/113 - 1s - loss: 0.7736 - accuracy: 0.6159 - val_loss: 0.7385 - val_accuracy: 0.6146 - 567ms/epoch - 5ms/step\n",
      "Epoch 7/10000\n",
      "113/113 - 1s - loss: 0.7533 - accuracy: 0.6146 - val_loss: 0.7326 - val_accuracy: 0.6159 - 617ms/epoch - 5ms/step\n",
      "Epoch 8/10000\n",
      "113/113 - 1s - loss: 0.7372 - accuracy: 0.6148 - val_loss: 0.7172 - val_accuracy: 0.6147 - 608ms/epoch - 5ms/step\n",
      "Epoch 9/10000\n",
      "113/113 - 1s - loss: 0.7251 - accuracy: 0.6156 - val_loss: 0.7134 - val_accuracy: 0.6174 - 574ms/epoch - 5ms/step\n",
      "Epoch 10/10000\n",
      "113/113 - 1s - loss: 0.7189 - accuracy: 0.6164 - val_loss: 0.7057 - val_accuracy: 0.6219 - 639ms/epoch - 6ms/step\n",
      "Epoch 11/10000\n",
      "113/113 - 1s - loss: 0.7118 - accuracy: 0.6173 - val_loss: 0.7047 - val_accuracy: 0.6214 - 638ms/epoch - 6ms/step\n",
      "Epoch 12/10000\n",
      "113/113 - 1s - loss: 0.7096 - accuracy: 0.6181 - val_loss: 0.6995 - val_accuracy: 0.6191 - 613ms/epoch - 5ms/step\n",
      "Epoch 13/10000\n",
      "113/113 - 1s - loss: 0.7039 - accuracy: 0.6175 - val_loss: 0.6921 - val_accuracy: 0.6218 - 547ms/epoch - 5ms/step\n",
      "Epoch 14/10000\n",
      "113/113 - 1s - loss: 0.7029 - accuracy: 0.6181 - val_loss: 0.6904 - val_accuracy: 0.6200 - 538ms/epoch - 5ms/step\n",
      "Epoch 15/10000\n",
      "113/113 - 1s - loss: 0.7061 - accuracy: 0.6168 - val_loss: 0.7026 - val_accuracy: 0.6185 - 513ms/epoch - 5ms/step\n",
      "Epoch 16/10000\n",
      "113/113 - 1s - loss: 0.7058 - accuracy: 0.6164 - val_loss: 0.6991 - val_accuracy: 0.6191 - 511ms/epoch - 5ms/step\n",
      "Epoch 17/10000\n",
      "113/113 - 1s - loss: 0.6999 - accuracy: 0.6161 - val_loss: 0.6925 - val_accuracy: 0.6183 - 536ms/epoch - 5ms/step\n",
      "Epoch 18/10000\n",
      "113/113 - 1s - loss: 0.6950 - accuracy: 0.6162 - val_loss: 0.6919 - val_accuracy: 0.6174 - 559ms/epoch - 5ms/step\n",
      "Epoch 19/10000\n",
      "113/113 - 0s - loss: 0.6884 - accuracy: 0.6190 - val_loss: 0.6857 - val_accuracy: 0.6205 - 495ms/epoch - 4ms/step\n",
      "Epoch 20/10000\n",
      "113/113 - 1s - loss: 0.6861 - accuracy: 0.6179 - val_loss: 0.6819 - val_accuracy: 0.6187 - 578ms/epoch - 5ms/step\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Iterazione 55/128 (03-11-2023_11-15-07)\n",
      "hidden_layer_size: 100\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 3\n",
      "activation_hidden: sigmoid\n",
      "activation_output: sigmoid\n",
      "batch_size: 1000\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 10\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 62.19%\n",
      "La recall di questo modello sul validation set è: 25.04%\n",
      "La f1 di questo modello sul validation set è: 33.88%\n",
      "La balanced accuracy di questo modello sul validation set è: 55.33%\n",
      "La precision di questo modello sul validation set è: 52.38%\n",
      "La AUC di questo modello sul validation set è: 59.74%\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.64    0.86      0.74  7,670.00\n",
      "1                  0.52    0.25      0.34  4,841.00\n",
      "accuracy           0.62    0.62      0.62      0.62\n",
      "macro avg          0.58    0.55      0.54 12,511.00\n",
      "weighted avg       0.60    0.62      0.58 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         6568         1102\n",
      "Actual 1         3629         1212\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 65.94% con il modello:\n",
      "1 - AC65.94%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 62.97% con il modello:\n",
      "1 - BA62.97%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 64.08% con il modello:\n",
      "13 - PR64.08%, L20, mV-10, MV10, P2, AHsigmoid, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-26-48).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "113/113 - 2s - loss: 13.4946 - accuracy: 0.4052 - val_loss: 3.9219 - val_accuracy: 0.4329 - 2s/epoch - 16ms/step\n",
      "Epoch 2/10000\n",
      "113/113 - 1s - loss: 2.1322 - accuracy: 0.5640 - val_loss: 1.1002 - val_accuracy: 0.6131 - 568ms/epoch - 5ms/step\n",
      "Epoch 3/10000\n",
      "113/113 - 1s - loss: 1.0078 - accuracy: 0.6115 - val_loss: 0.9064 - val_accuracy: 0.6154 - 601ms/epoch - 5ms/step\n",
      "Epoch 4/10000\n",
      "113/113 - 1s - loss: 0.8690 - accuracy: 0.6135 - val_loss: 0.7770 - val_accuracy: 0.6171 - 620ms/epoch - 5ms/step\n",
      "Epoch 5/10000\n",
      "113/113 - 1s - loss: 0.7907 - accuracy: 0.6143 - val_loss: 0.7605 - val_accuracy: 0.6157 - 576ms/epoch - 5ms/step\n",
      "Epoch 6/10000\n",
      "113/113 - 1s - loss: 0.7736 - accuracy: 0.6159 - val_loss: 0.7385 - val_accuracy: 0.6146 - 609ms/epoch - 5ms/step\n",
      "Epoch 7/10000\n",
      "113/113 - 1s - loss: 0.7533 - accuracy: 0.6146 - val_loss: 0.7326 - val_accuracy: 0.6159 - 698ms/epoch - 6ms/step\n",
      "Epoch 8/10000\n",
      "113/113 - 1s - loss: 0.7372 - accuracy: 0.6148 - val_loss: 0.7172 - val_accuracy: 0.6147 - 572ms/epoch - 5ms/step\n",
      "Epoch 9/10000\n",
      "113/113 - 1s - loss: 0.7251 - accuracy: 0.6156 - val_loss: 0.7134 - val_accuracy: 0.6174 - 645ms/epoch - 6ms/step\n",
      "Epoch 10/10000\n",
      "113/113 - 1s - loss: 0.7189 - accuracy: 0.6164 - val_loss: 0.7057 - val_accuracy: 0.6219 - 564ms/epoch - 5ms/step\n",
      "Epoch 11/10000\n",
      "113/113 - 1s - loss: 0.7118 - accuracy: 0.6173 - val_loss: 0.7047 - val_accuracy: 0.6214 - 638ms/epoch - 6ms/step\n",
      "Epoch 12/10000\n",
      "113/113 - 1s - loss: 0.7096 - accuracy: 0.6181 - val_loss: 0.6995 - val_accuracy: 0.6191 - 720ms/epoch - 6ms/step\n",
      "Epoch 13/10000\n",
      "113/113 - 1s - loss: 0.7039 - accuracy: 0.6175 - val_loss: 0.6921 - val_accuracy: 0.6218 - 770ms/epoch - 7ms/step\n",
      "Epoch 14/10000\n",
      "113/113 - 1s - loss: 0.7029 - accuracy: 0.6181 - val_loss: 0.6904 - val_accuracy: 0.6200 - 572ms/epoch - 5ms/step\n",
      "Epoch 15/10000\n",
      "113/113 - 0s - loss: 0.7061 - accuracy: 0.6168 - val_loss: 0.7026 - val_accuracy: 0.6185 - 499ms/epoch - 4ms/step\n",
      "Epoch 16/10000\n",
      "113/113 - 0s - loss: 0.7058 - accuracy: 0.6164 - val_loss: 0.6991 - val_accuracy: 0.6191 - 496ms/epoch - 4ms/step\n",
      "Epoch 17/10000\n",
      "113/113 - 1s - loss: 0.6999 - accuracy: 0.6161 - val_loss: 0.6925 - val_accuracy: 0.6183 - 503ms/epoch - 4ms/step\n",
      "Epoch 18/10000\n",
      "113/113 - 1s - loss: 0.6950 - accuracy: 0.6162 - val_loss: 0.6919 - val_accuracy: 0.6174 - 524ms/epoch - 5ms/step\n",
      "Epoch 19/10000\n",
      "113/113 - 0s - loss: 0.6884 - accuracy: 0.6190 - val_loss: 0.6857 - val_accuracy: 0.6205 - 496ms/epoch - 4ms/step\n",
      "Epoch 20/10000\n",
      "113/113 - 1s - loss: 0.6861 - accuracy: 0.6179 - val_loss: 0.6819 - val_accuracy: 0.6187 - 561ms/epoch - 5ms/step\n",
      "Epoch 21/10000\n",
      "113/113 - 1s - loss: 0.6835 - accuracy: 0.6193 - val_loss: 0.6809 - val_accuracy: 0.6171 - 525ms/epoch - 5ms/step\n",
      "Epoch 22/10000\n",
      "113/113 - 0s - loss: 0.6776 - accuracy: 0.6189 - val_loss: 0.6768 - val_accuracy: 0.6223 - 487ms/epoch - 4ms/step\n",
      "Epoch 23/10000\n",
      "113/113 - 1s - loss: 0.6757 - accuracy: 0.6202 - val_loss: 0.6755 - val_accuracy: 0.6232 - 508ms/epoch - 4ms/step\n",
      "Epoch 24/10000\n",
      "113/113 - 0s - loss: 0.6757 - accuracy: 0.6188 - val_loss: 0.6781 - val_accuracy: 0.6213 - 500ms/epoch - 4ms/step\n",
      "Epoch 25/10000\n",
      "113/113 - 0s - loss: 0.6759 - accuracy: 0.6188 - val_loss: 0.6800 - val_accuracy: 0.6198 - 497ms/epoch - 4ms/step\n",
      "Epoch 26/10000\n",
      "113/113 - 1s - loss: 0.6768 - accuracy: 0.6186 - val_loss: 0.6744 - val_accuracy: 0.6157 - 592ms/epoch - 5ms/step\n",
      "Epoch 27/10000\n",
      "113/113 - 0s - loss: 0.6746 - accuracy: 0.6195 - val_loss: 0.6769 - val_accuracy: 0.6207 - 497ms/epoch - 4ms/step\n",
      "Epoch 28/10000\n",
      "113/113 - 0s - loss: 0.6728 - accuracy: 0.6195 - val_loss: 0.6776 - val_accuracy: 0.6207 - 490ms/epoch - 4ms/step\n",
      "Epoch 29/10000\n",
      "113/113 - 1s - loss: 0.6732 - accuracy: 0.6176 - val_loss: 0.6686 - val_accuracy: 0.6219 - 588ms/epoch - 5ms/step\n",
      "Epoch 30/10000\n",
      "113/113 - 1s - loss: 0.6714 - accuracy: 0.6183 - val_loss: 0.6733 - val_accuracy: 0.6192 - 512ms/epoch - 5ms/step\n",
      "Epoch 31/10000\n",
      "113/113 - 1s - loss: 0.6711 - accuracy: 0.6186 - val_loss: 0.6732 - val_accuracy: 0.6211 - 516ms/epoch - 5ms/step\n",
      "Epoch 32/10000\n",
      "113/113 - 1s - loss: 0.6717 - accuracy: 0.6184 - val_loss: 0.6711 - val_accuracy: 0.6217 - 509ms/epoch - 5ms/step\n",
      "Epoch 33/10000\n",
      "113/113 - 0s - loss: 0.6690 - accuracy: 0.6189 - val_loss: 0.6692 - val_accuracy: 0.6207 - 496ms/epoch - 4ms/step\n",
      "Epoch 34/10000\n",
      "113/113 - 1s - loss: 0.6684 - accuracy: 0.6184 - val_loss: 0.6673 - val_accuracy: 0.6199 - 585ms/epoch - 5ms/step\n",
      "Epoch 35/10000\n",
      "113/113 - 0s - loss: 0.6657 - accuracy: 0.6203 - val_loss: 0.6670 - val_accuracy: 0.6209 - 496ms/epoch - 4ms/step\n",
      "Epoch 36/10000\n",
      "113/113 - 0s - loss: 0.6645 - accuracy: 0.6202 - val_loss: 0.6627 - val_accuracy: 0.6238 - 491ms/epoch - 4ms/step\n",
      "Epoch 37/10000\n",
      "113/113 - 1s - loss: 0.6635 - accuracy: 0.6213 - val_loss: 0.6683 - val_accuracy: 0.6213 - 509ms/epoch - 5ms/step\n",
      "Epoch 38/10000\n",
      "113/113 - 0s - loss: 0.6650 - accuracy: 0.6196 - val_loss: 0.6624 - val_accuracy: 0.6211 - 496ms/epoch - 4ms/step\n",
      "Epoch 39/10000\n",
      "113/113 - 1s - loss: 0.6627 - accuracy: 0.6212 - val_loss: 0.6641 - val_accuracy: 0.6174 - 633ms/epoch - 6ms/step\n",
      "Epoch 40/10000\n",
      "113/113 - 1s - loss: 0.6631 - accuracy: 0.6183 - val_loss: 0.6624 - val_accuracy: 0.6178 - 521ms/epoch - 5ms/step\n",
      "Epoch 41/10000\n",
      "113/113 - 0s - loss: 0.6629 - accuracy: 0.6197 - val_loss: 0.6590 - val_accuracy: 0.6191 - 498ms/epoch - 4ms/step\n",
      "Epoch 42/10000\n",
      "113/113 - 1s - loss: 0.6628 - accuracy: 0.6200 - val_loss: 0.6619 - val_accuracy: 0.6225 - 517ms/epoch - 5ms/step\n",
      "Epoch 43/10000\n",
      "113/113 - 0s - loss: 0.6610 - accuracy: 0.6201 - val_loss: 0.6580 - val_accuracy: 0.6231 - 493ms/epoch - 4ms/step\n",
      "Epoch 44/10000\n",
      "113/113 - 1s - loss: 0.6608 - accuracy: 0.6201 - val_loss: 0.6610 - val_accuracy: 0.6236 - 577ms/epoch - 5ms/step\n",
      "Epoch 45/10000\n",
      "113/113 - 0s - loss: 0.6604 - accuracy: 0.6206 - val_loss: 0.6580 - val_accuracy: 0.6258 - 488ms/epoch - 4ms/step\n",
      "Epoch 46/10000\n",
      "113/113 - 1s - loss: 0.6602 - accuracy: 0.6206 - val_loss: 0.6576 - val_accuracy: 0.6243 - 509ms/epoch - 5ms/step\n",
      "Epoch 47/10000\n",
      "113/113 - 0s - loss: 0.6586 - accuracy: 0.6214 - val_loss: 0.6555 - val_accuracy: 0.6207 - 498ms/epoch - 4ms/step\n",
      "Epoch 48/10000\n",
      "113/113 - 1s - loss: 0.6598 - accuracy: 0.6216 - val_loss: 0.6576 - val_accuracy: 0.6223 - 545ms/epoch - 5ms/step\n",
      "Epoch 49/10000\n",
      "113/113 - 1s - loss: 0.6599 - accuracy: 0.6221 - val_loss: 0.6584 - val_accuracy: 0.6203 - 592ms/epoch - 5ms/step\n",
      "Epoch 50/10000\n",
      "113/113 - 0s - loss: 0.6592 - accuracy: 0.6203 - val_loss: 0.6604 - val_accuracy: 0.6261 - 497ms/epoch - 4ms/step\n",
      "Epoch 51/10000\n",
      "113/113 - 1s - loss: 0.6577 - accuracy: 0.6217 - val_loss: 0.6589 - val_accuracy: 0.6180 - 548ms/epoch - 5ms/step\n",
      "Epoch 52/10000\n",
      "113/113 - 1s - loss: 0.6571 - accuracy: 0.6216 - val_loss: 0.6595 - val_accuracy: 0.6187 - 528ms/epoch - 5ms/step\n",
      "Epoch 53/10000\n",
      "113/113 - 0s - loss: 0.6571 - accuracy: 0.6219 - val_loss: 0.6587 - val_accuracy: 0.6270 - 493ms/epoch - 4ms/step\n",
      "Epoch 54/10000\n",
      "113/113 - 1s - loss: 0.6567 - accuracy: 0.6214 - val_loss: 0.6601 - val_accuracy: 0.6269 - 560ms/epoch - 5ms/step\n",
      "Epoch 55/10000\n",
      "113/113 - 1s - loss: 0.6566 - accuracy: 0.6216 - val_loss: 0.6571 - val_accuracy: 0.6281 - 524ms/epoch - 5ms/step\n",
      "Epoch 56/10000\n",
      "113/113 - 1s - loss: 0.6557 - accuracy: 0.6227 - val_loss: 0.6580 - val_accuracy: 0.6199 - 561ms/epoch - 5ms/step\n",
      "Epoch 57/10000\n",
      "113/113 - 1s - loss: 0.6566 - accuracy: 0.6228 - val_loss: 0.6596 - val_accuracy: 0.6208 - 537ms/epoch - 5ms/step\n",
      "Epoch 58/10000\n",
      "113/113 - 1s - loss: 0.6575 - accuracy: 0.6213 - val_loss: 0.6623 - val_accuracy: 0.6167 - 517ms/epoch - 5ms/step\n",
      "Epoch 59/10000\n",
      "113/113 - 1s - loss: 0.6581 - accuracy: 0.6205 - val_loss: 0.6603 - val_accuracy: 0.6241 - 545ms/epoch - 5ms/step\n",
      "Epoch 60/10000\n",
      "113/113 - 1s - loss: 0.6580 - accuracy: 0.6203 - val_loss: 0.6584 - val_accuracy: 0.6212 - 516ms/epoch - 5ms/step\n",
      "Epoch 61/10000\n",
      "113/113 - 1s - loss: 0.6586 - accuracy: 0.6205 - val_loss: 0.6598 - val_accuracy: 0.6196 - 509ms/epoch - 5ms/step\n",
      "Epoch 62/10000\n",
      "113/113 - 0s - loss: 0.6571 - accuracy: 0.6208 - val_loss: 0.6621 - val_accuracy: 0.6245 - 495ms/epoch - 4ms/step\n",
      "Epoch 63/10000\n",
      "113/113 - 1s - loss: 0.6571 - accuracy: 0.6217 - val_loss: 0.6590 - val_accuracy: 0.6215 - 502ms/epoch - 4ms/step\n",
      "Epoch 64/10000\n",
      "113/113 - 1s - loss: 0.6566 - accuracy: 0.6206 - val_loss: 0.6593 - val_accuracy: 0.6160 - 546ms/epoch - 5ms/step\n",
      "Epoch 65/10000\n",
      "113/113 - 1s - loss: 0.6570 - accuracy: 0.6213 - val_loss: 0.6550 - val_accuracy: 0.6194 - 539ms/epoch - 5ms/step\n",
      "Epoch 66/10000\n",
      "113/113 - 1s - loss: 0.6561 - accuracy: 0.6217 - val_loss: 0.6590 - val_accuracy: 0.6196 - 533ms/epoch - 5ms/step\n",
      "Epoch 67/10000\n",
      "113/113 - 1s - loss: 0.6563 - accuracy: 0.6228 - val_loss: 0.6597 - val_accuracy: 0.6262 - 531ms/epoch - 5ms/step\n",
      "Epoch 68/10000\n",
      "113/113 - 1s - loss: 0.6561 - accuracy: 0.6225 - val_loss: 0.6653 - val_accuracy: 0.6223 - 510ms/epoch - 5ms/step\n",
      "Epoch 69/10000\n",
      "113/113 - 1s - loss: 0.6562 - accuracy: 0.6223 - val_loss: 0.6641 - val_accuracy: 0.6249 - 562ms/epoch - 5ms/step\n",
      "Epoch 70/10000\n",
      "113/113 - 1s - loss: 0.6552 - accuracy: 0.6223 - val_loss: 0.6593 - val_accuracy: 0.6235 - 533ms/epoch - 5ms/step\n",
      "Epoch 71/10000\n",
      "113/113 - 1s - loss: 0.6537 - accuracy: 0.6209 - val_loss: 0.6543 - val_accuracy: 0.6247 - 509ms/epoch - 5ms/step\n",
      "Epoch 72/10000\n",
      "113/113 - 1s - loss: 0.6552 - accuracy: 0.6202 - val_loss: 0.6573 - val_accuracy: 0.6243 - 519ms/epoch - 5ms/step\n",
      "Epoch 73/10000\n",
      "113/113 - 1s - loss: 0.6549 - accuracy: 0.6208 - val_loss: 0.6539 - val_accuracy: 0.6232 - 523ms/epoch - 5ms/step\n",
      "Epoch 74/10000\n",
      "113/113 - 1s - loss: 0.6556 - accuracy: 0.6199 - val_loss: 0.6537 - val_accuracy: 0.6236 - 554ms/epoch - 5ms/step\n",
      "Epoch 75/10000\n",
      "113/113 - 1s - loss: 0.6547 - accuracy: 0.6215 - val_loss: 0.6546 - val_accuracy: 0.6208 - 507ms/epoch - 4ms/step\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Iterazione 56/128 (03-11-2023_11-15-52)\n",
      "hidden_layer_size: 100\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 3\n",
      "activation_hidden: sigmoid\n",
      "activation_output: sigmoid\n",
      "batch_size: 1000\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 20\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 62.81%\n",
      "La recall di questo modello sul validation set è: 24.91%\n",
      "La f1 di questo modello sul validation set è: 34.14%\n",
      "La balanced accuracy di questo modello sul validation set è: 55.82%\n",
      "La precision di questo modello sul validation set è: 54.23%\n",
      "La AUC di questo modello sul validation set è: 60.54%\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.65    0.87      0.74  7,670.00\n",
      "1                  0.54    0.25      0.34  4,841.00\n",
      "accuracy           0.63    0.63      0.63      0.63\n",
      "macro avg          0.59    0.56      0.54 12,511.00\n",
      "weighted avg       0.61    0.63      0.59 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         6652         1018\n",
      "Actual 1         3635         1206\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 65.94% con il modello:\n",
      "1 - AC65.94%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 62.97% con il modello:\n",
      "1 - BA62.97%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 64.08% con il modello:\n",
      "13 - PR64.08%, L20, mV-10, MV10, P2, AHsigmoid, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-26-48).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "1126/1126 - 4s - loss: 811268288.0000 - accuracy: 0.5568 - val_loss: 117003120.0000 - val_accuracy: 0.5195 - 4s/epoch - 4ms/step\n",
      "Epoch 2/10000\n",
      "1126/1126 - 3s - loss: 93861408.0000 - accuracy: 0.5642 - val_loss: 199615184.0000 - val_accuracy: 0.4602 - 3s/epoch - 3ms/step\n",
      "Epoch 3/10000\n",
      "1126/1126 - 3s - loss: 82479312.0000 - accuracy: 0.5708 - val_loss: 41911404.0000 - val_accuracy: 0.5992 - 3s/epoch - 3ms/step\n",
      "Epoch 4/10000\n",
      "1126/1126 - 3s - loss: 85156816.0000 - accuracy: 0.5678 - val_loss: 58792588.0000 - val_accuracy: 0.6238 - 3s/epoch - 2ms/step\n",
      "Epoch 5/10000\n",
      "1126/1126 - 3s - loss: 80217944.0000 - accuracy: 0.5702 - val_loss: 87940816.0000 - val_accuracy: 0.6113 - 3s/epoch - 3ms/step\n",
      "Epoch 6/10000\n",
      "1126/1126 - 3s - loss: 70002104.0000 - accuracy: 0.5715 - val_loss: 89379152.0000 - val_accuracy: 0.4662 - 3s/epoch - 3ms/step\n",
      "Epoch 7/10000\n",
      "1126/1126 - 3s - loss: 79806944.0000 - accuracy: 0.5695 - val_loss: 57814260.0000 - val_accuracy: 0.5153 - 3s/epoch - 3ms/step\n",
      "Epoch 8/10000\n",
      "1126/1126 - 3s - loss: 74882336.0000 - accuracy: 0.5731 - val_loss: 168991424.0000 - val_accuracy: 0.6099 - 3s/epoch - 3ms/step\n",
      "Epoch 9/10000\n",
      "1126/1126 - 3s - loss: 69830952.0000 - accuracy: 0.5754 - val_loss: 67251032.0000 - val_accuracy: 0.5591 - 3s/epoch - 3ms/step\n",
      "Epoch 10/10000\n",
      "1126/1126 - 3s - loss: 76849040.0000 - accuracy: 0.5751 - val_loss: 103730736.0000 - val_accuracy: 0.4857 - 3s/epoch - 3ms/step\n",
      "Epoch 11/10000\n",
      "1126/1126 - 3s - loss: 79511168.0000 - accuracy: 0.5755 - val_loss: 140888208.0000 - val_accuracy: 0.5249 - 3s/epoch - 3ms/step\n",
      "Epoch 12/10000\n",
      "1126/1126 - 3s - loss: 73606504.0000 - accuracy: 0.5755 - val_loss: 33856328.0000 - val_accuracy: 0.6199 - 3s/epoch - 3ms/step\n",
      "Epoch 13/10000\n",
      "1126/1126 - 3s - loss: 70701184.0000 - accuracy: 0.5764 - val_loss: 77325344.0000 - val_accuracy: 0.6164 - 3s/epoch - 3ms/step\n",
      "Epoch 14/10000\n",
      "1126/1126 - 3s - loss: 70462056.0000 - accuracy: 0.5769 - val_loss: 105338800.0000 - val_accuracy: 0.4658 - 3s/epoch - 3ms/step\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Iterazione 57/128 (03-11-2023_11-16-36)\n",
      "hidden_layer_size: 100\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 4\n",
      "activation_hidden: relu\n",
      "activation_output: sigmoid\n",
      "batch_size: 100\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 10\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 62.38%\n",
      "La recall di questo modello sul validation set è: 24.75%\n",
      "La f1 di questo modello sul validation set è: 33.73%\n",
      "La balanced accuracy di questo modello sul validation set è: 55.44%\n",
      "La precision di questo modello sul validation set è: 52.96%\n",
      "La AUC di questo modello sul validation set è: 55.44%\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.64    0.86      0.74  7,670.00\n",
      "1                  0.53    0.25      0.34  4,841.00\n",
      "accuracy           0.62    0.62      0.62      0.62\n",
      "macro avg          0.59    0.55      0.54 12,511.00\n",
      "weighted avg       0.60    0.62      0.58 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         6606         1064\n",
      "Actual 1         3643         1198\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 65.94% con il modello:\n",
      "1 - AC65.94%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 62.97% con il modello:\n",
      "1 - BA62.97%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 64.08% con il modello:\n",
      "13 - PR64.08%, L20, mV-10, MV10, P2, AHsigmoid, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-26-48).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "1126/1126 - 5s - loss: 811268288.0000 - accuracy: 0.5568 - val_loss: 117003120.0000 - val_accuracy: 0.5195 - 5s/epoch - 4ms/step\n",
      "Epoch 2/10000\n",
      "1126/1126 - 3s - loss: 93861408.0000 - accuracy: 0.5642 - val_loss: 199615184.0000 - val_accuracy: 0.4602 - 3s/epoch - 3ms/step\n",
      "Epoch 3/10000\n",
      "1126/1126 - 3s - loss: 82479312.0000 - accuracy: 0.5708 - val_loss: 41911404.0000 - val_accuracy: 0.5992 - 3s/epoch - 3ms/step\n",
      "Epoch 4/10000\n",
      "1126/1126 - 3s - loss: 85156816.0000 - accuracy: 0.5678 - val_loss: 58792588.0000 - val_accuracy: 0.6238 - 3s/epoch - 3ms/step\n",
      "Epoch 5/10000\n",
      "1126/1126 - 3s - loss: 80217944.0000 - accuracy: 0.5702 - val_loss: 87940816.0000 - val_accuracy: 0.6113 - 3s/epoch - 3ms/step\n",
      "Epoch 6/10000\n",
      "1126/1126 - 3s - loss: 70002104.0000 - accuracy: 0.5715 - val_loss: 89379152.0000 - val_accuracy: 0.4662 - 3s/epoch - 3ms/step\n",
      "Epoch 7/10000\n",
      "1126/1126 - 3s - loss: 79806944.0000 - accuracy: 0.5695 - val_loss: 57814260.0000 - val_accuracy: 0.5153 - 3s/epoch - 3ms/step\n",
      "Epoch 8/10000\n",
      "1126/1126 - 3s - loss: 74882336.0000 - accuracy: 0.5731 - val_loss: 168991424.0000 - val_accuracy: 0.6099 - 3s/epoch - 3ms/step\n",
      "Epoch 9/10000\n",
      "1126/1126 - 3s - loss: 69830952.0000 - accuracy: 0.5754 - val_loss: 67251032.0000 - val_accuracy: 0.5591 - 3s/epoch - 2ms/step\n",
      "Epoch 10/10000\n",
      "1126/1126 - 3s - loss: 76849040.0000 - accuracy: 0.5751 - val_loss: 103730736.0000 - val_accuracy: 0.4857 - 3s/epoch - 3ms/step\n",
      "Epoch 11/10000\n",
      "1126/1126 - 3s - loss: 79511168.0000 - accuracy: 0.5755 - val_loss: 140888208.0000 - val_accuracy: 0.5249 - 3s/epoch - 3ms/step\n",
      "Epoch 12/10000\n",
      "1126/1126 - 3s - loss: 73606504.0000 - accuracy: 0.5755 - val_loss: 33856328.0000 - val_accuracy: 0.6199 - 3s/epoch - 3ms/step\n",
      "Epoch 13/10000\n",
      "1126/1126 - 3s - loss: 70701184.0000 - accuracy: 0.5764 - val_loss: 77325344.0000 - val_accuracy: 0.6164 - 3s/epoch - 3ms/step\n",
      "Epoch 14/10000\n",
      "1126/1126 - 3s - loss: 70462056.0000 - accuracy: 0.5769 - val_loss: 105338800.0000 - val_accuracy: 0.4658 - 3s/epoch - 3ms/step\n",
      "Epoch 15/10000\n",
      "1126/1126 - 3s - loss: 67950960.0000 - accuracy: 0.5789 - val_loss: 51578900.0000 - val_accuracy: 0.5357 - 3s/epoch - 2ms/step\n",
      "Epoch 16/10000\n",
      "1126/1126 - 3s - loss: 65696620.0000 - accuracy: 0.5821 - val_loss: 115101192.0000 - val_accuracy: 0.4735 - 3s/epoch - 3ms/step\n",
      "Epoch 17/10000\n",
      "1126/1126 - 3s - loss: 67778376.0000 - accuracy: 0.5785 - val_loss: 23455862.0000 - val_accuracy: 0.5832 - 3s/epoch - 3ms/step\n",
      "Epoch 18/10000\n",
      "1126/1126 - 3s - loss: 73855424.0000 - accuracy: 0.5797 - val_loss: 116419624.0000 - val_accuracy: 0.5276 - 3s/epoch - 3ms/step\n",
      "Epoch 19/10000\n",
      "1126/1126 - 3s - loss: 63124468.0000 - accuracy: 0.5841 - val_loss: 35797812.0000 - val_accuracy: 0.5248 - 3s/epoch - 3ms/step\n",
      "Epoch 20/10000\n",
      "1126/1126 - 3s - loss: 66855300.0000 - accuracy: 0.5832 - val_loss: 22202754.0000 - val_accuracy: 0.6270 - 3s/epoch - 3ms/step\n",
      "Epoch 21/10000\n",
      "1126/1126 - 3s - loss: 68433072.0000 - accuracy: 0.5812 - val_loss: 24333688.0000 - val_accuracy: 0.5829 - 3s/epoch - 3ms/step\n",
      "Epoch 22/10000\n",
      "1126/1126 - 3s - loss: 63083288.0000 - accuracy: 0.5855 - val_loss: 66987656.0000 - val_accuracy: 0.4841 - 3s/epoch - 2ms/step\n",
      "Epoch 23/10000\n",
      "1126/1126 - 3s - loss: 73905640.0000 - accuracy: 0.5813 - val_loss: 27962728.0000 - val_accuracy: 0.6141 - 3s/epoch - 2ms/step\n",
      "Epoch 24/10000\n",
      "1126/1126 - 3s - loss: 72915816.0000 - accuracy: 0.5824 - val_loss: 58547740.0000 - val_accuracy: 0.6305 - 3s/epoch - 3ms/step\n",
      "Epoch 25/10000\n",
      "1126/1126 - 3s - loss: 61852464.0000 - accuracy: 0.5857 - val_loss: 56952692.0000 - val_accuracy: 0.6283 - 3s/epoch - 2ms/step\n",
      "Epoch 26/10000\n",
      "1126/1126 - 3s - loss: 66253444.0000 - accuracy: 0.5835 - val_loss: 27581416.0000 - val_accuracy: 0.6267 - 3s/epoch - 3ms/step\n",
      "Epoch 27/10000\n",
      "1126/1126 - 3s - loss: 67124400.0000 - accuracy: 0.5848 - val_loss: 71420504.0000 - val_accuracy: 0.6295 - 3s/epoch - 3ms/step\n",
      "Epoch 28/10000\n",
      "1126/1126 - 3s - loss: 63769952.0000 - accuracy: 0.5860 - val_loss: 23408690.0000 - val_accuracy: 0.6128 - 3s/epoch - 2ms/step\n",
      "Epoch 29/10000\n",
      "1126/1126 - 3s - loss: 72014216.0000 - accuracy: 0.5825 - val_loss: 105547408.0000 - val_accuracy: 0.4612 - 3s/epoch - 3ms/step\n",
      "Epoch 30/10000\n",
      "1126/1126 - 3s - loss: 67014840.0000 - accuracy: 0.5858 - val_loss: 32005520.0000 - val_accuracy: 0.5860 - 3s/epoch - 2ms/step\n",
      "Epoch 31/10000\n",
      "1126/1126 - 3s - loss: 66474540.0000 - accuracy: 0.5875 - val_loss: 20527880.0000 - val_accuracy: 0.6362 - 3s/epoch - 3ms/step\n",
      "Epoch 32/10000\n",
      "1126/1126 - 3s - loss: 67772544.0000 - accuracy: 0.5866 - val_loss: 150884432.0000 - val_accuracy: 0.4372 - 3s/epoch - 3ms/step\n",
      "Epoch 33/10000\n",
      "1126/1126 - 3s - loss: 64658776.0000 - accuracy: 0.5877 - val_loss: 51884748.0000 - val_accuracy: 0.6296 - 3s/epoch - 2ms/step\n",
      "Epoch 34/10000\n",
      "1126/1126 - 3s - loss: 66699632.0000 - accuracy: 0.5867 - val_loss: 169397392.0000 - val_accuracy: 0.4450 - 3s/epoch - 2ms/step\n",
      "Epoch 35/10000\n",
      "1126/1126 - 3s - loss: 62752904.0000 - accuracy: 0.5892 - val_loss: 19988026.0000 - val_accuracy: 0.6431 - 3s/epoch - 2ms/step\n",
      "Epoch 36/10000\n",
      "1126/1126 - 3s - loss: 66428920.0000 - accuracy: 0.5855 - val_loss: 46779828.0000 - val_accuracy: 0.6356 - 3s/epoch - 2ms/step\n",
      "Epoch 37/10000\n",
      "1126/1126 - 3s - loss: 62411804.0000 - accuracy: 0.5880 - val_loss: 24525970.0000 - val_accuracy: 0.6420 - 3s/epoch - 2ms/step\n",
      "Epoch 38/10000\n",
      "1126/1126 - 3s - loss: 63406432.0000 - accuracy: 0.5861 - val_loss: 75821112.0000 - val_accuracy: 0.6176 - 3s/epoch - 2ms/step\n",
      "Epoch 39/10000\n",
      "1126/1126 - 3s - loss: 66439948.0000 - accuracy: 0.5878 - val_loss: 44644208.0000 - val_accuracy: 0.6327 - 3s/epoch - 2ms/step\n",
      "Epoch 40/10000\n",
      "1126/1126 - 3s - loss: 57426276.0000 - accuracy: 0.5908 - val_loss: 22022640.0000 - val_accuracy: 0.6413 - 3s/epoch - 2ms/step\n",
      "Epoch 41/10000\n",
      "1126/1126 - 3s - loss: 66123004.0000 - accuracy: 0.5877 - val_loss: 46378256.0000 - val_accuracy: 0.6035 - 3s/epoch - 2ms/step\n",
      "Epoch 42/10000\n",
      "1126/1126 - 3s - loss: 65950552.0000 - accuracy: 0.5869 - val_loss: 137496288.0000 - val_accuracy: 0.5302 - 3s/epoch - 2ms/step\n",
      "Epoch 43/10000\n",
      "1126/1126 - 3s - loss: 62622344.0000 - accuracy: 0.5869 - val_loss: 96366816.0000 - val_accuracy: 0.6286 - 3s/epoch - 2ms/step\n",
      "Epoch 44/10000\n",
      "1126/1126 - 3s - loss: 62778884.0000 - accuracy: 0.5885 - val_loss: 192319696.0000 - val_accuracy: 0.6140 - 3s/epoch - 2ms/step\n",
      "Epoch 45/10000\n",
      "1126/1126 - 3s - loss: 60545328.0000 - accuracy: 0.5899 - val_loss: 74052240.0000 - val_accuracy: 0.6179 - 3s/epoch - 2ms/step\n",
      "Epoch 46/10000\n",
      "1126/1126 - 3s - loss: 68475112.0000 - accuracy: 0.5876 - val_loss: 31869070.0000 - val_accuracy: 0.6410 - 3s/epoch - 2ms/step\n",
      "Epoch 47/10000\n",
      "1126/1126 - 3s - loss: 66766524.0000 - accuracy: 0.5894 - val_loss: 91288824.0000 - val_accuracy: 0.6180 - 3s/epoch - 2ms/step\n",
      "Epoch 48/10000\n",
      "1126/1126 - 3s - loss: 58112168.0000 - accuracy: 0.5912 - val_loss: 25489730.0000 - val_accuracy: 0.6458 - 3s/epoch - 2ms/step\n",
      "Epoch 49/10000\n",
      "1126/1126 - 3s - loss: 59260276.0000 - accuracy: 0.5908 - val_loss: 132604768.0000 - val_accuracy: 0.6227 - 3s/epoch - 2ms/step\n",
      "Epoch 50/10000\n",
      "1126/1126 - 3s - loss: 63262996.0000 - accuracy: 0.5909 - val_loss: 57844384.0000 - val_accuracy: 0.6220 - 3s/epoch - 2ms/step\n",
      "Epoch 51/10000\n",
      "1126/1126 - 3s - loss: 58356860.0000 - accuracy: 0.5918 - val_loss: 29233648.0000 - val_accuracy: 0.6158 - 3s/epoch - 2ms/step\n",
      "Epoch 52/10000\n",
      "1126/1126 - 3s - loss: 66362364.0000 - accuracy: 0.5875 - val_loss: 175918880.0000 - val_accuracy: 0.6142 - 3s/epoch - 2ms/step\n",
      "Epoch 53/10000\n",
      "1126/1126 - 3s - loss: 62675432.0000 - accuracy: 0.5884 - val_loss: 34167776.0000 - val_accuracy: 0.5510 - 3s/epoch - 2ms/step\n",
      "Epoch 54/10000\n",
      "1126/1126 - 3s - loss: 61151024.0000 - accuracy: 0.5905 - val_loss: 115278280.0000 - val_accuracy: 0.6155 - 3s/epoch - 2ms/step\n",
      "Epoch 55/10000\n",
      "1126/1126 - 3s - loss: 59989180.0000 - accuracy: 0.5920 - val_loss: 125399192.0000 - val_accuracy: 0.6164 - 3s/epoch - 2ms/step\n",
      "Epoch 56/10000\n",
      "1126/1126 - 3s - loss: 71477840.0000 - accuracy: 0.5886 - val_loss: 46507340.0000 - val_accuracy: 0.6282 - 3s/epoch - 2ms/step\n",
      "Epoch 57/10000\n",
      "1126/1126 - 3s - loss: 56854180.0000 - accuracy: 0.5909 - val_loss: 55611808.0000 - val_accuracy: 0.6322 - 3s/epoch - 2ms/step\n",
      "Epoch 58/10000\n",
      "1126/1126 - 3s - loss: 57746156.0000 - accuracy: 0.5920 - val_loss: 34710444.0000 - val_accuracy: 0.6280 - 3s/epoch - 2ms/step\n",
      "Epoch 59/10000\n",
      "1126/1126 - 3s - loss: 59363928.0000 - accuracy: 0.5906 - val_loss: 178299616.0000 - val_accuracy: 0.6140 - 3s/epoch - 2ms/step\n",
      "Epoch 60/10000\n",
      "1126/1126 - 3s - loss: 63793916.0000 - accuracy: 0.5896 - val_loss: 26015060.0000 - val_accuracy: 0.6410 - 3s/epoch - 2ms/step\n",
      "Epoch 61/10000\n",
      "1126/1126 - 3s - loss: 64554596.0000 - accuracy: 0.5908 - val_loss: 92034584.0000 - val_accuracy: 0.6225 - 3s/epoch - 2ms/step\n",
      "Epoch 62/10000\n",
      "1126/1126 - 3s - loss: 58971236.0000 - accuracy: 0.5937 - val_loss: 39431628.0000 - val_accuracy: 0.5423 - 3s/epoch - 2ms/step\n",
      "Epoch 63/10000\n",
      "1126/1126 - 3s - loss: 58865448.0000 - accuracy: 0.5910 - val_loss: 130928112.0000 - val_accuracy: 0.6164 - 3s/epoch - 2ms/step\n",
      "Epoch 64/10000\n",
      "1126/1126 - 3s - loss: 59909760.0000 - accuracy: 0.5883 - val_loss: 38816928.0000 - val_accuracy: 0.5247 - 3s/epoch - 2ms/step\n",
      "Epoch 65/10000\n",
      "1126/1126 - 3s - loss: 64434676.0000 - accuracy: 0.5917 - val_loss: 25860722.0000 - val_accuracy: 0.6375 - 3s/epoch - 3ms/step\n",
      "Epoch 66/10000\n",
      "1126/1126 - 3s - loss: 59638748.0000 - accuracy: 0.5926 - val_loss: 152288112.0000 - val_accuracy: 0.4508 - 3s/epoch - 2ms/step\n",
      "Epoch 67/10000\n",
      "1126/1126 - 3s - loss: 63229672.0000 - accuracy: 0.5908 - val_loss: 21209814.0000 - val_accuracy: 0.6458 - 3s/epoch - 2ms/step\n",
      "Epoch 68/10000\n",
      "1126/1126 - 3s - loss: 56915740.0000 - accuracy: 0.5917 - val_loss: 68638824.0000 - val_accuracy: 0.6183 - 3s/epoch - 2ms/step\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Iterazione 58/128 (03-11-2023_11-19-49)\n",
      "hidden_layer_size: 100\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 4\n",
      "activation_hidden: relu\n",
      "activation_output: sigmoid\n",
      "batch_size: 100\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 20\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 64.58%\n",
      "La recall di questo modello sul validation set è: 29.23%\n",
      "La f1 di questo modello sul validation set è: 38.98%\n",
      "La balanced accuracy di questo modello sul validation set è: 58.06%\n",
      "La precision di questo modello sul validation set è: 58.47%\n",
      "La AUC di questo modello sul validation set è: 58.06%\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.66    0.87      0.75  7,670.00\n",
      "1                  0.58    0.29      0.39  4,841.00\n",
      "accuracy           0.65    0.65      0.65      0.65\n",
      "macro avg          0.62    0.58      0.57 12,511.00\n",
      "weighted avg       0.63    0.65      0.61 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         6665         1005\n",
      "Actual 1         3426         1415\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 65.94% con il modello:\n",
      "1 - AC65.94%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 62.97% con il modello:\n",
      "1 - BA62.97%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 64.08% con il modello:\n",
      "13 - PR64.08%, L20, mV-10, MV10, P2, AHsigmoid, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-26-48).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "113/113 - 2s - loss: 5449523712.0000 - accuracy: 0.5097 - val_loss: 629431104.0000 - val_accuracy: 0.5632 - 2s/epoch - 19ms/step\n",
      "Epoch 2/10000\n",
      "113/113 - 1s - loss: 522243360.0000 - accuracy: 0.5591 - val_loss: 437589760.0000 - val_accuracy: 0.5694 - 656ms/epoch - 6ms/step\n",
      "Epoch 3/10000\n",
      "113/113 - 1s - loss: 399768352.0000 - accuracy: 0.5653 - val_loss: 346153984.0000 - val_accuracy: 0.5762 - 592ms/epoch - 5ms/step\n",
      "Epoch 4/10000\n",
      "113/113 - 1s - loss: 337024864.0000 - accuracy: 0.5628 - val_loss: 332346592.0000 - val_accuracy: 0.5962 - 599ms/epoch - 5ms/step\n",
      "Epoch 5/10000\n",
      "113/113 - 1s - loss: 256931184.0000 - accuracy: 0.5657 - val_loss: 225860096.0000 - val_accuracy: 0.5805 - 677ms/epoch - 6ms/step\n",
      "Epoch 6/10000\n",
      "113/113 - 1s - loss: 194830256.0000 - accuracy: 0.5614 - val_loss: 159867008.0000 - val_accuracy: 0.5804 - 616ms/epoch - 5ms/step\n",
      "Epoch 7/10000\n",
      "113/113 - 1s - loss: 153758128.0000 - accuracy: 0.5601 - val_loss: 104125824.0000 - val_accuracy: 0.5328 - 602ms/epoch - 5ms/step\n",
      "Epoch 8/10000\n",
      "113/113 - 1s - loss: 106247288.0000 - accuracy: 0.5618 - val_loss: 78987320.0000 - val_accuracy: 0.5735 - 659ms/epoch - 6ms/step\n",
      "Epoch 9/10000\n",
      "113/113 - 1s - loss: 78866592.0000 - accuracy: 0.5616 - val_loss: 124262984.0000 - val_accuracy: 0.5002 - 670ms/epoch - 6ms/step\n",
      "Epoch 10/10000\n",
      "113/113 - 1s - loss: 101467808.0000 - accuracy: 0.5643 - val_loss: 71091952.0000 - val_accuracy: 0.6136 - 683ms/epoch - 6ms/step\n",
      "Epoch 11/10000\n",
      "113/113 - 1s - loss: 91611464.0000 - accuracy: 0.5660 - val_loss: 93417544.0000 - val_accuracy: 0.6043 - 699ms/epoch - 6ms/step\n",
      "Epoch 12/10000\n",
      "113/113 - 1s - loss: 75140184.0000 - accuracy: 0.5649 - val_loss: 81352384.0000 - val_accuracy: 0.5310 - 809ms/epoch - 7ms/step\n",
      "Epoch 13/10000\n",
      "113/113 - 1s - loss: 64408408.0000 - accuracy: 0.5664 - val_loss: 125068328.0000 - val_accuracy: 0.4857 - 692ms/epoch - 6ms/step\n",
      "Epoch 14/10000\n",
      "113/113 - 1s - loss: 65826288.0000 - accuracy: 0.5672 - val_loss: 42102036.0000 - val_accuracy: 0.5737 - 621ms/epoch - 5ms/step\n",
      "Epoch 15/10000\n",
      "113/113 - 1s - loss: 64881000.0000 - accuracy: 0.5682 - val_loss: 44880672.0000 - val_accuracy: 0.5673 - 699ms/epoch - 6ms/step\n",
      "Epoch 16/10000\n",
      "113/113 - 1s - loss: 47543364.0000 - accuracy: 0.5697 - val_loss: 78624032.0000 - val_accuracy: 0.4992 - 542ms/epoch - 5ms/step\n",
      "Epoch 17/10000\n",
      "113/113 - 1s - loss: 56428172.0000 - accuracy: 0.5695 - val_loss: 56478152.0000 - val_accuracy: 0.6046 - 640ms/epoch - 6ms/step\n",
      "Epoch 18/10000\n",
      "113/113 - 1s - loss: 64360204.0000 - accuracy: 0.5689 - val_loss: 89886584.0000 - val_accuracy: 0.6127 - 551ms/epoch - 5ms/step\n",
      "Epoch 19/10000\n",
      "113/113 - 1s - loss: 47918180.0000 - accuracy: 0.5701 - val_loss: 49821932.0000 - val_accuracy: 0.5975 - 544ms/epoch - 5ms/step\n",
      "Epoch 20/10000\n",
      "113/113 - 1s - loss: 46531324.0000 - accuracy: 0.5711 - val_loss: 51140772.0000 - val_accuracy: 0.5176 - 576ms/epoch - 5ms/step\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Iterazione 59/128 (03-11-2023_11-20-05)\n",
      "hidden_layer_size: 100\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 4\n",
      "activation_hidden: relu\n",
      "activation_output: sigmoid\n",
      "batch_size: 1000\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 10\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 61.36%\n",
      "La recall di questo modello sul validation set è: 37.93%\n",
      "La f1 di questo modello sul validation set è: 43.17%\n",
      "La balanced accuracy di questo modello sul validation set è: 57.04%\n",
      "La precision di questo modello sul validation set è: 50.1%\n",
      "La AUC di questo modello sul validation set è: 57.04%\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.66    0.76      0.71  7,670.00\n",
      "1                  0.50    0.38      0.43  4,841.00\n",
      "accuracy           0.61    0.61      0.61      0.61\n",
      "macro avg          0.58    0.57      0.57 12,511.00\n",
      "weighted avg       0.60    0.61      0.60 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         5841         1829\n",
      "Actual 1         3005         1836\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 65.94% con il modello:\n",
      "1 - AC65.94%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 62.97% con il modello:\n",
      "1 - BA62.97%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 64.08% con il modello:\n",
      "13 - PR64.08%, L20, mV-10, MV10, P2, AHsigmoid, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-26-48).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "113/113 - 2s - loss: 5449523712.0000 - accuracy: 0.5097 - val_loss: 629431104.0000 - val_accuracy: 0.5632 - 2s/epoch - 18ms/step\n",
      "Epoch 2/10000\n",
      "113/113 - 1s - loss: 522243360.0000 - accuracy: 0.5591 - val_loss: 437589760.0000 - val_accuracy: 0.5694 - 644ms/epoch - 6ms/step\n",
      "Epoch 3/10000\n",
      "113/113 - 1s - loss: 399768352.0000 - accuracy: 0.5653 - val_loss: 346153984.0000 - val_accuracy: 0.5762 - 663ms/epoch - 6ms/step\n",
      "Epoch 4/10000\n",
      "113/113 - 1s - loss: 337024864.0000 - accuracy: 0.5628 - val_loss: 332346592.0000 - val_accuracy: 0.5962 - 635ms/epoch - 6ms/step\n",
      "Epoch 5/10000\n",
      "113/113 - 1s - loss: 256931184.0000 - accuracy: 0.5657 - val_loss: 225860096.0000 - val_accuracy: 0.5805 - 592ms/epoch - 5ms/step\n",
      "Epoch 6/10000\n",
      "113/113 - 1s - loss: 194830256.0000 - accuracy: 0.5614 - val_loss: 159867008.0000 - val_accuracy: 0.5804 - 691ms/epoch - 6ms/step\n",
      "Epoch 7/10000\n",
      "113/113 - 1s - loss: 153758128.0000 - accuracy: 0.5601 - val_loss: 104125824.0000 - val_accuracy: 0.5328 - 601ms/epoch - 5ms/step\n",
      "Epoch 8/10000\n",
      "113/113 - 1s - loss: 106247288.0000 - accuracy: 0.5618 - val_loss: 78987320.0000 - val_accuracy: 0.5735 - 590ms/epoch - 5ms/step\n",
      "Epoch 9/10000\n",
      "113/113 - 1s - loss: 78866592.0000 - accuracy: 0.5616 - val_loss: 124262984.0000 - val_accuracy: 0.5002 - 671ms/epoch - 6ms/step\n",
      "Epoch 10/10000\n",
      "113/113 - 1s - loss: 101467808.0000 - accuracy: 0.5643 - val_loss: 71091952.0000 - val_accuracy: 0.6136 - 602ms/epoch - 5ms/step\n",
      "Epoch 11/10000\n",
      "113/113 - 1s - loss: 91611464.0000 - accuracy: 0.5660 - val_loss: 93417544.0000 - val_accuracy: 0.6043 - 658ms/epoch - 6ms/step\n",
      "Epoch 12/10000\n",
      "113/113 - 1s - loss: 75140184.0000 - accuracy: 0.5649 - val_loss: 81352384.0000 - val_accuracy: 0.5310 - 747ms/epoch - 7ms/step\n",
      "Epoch 13/10000\n",
      "113/113 - 1s - loss: 64408408.0000 - accuracy: 0.5664 - val_loss: 125068328.0000 - val_accuracy: 0.4857 - 643ms/epoch - 6ms/step\n",
      "Epoch 14/10000\n",
      "113/113 - 1s - loss: 65826288.0000 - accuracy: 0.5672 - val_loss: 42102036.0000 - val_accuracy: 0.5737 - 590ms/epoch - 5ms/step\n",
      "Epoch 15/10000\n",
      "113/113 - 1s - loss: 64881000.0000 - accuracy: 0.5682 - val_loss: 44880672.0000 - val_accuracy: 0.5673 - 537ms/epoch - 5ms/step\n",
      "Epoch 16/10000\n",
      "113/113 - 1s - loss: 47543364.0000 - accuracy: 0.5697 - val_loss: 78624032.0000 - val_accuracy: 0.4992 - 553ms/epoch - 5ms/step\n",
      "Epoch 17/10000\n",
      "113/113 - 1s - loss: 56428172.0000 - accuracy: 0.5695 - val_loss: 56478152.0000 - val_accuracy: 0.6046 - 561ms/epoch - 5ms/step\n",
      "Epoch 18/10000\n",
      "113/113 - 1s - loss: 64360204.0000 - accuracy: 0.5689 - val_loss: 89886584.0000 - val_accuracy: 0.6127 - 531ms/epoch - 5ms/step\n",
      "Epoch 19/10000\n",
      "113/113 - 1s - loss: 47918180.0000 - accuracy: 0.5701 - val_loss: 49821932.0000 - val_accuracy: 0.5975 - 533ms/epoch - 5ms/step\n",
      "Epoch 20/10000\n",
      "113/113 - 1s - loss: 46531324.0000 - accuracy: 0.5711 - val_loss: 51140772.0000 - val_accuracy: 0.5176 - 606ms/epoch - 5ms/step\n",
      "Epoch 21/10000\n",
      "113/113 - 1s - loss: 50410492.0000 - accuracy: 0.5707 - val_loss: 69364072.0000 - val_accuracy: 0.4878 - 561ms/epoch - 5ms/step\n",
      "Epoch 22/10000\n",
      "113/113 - 1s - loss: 54591008.0000 - accuracy: 0.5695 - val_loss: 31885978.0000 - val_accuracy: 0.5625 - 550ms/epoch - 5ms/step\n",
      "Epoch 23/10000\n",
      "113/113 - 1s - loss: 61129716.0000 - accuracy: 0.5684 - val_loss: 43608068.0000 - val_accuracy: 0.6232 - 547ms/epoch - 5ms/step\n",
      "Epoch 24/10000\n",
      "113/113 - 1s - loss: 65415520.0000 - accuracy: 0.5741 - val_loss: 32382472.0000 - val_accuracy: 0.6074 - 619ms/epoch - 5ms/step\n",
      "Epoch 25/10000\n",
      "113/113 - 1s - loss: 72335352.0000 - accuracy: 0.5733 - val_loss: 42285828.0000 - val_accuracy: 0.5442 - 538ms/epoch - 5ms/step\n",
      "Epoch 26/10000\n",
      "113/113 - 1s - loss: 67442216.0000 - accuracy: 0.5701 - val_loss: 242287584.0000 - val_accuracy: 0.6133 - 589ms/epoch - 5ms/step\n",
      "Epoch 27/10000\n",
      "113/113 - 1s - loss: 63658744.0000 - accuracy: 0.5754 - val_loss: 29857088.0000 - val_accuracy: 0.5923 - 540ms/epoch - 5ms/step\n",
      "Epoch 28/10000\n",
      "113/113 - 1s - loss: 50866184.0000 - accuracy: 0.5742 - val_loss: 28048706.0000 - val_accuracy: 0.5884 - 542ms/epoch - 5ms/step\n",
      "Epoch 29/10000\n",
      "113/113 - 1s - loss: 68272472.0000 - accuracy: 0.5726 - val_loss: 42494660.0000 - val_accuracy: 0.6036 - 613ms/epoch - 5ms/step\n",
      "Epoch 30/10000\n",
      "113/113 - 1s - loss: 74800160.0000 - accuracy: 0.5731 - val_loss: 38398440.0000 - val_accuracy: 0.5850 - 534ms/epoch - 5ms/step\n",
      "Epoch 31/10000\n",
      "113/113 - 1s - loss: 42604712.0000 - accuracy: 0.5768 - val_loss: 39888888.0000 - val_accuracy: 0.6220 - 558ms/epoch - 5ms/step\n",
      "Epoch 32/10000\n",
      "113/113 - 1s - loss: 47871180.0000 - accuracy: 0.5769 - val_loss: 31317434.0000 - val_accuracy: 0.6094 - 543ms/epoch - 5ms/step\n",
      "Epoch 33/10000\n",
      "113/113 - 1s - loss: 71801120.0000 - accuracy: 0.5713 - val_loss: 96289832.0000 - val_accuracy: 0.5318 - 559ms/epoch - 5ms/step\n",
      "Epoch 34/10000\n",
      "113/113 - 1s - loss: 73453296.0000 - accuracy: 0.5720 - val_loss: 36283788.0000 - val_accuracy: 0.5504 - 639ms/epoch - 6ms/step\n",
      "Epoch 35/10000\n",
      "113/113 - 1s - loss: 41422528.0000 - accuracy: 0.5754 - val_loss: 63093736.0000 - val_accuracy: 0.6250 - 554ms/epoch - 5ms/step\n",
      "Epoch 36/10000\n",
      "113/113 - 1s - loss: 70294448.0000 - accuracy: 0.5721 - val_loss: 30880028.0000 - val_accuracy: 0.5710 - 605ms/epoch - 5ms/step\n",
      "Epoch 37/10000\n",
      "113/113 - 1s - loss: 43303776.0000 - accuracy: 0.5762 - val_loss: 113410640.0000 - val_accuracy: 0.6104 - 556ms/epoch - 5ms/step\n",
      "Epoch 38/10000\n",
      "113/113 - 1s - loss: 47714680.0000 - accuracy: 0.5756 - val_loss: 89059656.0000 - val_accuracy: 0.4749 - 534ms/epoch - 5ms/step\n",
      "Epoch 39/10000\n",
      "113/113 - 1s - loss: 56229812.0000 - accuracy: 0.5753 - val_loss: 28902604.0000 - val_accuracy: 0.5901 - 593ms/epoch - 5ms/step\n",
      "Epoch 40/10000\n",
      "113/113 - 1s - loss: 61579708.0000 - accuracy: 0.5731 - val_loss: 48915724.0000 - val_accuracy: 0.5626 - 548ms/epoch - 5ms/step\n",
      "Epoch 41/10000\n",
      "113/113 - 1s - loss: 58095040.0000 - accuracy: 0.5745 - val_loss: 123727568.0000 - val_accuracy: 0.6105 - 543ms/epoch - 5ms/step\n",
      "Epoch 42/10000\n",
      "113/113 - 1s - loss: 50168268.0000 - accuracy: 0.5787 - val_loss: 33558028.0000 - val_accuracy: 0.5511 - 542ms/epoch - 5ms/step\n",
      "Epoch 43/10000\n",
      "113/113 - 1s - loss: 60546464.0000 - accuracy: 0.5768 - val_loss: 24412126.0000 - val_accuracy: 0.5830 - 604ms/epoch - 5ms/step\n",
      "Epoch 44/10000\n",
      "113/113 - 1s - loss: 44889068.0000 - accuracy: 0.5758 - val_loss: 53644992.0000 - val_accuracy: 0.6250 - 606ms/epoch - 5ms/step\n",
      "Epoch 45/10000\n",
      "113/113 - 1s - loss: 56158056.0000 - accuracy: 0.5762 - val_loss: 53608620.0000 - val_accuracy: 0.6073 - 558ms/epoch - 5ms/step\n",
      "Epoch 46/10000\n",
      "113/113 - 1s - loss: 65352744.0000 - accuracy: 0.5734 - val_loss: 22693682.0000 - val_accuracy: 0.5935 - 547ms/epoch - 5ms/step\n",
      "Epoch 47/10000\n",
      "113/113 - 1s - loss: 53457936.0000 - accuracy: 0.5760 - val_loss: 26937588.0000 - val_accuracy: 0.5670 - 545ms/epoch - 5ms/step\n",
      "Epoch 48/10000\n",
      "113/113 - 1s - loss: 65192212.0000 - accuracy: 0.5773 - val_loss: 25097000.0000 - val_accuracy: 0.6161 - 540ms/epoch - 5ms/step\n",
      "Epoch 49/10000\n",
      "113/113 - 1s - loss: 35373652.0000 - accuracy: 0.5774 - val_loss: 38841684.0000 - val_accuracy: 0.6056 - 691ms/epoch - 6ms/step\n",
      "Epoch 50/10000\n",
      "113/113 - 1s - loss: 42971896.0000 - accuracy: 0.5792 - val_loss: 22813008.0000 - val_accuracy: 0.5920 - 563ms/epoch - 5ms/step\n",
      "Epoch 51/10000\n",
      "113/113 - 1s - loss: 61027044.0000 - accuracy: 0.5721 - val_loss: 59898280.0000 - val_accuracy: 0.6123 - 538ms/epoch - 5ms/step\n",
      "Epoch 52/10000\n",
      "113/113 - 1s - loss: 58976464.0000 - accuracy: 0.5752 - val_loss: 103268768.0000 - val_accuracy: 0.4673 - 540ms/epoch - 5ms/step\n",
      "Epoch 53/10000\n",
      "113/113 - 1s - loss: 47696308.0000 - accuracy: 0.5809 - val_loss: 48630188.0000 - val_accuracy: 0.6206 - 555ms/epoch - 5ms/step\n",
      "Epoch 54/10000\n",
      "113/113 - 1s - loss: 62939656.0000 - accuracy: 0.5754 - val_loss: 62775892.0000 - val_accuracy: 0.6073 - 603ms/epoch - 5ms/step\n",
      "Epoch 55/10000\n",
      "113/113 - 1s - loss: 74220288.0000 - accuracy: 0.5750 - val_loss: 54828284.0000 - val_accuracy: 0.5708 - 564ms/epoch - 5ms/step\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Iterazione 60/128 (03-11-2023_11-20-41)\n",
      "hidden_layer_size: 100\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 4\n",
      "activation_hidden: relu\n",
      "activation_output: sigmoid\n",
      "batch_size: 1000\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 20\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 62.5%\n",
      "La recall di questo modello sul validation set è: 17.19%\n",
      "La f1 di questo modello sul validation set è: 26.18%\n",
      "La balanced accuracy di questo modello sul validation set è: 54.15%\n",
      "La precision di questo modello sul validation set è: 54.95%\n",
      "La AUC di questo modello sul validation set è: 54.15%\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.64    0.91      0.75  7,670.00\n",
      "1                  0.55    0.17      0.26  4,841.00\n",
      "accuracy           0.63    0.63      0.63      0.63\n",
      "macro avg          0.59    0.54      0.51 12,511.00\n",
      "weighted avg       0.60    0.63      0.56 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         6988          682\n",
      "Actual 1         4009          832\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 65.94% con il modello:\n",
      "1 - AC65.94%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 62.97% con il modello:\n",
      "1 - BA62.97%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 64.08% con il modello:\n",
      "13 - PR64.08%, L20, mV-10, MV10, P2, AHsigmoid, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-26-48).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "1126/1126 - 5s - loss: 7.1700 - accuracy: 0.5491 - val_loss: 0.8056 - val_accuracy: 0.6119 - 5s/epoch - 4ms/step\n",
      "Epoch 2/10000\n",
      "1126/1126 - 3s - loss: 0.7638 - accuracy: 0.6097 - val_loss: 0.7254 - val_accuracy: 0.6064 - 3s/epoch - 3ms/step\n",
      "Epoch 3/10000\n",
      "1126/1126 - 3s - loss: 0.7127 - accuracy: 0.6151 - val_loss: 0.6948 - val_accuracy: 0.6175 - 3s/epoch - 2ms/step\n",
      "Epoch 4/10000\n",
      "1126/1126 - 3s - loss: 0.7026 - accuracy: 0.6135 - val_loss: 0.6935 - val_accuracy: 0.6211 - 3s/epoch - 3ms/step\n",
      "Epoch 5/10000\n",
      "1126/1126 - 3s - loss: 0.6985 - accuracy: 0.6147 - val_loss: 0.6901 - val_accuracy: 0.6210 - 3s/epoch - 3ms/step\n",
      "Epoch 6/10000\n",
      "1126/1126 - 3s - loss: 0.7019 - accuracy: 0.6135 - val_loss: 0.6875 - val_accuracy: 0.6144 - 3s/epoch - 3ms/step\n",
      "Epoch 7/10000\n",
      "1126/1126 - 3s - loss: 0.6864 - accuracy: 0.6157 - val_loss: 0.7103 - val_accuracy: 0.6075 - 3s/epoch - 3ms/step\n",
      "Epoch 8/10000\n",
      "1126/1126 - 3s - loss: 0.6873 - accuracy: 0.6114 - val_loss: 0.6885 - val_accuracy: 0.6161 - 3s/epoch - 3ms/step\n",
      "Epoch 9/10000\n",
      "1126/1126 - 3s - loss: 0.6836 - accuracy: 0.6118 - val_loss: 0.6865 - val_accuracy: 0.6184 - 3s/epoch - 3ms/step\n",
      "Epoch 10/10000\n",
      "1126/1126 - 3s - loss: 0.6813 - accuracy: 0.6130 - val_loss: 0.6826 - val_accuracy: 0.6096 - 3s/epoch - 3ms/step\n",
      "Epoch 11/10000\n",
      "1126/1126 - 3s - loss: 0.6860 - accuracy: 0.6109 - val_loss: 0.6796 - val_accuracy: 0.6171 - 3s/epoch - 3ms/step\n",
      "Epoch 12/10000\n",
      "1126/1126 - 3s - loss: 0.6773 - accuracy: 0.6134 - val_loss: 0.6742 - val_accuracy: 0.6163 - 3s/epoch - 3ms/step\n",
      "Epoch 13/10000\n",
      "1126/1126 - 3s - loss: 0.6776 - accuracy: 0.6128 - val_loss: 0.6660 - val_accuracy: 0.6167 - 3s/epoch - 3ms/step\n",
      "Epoch 14/10000\n",
      "1126/1126 - 3s - loss: 0.6782 - accuracy: 0.6118 - val_loss: 0.6706 - val_accuracy: 0.6189 - 3s/epoch - 3ms/step\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Iterazione 61/128 (03-11-2023_11-21-28)\n",
      "hidden_layer_size: 100\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 4\n",
      "activation_hidden: sigmoid\n",
      "activation_output: sigmoid\n",
      "batch_size: 100\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 10\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 62.11%\n",
      "La recall di questo modello sul validation set è: 22.6%\n",
      "La f1 di questo modello sul validation set è: 31.58%\n",
      "La balanced accuracy di questo modello sul validation set è: 54.83%\n",
      "La precision di questo modello sul validation set è: 52.42%\n",
      "La AUC di questo modello sul validation set è: 59.46%\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.64    0.87      0.74  7,670.00\n",
      "1                  0.52    0.23      0.32  4,841.00\n",
      "accuracy           0.62    0.62      0.62      0.62\n",
      "macro avg          0.58    0.55      0.53 12,511.00\n",
      "weighted avg       0.60    0.62      0.57 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         6677          993\n",
      "Actual 1         3747         1094\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 65.94% con il modello:\n",
      "1 - AC65.94%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 62.97% con il modello:\n",
      "1 - BA62.97%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 64.08% con il modello:\n",
      "13 - PR64.08%, L20, mV-10, MV10, P2, AHsigmoid, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-26-48).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "1126/1126 - 5s - loss: 7.1700 - accuracy: 0.5491 - val_loss: 0.8056 - val_accuracy: 0.6119 - 5s/epoch - 5ms/step\n",
      "Epoch 2/10000\n",
      "1126/1126 - 3s - loss: 0.7638 - accuracy: 0.6097 - val_loss: 0.7254 - val_accuracy: 0.6064 - 3s/epoch - 3ms/step\n",
      "Epoch 3/10000\n",
      "1126/1126 - 3s - loss: 0.7127 - accuracy: 0.6151 - val_loss: 0.6948 - val_accuracy: 0.6175 - 3s/epoch - 3ms/step\n",
      "Epoch 4/10000\n",
      "1126/1126 - 3s - loss: 0.7026 - accuracy: 0.6135 - val_loss: 0.6935 - val_accuracy: 0.6211 - 3s/epoch - 3ms/step\n",
      "Epoch 5/10000\n",
      "1126/1126 - 3s - loss: 0.6985 - accuracy: 0.6147 - val_loss: 0.6901 - val_accuracy: 0.6210 - 3s/epoch - 3ms/step\n",
      "Epoch 6/10000\n",
      "1126/1126 - 3s - loss: 0.7019 - accuracy: 0.6135 - val_loss: 0.6875 - val_accuracy: 0.6144 - 3s/epoch - 3ms/step\n",
      "Epoch 7/10000\n",
      "1126/1126 - 3s - loss: 0.6864 - accuracy: 0.6157 - val_loss: 0.7103 - val_accuracy: 0.6075 - 3s/epoch - 3ms/step\n",
      "Epoch 8/10000\n",
      "1126/1126 - 3s - loss: 0.6873 - accuracy: 0.6114 - val_loss: 0.6885 - val_accuracy: 0.6161 - 3s/epoch - 3ms/step\n",
      "Epoch 9/10000\n",
      "1126/1126 - 3s - loss: 0.6836 - accuracy: 0.6118 - val_loss: 0.6865 - val_accuracy: 0.6184 - 3s/epoch - 3ms/step\n",
      "Epoch 10/10000\n",
      "1126/1126 - 3s - loss: 0.6813 - accuracy: 0.6130 - val_loss: 0.6826 - val_accuracy: 0.6096 - 3s/epoch - 3ms/step\n",
      "Epoch 11/10000\n",
      "1126/1126 - 3s - loss: 0.6860 - accuracy: 0.6109 - val_loss: 0.6796 - val_accuracy: 0.6171 - 3s/epoch - 3ms/step\n",
      "Epoch 12/10000\n",
      "1126/1126 - 3s - loss: 0.6773 - accuracy: 0.6134 - val_loss: 0.6742 - val_accuracy: 0.6163 - 3s/epoch - 3ms/step\n",
      "Epoch 13/10000\n",
      "1126/1126 - 3s - loss: 0.6776 - accuracy: 0.6128 - val_loss: 0.6660 - val_accuracy: 0.6167 - 3s/epoch - 3ms/step\n",
      "Epoch 14/10000\n",
      "1126/1126 - 3s - loss: 0.6782 - accuracy: 0.6118 - val_loss: 0.6706 - val_accuracy: 0.6189 - 3s/epoch - 3ms/step\n",
      "Epoch 15/10000\n",
      "1126/1126 - 3s - loss: 0.6740 - accuracy: 0.6141 - val_loss: 0.6784 - val_accuracy: 0.6171 - 3s/epoch - 3ms/step\n",
      "Epoch 16/10000\n",
      "1126/1126 - 3s - loss: 0.6717 - accuracy: 0.6154 - val_loss: 0.6801 - val_accuracy: 0.6176 - 3s/epoch - 3ms/step\n",
      "Epoch 17/10000\n",
      "1126/1126 - 3s - loss: 0.6726 - accuracy: 0.6159 - val_loss: 0.6733 - val_accuracy: 0.6122 - 3s/epoch - 3ms/step\n",
      "Epoch 18/10000\n",
      "1126/1126 - 3s - loss: 0.6715 - accuracy: 0.6155 - val_loss: 0.6732 - val_accuracy: 0.6185 - 3s/epoch - 3ms/step\n",
      "Epoch 19/10000\n",
      "1126/1126 - 3s - loss: 0.6707 - accuracy: 0.6142 - val_loss: 0.6768 - val_accuracy: 0.6157 - 3s/epoch - 3ms/step\n",
      "Epoch 20/10000\n",
      "1126/1126 - 3s - loss: 0.6707 - accuracy: 0.6147 - val_loss: 0.6743 - val_accuracy: 0.6169 - 3s/epoch - 3ms/step\n",
      "Epoch 21/10000\n",
      "1126/1126 - 3s - loss: 0.6725 - accuracy: 0.6114 - val_loss: 0.6681 - val_accuracy: 0.6146 - 3s/epoch - 3ms/step\n",
      "Epoch 22/10000\n",
      "1126/1126 - 3s - loss: 0.6739 - accuracy: 0.6115 - val_loss: 0.6737 - val_accuracy: 0.6111 - 3s/epoch - 3ms/step\n",
      "Epoch 23/10000\n",
      "1126/1126 - 3s - loss: 0.6708 - accuracy: 0.6123 - val_loss: 0.6708 - val_accuracy: 0.6098 - 3s/epoch - 3ms/step\n",
      "Epoch 24/10000\n",
      "1126/1126 - 3s - loss: 0.6695 - accuracy: 0.6088 - val_loss: 0.6725 - val_accuracy: 0.6028 - 3s/epoch - 2ms/step\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Iterazione 62/128 (03-11-2023_11-22-46)\n",
      "hidden_layer_size: 100\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 4\n",
      "activation_hidden: sigmoid\n",
      "activation_output: sigmoid\n",
      "batch_size: 100\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 20\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 62.11%\n",
      "La recall di questo modello sul validation set è: 22.6%\n",
      "La f1 di questo modello sul validation set è: 31.58%\n",
      "La balanced accuracy di questo modello sul validation set è: 54.83%\n",
      "La precision di questo modello sul validation set è: 52.42%\n",
      "La AUC di questo modello sul validation set è: 59.46%\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.64    0.87      0.74  7,670.00\n",
      "1                  0.52    0.23      0.32  4,841.00\n",
      "accuracy           0.62    0.62      0.62      0.62\n",
      "macro avg          0.58    0.55      0.53 12,511.00\n",
      "weighted avg       0.60    0.62      0.57 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         6677          993\n",
      "Actual 1         3747         1094\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 65.94% con il modello:\n",
      "1 - AC65.94%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 62.97% con il modello:\n",
      "1 - BA62.97%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 64.08% con il modello:\n",
      "13 - PR64.08%, L20, mV-10, MV10, P2, AHsigmoid, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-26-48).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "113/113 - 2s - loss: 40.8352 - accuracy: 0.3893 - val_loss: 23.0546 - val_accuracy: 0.3948 - 2s/epoch - 20ms/step\n",
      "Epoch 2/10000\n",
      "113/113 - 1s - loss: 14.0312 - accuracy: 0.4562 - val_loss: 7.0963 - val_accuracy: 0.4598 - 716ms/epoch - 6ms/step\n",
      "Epoch 3/10000\n",
      "113/113 - 1s - loss: 4.9248 - accuracy: 0.5571 - val_loss: 2.5191 - val_accuracy: 0.5653 - 687ms/epoch - 6ms/step\n",
      "Epoch 4/10000\n",
      "113/113 - 1s - loss: 1.1374 - accuracy: 0.5955 - val_loss: 0.9967 - val_accuracy: 0.6047 - 664ms/epoch - 6ms/step\n",
      "Epoch 5/10000\n",
      "113/113 - 1s - loss: 0.8904 - accuracy: 0.6069 - val_loss: 0.8424 - val_accuracy: 0.6110 - 751ms/epoch - 7ms/step\n",
      "Epoch 6/10000\n",
      "113/113 - 1s - loss: 0.7824 - accuracy: 0.6126 - val_loss: 0.7767 - val_accuracy: 0.6142 - 671ms/epoch - 6ms/step\n",
      "Epoch 7/10000\n",
      "113/113 - 1s - loss: 0.7517 - accuracy: 0.6161 - val_loss: 0.7463 - val_accuracy: 0.6171 - 717ms/epoch - 6ms/step\n",
      "Epoch 8/10000\n",
      "113/113 - 1s - loss: 0.7325 - accuracy: 0.6162 - val_loss: 0.7237 - val_accuracy: 0.6156 - 711ms/epoch - 6ms/step\n",
      "Epoch 9/10000\n",
      "113/113 - 1s - loss: 0.7191 - accuracy: 0.6157 - val_loss: 0.7163 - val_accuracy: 0.6179 - 805ms/epoch - 7ms/step\n",
      "Epoch 10/10000\n",
      "113/113 - 1s - loss: 0.7165 - accuracy: 0.6150 - val_loss: 0.7184 - val_accuracy: 0.6175 - 617ms/epoch - 5ms/step\n",
      "Epoch 11/10000\n",
      "113/113 - 1s - loss: 0.7082 - accuracy: 0.6172 - val_loss: 0.7051 - val_accuracy: 0.6200 - 635ms/epoch - 6ms/step\n",
      "Epoch 12/10000\n",
      "113/113 - 1s - loss: 0.7055 - accuracy: 0.6161 - val_loss: 0.6993 - val_accuracy: 0.6155 - 593ms/epoch - 5ms/step\n",
      "Epoch 13/10000\n",
      "113/113 - 1s - loss: 0.7026 - accuracy: 0.6178 - val_loss: 0.6967 - val_accuracy: 0.6199 - 600ms/epoch - 5ms/step\n",
      "Epoch 14/10000\n",
      "113/113 - 1s - loss: 0.7009 - accuracy: 0.6177 - val_loss: 0.7007 - val_accuracy: 0.6207 - 583ms/epoch - 5ms/step\n",
      "Epoch 15/10000\n",
      "113/113 - 1s - loss: 0.7013 - accuracy: 0.6191 - val_loss: 0.6996 - val_accuracy: 0.6158 - 651ms/epoch - 6ms/step\n",
      "Epoch 16/10000\n",
      "113/113 - 1s - loss: 0.6946 - accuracy: 0.6194 - val_loss: 0.7018 - val_accuracy: 0.6224 - 726ms/epoch - 6ms/step\n",
      "Epoch 17/10000\n",
      "113/113 - 1s - loss: 0.6944 - accuracy: 0.6194 - val_loss: 0.6923 - val_accuracy: 0.6230 - 661ms/epoch - 6ms/step\n",
      "Epoch 18/10000\n",
      "113/113 - 1s - loss: 0.6925 - accuracy: 0.6182 - val_loss: 0.6891 - val_accuracy: 0.6221 - 600ms/epoch - 5ms/step\n",
      "Epoch 19/10000\n",
      "113/113 - 1s - loss: 0.6901 - accuracy: 0.6196 - val_loss: 0.6953 - val_accuracy: 0.6240 - 612ms/epoch - 5ms/step\n",
      "Epoch 20/10000\n",
      "113/113 - 1s - loss: 0.6943 - accuracy: 0.6197 - val_loss: 0.7021 - val_accuracy: 0.6231 - 600ms/epoch - 5ms/step\n",
      "Epoch 21/10000\n",
      "113/113 - 1s - loss: 0.6906 - accuracy: 0.6209 - val_loss: 0.6976 - val_accuracy: 0.6218 - 622ms/epoch - 6ms/step\n",
      "Epoch 22/10000\n",
      "113/113 - 1s - loss: 0.6877 - accuracy: 0.6225 - val_loss: 0.6880 - val_accuracy: 0.6258 - 623ms/epoch - 6ms/step\n",
      "Epoch 23/10000\n",
      "113/113 - 1s - loss: 0.6852 - accuracy: 0.6192 - val_loss: 0.6935 - val_accuracy: 0.6251 - 620ms/epoch - 5ms/step\n",
      "Epoch 24/10000\n",
      "113/113 - 1s - loss: 0.6881 - accuracy: 0.6192 - val_loss: 0.7038 - val_accuracy: 0.6246 - 655ms/epoch - 6ms/step\n",
      "Epoch 25/10000\n",
      "113/113 - 1s - loss: 0.6846 - accuracy: 0.6205 - val_loss: 0.7020 - val_accuracy: 0.6234 - 621ms/epoch - 5ms/step\n",
      "Epoch 26/10000\n",
      "113/113 - 1s - loss: 0.6830 - accuracy: 0.6208 - val_loss: 0.6960 - val_accuracy: 0.6143 - 606ms/epoch - 5ms/step\n",
      "Epoch 27/10000\n",
      "113/113 - 1s - loss: 0.6827 - accuracy: 0.6207 - val_loss: 0.6987 - val_accuracy: 0.6235 - 604ms/epoch - 5ms/step\n",
      "Epoch 28/10000\n",
      "113/113 - 1s - loss: 0.6775 - accuracy: 0.6206 - val_loss: 0.6986 - val_accuracy: 0.6148 - 613ms/epoch - 5ms/step\n",
      "Epoch 29/10000\n",
      "113/113 - 1s - loss: 0.6758 - accuracy: 0.6194 - val_loss: 0.6955 - val_accuracy: 0.6203 - 651ms/epoch - 6ms/step\n",
      "Epoch 30/10000\n",
      "113/113 - 1s - loss: 0.6743 - accuracy: 0.6197 - val_loss: 0.6889 - val_accuracy: 0.6214 - 599ms/epoch - 5ms/step\n",
      "Epoch 31/10000\n",
      "113/113 - 1s - loss: 0.6730 - accuracy: 0.6199 - val_loss: 0.6902 - val_accuracy: 0.6172 - 583ms/epoch - 5ms/step\n",
      "Epoch 32/10000\n",
      "113/113 - 1s - loss: 0.6781 - accuracy: 0.6194 - val_loss: 0.6821 - val_accuracy: 0.6232 - 632ms/epoch - 6ms/step\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Iterazione 63/128 (03-11-2023_11-23-10)\n",
      "hidden_layer_size: 100\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 4\n",
      "activation_hidden: sigmoid\n",
      "activation_output: sigmoid\n",
      "batch_size: 1000\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 10\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 62.58%\n",
      "La recall di questo modello sul validation set è: 30.26%\n",
      "La f1 di questo modello sul validation set è: 38.49%\n",
      "La balanced accuracy di questo modello sul validation set è: 56.62%\n",
      "La precision di questo modello sul validation set è: 52.87%\n",
      "La AUC di questo modello sul validation set è: 60.45%\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.65    0.83      0.73  7,670.00\n",
      "1                  0.53    0.30      0.38  4,841.00\n",
      "accuracy           0.63    0.63      0.63      0.63\n",
      "macro avg          0.59    0.57      0.56 12,511.00\n",
      "weighted avg       0.61    0.63      0.60 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         6364         1306\n",
      "Actual 1         3376         1465\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 65.94% con il modello:\n",
      "1 - AC65.94%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 62.97% con il modello:\n",
      "1 - BA62.97%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 64.08% con il modello:\n",
      "13 - PR64.08%, L20, mV-10, MV10, P2, AHsigmoid, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-26-48).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "113/113 - 2s - loss: 40.8352 - accuracy: 0.3893 - val_loss: 23.0546 - val_accuracy: 0.3948 - 2s/epoch - 17ms/step\n",
      "Epoch 2/10000\n",
      "113/113 - 1s - loss: 14.0312 - accuracy: 0.4562 - val_loss: 7.0963 - val_accuracy: 0.4598 - 699ms/epoch - 6ms/step\n",
      "Epoch 3/10000\n",
      "113/113 - 1s - loss: 4.9248 - accuracy: 0.5571 - val_loss: 2.5191 - val_accuracy: 0.5653 - 733ms/epoch - 6ms/step\n",
      "Epoch 4/10000\n",
      "113/113 - 1s - loss: 1.1374 - accuracy: 0.5955 - val_loss: 0.9967 - val_accuracy: 0.6047 - 661ms/epoch - 6ms/step\n",
      "Epoch 5/10000\n",
      "113/113 - 1s - loss: 0.8904 - accuracy: 0.6069 - val_loss: 0.8424 - val_accuracy: 0.6110 - 652ms/epoch - 6ms/step\n",
      "Epoch 6/10000\n",
      "113/113 - 1s - loss: 0.7824 - accuracy: 0.6126 - val_loss: 0.7767 - val_accuracy: 0.6142 - 714ms/epoch - 6ms/step\n",
      "Epoch 7/10000\n",
      "113/113 - 1s - loss: 0.7517 - accuracy: 0.6161 - val_loss: 0.7463 - val_accuracy: 0.6171 - 649ms/epoch - 6ms/step\n",
      "Epoch 8/10000\n",
      "113/113 - 1s - loss: 0.7325 - accuracy: 0.6162 - val_loss: 0.7237 - val_accuracy: 0.6156 - 658ms/epoch - 6ms/step\n",
      "Epoch 9/10000\n",
      "113/113 - 1s - loss: 0.7191 - accuracy: 0.6157 - val_loss: 0.7163 - val_accuracy: 0.6179 - 754ms/epoch - 7ms/step\n",
      "Epoch 10/10000\n",
      "113/113 - 1s - loss: 0.7165 - accuracy: 0.6150 - val_loss: 0.7184 - val_accuracy: 0.6175 - 684ms/epoch - 6ms/step\n",
      "Epoch 11/10000\n",
      "113/113 - 1s - loss: 0.7082 - accuracy: 0.6172 - val_loss: 0.7051 - val_accuracy: 0.6200 - 741ms/epoch - 7ms/step\n",
      "Epoch 12/10000\n",
      "113/113 - 1s - loss: 0.7055 - accuracy: 0.6161 - val_loss: 0.6993 - val_accuracy: 0.6155 - 808ms/epoch - 7ms/step\n",
      "Epoch 13/10000\n",
      "113/113 - 1s - loss: 0.7026 - accuracy: 0.6178 - val_loss: 0.6967 - val_accuracy: 0.6199 - 661ms/epoch - 6ms/step\n",
      "Epoch 14/10000\n",
      "113/113 - 1s - loss: 0.7009 - accuracy: 0.6177 - val_loss: 0.7007 - val_accuracy: 0.6207 - 681ms/epoch - 6ms/step\n",
      "Epoch 15/10000\n",
      "113/113 - 1s - loss: 0.7013 - accuracy: 0.6191 - val_loss: 0.6996 - val_accuracy: 0.6158 - 599ms/epoch - 5ms/step\n",
      "Epoch 16/10000\n",
      "113/113 - 1s - loss: 0.6946 - accuracy: 0.6194 - val_loss: 0.7018 - val_accuracy: 0.6224 - 617ms/epoch - 5ms/step\n",
      "Epoch 17/10000\n",
      "113/113 - 1s - loss: 0.6944 - accuracy: 0.6194 - val_loss: 0.6923 - val_accuracy: 0.6230 - 583ms/epoch - 5ms/step\n",
      "Epoch 18/10000\n",
      "113/113 - 1s - loss: 0.6925 - accuracy: 0.6182 - val_loss: 0.6891 - val_accuracy: 0.6221 - 600ms/epoch - 5ms/step\n",
      "Epoch 19/10000\n",
      "113/113 - 1s - loss: 0.6901 - accuracy: 0.6196 - val_loss: 0.6953 - val_accuracy: 0.6240 - 651ms/epoch - 6ms/step\n",
      "Epoch 20/10000\n",
      "113/113 - 1s - loss: 0.6943 - accuracy: 0.6197 - val_loss: 0.7021 - val_accuracy: 0.6231 - 595ms/epoch - 5ms/step\n",
      "Epoch 21/10000\n",
      "113/113 - 1s - loss: 0.6906 - accuracy: 0.6209 - val_loss: 0.6976 - val_accuracy: 0.6218 - 594ms/epoch - 5ms/step\n",
      "Epoch 22/10000\n",
      "113/113 - 1s - loss: 0.6877 - accuracy: 0.6225 - val_loss: 0.6880 - val_accuracy: 0.6258 - 596ms/epoch - 5ms/step\n",
      "Epoch 23/10000\n",
      "113/113 - 1s - loss: 0.6852 - accuracy: 0.6192 - val_loss: 0.6935 - val_accuracy: 0.6251 - 601ms/epoch - 5ms/step\n",
      "Epoch 24/10000\n",
      "113/113 - 1s - loss: 0.6881 - accuracy: 0.6192 - val_loss: 0.7038 - val_accuracy: 0.6246 - 600ms/epoch - 5ms/step\n",
      "Epoch 25/10000\n",
      "113/113 - 1s - loss: 0.6846 - accuracy: 0.6205 - val_loss: 0.7020 - val_accuracy: 0.6234 - 621ms/epoch - 5ms/step\n",
      "Epoch 26/10000\n",
      "113/113 - 1s - loss: 0.6830 - accuracy: 0.6208 - val_loss: 0.6960 - val_accuracy: 0.6143 - 578ms/epoch - 5ms/step\n",
      "Epoch 27/10000\n",
      "113/113 - 1s - loss: 0.6827 - accuracy: 0.6207 - val_loss: 0.6987 - val_accuracy: 0.6235 - 732ms/epoch - 6ms/step\n",
      "Epoch 28/10000\n",
      "113/113 - 1s - loss: 0.6775 - accuracy: 0.6206 - val_loss: 0.6986 - val_accuracy: 0.6148 - 611ms/epoch - 5ms/step\n",
      "Epoch 29/10000\n",
      "113/113 - 1s - loss: 0.6758 - accuracy: 0.6194 - val_loss: 0.6955 - val_accuracy: 0.6203 - 606ms/epoch - 5ms/step\n",
      "Epoch 30/10000\n",
      "113/113 - 1s - loss: 0.6743 - accuracy: 0.6197 - val_loss: 0.6889 - val_accuracy: 0.6214 - 601ms/epoch - 5ms/step\n",
      "Epoch 31/10000\n",
      "113/113 - 1s - loss: 0.6730 - accuracy: 0.6199 - val_loss: 0.6902 - val_accuracy: 0.6172 - 598ms/epoch - 5ms/step\n",
      "Epoch 32/10000\n",
      "113/113 - 1s - loss: 0.6781 - accuracy: 0.6194 - val_loss: 0.6821 - val_accuracy: 0.6232 - 671ms/epoch - 6ms/step\n",
      "Epoch 33/10000\n",
      "113/113 - 1s - loss: 0.6761 - accuracy: 0.6191 - val_loss: 0.6891 - val_accuracy: 0.6123 - 604ms/epoch - 5ms/step\n",
      "Epoch 34/10000\n",
      "113/113 - 1s - loss: 0.6727 - accuracy: 0.6196 - val_loss: 0.6855 - val_accuracy: 0.6218 - 602ms/epoch - 5ms/step\n",
      "Epoch 35/10000\n",
      "113/113 - 1s - loss: 0.6750 - accuracy: 0.6212 - val_loss: 0.6882 - val_accuracy: 0.6219 - 619ms/epoch - 5ms/step\n",
      "Epoch 36/10000\n",
      "113/113 - 1s - loss: 0.6750 - accuracy: 0.6208 - val_loss: 0.6858 - val_accuracy: 0.6228 - 605ms/epoch - 5ms/step\n",
      "Epoch 37/10000\n",
      "113/113 - 1s - loss: 0.6739 - accuracy: 0.6202 - val_loss: 0.6803 - val_accuracy: 0.6231 - 662ms/epoch - 6ms/step\n",
      "Epoch 38/10000\n",
      "113/113 - 1s - loss: 0.6709 - accuracy: 0.6198 - val_loss: 0.6790 - val_accuracy: 0.6205 - 616ms/epoch - 5ms/step\n",
      "Epoch 39/10000\n",
      "113/113 - 1s - loss: 0.6695 - accuracy: 0.6208 - val_loss: 0.6723 - val_accuracy: 0.6167 - 601ms/epoch - 5ms/step\n",
      "Epoch 40/10000\n",
      "113/113 - 1s - loss: 0.6709 - accuracy: 0.6191 - val_loss: 0.6728 - val_accuracy: 0.6201 - 667ms/epoch - 6ms/step\n",
      "Epoch 41/10000\n",
      "113/113 - 1s - loss: 0.6723 - accuracy: 0.6195 - val_loss: 0.6763 - val_accuracy: 0.6204 - 616ms/epoch - 5ms/step\n",
      "Epoch 42/10000\n",
      "113/113 - 1s - loss: 0.6698 - accuracy: 0.6207 - val_loss: 0.6832 - val_accuracy: 0.6199 - 671ms/epoch - 6ms/step\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Iterazione 64/128 (03-11-2023_11-23-41)\n",
      "hidden_layer_size: 100\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 4\n",
      "activation_hidden: sigmoid\n",
      "activation_output: sigmoid\n",
      "batch_size: 1000\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 20\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 62.58%\n",
      "La recall di questo modello sul validation set è: 30.26%\n",
      "La f1 di questo modello sul validation set è: 38.49%\n",
      "La balanced accuracy di questo modello sul validation set è: 56.62%\n",
      "La precision di questo modello sul validation set è: 52.87%\n",
      "La AUC di questo modello sul validation set è: 60.45%\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.65    0.83      0.73  7,670.00\n",
      "1                  0.53    0.30      0.38  4,841.00\n",
      "accuracy           0.63    0.63      0.63      0.63\n",
      "macro avg          0.59    0.57      0.56 12,511.00\n",
      "weighted avg       0.61    0.63      0.60 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         6364         1306\n",
      "Actual 1         3376         1465\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 65.94% con il modello:\n",
      "1 - AC65.94%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 62.97% con il modello:\n",
      "1 - BA62.97%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 64.08% con il modello:\n",
      "13 - PR64.08%, L20, mV-10, MV10, P2, AHsigmoid, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-26-48).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "1126/1126 - 4s - loss: 66921.5156 - accuracy: 0.5391 - val_loss: 1717.1847 - val_accuracy: 0.5911 - 4s/epoch - 3ms/step\n",
      "Epoch 2/10000\n",
      "1126/1126 - 3s - loss: 1749.1656 - accuracy: 0.5733 - val_loss: 1250.6667 - val_accuracy: 0.5575 - 3s/epoch - 2ms/step\n",
      "Epoch 3/10000\n",
      "1126/1126 - 2s - loss: 1341.0406 - accuracy: 0.5757 - val_loss: 767.9097 - val_accuracy: 0.6090 - 2s/epoch - 2ms/step\n",
      "Epoch 4/10000\n",
      "1126/1126 - 2s - loss: 1169.0867 - accuracy: 0.5767 - val_loss: 756.3774 - val_accuracy: 0.5940 - 2s/epoch - 2ms/step\n",
      "Epoch 5/10000\n",
      "1126/1126 - 2s - loss: 1164.6843 - accuracy: 0.5774 - val_loss: 645.1292 - val_accuracy: 0.6072 - 2s/epoch - 2ms/step\n",
      "Epoch 6/10000\n",
      "1126/1126 - 2s - loss: 1137.5796 - accuracy: 0.5766 - val_loss: 750.7300 - val_accuracy: 0.6157 - 2s/epoch - 2ms/step\n",
      "Epoch 7/10000\n",
      "1126/1126 - 2s - loss: 1110.4401 - accuracy: 0.5795 - val_loss: 533.6027 - val_accuracy: 0.6091 - 2s/epoch - 2ms/step\n",
      "Epoch 8/10000\n",
      "1126/1126 - 2s - loss: 1293.2806 - accuracy: 0.5786 - val_loss: 900.9355 - val_accuracy: 0.6132 - 2s/epoch - 2ms/step\n",
      "Epoch 9/10000\n",
      "1126/1126 - 2s - loss: 1022.4526 - accuracy: 0.5813 - val_loss: 514.7114 - val_accuracy: 0.5851 - 2s/epoch - 2ms/step\n",
      "Epoch 10/10000\n",
      "1126/1126 - 2s - loss: 1066.1085 - accuracy: 0.5813 - val_loss: 800.0114 - val_accuracy: 0.6160 - 2s/epoch - 2ms/step\n",
      "Epoch 11/10000\n",
      "1126/1126 - 2s - loss: 1042.8783 - accuracy: 0.5832 - val_loss: 954.5331 - val_accuracy: 0.6189 - 2s/epoch - 2ms/step\n",
      "Epoch 12/10000\n",
      "1126/1126 - 2s - loss: 1068.1947 - accuracy: 0.5828 - val_loss: 696.2319 - val_accuracy: 0.6156 - 2s/epoch - 2ms/step\n",
      "Epoch 13/10000\n",
      "1126/1126 - 2s - loss: 980.6830 - accuracy: 0.5867 - val_loss: 387.2534 - val_accuracy: 0.6211 - 2s/epoch - 2ms/step\n",
      "Epoch 14/10000\n",
      "1126/1126 - 2s - loss: 1070.6021 - accuracy: 0.5858 - val_loss: 1353.7283 - val_accuracy: 0.6199 - 2s/epoch - 2ms/step\n",
      "Epoch 15/10000\n",
      "1126/1126 - 2s - loss: 1077.6831 - accuracy: 0.5830 - val_loss: 1030.3752 - val_accuracy: 0.6211 - 2s/epoch - 2ms/step\n",
      "Epoch 16/10000\n",
      "1126/1126 - 2s - loss: 1076.7826 - accuracy: 0.5840 - val_loss: 1125.3580 - val_accuracy: 0.5106 - 2s/epoch - 2ms/step\n",
      "Epoch 17/10000\n",
      "1126/1126 - 2s - loss: 1082.1926 - accuracy: 0.5841 - val_loss: 408.0424 - val_accuracy: 0.6318 - 2s/epoch - 2ms/step\n",
      "Epoch 18/10000\n",
      "1126/1126 - 2s - loss: 928.5602 - accuracy: 0.5871 - val_loss: 505.7067 - val_accuracy: 0.6247 - 2s/epoch - 2ms/step\n",
      "Epoch 19/10000\n",
      "1126/1126 - 2s - loss: 1112.8386 - accuracy: 0.5832 - val_loss: 606.3779 - val_accuracy: 0.5732 - 2s/epoch - 2ms/step\n",
      "Epoch 20/10000\n",
      "1126/1126 - 2s - loss: 917.4910 - accuracy: 0.5909 - val_loss: 400.5390 - val_accuracy: 0.5734 - 2s/epoch - 2ms/step\n",
      "Epoch 21/10000\n",
      "1126/1126 - 2s - loss: 924.4166 - accuracy: 0.5874 - val_loss: 992.7320 - val_accuracy: 0.5598 - 2s/epoch - 2ms/step\n",
      "Epoch 22/10000\n",
      "1126/1126 - 2s - loss: 915.2155 - accuracy: 0.5876 - val_loss: 1042.8108 - val_accuracy: 0.6268 - 2s/epoch - 2ms/step\n",
      "Epoch 23/10000\n",
      "1126/1126 - 2s - loss: 940.4225 - accuracy: 0.5899 - val_loss: 424.2138 - val_accuracy: 0.6395 - 2s/epoch - 2ms/step\n",
      "Epoch 24/10000\n",
      "1126/1126 - 2s - loss: 906.9357 - accuracy: 0.5896 - val_loss: 1865.8270 - val_accuracy: 0.4806 - 2s/epoch - 2ms/step\n",
      "Epoch 25/10000\n",
      "1126/1126 - 2s - loss: 902.7668 - accuracy: 0.5890 - val_loss: 1666.9291 - val_accuracy: 0.4825 - 2s/epoch - 2ms/step\n",
      "Epoch 26/10000\n",
      "1126/1126 - 2s - loss: 894.0869 - accuracy: 0.5918 - val_loss: 1061.6157 - val_accuracy: 0.6229 - 2s/epoch - 2ms/step\n",
      "Epoch 27/10000\n",
      "1126/1126 - 2s - loss: 940.4803 - accuracy: 0.5899 - val_loss: 650.7890 - val_accuracy: 0.5355 - 2s/epoch - 2ms/step\n",
      "Epoch 28/10000\n",
      "1126/1126 - 2s - loss: 1084.8488 - accuracy: 0.5912 - val_loss: 512.1691 - val_accuracy: 0.5825 - 2s/epoch - 2ms/step\n",
      "Epoch 29/10000\n",
      "1126/1126 - 2s - loss: 998.2719 - accuracy: 0.5881 - val_loss: 291.3951 - val_accuracy: 0.6483 - 2s/epoch - 2ms/step\n",
      "Epoch 30/10000\n",
      "1126/1126 - 2s - loss: 984.0137 - accuracy: 0.5881 - val_loss: 522.8982 - val_accuracy: 0.6277 - 2s/epoch - 2ms/step\n",
      "Epoch 31/10000\n",
      "1126/1126 - 2s - loss: 955.2786 - accuracy: 0.5881 - val_loss: 863.8533 - val_accuracy: 0.5609 - 2s/epoch - 2ms/step\n",
      "Epoch 32/10000\n",
      "1126/1126 - 2s - loss: 975.7389 - accuracy: 0.5907 - val_loss: 740.2094 - val_accuracy: 0.6251 - 2s/epoch - 2ms/step\n",
      "Epoch 33/10000\n",
      "1126/1126 - 2s - loss: 877.5501 - accuracy: 0.5931 - val_loss: 311.5603 - val_accuracy: 0.6177 - 2s/epoch - 2ms/step\n",
      "Epoch 34/10000\n",
      "1126/1126 - 2s - loss: 962.2048 - accuracy: 0.5892 - val_loss: 343.4294 - val_accuracy: 0.6509 - 2s/epoch - 2ms/step\n",
      "Epoch 35/10000\n",
      "1126/1126 - 2s - loss: 938.6786 - accuracy: 0.5910 - val_loss: 531.6105 - val_accuracy: 0.5581 - 2s/epoch - 2ms/step\n",
      "Epoch 36/10000\n",
      "1126/1126 - 2s - loss: 928.9843 - accuracy: 0.5915 - val_loss: 849.3420 - val_accuracy: 0.6359 - 2s/epoch - 2ms/step\n",
      "Epoch 37/10000\n",
      "1126/1126 - 2s - loss: 946.7754 - accuracy: 0.5930 - val_loss: 699.2391 - val_accuracy: 0.5390 - 2s/epoch - 2ms/step\n",
      "Epoch 38/10000\n",
      "1126/1126 - 2s - loss: 1015.8685 - accuracy: 0.5875 - val_loss: 1522.9584 - val_accuracy: 0.6246 - 2s/epoch - 2ms/step\n",
      "Epoch 39/10000\n",
      "1126/1126 - 2s - loss: 982.4305 - accuracy: 0.5927 - val_loss: 357.1824 - val_accuracy: 0.6526 - 2s/epoch - 2ms/step\n",
      "Epoch 40/10000\n",
      "1126/1126 - 2s - loss: 863.7259 - accuracy: 0.5929 - val_loss: 509.0650 - val_accuracy: 0.5959 - 2s/epoch - 2ms/step\n",
      "Epoch 41/10000\n",
      "1126/1126 - 2s - loss: 992.5944 - accuracy: 0.5906 - val_loss: 326.1852 - val_accuracy: 0.6469 - 2s/epoch - 2ms/step\n",
      "Epoch 42/10000\n",
      "1126/1126 - 2s - loss: 953.7074 - accuracy: 0.5936 - val_loss: 2121.4265 - val_accuracy: 0.6160 - 2s/epoch - 2ms/step\n",
      "Epoch 43/10000\n",
      "1126/1126 - 2s - loss: 1042.9307 - accuracy: 0.5906 - val_loss: 469.6729 - val_accuracy: 0.6346 - 2s/epoch - 2ms/step\n",
      "Epoch 44/10000\n",
      "1126/1126 - 2s - loss: 888.4523 - accuracy: 0.5950 - val_loss: 1894.5968 - val_accuracy: 0.6191 - 2s/epoch - 2ms/step\n",
      "Epoch 45/10000\n",
      "1126/1126 - 2s - loss: 1022.6636 - accuracy: 0.5937 - val_loss: 712.3823 - val_accuracy: 0.6270 - 2s/epoch - 2ms/step\n",
      "Epoch 46/10000\n",
      "1126/1126 - 2s - loss: 857.5526 - accuracy: 0.5920 - val_loss: 833.9176 - val_accuracy: 0.5549 - 2s/epoch - 2ms/step\n",
      "Epoch 47/10000\n",
      "1126/1126 - 2s - loss: 876.4486 - accuracy: 0.5942 - val_loss: 1728.2263 - val_accuracy: 0.4787 - 2s/epoch - 2ms/step\n",
      "Epoch 48/10000\n",
      "1126/1126 - 2s - loss: 826.3303 - accuracy: 0.5963 - val_loss: 592.0454 - val_accuracy: 0.6315 - 2s/epoch - 2ms/step\n",
      "Epoch 49/10000\n",
      "1126/1126 - 2s - loss: 937.0411 - accuracy: 0.5949 - val_loss: 681.6010 - val_accuracy: 0.6277 - 2s/epoch - 2ms/step\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Iterazione 65/128 (03-11-2023_11-25-34)\n",
      "hidden_layer_size: 500\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 1\n",
      "activation_hidden: relu\n",
      "activation_output: sigmoid\n",
      "batch_size: 100\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 10\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 65.26%\n",
      "La recall di questo modello sul validation set è: 29.25%\n",
      "La f1 di questo modello sul validation set è: 39.45%\n",
      "La balanced accuracy di questo modello sul validation set è: 58.62%\n",
      "La precision di questo modello sul validation set è: 60.59%\n",
      "La AUC di questo modello sul validation set è: 59.56%\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.66    0.88      0.76  7,670.00\n",
      "1                  0.61    0.29      0.39  4,841.00\n",
      "accuracy           0.65    0.65      0.65      0.65\n",
      "macro avg          0.63    0.59      0.58 12,511.00\n",
      "weighted avg       0.64    0.65      0.62 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         6749          921\n",
      "Actual 1         3425         1416\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 65.94% con il modello:\n",
      "1 - AC65.94%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 62.97% con il modello:\n",
      "1 - BA62.97%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 64.08% con il modello:\n",
      "13 - PR64.08%, L20, mV-10, MV10, P2, AHsigmoid, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-26-48).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "1126/1126 - 3s - loss: 66921.5156 - accuracy: 0.5391 - val_loss: 1717.1847 - val_accuracy: 0.5911 - 3s/epoch - 3ms/step\n",
      "Epoch 2/10000\n",
      "1126/1126 - 2s - loss: 1749.1656 - accuracy: 0.5733 - val_loss: 1250.6667 - val_accuracy: 0.5575 - 2s/epoch - 2ms/step\n",
      "Epoch 3/10000\n",
      "1126/1126 - 2s - loss: 1341.0406 - accuracy: 0.5757 - val_loss: 767.9097 - val_accuracy: 0.6090 - 2s/epoch - 2ms/step\n",
      "Epoch 4/10000\n",
      "1126/1126 - 2s - loss: 1169.0867 - accuracy: 0.5767 - val_loss: 756.3774 - val_accuracy: 0.5940 - 2s/epoch - 2ms/step\n",
      "Epoch 5/10000\n",
      "1126/1126 - 2s - loss: 1164.6843 - accuracy: 0.5774 - val_loss: 645.1292 - val_accuracy: 0.6072 - 2s/epoch - 2ms/step\n",
      "Epoch 6/10000\n",
      "1126/1126 - 2s - loss: 1137.5796 - accuracy: 0.5766 - val_loss: 750.7300 - val_accuracy: 0.6157 - 2s/epoch - 2ms/step\n",
      "Epoch 7/10000\n",
      "1126/1126 - 2s - loss: 1110.4401 - accuracy: 0.5795 - val_loss: 533.6027 - val_accuracy: 0.6091 - 2s/epoch - 2ms/step\n",
      "Epoch 8/10000\n",
      "1126/1126 - 2s - loss: 1293.2806 - accuracy: 0.5786 - val_loss: 900.9355 - val_accuracy: 0.6132 - 2s/epoch - 2ms/step\n",
      "Epoch 9/10000\n",
      "1126/1126 - 2s - loss: 1022.4526 - accuracy: 0.5813 - val_loss: 514.7114 - val_accuracy: 0.5851 - 2s/epoch - 2ms/step\n",
      "Epoch 10/10000\n",
      "1126/1126 - 2s - loss: 1066.1085 - accuracy: 0.5813 - val_loss: 800.0114 - val_accuracy: 0.6160 - 2s/epoch - 2ms/step\n",
      "Epoch 11/10000\n",
      "1126/1126 - 2s - loss: 1042.8783 - accuracy: 0.5832 - val_loss: 954.5331 - val_accuracy: 0.6189 - 2s/epoch - 2ms/step\n",
      "Epoch 12/10000\n",
      "1126/1126 - 2s - loss: 1068.1947 - accuracy: 0.5828 - val_loss: 696.2319 - val_accuracy: 0.6156 - 2s/epoch - 2ms/step\n",
      "Epoch 13/10000\n",
      "1126/1126 - 2s - loss: 980.6830 - accuracy: 0.5867 - val_loss: 387.2534 - val_accuracy: 0.6211 - 2s/epoch - 2ms/step\n",
      "Epoch 14/10000\n",
      "1126/1126 - 2s - loss: 1070.6021 - accuracy: 0.5858 - val_loss: 1353.7283 - val_accuracy: 0.6199 - 2s/epoch - 2ms/step\n",
      "Epoch 15/10000\n",
      "1126/1126 - 2s - loss: 1077.6831 - accuracy: 0.5830 - val_loss: 1030.3752 - val_accuracy: 0.6211 - 2s/epoch - 2ms/step\n",
      "Epoch 16/10000\n",
      "1126/1126 - 2s - loss: 1076.7826 - accuracy: 0.5840 - val_loss: 1125.3580 - val_accuracy: 0.5106 - 2s/epoch - 2ms/step\n",
      "Epoch 17/10000\n",
      "1126/1126 - 2s - loss: 1082.1926 - accuracy: 0.5841 - val_loss: 408.0424 - val_accuracy: 0.6318 - 2s/epoch - 2ms/step\n",
      "Epoch 18/10000\n",
      "1126/1126 - 2s - loss: 928.5602 - accuracy: 0.5871 - val_loss: 505.7067 - val_accuracy: 0.6247 - 2s/epoch - 2ms/step\n",
      "Epoch 19/10000\n",
      "1126/1126 - 3s - loss: 1112.8386 - accuracy: 0.5832 - val_loss: 606.3779 - val_accuracy: 0.5732 - 3s/epoch - 2ms/step\n",
      "Epoch 20/10000\n",
      "1126/1126 - 2s - loss: 917.4910 - accuracy: 0.5909 - val_loss: 400.5390 - val_accuracy: 0.5734 - 2s/epoch - 2ms/step\n",
      "Epoch 21/10000\n",
      "1126/1126 - 2s - loss: 924.4166 - accuracy: 0.5874 - val_loss: 992.7320 - val_accuracy: 0.5598 - 2s/epoch - 2ms/step\n",
      "Epoch 22/10000\n",
      "1126/1126 - 2s - loss: 915.2155 - accuracy: 0.5876 - val_loss: 1042.8108 - val_accuracy: 0.6268 - 2s/epoch - 2ms/step\n",
      "Epoch 23/10000\n",
      "1126/1126 - 2s - loss: 940.4225 - accuracy: 0.5899 - val_loss: 424.2138 - val_accuracy: 0.6395 - 2s/epoch - 2ms/step\n",
      "Epoch 24/10000\n",
      "1126/1126 - 2s - loss: 906.9357 - accuracy: 0.5896 - val_loss: 1865.8270 - val_accuracy: 0.4806 - 2s/epoch - 2ms/step\n",
      "Epoch 25/10000\n",
      "1126/1126 - 2s - loss: 902.7668 - accuracy: 0.5890 - val_loss: 1666.9291 - val_accuracy: 0.4825 - 2s/epoch - 2ms/step\n",
      "Epoch 26/10000\n",
      "1126/1126 - 3s - loss: 894.0869 - accuracy: 0.5918 - val_loss: 1061.6157 - val_accuracy: 0.6229 - 3s/epoch - 2ms/step\n",
      "Epoch 27/10000\n",
      "1126/1126 - 2s - loss: 940.4803 - accuracy: 0.5899 - val_loss: 650.7890 - val_accuracy: 0.5355 - 2s/epoch - 2ms/step\n",
      "Epoch 28/10000\n",
      "1126/1126 - 2s - loss: 1084.8488 - accuracy: 0.5912 - val_loss: 512.1691 - val_accuracy: 0.5825 - 2s/epoch - 2ms/step\n",
      "Epoch 29/10000\n",
      "1126/1126 - 2s - loss: 998.2719 - accuracy: 0.5881 - val_loss: 291.3951 - val_accuracy: 0.6483 - 2s/epoch - 2ms/step\n",
      "Epoch 30/10000\n",
      "1126/1126 - 2s - loss: 984.0137 - accuracy: 0.5881 - val_loss: 522.8982 - val_accuracy: 0.6277 - 2s/epoch - 2ms/step\n",
      "Epoch 31/10000\n",
      "1126/1126 - 2s - loss: 955.2786 - accuracy: 0.5881 - val_loss: 863.8533 - val_accuracy: 0.5609 - 2s/epoch - 2ms/step\n",
      "Epoch 32/10000\n",
      "1126/1126 - 2s - loss: 975.7389 - accuracy: 0.5907 - val_loss: 740.2094 - val_accuracy: 0.6251 - 2s/epoch - 2ms/step\n",
      "Epoch 33/10000\n",
      "1126/1126 - 2s - loss: 877.5501 - accuracy: 0.5931 - val_loss: 311.5603 - val_accuracy: 0.6177 - 2s/epoch - 2ms/step\n",
      "Epoch 34/10000\n",
      "1126/1126 - 2s - loss: 962.2048 - accuracy: 0.5892 - val_loss: 343.4294 - val_accuracy: 0.6509 - 2s/epoch - 2ms/step\n",
      "Epoch 35/10000\n",
      "1126/1126 - 2s - loss: 938.6786 - accuracy: 0.5910 - val_loss: 531.6105 - val_accuracy: 0.5581 - 2s/epoch - 2ms/step\n",
      "Epoch 36/10000\n",
      "1126/1126 - 2s - loss: 928.9843 - accuracy: 0.5915 - val_loss: 849.3420 - val_accuracy: 0.6359 - 2s/epoch - 2ms/step\n",
      "Epoch 37/10000\n",
      "1126/1126 - 2s - loss: 946.7754 - accuracy: 0.5930 - val_loss: 699.2391 - val_accuracy: 0.5390 - 2s/epoch - 2ms/step\n",
      "Epoch 38/10000\n",
      "1126/1126 - 2s - loss: 1015.8685 - accuracy: 0.5875 - val_loss: 1522.9584 - val_accuracy: 0.6246 - 2s/epoch - 2ms/step\n",
      "Epoch 39/10000\n",
      "1126/1126 - 2s - loss: 982.4305 - accuracy: 0.5927 - val_loss: 357.1824 - val_accuracy: 0.6526 - 2s/epoch - 2ms/step\n",
      "Epoch 40/10000\n",
      "1126/1126 - 3s - loss: 863.7259 - accuracy: 0.5929 - val_loss: 509.0650 - val_accuracy: 0.5959 - 3s/epoch - 2ms/step\n",
      "Epoch 41/10000\n",
      "1126/1126 - 3s - loss: 992.5944 - accuracy: 0.5906 - val_loss: 326.1852 - val_accuracy: 0.6469 - 3s/epoch - 2ms/step\n",
      "Epoch 42/10000\n",
      "1126/1126 - 3s - loss: 953.7074 - accuracy: 0.5936 - val_loss: 2121.4265 - val_accuracy: 0.6160 - 3s/epoch - 2ms/step\n",
      "Epoch 43/10000\n",
      "1126/1126 - 3s - loss: 1042.9307 - accuracy: 0.5906 - val_loss: 469.6729 - val_accuracy: 0.6346 - 3s/epoch - 2ms/step\n",
      "Epoch 44/10000\n",
      "1126/1126 - 3s - loss: 888.4523 - accuracy: 0.5950 - val_loss: 1894.5968 - val_accuracy: 0.6191 - 3s/epoch - 2ms/step\n",
      "Epoch 45/10000\n",
      "1126/1126 - 3s - loss: 1022.6636 - accuracy: 0.5937 - val_loss: 712.3823 - val_accuracy: 0.6270 - 3s/epoch - 2ms/step\n",
      "Epoch 46/10000\n",
      "1126/1126 - 3s - loss: 857.5526 - accuracy: 0.5920 - val_loss: 833.9176 - val_accuracy: 0.5549 - 3s/epoch - 2ms/step\n",
      "Epoch 47/10000\n",
      "1126/1126 - 3s - loss: 876.4486 - accuracy: 0.5942 - val_loss: 1728.2263 - val_accuracy: 0.4787 - 3s/epoch - 2ms/step\n",
      "Epoch 48/10000\n",
      "1126/1126 - 3s - loss: 826.3303 - accuracy: 0.5963 - val_loss: 592.0454 - val_accuracy: 0.6315 - 3s/epoch - 2ms/step\n",
      "Epoch 49/10000\n",
      "1126/1126 - 3s - loss: 937.0411 - accuracy: 0.5949 - val_loss: 681.6010 - val_accuracy: 0.6277 - 3s/epoch - 2ms/step\n",
      "Epoch 50/10000\n",
      "1126/1126 - 3s - loss: 926.5283 - accuracy: 0.5935 - val_loss: 352.0200 - val_accuracy: 0.6262 - 3s/epoch - 2ms/step\n",
      "Epoch 51/10000\n",
      "1126/1126 - 3s - loss: 853.2471 - accuracy: 0.5956 - val_loss: 307.4392 - val_accuracy: 0.6326 - 3s/epoch - 2ms/step\n",
      "Epoch 52/10000\n",
      "1126/1126 - 2s - loss: 892.4537 - accuracy: 0.5925 - val_loss: 650.4541 - val_accuracy: 0.5825 - 2s/epoch - 2ms/step\n",
      "Epoch 53/10000\n",
      "1126/1126 - 2s - loss: 892.0854 - accuracy: 0.5950 - val_loss: 933.6150 - val_accuracy: 0.6252 - 2s/epoch - 2ms/step\n",
      "Epoch 54/10000\n",
      "1126/1126 - 2s - loss: 998.7204 - accuracy: 0.5909 - val_loss: 899.0228 - val_accuracy: 0.6268 - 2s/epoch - 2ms/step\n",
      "Epoch 55/10000\n",
      "1126/1126 - 2s - loss: 833.9517 - accuracy: 0.5960 - val_loss: 534.2770 - val_accuracy: 0.6302 - 2s/epoch - 2ms/step\n",
      "Epoch 56/10000\n",
      "1126/1126 - 2s - loss: 970.3427 - accuracy: 0.5978 - val_loss: 283.6010 - val_accuracy: 0.6573 - 2s/epoch - 2ms/step\n",
      "Epoch 57/10000\n",
      "1126/1126 - 2s - loss: 908.3792 - accuracy: 0.5953 - val_loss: 1221.9803 - val_accuracy: 0.5343 - 2s/epoch - 2ms/step\n",
      "Epoch 58/10000\n",
      "1126/1126 - 2s - loss: 978.3553 - accuracy: 0.5951 - val_loss: 442.6389 - val_accuracy: 0.6144 - 2s/epoch - 2ms/step\n",
      "Epoch 59/10000\n",
      "1126/1126 - 2s - loss: 895.6077 - accuracy: 0.5954 - val_loss: 300.6346 - val_accuracy: 0.6501 - 2s/epoch - 2ms/step\n",
      "Epoch 60/10000\n",
      "1126/1126 - 3s - loss: 869.7468 - accuracy: 0.5948 - val_loss: 321.4566 - val_accuracy: 0.6439 - 3s/epoch - 2ms/step\n",
      "Epoch 61/10000\n",
      "1126/1126 - 2s - loss: 909.9449 - accuracy: 0.5955 - val_loss: 2318.8140 - val_accuracy: 0.6229 - 2s/epoch - 2ms/step\n",
      "Epoch 62/10000\n",
      "1126/1126 - 2s - loss: 956.0247 - accuracy: 0.5941 - val_loss: 1736.0776 - val_accuracy: 0.4781 - 2s/epoch - 2ms/step\n",
      "Epoch 63/10000\n",
      "1126/1126 - 3s - loss: 922.7828 - accuracy: 0.5951 - val_loss: 349.9646 - val_accuracy: 0.6610 - 3s/epoch - 2ms/step\n",
      "Epoch 64/10000\n",
      "1126/1126 - 3s - loss: 886.2540 - accuracy: 0.5953 - val_loss: 1442.2030 - val_accuracy: 0.5219 - 3s/epoch - 2ms/step\n",
      "Epoch 65/10000\n",
      "1126/1126 - 2s - loss: 906.4959 - accuracy: 0.5936 - val_loss: 338.6573 - val_accuracy: 0.6095 - 2s/epoch - 2ms/step\n",
      "Epoch 66/10000\n",
      "1126/1126 - 2s - loss: 1068.3275 - accuracy: 0.5929 - val_loss: 463.2471 - val_accuracy: 0.6345 - 2s/epoch - 2ms/step\n",
      "Epoch 67/10000\n",
      "1126/1126 - 2s - loss: 877.9141 - accuracy: 0.5973 - val_loss: 2237.1826 - val_accuracy: 0.4627 - 2s/epoch - 2ms/step\n",
      "Epoch 68/10000\n",
      "1126/1126 - 2s - loss: 978.9470 - accuracy: 0.5968 - val_loss: 2798.0459 - val_accuracy: 0.6171 - 2s/epoch - 2ms/step\n",
      "Epoch 69/10000\n",
      "1126/1126 - 2s - loss: 926.8770 - accuracy: 0.5976 - val_loss: 437.1850 - val_accuracy: 0.6557 - 2s/epoch - 2ms/step\n",
      "Epoch 70/10000\n",
      "1126/1126 - 2s - loss: 913.2715 - accuracy: 0.5965 - val_loss: 286.2161 - val_accuracy: 0.6532 - 2s/epoch - 2ms/step\n",
      "Epoch 71/10000\n",
      "1126/1126 - 2s - loss: 971.3312 - accuracy: 0.5928 - val_loss: 1455.9650 - val_accuracy: 0.6216 - 2s/epoch - 2ms/step\n",
      "Epoch 72/10000\n",
      "1126/1126 - 3s - loss: 873.1713 - accuracy: 0.5962 - val_loss: 822.2755 - val_accuracy: 0.6362 - 3s/epoch - 2ms/step\n",
      "Epoch 73/10000\n",
      "1126/1126 - 2s - loss: 1073.8456 - accuracy: 0.5917 - val_loss: 369.9972 - val_accuracy: 0.6465 - 2s/epoch - 2ms/step\n",
      "Epoch 74/10000\n",
      "1126/1126 - 2s - loss: 845.6011 - accuracy: 0.5980 - val_loss: 560.9518 - val_accuracy: 0.6314 - 2s/epoch - 2ms/step\n",
      "Epoch 75/10000\n",
      "1126/1126 - 2s - loss: 862.5553 - accuracy: 0.5954 - val_loss: 522.8799 - val_accuracy: 0.6306 - 2s/epoch - 2ms/step\n",
      "Epoch 76/10000\n",
      "1126/1126 - 2s - loss: 828.3226 - accuracy: 0.5993 - val_loss: 478.5745 - val_accuracy: 0.6414 - 2s/epoch - 2ms/step\n",
      "Epoch 77/10000\n",
      "1126/1126 - 2s - loss: 914.5189 - accuracy: 0.5945 - val_loss: 526.9504 - val_accuracy: 0.6405 - 2s/epoch - 2ms/step\n",
      "Epoch 78/10000\n",
      "1126/1126 - 2s - loss: 839.4825 - accuracy: 0.5976 - val_loss: 1677.3291 - val_accuracy: 0.4578 - 2s/epoch - 2ms/step\n",
      "Epoch 79/10000\n",
      "1126/1126 - 2s - loss: 819.6058 - accuracy: 0.5973 - val_loss: 1886.3376 - val_accuracy: 0.5008 - 2s/epoch - 2ms/step\n",
      "Epoch 80/10000\n",
      "1126/1126 - 2s - loss: 844.7037 - accuracy: 0.5974 - val_loss: 1674.1587 - val_accuracy: 0.4795 - 2s/epoch - 2ms/step\n",
      "Epoch 81/10000\n",
      "1126/1126 - 2s - loss: 1030.3197 - accuracy: 0.5959 - val_loss: 2186.3064 - val_accuracy: 0.4833 - 2s/epoch - 2ms/step\n",
      "Epoch 82/10000\n",
      "1126/1126 - 2s - loss: 883.5678 - accuracy: 0.5985 - val_loss: 506.5706 - val_accuracy: 0.6086 - 2s/epoch - 2ms/step\n",
      "Epoch 83/10000\n",
      "1126/1126 - 2s - loss: 895.5931 - accuracy: 0.5979 - val_loss: 851.9306 - val_accuracy: 0.6366 - 2s/epoch - 2ms/step\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Iterazione 66/128 (03-11-2023_11-28-58)\n",
      "hidden_layer_size: 500\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 1\n",
      "activation_hidden: relu\n",
      "activation_output: sigmoid\n",
      "batch_size: 100\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 20\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 66.1%\n",
      "La recall di questo modello sul validation set è: 36.05%\n",
      "La f1 di questo modello sul validation set è: 45.14%\n",
      "La balanced accuracy di questo modello sul validation set è: 60.56%\n",
      "La precision di questo modello sul validation set è: 60.38%\n",
      "La AUC di questo modello sul validation set è: 61.12%\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.68    0.85      0.75  7,670.00\n",
      "1                  0.60    0.36      0.45  4,841.00\n",
      "accuracy           0.66    0.66      0.66      0.66\n",
      "macro avg          0.64    0.61      0.60 12,511.00\n",
      "weighted avg       0.65    0.66      0.64 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         6525         1145\n",
      "Actual 1         3096         1745\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 66.1% con il modello:\n",
      "66 - AC66.1%, L500, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA20, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_11-28-58).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 62.97% con il modello:\n",
      "1 - BA62.97%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 64.08% con il modello:\n",
      "13 - PR64.08%, L20, mV-10, MV10, P2, AHsigmoid, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-26-48).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "113/113 - 2s - loss: 419449.3438 - accuracy: 0.4099 - val_loss: 169776.7969 - val_accuracy: 0.4320 - 2s/epoch - 18ms/step\n",
      "Epoch 2/10000\n",
      "113/113 - 1s - loss: 137071.7344 - accuracy: 0.4298 - val_loss: 99716.0938 - val_accuracy: 0.4328 - 836ms/epoch - 7ms/step\n",
      "Epoch 3/10000\n",
      "113/113 - 1s - loss: 64056.5586 - accuracy: 0.4434 - val_loss: 29451.6914 - val_accuracy: 0.4677 - 916ms/epoch - 8ms/step\n",
      "Epoch 4/10000\n",
      "113/113 - 1s - loss: 14243.3584 - accuracy: 0.5454 - val_loss: 7842.9985 - val_accuracy: 0.5974 - 760ms/epoch - 7ms/step\n",
      "Epoch 5/10000\n",
      "113/113 - 1s - loss: 6431.4600 - accuracy: 0.6039 - val_loss: 5321.2856 - val_accuracy: 0.6123 - 789ms/epoch - 7ms/step\n",
      "Epoch 6/10000\n",
      "113/113 - 1s - loss: 4569.0220 - accuracy: 0.6055 - val_loss: 3995.9309 - val_accuracy: 0.6135 - 751ms/epoch - 7ms/step\n",
      "Epoch 7/10000\n",
      "113/113 - 1s - loss: 3358.0569 - accuracy: 0.6040 - val_loss: 2874.7288 - val_accuracy: 0.6099 - 683ms/epoch - 6ms/step\n",
      "Epoch 8/10000\n",
      "113/113 - 1s - loss: 2519.7837 - accuracy: 0.5996 - val_loss: 2547.0205 - val_accuracy: 0.6069 - 747ms/epoch - 7ms/step\n",
      "Epoch 9/10000\n",
      "113/113 - 1s - loss: 1960.9266 - accuracy: 0.5896 - val_loss: 2053.8579 - val_accuracy: 0.6055 - 750ms/epoch - 7ms/step\n",
      "Epoch 10/10000\n",
      "113/113 - 1s - loss: 2524.9819 - accuracy: 0.5740 - val_loss: 3283.9604 - val_accuracy: 0.6179 - 582ms/epoch - 5ms/step\n",
      "Epoch 11/10000\n",
      "113/113 - 1s - loss: 1782.8923 - accuracy: 0.5755 - val_loss: 1427.6516 - val_accuracy: 0.6087 - 631ms/epoch - 6ms/step\n",
      "Epoch 12/10000\n",
      "113/113 - 1s - loss: 1487.4423 - accuracy: 0.5731 - val_loss: 1125.9818 - val_accuracy: 0.5785 - 600ms/epoch - 5ms/step\n",
      "Epoch 13/10000\n",
      "113/113 - 1s - loss: 1411.4863 - accuracy: 0.5720 - val_loss: 1230.2314 - val_accuracy: 0.5490 - 604ms/epoch - 5ms/step\n",
      "Epoch 14/10000\n",
      "113/113 - 1s - loss: 1452.7874 - accuracy: 0.5706 - val_loss: 1164.3550 - val_accuracy: 0.6057 - 602ms/epoch - 5ms/step\n",
      "Epoch 15/10000\n",
      "113/113 - 1s - loss: 1286.5060 - accuracy: 0.5740 - val_loss: 1307.1156 - val_accuracy: 0.5430 - 672ms/epoch - 6ms/step\n",
      "Epoch 16/10000\n",
      "113/113 - 1s - loss: 1196.2773 - accuracy: 0.5764 - val_loss: 1120.3752 - val_accuracy: 0.5327 - 611ms/epoch - 5ms/step\n",
      "Epoch 17/10000\n",
      "113/113 - 1s - loss: 1176.2462 - accuracy: 0.5738 - val_loss: 1000.1036 - val_accuracy: 0.5493 - 677ms/epoch - 6ms/step\n",
      "Epoch 18/10000\n",
      "113/113 - 1s - loss: 1035.0726 - accuracy: 0.5738 - val_loss: 1551.7982 - val_accuracy: 0.6111 - 594ms/epoch - 5ms/step\n",
      "Epoch 19/10000\n",
      "113/113 - 1s - loss: 1024.8107 - accuracy: 0.5785 - val_loss: 921.8513 - val_accuracy: 0.5506 - 632ms/epoch - 6ms/step\n",
      "Epoch 20/10000\n",
      "113/113 - 1s - loss: 900.2286 - accuracy: 0.5776 - val_loss: 827.6695 - val_accuracy: 0.5991 - 672ms/epoch - 6ms/step\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Iterazione 67/128 (03-11-2023_11-29-15)\n",
      "hidden_layer_size: 500\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 1\n",
      "activation_hidden: relu\n",
      "activation_output: sigmoid\n",
      "batch_size: 1000\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 10\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 61.79%\n",
      "La recall di questo modello sul validation set è: 6.51%\n",
      "La f1 di questo modello sul validation set è: 11.65%\n",
      "La balanced accuracy di questo modello sul validation set è: 51.6%\n",
      "La precision di questo modello sul validation set è: 55.36%\n",
      "La AUC di questo modello sul validation set è: 51.56%\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.62    0.97      0.76  7,670.00\n",
      "1                  0.55    0.07      0.12  4,841.00\n",
      "accuracy           0.62    0.62      0.62      0.62\n",
      "macro avg          0.59    0.52      0.44 12,511.00\n",
      "weighted avg       0.59    0.62      0.51 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         7416          254\n",
      "Actual 1         4526          315\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 66.1% con il modello:\n",
      "66 - AC66.1%, L500, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA20, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_11-28-58).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 62.97% con il modello:\n",
      "1 - BA62.97%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 64.08% con il modello:\n",
      "13 - PR64.08%, L20, mV-10, MV10, P2, AHsigmoid, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-26-48).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "113/113 - 2s - loss: 419449.3438 - accuracy: 0.4099 - val_loss: 169776.7969 - val_accuracy: 0.4320 - 2s/epoch - 14ms/step\n",
      "Epoch 2/10000\n",
      "113/113 - 1s - loss: 137071.7344 - accuracy: 0.4298 - val_loss: 99716.0938 - val_accuracy: 0.4328 - 691ms/epoch - 6ms/step\n",
      "Epoch 3/10000\n",
      "113/113 - 1s - loss: 64056.5586 - accuracy: 0.4434 - val_loss: 29451.6914 - val_accuracy: 0.4677 - 638ms/epoch - 6ms/step\n",
      "Epoch 4/10000\n",
      "113/113 - 1s - loss: 14243.3584 - accuracy: 0.5454 - val_loss: 7842.9985 - val_accuracy: 0.5974 - 722ms/epoch - 6ms/step\n",
      "Epoch 5/10000\n",
      "113/113 - 1s - loss: 6431.4600 - accuracy: 0.6039 - val_loss: 5321.2856 - val_accuracy: 0.6123 - 681ms/epoch - 6ms/step\n",
      "Epoch 6/10000\n",
      "113/113 - 1s - loss: 4569.0220 - accuracy: 0.6055 - val_loss: 3995.9309 - val_accuracy: 0.6135 - 671ms/epoch - 6ms/step\n",
      "Epoch 7/10000\n",
      "113/113 - 1s - loss: 3358.0569 - accuracy: 0.6040 - val_loss: 2874.7288 - val_accuracy: 0.6099 - 720ms/epoch - 6ms/step\n",
      "Epoch 8/10000\n",
      "113/113 - 1s - loss: 2519.7837 - accuracy: 0.5996 - val_loss: 2547.0205 - val_accuracy: 0.6069 - 644ms/epoch - 6ms/step\n",
      "Epoch 9/10000\n",
      "113/113 - 1s - loss: 1960.9266 - accuracy: 0.5896 - val_loss: 2053.8579 - val_accuracy: 0.6055 - 707ms/epoch - 6ms/step\n",
      "Epoch 10/10000\n",
      "113/113 - 1s - loss: 2524.9819 - accuracy: 0.5740 - val_loss: 3283.9604 - val_accuracy: 0.6179 - 699ms/epoch - 6ms/step\n",
      "Epoch 11/10000\n",
      "113/113 - 1s - loss: 1782.8923 - accuracy: 0.5755 - val_loss: 1427.6516 - val_accuracy: 0.6087 - 794ms/epoch - 7ms/step\n",
      "Epoch 12/10000\n",
      "113/113 - 1s - loss: 1487.4423 - accuracy: 0.5731 - val_loss: 1125.9818 - val_accuracy: 0.5785 - 642ms/epoch - 6ms/step\n",
      "Epoch 13/10000\n",
      "113/113 - 1s - loss: 1411.4863 - accuracy: 0.5720 - val_loss: 1230.2314 - val_accuracy: 0.5490 - 615ms/epoch - 5ms/step\n",
      "Epoch 14/10000\n",
      "113/113 - 1s - loss: 1452.7874 - accuracy: 0.5706 - val_loss: 1164.3550 - val_accuracy: 0.6057 - 588ms/epoch - 5ms/step\n",
      "Epoch 15/10000\n",
      "113/113 - 1s - loss: 1286.5060 - accuracy: 0.5740 - val_loss: 1307.1156 - val_accuracy: 0.5430 - 645ms/epoch - 6ms/step\n",
      "Epoch 16/10000\n",
      "113/113 - 1s - loss: 1196.2773 - accuracy: 0.5764 - val_loss: 1120.3752 - val_accuracy: 0.5327 - 584ms/epoch - 5ms/step\n",
      "Epoch 17/10000\n",
      "113/113 - 1s - loss: 1176.2462 - accuracy: 0.5738 - val_loss: 1000.1036 - val_accuracy: 0.5493 - 593ms/epoch - 5ms/step\n",
      "Epoch 18/10000\n",
      "113/113 - 1s - loss: 1035.0726 - accuracy: 0.5738 - val_loss: 1551.7982 - val_accuracy: 0.6111 - 650ms/epoch - 6ms/step\n",
      "Epoch 19/10000\n",
      "113/113 - 1s - loss: 1024.8107 - accuracy: 0.5785 - val_loss: 921.8513 - val_accuracy: 0.5506 - 564ms/epoch - 5ms/step\n",
      "Epoch 20/10000\n",
      "113/113 - 1s - loss: 900.2286 - accuracy: 0.5776 - val_loss: 827.6695 - val_accuracy: 0.5991 - 588ms/epoch - 5ms/step\n",
      "Epoch 21/10000\n",
      "113/113 - 1s - loss: 915.7888 - accuracy: 0.5782 - val_loss: 776.3094 - val_accuracy: 0.5949 - 578ms/epoch - 5ms/step\n",
      "Epoch 22/10000\n",
      "113/113 - 1s - loss: 994.7616 - accuracy: 0.5751 - val_loss: 892.8110 - val_accuracy: 0.6129 - 570ms/epoch - 5ms/step\n",
      "Epoch 23/10000\n",
      "113/113 - 1s - loss: 1083.2550 - accuracy: 0.5781 - val_loss: 1333.7018 - val_accuracy: 0.5072 - 624ms/epoch - 6ms/step\n",
      "Epoch 24/10000\n",
      "113/113 - 1s - loss: 969.1133 - accuracy: 0.5804 - val_loss: 679.0459 - val_accuracy: 0.5901 - 560ms/epoch - 5ms/step\n",
      "Epoch 25/10000\n",
      "113/113 - 1s - loss: 841.6279 - accuracy: 0.5797 - val_loss: 916.0961 - val_accuracy: 0.5329 - 602ms/epoch - 5ms/step\n",
      "Epoch 26/10000\n",
      "113/113 - 1s - loss: 984.7174 - accuracy: 0.5760 - val_loss: 756.3215 - val_accuracy: 0.6239 - 633ms/epoch - 6ms/step\n",
      "Epoch 27/10000\n",
      "113/113 - 1s - loss: 1170.8838 - accuracy: 0.5803 - val_loss: 635.6618 - val_accuracy: 0.5972 - 646ms/epoch - 6ms/step\n",
      "Epoch 28/10000\n",
      "113/113 - 1s - loss: 1337.2301 - accuracy: 0.5725 - val_loss: 1136.1899 - val_accuracy: 0.5392 - 584ms/epoch - 5ms/step\n",
      "Epoch 29/10000\n",
      "113/113 - 1s - loss: 981.7086 - accuracy: 0.5800 - val_loss: 1177.0701 - val_accuracy: 0.5217 - 600ms/epoch - 5ms/step\n",
      "Epoch 30/10000\n",
      "113/113 - 1s - loss: 886.3622 - accuracy: 0.5814 - val_loss: 687.4474 - val_accuracy: 0.5993 - 654ms/epoch - 6ms/step\n",
      "Epoch 31/10000\n",
      "113/113 - 1s - loss: 866.3509 - accuracy: 0.5784 - val_loss: 1838.4907 - val_accuracy: 0.4704 - 630ms/epoch - 6ms/step\n",
      "Epoch 32/10000\n",
      "113/113 - 1s - loss: 1095.8400 - accuracy: 0.5759 - val_loss: 1119.4995 - val_accuracy: 0.6102 - 713ms/epoch - 6ms/step\n",
      "Epoch 33/10000\n",
      "113/113 - 1s - loss: 1109.2584 - accuracy: 0.5800 - val_loss: 660.8809 - val_accuracy: 0.5685 - 590ms/epoch - 5ms/step\n",
      "Epoch 34/10000\n",
      "113/113 - 1s - loss: 928.2126 - accuracy: 0.5775 - val_loss: 1247.3848 - val_accuracy: 0.6116 - 637ms/epoch - 6ms/step\n",
      "Epoch 35/10000\n",
      "113/113 - 1s - loss: 1071.2272 - accuracy: 0.5819 - val_loss: 556.8639 - val_accuracy: 0.5952 - 577ms/epoch - 5ms/step\n",
      "Epoch 36/10000\n",
      "113/113 - 1s - loss: 1120.1256 - accuracy: 0.5790 - val_loss: 556.3635 - val_accuracy: 0.5884 - 605ms/epoch - 5ms/step\n",
      "Epoch 37/10000\n",
      "113/113 - 1s - loss: 922.0243 - accuracy: 0.5771 - val_loss: 1070.8446 - val_accuracy: 0.5499 - 629ms/epoch - 6ms/step\n",
      "Epoch 38/10000\n",
      "113/113 - 1s - loss: 1133.5189 - accuracy: 0.5806 - val_loss: 602.5611 - val_accuracy: 0.6016 - 667ms/epoch - 6ms/step\n",
      "Epoch 39/10000\n",
      "113/113 - 1s - loss: 725.4792 - accuracy: 0.5826 - val_loss: 1579.6787 - val_accuracy: 0.4774 - 633ms/epoch - 6ms/step\n",
      "Epoch 40/10000\n",
      "113/113 - 1s - loss: 1049.8269 - accuracy: 0.5800 - val_loss: 1620.1672 - val_accuracy: 0.6099 - 606ms/epoch - 5ms/step\n",
      "Epoch 41/10000\n",
      "113/113 - 1s - loss: 811.7237 - accuracy: 0.5798 - val_loss: 708.5429 - val_accuracy: 0.5843 - 582ms/epoch - 5ms/step\n",
      "Epoch 42/10000\n",
      "113/113 - 1s - loss: 937.8938 - accuracy: 0.5776 - val_loss: 1155.2313 - val_accuracy: 0.6123 - 629ms/epoch - 6ms/step\n",
      "Epoch 43/10000\n",
      "113/113 - 1s - loss: 999.3054 - accuracy: 0.5819 - val_loss: 507.5501 - val_accuracy: 0.5901 - 600ms/epoch - 5ms/step\n",
      "Epoch 44/10000\n",
      "113/113 - 1s - loss: 604.5991 - accuracy: 0.5844 - val_loss: 515.5062 - val_accuracy: 0.6080 - 565ms/epoch - 5ms/step\n",
      "Epoch 45/10000\n",
      "113/113 - 1s - loss: 607.8567 - accuracy: 0.5822 - val_loss: 502.6044 - val_accuracy: 0.6142 - 583ms/epoch - 5ms/step\n",
      "Epoch 46/10000\n",
      "113/113 - 1s - loss: 906.1326 - accuracy: 0.5770 - val_loss: 553.9087 - val_accuracy: 0.6133 - 704ms/epoch - 6ms/step\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Iterazione 68/128 (03-11-2023_11-29-48)\n",
      "hidden_layer_size: 500\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 1\n",
      "activation_hidden: relu\n",
      "activation_output: sigmoid\n",
      "batch_size: 1000\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 20\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 62.38%\n",
      "La recall di questo modello sul validation set è: 42.59%\n",
      "La f1 di questo modello sul validation set è: 46.7%\n",
      "La balanced accuracy di questo modello sul validation set è: 58.73%\n",
      "La precision di questo modello sul validation set è: 51.68%\n",
      "La AUC di questo modello sul validation set è: 59.02%\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.67    0.75      0.71  7,670.00\n",
      "1                  0.52    0.43      0.47  4,841.00\n",
      "accuracy           0.62    0.62      0.62      0.62\n",
      "macro avg          0.60    0.59      0.59 12,511.00\n",
      "weighted avg       0.61    0.62      0.62 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         5742         1928\n",
      "Actual 1         2779         2062\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 66.1% con il modello:\n",
      "66 - AC66.1%, L500, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA20, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_11-28-58).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 62.97% con il modello:\n",
      "1 - BA62.97%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 64.08% con il modello:\n",
      "13 - PR64.08%, L20, mV-10, MV10, P2, AHsigmoid, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-26-48).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "1126/1126 - 4s - loss: 4.3268 - accuracy: 0.4922 - val_loss: 2.0051 - val_accuracy: 0.5358 - 4s/epoch - 3ms/step\n",
      "Epoch 2/10000\n",
      "1126/1126 - 3s - loss: 1.7154 - accuracy: 0.5702 - val_loss: 1.5700 - val_accuracy: 0.5661 - 3s/epoch - 2ms/step\n",
      "Epoch 3/10000\n",
      "1126/1126 - 2s - loss: 1.5043 - accuracy: 0.5834 - val_loss: 1.4380 - val_accuracy: 0.5534 - 2s/epoch - 2ms/step\n",
      "Epoch 4/10000\n",
      "1126/1126 - 3s - loss: 1.4190 - accuracy: 0.5875 - val_loss: 1.4191 - val_accuracy: 0.5932 - 3s/epoch - 2ms/step\n",
      "Epoch 5/10000\n",
      "1126/1126 - 3s - loss: 1.3790 - accuracy: 0.5877 - val_loss: 1.3663 - val_accuracy: 0.5942 - 3s/epoch - 2ms/step\n",
      "Epoch 6/10000\n",
      "1126/1126 - 3s - loss: 1.3325 - accuracy: 0.5878 - val_loss: 1.3284 - val_accuracy: 0.5996 - 3s/epoch - 2ms/step\n",
      "Epoch 7/10000\n",
      "1126/1126 - 2s - loss: 1.2745 - accuracy: 0.5903 - val_loss: 1.3223 - val_accuracy: 0.5936 - 2s/epoch - 2ms/step\n",
      "Epoch 8/10000\n",
      "1126/1126 - 3s - loss: 1.2444 - accuracy: 0.5904 - val_loss: 1.2209 - val_accuracy: 0.5845 - 3s/epoch - 2ms/step\n",
      "Epoch 9/10000\n",
      "1126/1126 - 3s - loss: 1.2316 - accuracy: 0.5886 - val_loss: 1.1940 - val_accuracy: 0.5991 - 3s/epoch - 3ms/step\n",
      "Epoch 10/10000\n",
      "1126/1126 - 3s - loss: 1.1980 - accuracy: 0.5927 - val_loss: 1.1824 - val_accuracy: 0.5937 - 3s/epoch - 2ms/step\n",
      "Epoch 11/10000\n",
      "1126/1126 - 3s - loss: 1.1749 - accuracy: 0.5937 - val_loss: 1.1754 - val_accuracy: 0.5931 - 3s/epoch - 2ms/step\n",
      "Epoch 12/10000\n",
      "1126/1126 - 3s - loss: 1.1499 - accuracy: 0.5958 - val_loss: 1.1293 - val_accuracy: 0.6040 - 3s/epoch - 2ms/step\n",
      "Epoch 13/10000\n",
      "1126/1126 - 3s - loss: 1.1346 - accuracy: 0.5954 - val_loss: 1.1128 - val_accuracy: 0.5864 - 3s/epoch - 2ms/step\n",
      "Epoch 14/10000\n",
      "1126/1126 - 3s - loss: 1.1224 - accuracy: 0.5943 - val_loss: 1.0866 - val_accuracy: 0.5955 - 3s/epoch - 2ms/step\n",
      "Epoch 15/10000\n",
      "1126/1126 - 3s - loss: 1.0870 - accuracy: 0.5982 - val_loss: 1.0888 - val_accuracy: 0.6031 - 3s/epoch - 2ms/step\n",
      "Epoch 16/10000\n",
      "1126/1126 - 3s - loss: 1.0869 - accuracy: 0.5964 - val_loss: 1.0988 - val_accuracy: 0.5797 - 3s/epoch - 2ms/step\n",
      "Epoch 17/10000\n",
      "1126/1126 - 3s - loss: 1.0613 - accuracy: 0.5964 - val_loss: 1.0543 - val_accuracy: 0.6023 - 3s/epoch - 3ms/step\n",
      "Epoch 18/10000\n",
      "1126/1126 - 3s - loss: 1.0420 - accuracy: 0.6004 - val_loss: 1.0726 - val_accuracy: 0.6045 - 3s/epoch - 3ms/step\n",
      "Epoch 19/10000\n",
      "1126/1126 - 3s - loss: 1.0281 - accuracy: 0.5982 - val_loss: 1.0116 - val_accuracy: 0.6027 - 3s/epoch - 2ms/step\n",
      "Epoch 20/10000\n",
      "1126/1126 - 3s - loss: 1.0273 - accuracy: 0.5963 - val_loss: 1.0167 - val_accuracy: 0.5946 - 3s/epoch - 2ms/step\n",
      "Epoch 21/10000\n",
      "1126/1126 - 3s - loss: 1.0131 - accuracy: 0.5962 - val_loss: 1.0008 - val_accuracy: 0.6062 - 3s/epoch - 3ms/step\n",
      "Epoch 22/10000\n",
      "1126/1126 - 3s - loss: 0.9978 - accuracy: 0.6018 - val_loss: 0.9809 - val_accuracy: 0.5922 - 3s/epoch - 3ms/step\n",
      "Epoch 23/10000\n",
      "1126/1126 - 3s - loss: 0.9808 - accuracy: 0.6003 - val_loss: 0.9835 - val_accuracy: 0.5976 - 3s/epoch - 2ms/step\n",
      "Epoch 24/10000\n",
      "1126/1126 - 3s - loss: 0.9698 - accuracy: 0.5984 - val_loss: 0.9637 - val_accuracy: 0.5963 - 3s/epoch - 2ms/step\n",
      "Epoch 25/10000\n",
      "1126/1126 - 3s - loss: 0.9662 - accuracy: 0.5969 - val_loss: 0.9544 - val_accuracy: 0.5992 - 3s/epoch - 2ms/step\n",
      "Epoch 26/10000\n",
      "1126/1126 - 3s - loss: 0.9454 - accuracy: 0.6034 - val_loss: 0.9521 - val_accuracy: 0.6020 - 3s/epoch - 3ms/step\n",
      "Epoch 27/10000\n",
      "1126/1126 - 3s - loss: 0.9434 - accuracy: 0.6003 - val_loss: 0.9575 - val_accuracy: 0.6049 - 3s/epoch - 2ms/step\n",
      "Epoch 28/10000\n",
      "1126/1126 - 3s - loss: 0.9218 - accuracy: 0.6061 - val_loss: 0.9253 - val_accuracy: 0.6040 - 3s/epoch - 3ms/step\n",
      "Epoch 29/10000\n",
      "1126/1126 - 3s - loss: 0.9129 - accuracy: 0.6097 - val_loss: 0.9361 - val_accuracy: 0.6118 - 3s/epoch - 2ms/step\n",
      "Epoch 30/10000\n",
      "1126/1126 - 3s - loss: 0.9159 - accuracy: 0.6079 - val_loss: 0.9161 - val_accuracy: 0.6111 - 3s/epoch - 2ms/step\n",
      "Epoch 31/10000\n",
      "1126/1126 - 3s - loss: 0.9165 - accuracy: 0.6076 - val_loss: 0.9283 - val_accuracy: 0.6116 - 3s/epoch - 2ms/step\n",
      "Epoch 32/10000\n",
      "1126/1126 - 3s - loss: 0.9085 - accuracy: 0.6076 - val_loss: 0.9047 - val_accuracy: 0.6094 - 3s/epoch - 2ms/step\n",
      "Epoch 33/10000\n",
      "1126/1126 - 3s - loss: 0.8899 - accuracy: 0.6080 - val_loss: 0.8928 - val_accuracy: 0.6023 - 3s/epoch - 3ms/step\n",
      "Epoch 34/10000\n",
      "1126/1126 - 3s - loss: 0.8800 - accuracy: 0.6102 - val_loss: 0.8851 - val_accuracy: 0.6023 - 3s/epoch - 2ms/step\n",
      "Epoch 35/10000\n",
      "1126/1126 - 3s - loss: 0.8689 - accuracy: 0.6089 - val_loss: 0.8515 - val_accuracy: 0.6133 - 3s/epoch - 2ms/step\n",
      "Epoch 36/10000\n",
      "1126/1126 - 3s - loss: 0.8673 - accuracy: 0.6068 - val_loss: 0.8918 - val_accuracy: 0.6073 - 3s/epoch - 2ms/step\n",
      "Epoch 37/10000\n",
      "1126/1126 - 3s - loss: 0.8753 - accuracy: 0.6030 - val_loss: 0.8567 - val_accuracy: 0.6081 - 3s/epoch - 2ms/step\n",
      "Epoch 38/10000\n",
      "1126/1126 - 3s - loss: 0.8609 - accuracy: 0.6071 - val_loss: 0.8528 - val_accuracy: 0.5825 - 3s/epoch - 2ms/step\n",
      "Epoch 39/10000\n",
      "1126/1126 - 2s - loss: 0.8625 - accuracy: 0.6105 - val_loss: 0.8522 - val_accuracy: 0.6074 - 2s/epoch - 2ms/step\n",
      "Epoch 40/10000\n",
      "1126/1126 - 2s - loss: 0.8499 - accuracy: 0.6095 - val_loss: 0.8413 - val_accuracy: 0.6104 - 2s/epoch - 2ms/step\n",
      "Epoch 41/10000\n",
      "1126/1126 - 2s - loss: 0.8519 - accuracy: 0.6078 - val_loss: 0.8435 - val_accuracy: 0.6135 - 2s/epoch - 2ms/step\n",
      "Epoch 42/10000\n",
      "1126/1126 - 2s - loss: 0.8519 - accuracy: 0.6078 - val_loss: 0.8274 - val_accuracy: 0.6158 - 2s/epoch - 2ms/step\n",
      "Epoch 43/10000\n",
      "1126/1126 - 2s - loss: 0.8439 - accuracy: 0.6077 - val_loss: 0.8334 - val_accuracy: 0.6075 - 2s/epoch - 2ms/step\n",
      "Epoch 44/10000\n",
      "1126/1126 - 2s - loss: 0.8442 - accuracy: 0.6088 - val_loss: 0.8388 - val_accuracy: 0.6127 - 2s/epoch - 2ms/step\n",
      "Epoch 45/10000\n",
      "1126/1126 - 2s - loss: 0.8310 - accuracy: 0.6120 - val_loss: 0.8403 - val_accuracy: 0.6134 - 2s/epoch - 2ms/step\n",
      "Epoch 46/10000\n",
      "1126/1126 - 2s - loss: 0.8320 - accuracy: 0.6083 - val_loss: 0.8305 - val_accuracy: 0.6023 - 2s/epoch - 2ms/step\n",
      "Epoch 47/10000\n",
      "1126/1126 - 2s - loss: 0.8277 - accuracy: 0.6056 - val_loss: 0.8277 - val_accuracy: 0.6057 - 2s/epoch - 2ms/step\n",
      "Epoch 48/10000\n",
      "1126/1126 - 2s - loss: 0.8241 - accuracy: 0.6087 - val_loss: 0.8139 - val_accuracy: 0.6113 - 2s/epoch - 2ms/step\n",
      "Epoch 49/10000\n",
      "1126/1126 - 3s - loss: 0.8224 - accuracy: 0.6099 - val_loss: 0.8323 - val_accuracy: 0.6017 - 3s/epoch - 2ms/step\n",
      "Epoch 50/10000\n",
      "1126/1126 - 3s - loss: 0.8251 - accuracy: 0.6130 - val_loss: 0.8260 - val_accuracy: 0.6123 - 3s/epoch - 2ms/step\n",
      "Epoch 51/10000\n",
      "1126/1126 - 3s - loss: 0.8168 - accuracy: 0.6101 - val_loss: 0.8092 - val_accuracy: 0.6098 - 3s/epoch - 2ms/step\n",
      "Epoch 52/10000\n",
      "1126/1126 - 2s - loss: 0.8147 - accuracy: 0.6118 - val_loss: 0.8118 - val_accuracy: 0.6125 - 2s/epoch - 2ms/step\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Iterazione 69/128 (03-11-2023_11-32-10)\n",
      "hidden_layer_size: 500\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 1\n",
      "activation_hidden: sigmoid\n",
      "activation_output: sigmoid\n",
      "batch_size: 100\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 10\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 61.58%\n",
      "La recall di questo modello sul validation set è: 45.11%\n",
      "La f1 di questo modello sul validation set è: 47.61%\n",
      "La balanced accuracy di questo modello sul validation set è: 58.54%\n",
      "La precision di questo modello sul validation set è: 50.39%\n",
      "La AUC di questo modello sul validation set è: 60.87%\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.68    0.72      0.70  7,670.00\n",
      "1                  0.50    0.45      0.48  4,841.00\n",
      "accuracy           0.62    0.62      0.62      0.62\n",
      "macro avg          0.59    0.59      0.59 12,511.00\n",
      "weighted avg       0.61    0.62      0.61 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         5520         2150\n",
      "Actual 1         2657         2184\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 66.1% con il modello:\n",
      "66 - AC66.1%, L500, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA20, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_11-28-58).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 62.97% con il modello:\n",
      "1 - BA62.97%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 64.08% con il modello:\n",
      "13 - PR64.08%, L20, mV-10, MV10, P2, AHsigmoid, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-26-48).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "1126/1126 - 4s - loss: 4.3268 - accuracy: 0.4922 - val_loss: 2.0051 - val_accuracy: 0.5358 - 4s/epoch - 3ms/step\n",
      "Epoch 2/10000\n",
      "1126/1126 - 2s - loss: 1.7154 - accuracy: 0.5702 - val_loss: 1.5700 - val_accuracy: 0.5661 - 2s/epoch - 2ms/step\n",
      "Epoch 3/10000\n",
      "1126/1126 - 3s - loss: 1.5043 - accuracy: 0.5834 - val_loss: 1.4380 - val_accuracy: 0.5534 - 3s/epoch - 2ms/step\n",
      "Epoch 4/10000\n",
      "1126/1126 - 3s - loss: 1.4190 - accuracy: 0.5875 - val_loss: 1.4191 - val_accuracy: 0.5932 - 3s/epoch - 2ms/step\n",
      "Epoch 5/10000\n",
      "1126/1126 - 3s - loss: 1.3790 - accuracy: 0.5877 - val_loss: 1.3663 - val_accuracy: 0.5942 - 3s/epoch - 2ms/step\n",
      "Epoch 6/10000\n",
      "1126/1126 - 3s - loss: 1.3325 - accuracy: 0.5878 - val_loss: 1.3284 - val_accuracy: 0.5996 - 3s/epoch - 2ms/step\n",
      "Epoch 7/10000\n",
      "1126/1126 - 3s - loss: 1.2745 - accuracy: 0.5903 - val_loss: 1.3223 - val_accuracy: 0.5936 - 3s/epoch - 2ms/step\n",
      "Epoch 8/10000\n",
      "1126/1126 - 2s - loss: 1.2444 - accuracy: 0.5904 - val_loss: 1.2209 - val_accuracy: 0.5845 - 2s/epoch - 2ms/step\n",
      "Epoch 9/10000\n",
      "1126/1126 - 2s - loss: 1.2316 - accuracy: 0.5886 - val_loss: 1.1940 - val_accuracy: 0.5991 - 2s/epoch - 2ms/step\n",
      "Epoch 10/10000\n",
      "1126/1126 - 2s - loss: 1.1980 - accuracy: 0.5927 - val_loss: 1.1824 - val_accuracy: 0.5937 - 2s/epoch - 2ms/step\n",
      "Epoch 11/10000\n",
      "1126/1126 - 2s - loss: 1.1749 - accuracy: 0.5937 - val_loss: 1.1754 - val_accuracy: 0.5931 - 2s/epoch - 2ms/step\n",
      "Epoch 12/10000\n",
      "1126/1126 - 3s - loss: 1.1499 - accuracy: 0.5958 - val_loss: 1.1293 - val_accuracy: 0.6040 - 3s/epoch - 2ms/step\n",
      "Epoch 13/10000\n",
      "1126/1126 - 3s - loss: 1.1346 - accuracy: 0.5954 - val_loss: 1.1128 - val_accuracy: 0.5864 - 3s/epoch - 2ms/step\n",
      "Epoch 14/10000\n",
      "1126/1126 - 2s - loss: 1.1224 - accuracy: 0.5943 - val_loss: 1.0866 - val_accuracy: 0.5955 - 2s/epoch - 2ms/step\n",
      "Epoch 15/10000\n",
      "1126/1126 - 2s - loss: 1.0870 - accuracy: 0.5982 - val_loss: 1.0888 - val_accuracy: 0.6031 - 2s/epoch - 2ms/step\n",
      "Epoch 16/10000\n",
      "1126/1126 - 3s - loss: 1.0869 - accuracy: 0.5964 - val_loss: 1.0988 - val_accuracy: 0.5797 - 3s/epoch - 2ms/step\n",
      "Epoch 17/10000\n",
      "1126/1126 - 3s - loss: 1.0613 - accuracy: 0.5964 - val_loss: 1.0543 - val_accuracy: 0.6023 - 3s/epoch - 2ms/step\n",
      "Epoch 18/10000\n",
      "1126/1126 - 3s - loss: 1.0420 - accuracy: 0.6004 - val_loss: 1.0726 - val_accuracy: 0.6045 - 3s/epoch - 2ms/step\n",
      "Epoch 19/10000\n",
      "1126/1126 - 3s - loss: 1.0281 - accuracy: 0.5982 - val_loss: 1.0116 - val_accuracy: 0.6027 - 3s/epoch - 2ms/step\n",
      "Epoch 20/10000\n",
      "1126/1126 - 2s - loss: 1.0273 - accuracy: 0.5963 - val_loss: 1.0167 - val_accuracy: 0.5946 - 2s/epoch - 2ms/step\n",
      "Epoch 21/10000\n",
      "1126/1126 - 3s - loss: 1.0131 - accuracy: 0.5962 - val_loss: 1.0008 - val_accuracy: 0.6062 - 3s/epoch - 2ms/step\n",
      "Epoch 22/10000\n",
      "1126/1126 - 2s - loss: 0.9978 - accuracy: 0.6018 - val_loss: 0.9809 - val_accuracy: 0.5922 - 2s/epoch - 2ms/step\n",
      "Epoch 23/10000\n",
      "1126/1126 - 2s - loss: 0.9808 - accuracy: 0.6003 - val_loss: 0.9835 - val_accuracy: 0.5976 - 2s/epoch - 2ms/step\n",
      "Epoch 24/10000\n",
      "1126/1126 - 2s - loss: 0.9698 - accuracy: 0.5984 - val_loss: 0.9637 - val_accuracy: 0.5963 - 2s/epoch - 2ms/step\n",
      "Epoch 25/10000\n",
      "1126/1126 - 3s - loss: 0.9662 - accuracy: 0.5969 - val_loss: 0.9544 - val_accuracy: 0.5992 - 3s/epoch - 2ms/step\n",
      "Epoch 26/10000\n",
      "1126/1126 - 2s - loss: 0.9454 - accuracy: 0.6034 - val_loss: 0.9521 - val_accuracy: 0.6020 - 2s/epoch - 2ms/step\n",
      "Epoch 27/10000\n",
      "1126/1126 - 2s - loss: 0.9434 - accuracy: 0.6003 - val_loss: 0.9575 - val_accuracy: 0.6049 - 2s/epoch - 2ms/step\n",
      "Epoch 28/10000\n",
      "1126/1126 - 2s - loss: 0.9218 - accuracy: 0.6061 - val_loss: 0.9253 - val_accuracy: 0.6040 - 2s/epoch - 2ms/step\n",
      "Epoch 29/10000\n",
      "1126/1126 - 2s - loss: 0.9129 - accuracy: 0.6097 - val_loss: 0.9361 - val_accuracy: 0.6118 - 2s/epoch - 2ms/step\n",
      "Epoch 30/10000\n",
      "1126/1126 - 2s - loss: 0.9159 - accuracy: 0.6079 - val_loss: 0.9161 - val_accuracy: 0.6111 - 2s/epoch - 2ms/step\n",
      "Epoch 31/10000\n",
      "1126/1126 - 3s - loss: 0.9165 - accuracy: 0.6076 - val_loss: 0.9283 - val_accuracy: 0.6116 - 3s/epoch - 2ms/step\n",
      "Epoch 32/10000\n",
      "1126/1126 - 3s - loss: 0.9085 - accuracy: 0.6076 - val_loss: 0.9047 - val_accuracy: 0.6094 - 3s/epoch - 2ms/step\n",
      "Epoch 33/10000\n",
      "1126/1126 - 3s - loss: 0.8899 - accuracy: 0.6080 - val_loss: 0.8928 - val_accuracy: 0.6023 - 3s/epoch - 2ms/step\n",
      "Epoch 34/10000\n",
      "1126/1126 - 3s - loss: 0.8800 - accuracy: 0.6102 - val_loss: 0.8851 - val_accuracy: 0.6023 - 3s/epoch - 2ms/step\n",
      "Epoch 35/10000\n",
      "1126/1126 - 3s - loss: 0.8689 - accuracy: 0.6089 - val_loss: 0.8515 - val_accuracy: 0.6133 - 3s/epoch - 2ms/step\n",
      "Epoch 36/10000\n",
      "1126/1126 - 3s - loss: 0.8673 - accuracy: 0.6068 - val_loss: 0.8918 - val_accuracy: 0.6073 - 3s/epoch - 2ms/step\n",
      "Epoch 37/10000\n",
      "1126/1126 - 2s - loss: 0.8753 - accuracy: 0.6030 - val_loss: 0.8567 - val_accuracy: 0.6081 - 2s/epoch - 2ms/step\n",
      "Epoch 38/10000\n",
      "1126/1126 - 2s - loss: 0.8609 - accuracy: 0.6071 - val_loss: 0.8528 - val_accuracy: 0.5825 - 2s/epoch - 2ms/step\n",
      "Epoch 39/10000\n",
      "1126/1126 - 2s - loss: 0.8625 - accuracy: 0.6105 - val_loss: 0.8522 - val_accuracy: 0.6074 - 2s/epoch - 2ms/step\n",
      "Epoch 40/10000\n",
      "1126/1126 - 2s - loss: 0.8499 - accuracy: 0.6095 - val_loss: 0.8413 - val_accuracy: 0.6104 - 2s/epoch - 2ms/step\n",
      "Epoch 41/10000\n",
      "1126/1126 - 2s - loss: 0.8519 - accuracy: 0.6078 - val_loss: 0.8435 - val_accuracy: 0.6135 - 2s/epoch - 2ms/step\n",
      "Epoch 42/10000\n",
      "1126/1126 - 2s - loss: 0.8519 - accuracy: 0.6078 - val_loss: 0.8274 - val_accuracy: 0.6158 - 2s/epoch - 2ms/step\n",
      "Epoch 43/10000\n",
      "1126/1126 - 3s - loss: 0.8439 - accuracy: 0.6077 - val_loss: 0.8334 - val_accuracy: 0.6075 - 3s/epoch - 2ms/step\n",
      "Epoch 44/10000\n",
      "1126/1126 - 3s - loss: 0.8442 - accuracy: 0.6088 - val_loss: 0.8388 - val_accuracy: 0.6127 - 3s/epoch - 2ms/step\n",
      "Epoch 45/10000\n",
      "1126/1126 - 2s - loss: 0.8310 - accuracy: 0.6120 - val_loss: 0.8403 - val_accuracy: 0.6134 - 2s/epoch - 2ms/step\n",
      "Epoch 46/10000\n",
      "1126/1126 - 2s - loss: 0.8320 - accuracy: 0.6083 - val_loss: 0.8305 - val_accuracy: 0.6023 - 2s/epoch - 2ms/step\n",
      "Epoch 47/10000\n",
      "1126/1126 - 2s - loss: 0.8277 - accuracy: 0.6056 - val_loss: 0.8277 - val_accuracy: 0.6057 - 2s/epoch - 2ms/step\n",
      "Epoch 48/10000\n",
      "1126/1126 - 2s - loss: 0.8241 - accuracy: 0.6087 - val_loss: 0.8139 - val_accuracy: 0.6113 - 2s/epoch - 2ms/step\n",
      "Epoch 49/10000\n",
      "1126/1126 - 2s - loss: 0.8224 - accuracy: 0.6099 - val_loss: 0.8323 - val_accuracy: 0.6017 - 2s/epoch - 2ms/step\n",
      "Epoch 50/10000\n",
      "1126/1126 - 2s - loss: 0.8251 - accuracy: 0.6130 - val_loss: 0.8260 - val_accuracy: 0.6123 - 2s/epoch - 2ms/step\n",
      "Epoch 51/10000\n",
      "1126/1126 - 2s - loss: 0.8168 - accuracy: 0.6101 - val_loss: 0.8092 - val_accuracy: 0.6098 - 2s/epoch - 2ms/step\n",
      "Epoch 52/10000\n",
      "1126/1126 - 2s - loss: 0.8147 - accuracy: 0.6118 - val_loss: 0.8118 - val_accuracy: 0.6125 - 2s/epoch - 2ms/step\n",
      "Epoch 53/10000\n",
      "1126/1126 - 2s - loss: 0.8095 - accuracy: 0.6143 - val_loss: 0.8059 - val_accuracy: 0.6185 - 2s/epoch - 2ms/step\n",
      "Epoch 54/10000\n",
      "1126/1126 - 2s - loss: 0.8042 - accuracy: 0.6157 - val_loss: 0.8067 - val_accuracy: 0.6153 - 2s/epoch - 2ms/step\n",
      "Epoch 55/10000\n",
      "1126/1126 - 2s - loss: 0.8068 - accuracy: 0.6106 - val_loss: 0.7909 - val_accuracy: 0.6162 - 2s/epoch - 2ms/step\n",
      "Epoch 56/10000\n",
      "1126/1126 - 3s - loss: 0.8100 - accuracy: 0.6088 - val_loss: 0.8059 - val_accuracy: 0.6193 - 3s/epoch - 2ms/step\n",
      "Epoch 57/10000\n",
      "1126/1126 - 2s - loss: 0.8040 - accuracy: 0.6119 - val_loss: 0.8131 - val_accuracy: 0.6149 - 2s/epoch - 2ms/step\n",
      "Epoch 58/10000\n",
      "1126/1126 - 3s - loss: 0.8005 - accuracy: 0.6125 - val_loss: 0.8000 - val_accuracy: 0.6225 - 3s/epoch - 2ms/step\n",
      "Epoch 59/10000\n",
      "1126/1126 - 3s - loss: 0.7992 - accuracy: 0.6102 - val_loss: 0.8013 - val_accuracy: 0.6135 - 3s/epoch - 2ms/step\n",
      "Epoch 60/10000\n",
      "1126/1126 - 2s - loss: 0.8020 - accuracy: 0.6111 - val_loss: 0.8056 - val_accuracy: 0.5898 - 2s/epoch - 2ms/step\n",
      "Epoch 61/10000\n",
      "1126/1126 - 2s - loss: 0.7979 - accuracy: 0.6135 - val_loss: 0.8032 - val_accuracy: 0.6080 - 2s/epoch - 2ms/step\n",
      "Epoch 62/10000\n",
      "1126/1126 - 2s - loss: 0.7962 - accuracy: 0.6134 - val_loss: 0.7851 - val_accuracy: 0.6223 - 2s/epoch - 2ms/step\n",
      "Epoch 63/10000\n",
      "1126/1126 - 2s - loss: 0.7885 - accuracy: 0.6172 - val_loss: 0.7888 - val_accuracy: 0.6187 - 2s/epoch - 2ms/step\n",
      "Epoch 64/10000\n",
      "1126/1126 - 2s - loss: 0.7836 - accuracy: 0.6156 - val_loss: 0.7626 - val_accuracy: 0.6257 - 2s/epoch - 2ms/step\n",
      "Epoch 65/10000\n",
      "1126/1126 - 3s - loss: 0.7846 - accuracy: 0.6162 - val_loss: 0.7786 - val_accuracy: 0.6195 - 3s/epoch - 2ms/step\n",
      "Epoch 66/10000\n",
      "1126/1126 - 2s - loss: 0.7833 - accuracy: 0.6172 - val_loss: 0.7866 - val_accuracy: 0.6056 - 2s/epoch - 2ms/step\n",
      "Epoch 67/10000\n",
      "1126/1126 - 3s - loss: 0.7793 - accuracy: 0.6151 - val_loss: 0.7663 - val_accuracy: 0.6204 - 3s/epoch - 2ms/step\n",
      "Epoch 68/10000\n",
      "1126/1126 - 2s - loss: 0.7783 - accuracy: 0.6181 - val_loss: 0.7752 - val_accuracy: 0.6219 - 2s/epoch - 2ms/step\n",
      "Epoch 69/10000\n",
      "1126/1126 - 3s - loss: 0.7820 - accuracy: 0.6187 - val_loss: 0.7819 - val_accuracy: 0.6290 - 3s/epoch - 2ms/step\n",
      "Epoch 70/10000\n",
      "1126/1126 - 2s - loss: 0.7844 - accuracy: 0.6177 - val_loss: 0.7952 - val_accuracy: 0.6175 - 2s/epoch - 2ms/step\n",
      "Epoch 71/10000\n",
      "1126/1126 - 2s - loss: 0.7765 - accuracy: 0.6218 - val_loss: 0.7891 - val_accuracy: 0.6255 - 2s/epoch - 2ms/step\n",
      "Epoch 72/10000\n",
      "1126/1126 - 2s - loss: 0.7714 - accuracy: 0.6204 - val_loss: 0.7754 - val_accuracy: 0.6187 - 2s/epoch - 2ms/step\n",
      "Epoch 73/10000\n",
      "1126/1126 - 2s - loss: 0.7668 - accuracy: 0.6203 - val_loss: 0.7768 - val_accuracy: 0.6283 - 2s/epoch - 2ms/step\n",
      "Epoch 74/10000\n",
      "1126/1126 - 2s - loss: 0.7660 - accuracy: 0.6222 - val_loss: 0.7619 - val_accuracy: 0.6266 - 2s/epoch - 2ms/step\n",
      "Epoch 75/10000\n",
      "1126/1126 - 2s - loss: 0.7728 - accuracy: 0.6192 - val_loss: 0.7667 - val_accuracy: 0.6227 - 2s/epoch - 2ms/step\n",
      "Epoch 76/10000\n",
      "1126/1126 - 3s - loss: 0.7677 - accuracy: 0.6214 - val_loss: 0.7806 - val_accuracy: 0.6298 - 3s/epoch - 2ms/step\n",
      "Epoch 77/10000\n",
      "1126/1126 - 2s - loss: 0.7703 - accuracy: 0.6203 - val_loss: 0.7679 - val_accuracy: 0.6231 - 2s/epoch - 2ms/step\n",
      "Epoch 78/10000\n",
      "1126/1126 - 2s - loss: 0.7582 - accuracy: 0.6209 - val_loss: 0.7618 - val_accuracy: 0.6302 - 2s/epoch - 2ms/step\n",
      "Epoch 79/10000\n",
      "1126/1126 - 2s - loss: 0.7566 - accuracy: 0.6194 - val_loss: 0.7507 - val_accuracy: 0.6243 - 2s/epoch - 2ms/step\n",
      "Epoch 80/10000\n",
      "1126/1126 - 2s - loss: 0.7569 - accuracy: 0.6191 - val_loss: 0.7593 - val_accuracy: 0.6212 - 2s/epoch - 2ms/step\n",
      "Epoch 81/10000\n",
      "1126/1126 - 2s - loss: 0.7510 - accuracy: 0.6212 - val_loss: 0.7518 - val_accuracy: 0.6212 - 2s/epoch - 2ms/step\n",
      "Epoch 82/10000\n",
      "1126/1126 - 2s - loss: 0.7513 - accuracy: 0.6220 - val_loss: 0.7520 - val_accuracy: 0.6181 - 2s/epoch - 2ms/step\n",
      "Epoch 83/10000\n",
      "1126/1126 - 2s - loss: 0.7499 - accuracy: 0.6205 - val_loss: 0.7608 - val_accuracy: 0.6035 - 2s/epoch - 2ms/step\n",
      "Epoch 84/10000\n",
      "1126/1126 - 2s - loss: 0.7551 - accuracy: 0.6220 - val_loss: 0.7489 - val_accuracy: 0.6239 - 2s/epoch - 2ms/step\n",
      "Epoch 85/10000\n",
      "1126/1126 - 2s - loss: 0.7497 - accuracy: 0.6247 - val_loss: 0.7503 - val_accuracy: 0.6197 - 2s/epoch - 2ms/step\n",
      "Epoch 86/10000\n",
      "1126/1126 - 2s - loss: 0.7440 - accuracy: 0.6245 - val_loss: 0.7537 - val_accuracy: 0.6236 - 2s/epoch - 2ms/step\n",
      "Epoch 87/10000\n",
      "1126/1126 - 2s - loss: 0.7556 - accuracy: 0.6206 - val_loss: 0.7493 - val_accuracy: 0.6178 - 2s/epoch - 2ms/step\n",
      "Epoch 88/10000\n",
      "1126/1126 - 2s - loss: 0.7508 - accuracy: 0.6211 - val_loss: 0.7662 - val_accuracy: 0.6173 - 2s/epoch - 2ms/step\n",
      "Epoch 89/10000\n",
      "1126/1126 - 2s - loss: 0.7482 - accuracy: 0.6206 - val_loss: 0.7534 - val_accuracy: 0.6250 - 2s/epoch - 2ms/step\n",
      "Epoch 90/10000\n",
      "1126/1126 - 2s - loss: 0.7478 - accuracy: 0.6223 - val_loss: 0.7501 - val_accuracy: 0.6183 - 2s/epoch - 2ms/step\n",
      "Epoch 91/10000\n",
      "1126/1126 - 2s - loss: 0.7434 - accuracy: 0.6222 - val_loss: 0.7480 - val_accuracy: 0.6288 - 2s/epoch - 2ms/step\n",
      "Epoch 92/10000\n",
      "1126/1126 - 2s - loss: 0.7396 - accuracy: 0.6230 - val_loss: 0.7284 - val_accuracy: 0.6262 - 2s/epoch - 2ms/step\n",
      "Epoch 93/10000\n",
      "1126/1126 - 2s - loss: 0.7402 - accuracy: 0.6221 - val_loss: 0.7419 - val_accuracy: 0.6231 - 2s/epoch - 2ms/step\n",
      "Epoch 94/10000\n",
      "1126/1126 - 2s - loss: 0.7424 - accuracy: 0.6229 - val_loss: 0.7377 - val_accuracy: 0.6203 - 2s/epoch - 2ms/step\n",
      "Epoch 95/10000\n",
      "1126/1126 - 2s - loss: 0.7322 - accuracy: 0.6247 - val_loss: 0.7438 - val_accuracy: 0.6269 - 2s/epoch - 2ms/step\n",
      "Epoch 96/10000\n",
      "1126/1126 - 2s - loss: 0.7375 - accuracy: 0.6224 - val_loss: 0.7372 - val_accuracy: 0.6212 - 2s/epoch - 2ms/step\n",
      "Epoch 97/10000\n",
      "1126/1126 - 2s - loss: 0.7336 - accuracy: 0.6241 - val_loss: 0.7431 - val_accuracy: 0.6247 - 2s/epoch - 2ms/step\n",
      "Epoch 98/10000\n",
      "1126/1126 - 2s - loss: 0.7299 - accuracy: 0.6226 - val_loss: 0.7378 - val_accuracy: 0.6249 - 2s/epoch - 2ms/step\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Iterazione 70/128 (03-11-2023_11-36-15)\n",
      "hidden_layer_size: 500\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 1\n",
      "activation_hidden: sigmoid\n",
      "activation_output: sigmoid\n",
      "batch_size: 100\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 20\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 63.02%\n",
      "La recall di questo modello sul validation set è: 33.32%\n",
      "La f1 di questo modello sul validation set è: 41.09%\n",
      "La balanced accuracy di questo modello sul validation set è: 57.55%\n",
      "La precision di questo modello sul validation set è: 53.57%\n",
      "La AUC di questo modello sul validation set è: 62.19%\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.66    0.82      0.73  7,670.00\n",
      "1                  0.54    0.33      0.41  4,841.00\n",
      "accuracy           0.63    0.63      0.63      0.63\n",
      "macro avg          0.60    0.58      0.57 12,511.00\n",
      "weighted avg       0.61    0.63      0.61 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         6272         1398\n",
      "Actual 1         3228         1613\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 66.1% con il modello:\n",
      "66 - AC66.1%, L500, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA20, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_11-28-58).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 62.97% con il modello:\n",
      "1 - BA62.97%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 64.08% con il modello:\n",
      "13 - PR64.08%, L20, mV-10, MV10, P2, AHsigmoid, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-26-48).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "113/113 - 2s - loss: 13.3989 - accuracy: 0.4012 - val_loss: 6.6187 - val_accuracy: 0.4389 - 2s/epoch - 16ms/step\n",
      "Epoch 2/10000\n",
      "113/113 - 1s - loss: 5.2977 - accuracy: 0.4634 - val_loss: 4.2698 - val_accuracy: 0.4828 - 812ms/epoch - 7ms/step\n",
      "Epoch 3/10000\n",
      "113/113 - 1s - loss: 3.7261 - accuracy: 0.4866 - val_loss: 3.1344 - val_accuracy: 0.4980 - 775ms/epoch - 7ms/step\n",
      "Epoch 4/10000\n",
      "113/113 - 1s - loss: 2.8980 - accuracy: 0.5040 - val_loss: 2.6307 - val_accuracy: 0.5126 - 740ms/epoch - 7ms/step\n",
      "Epoch 5/10000\n",
      "113/113 - 1s - loss: 2.5100 - accuracy: 0.5140 - val_loss: 2.2977 - val_accuracy: 0.5176 - 815ms/epoch - 7ms/step\n",
      "Epoch 6/10000\n",
      "113/113 - 1s - loss: 2.1923 - accuracy: 0.5196 - val_loss: 2.0261 - val_accuracy: 0.5199 - 830ms/epoch - 7ms/step\n",
      "Epoch 7/10000\n",
      "113/113 - 1s - loss: 2.0093 - accuracy: 0.5219 - val_loss: 1.9187 - val_accuracy: 0.5273 - 712ms/epoch - 6ms/step\n",
      "Epoch 8/10000\n",
      "113/113 - 1s - loss: 1.9207 - accuracy: 0.5246 - val_loss: 1.8579 - val_accuracy: 0.5283 - 759ms/epoch - 7ms/step\n",
      "Epoch 9/10000\n",
      "113/113 - 1s - loss: 1.8523 - accuracy: 0.5275 - val_loss: 1.7856 - val_accuracy: 0.5330 - 787ms/epoch - 7ms/step\n",
      "Epoch 10/10000\n",
      "113/113 - 1s - loss: 1.7783 - accuracy: 0.5310 - val_loss: 1.7284 - val_accuracy: 0.5373 - 726ms/epoch - 6ms/step\n",
      "Epoch 11/10000\n",
      "113/113 - 1s - loss: 1.7316 - accuracy: 0.5359 - val_loss: 1.6667 - val_accuracy: 0.5346 - 636ms/epoch - 6ms/step\n",
      "Epoch 12/10000\n",
      "113/113 - 1s - loss: 1.6802 - accuracy: 0.5400 - val_loss: 1.6476 - val_accuracy: 0.5476 - 677ms/epoch - 6ms/step\n",
      "Epoch 13/10000\n",
      "113/113 - 1s - loss: 1.6603 - accuracy: 0.5412 - val_loss: 1.6144 - val_accuracy: 0.5462 - 617ms/epoch - 5ms/step\n",
      "Epoch 14/10000\n",
      "113/113 - 1s - loss: 1.6326 - accuracy: 0.5410 - val_loss: 1.5937 - val_accuracy: 0.5466 - 737ms/epoch - 7ms/step\n",
      "Epoch 15/10000\n",
      "113/113 - 1s - loss: 1.6365 - accuracy: 0.5355 - val_loss: 1.6070 - val_accuracy: 0.5434 - 850ms/epoch - 8ms/step\n",
      "Epoch 16/10000\n",
      "113/113 - 1s - loss: 1.6070 - accuracy: 0.5383 - val_loss: 1.5696 - val_accuracy: 0.5476 - 800ms/epoch - 7ms/step\n",
      "Epoch 17/10000\n",
      "113/113 - 1s - loss: 1.5818 - accuracy: 0.5397 - val_loss: 1.5667 - val_accuracy: 0.5465 - 701ms/epoch - 6ms/step\n",
      "Epoch 18/10000\n",
      "113/113 - 1s - loss: 1.5690 - accuracy: 0.5484 - val_loss: 1.5284 - val_accuracy: 0.5768 - 748ms/epoch - 7ms/step\n",
      "Epoch 19/10000\n",
      "113/113 - 1s - loss: 1.5534 - accuracy: 0.5620 - val_loss: 1.5423 - val_accuracy: 0.5456 - 670ms/epoch - 6ms/step\n",
      "Epoch 20/10000\n",
      "113/113 - 1s - loss: 1.5393 - accuracy: 0.5664 - val_loss: 1.5205 - val_accuracy: 0.5800 - 679ms/epoch - 6ms/step\n",
      "Epoch 21/10000\n",
      "113/113 - 1s - loss: 1.5272 - accuracy: 0.5588 - val_loss: 1.5271 - val_accuracy: 0.5461 - 721ms/epoch - 6ms/step\n",
      "Epoch 22/10000\n",
      "113/113 - 1s - loss: 1.5120 - accuracy: 0.5566 - val_loss: 1.4734 - val_accuracy: 0.5494 - 663ms/epoch - 6ms/step\n",
      "Epoch 23/10000\n",
      "113/113 - 1s - loss: 1.4954 - accuracy: 0.5473 - val_loss: 1.4716 - val_accuracy: 0.5537 - 700ms/epoch - 6ms/step\n",
      "Epoch 24/10000\n",
      "113/113 - 1s - loss: 1.4823 - accuracy: 0.5427 - val_loss: 1.4579 - val_accuracy: 0.5526 - 670ms/epoch - 6ms/step\n",
      "Epoch 25/10000\n",
      "113/113 - 1s - loss: 1.4727 - accuracy: 0.5404 - val_loss: 1.4623 - val_accuracy: 0.5307 - 679ms/epoch - 6ms/step\n",
      "Epoch 26/10000\n",
      "113/113 - 1s - loss: 1.4614 - accuracy: 0.5465 - val_loss: 1.4319 - val_accuracy: 0.5421 - 666ms/epoch - 6ms/step\n",
      "Epoch 27/10000\n",
      "113/113 - 1s - loss: 1.4443 - accuracy: 0.5501 - val_loss: 1.4126 - val_accuracy: 0.5855 - 661ms/epoch - 6ms/step\n",
      "Epoch 28/10000\n",
      "113/113 - 1s - loss: 1.4354 - accuracy: 0.5615 - val_loss: 1.4211 - val_accuracy: 0.5852 - 767ms/epoch - 7ms/step\n",
      "Epoch 29/10000\n",
      "113/113 - 1s - loss: 1.4277 - accuracy: 0.5784 - val_loss: 1.3987 - val_accuracy: 0.5904 - 754ms/epoch - 7ms/step\n",
      "Epoch 30/10000\n",
      "113/113 - 1s - loss: 1.4117 - accuracy: 0.5867 - val_loss: 1.3768 - val_accuracy: 0.5932 - 662ms/epoch - 6ms/step\n",
      "Epoch 31/10000\n",
      "113/113 - 1s - loss: 1.4074 - accuracy: 0.5809 - val_loss: 1.3765 - val_accuracy: 0.5840 - 661ms/epoch - 6ms/step\n",
      "Epoch 32/10000\n",
      "113/113 - 1s - loss: 1.3929 - accuracy: 0.5756 - val_loss: 1.3814 - val_accuracy: 0.5814 - 752ms/epoch - 7ms/step\n",
      "Epoch 33/10000\n",
      "113/113 - 1s - loss: 1.3795 - accuracy: 0.5760 - val_loss: 1.3682 - val_accuracy: 0.5835 - 661ms/epoch - 6ms/step\n",
      "Epoch 34/10000\n",
      "113/113 - 1s - loss: 1.3603 - accuracy: 0.5837 - val_loss: 1.3725 - val_accuracy: 0.5796 - 688ms/epoch - 6ms/step\n",
      "Epoch 35/10000\n",
      "113/113 - 1s - loss: 1.3503 - accuracy: 0.5859 - val_loss: 1.3515 - val_accuracy: 0.5879 - 684ms/epoch - 6ms/step\n",
      "Epoch 36/10000\n",
      "113/113 - 1s - loss: 1.3479 - accuracy: 0.5869 - val_loss: 1.3426 - val_accuracy: 0.5815 - 718ms/epoch - 6ms/step\n",
      "Epoch 37/10000\n",
      "113/113 - 1s - loss: 1.3356 - accuracy: 0.5867 - val_loss: 1.3242 - val_accuracy: 0.5912 - 685ms/epoch - 6ms/step\n",
      "Epoch 38/10000\n",
      "113/113 - 1s - loss: 1.3311 - accuracy: 0.5869 - val_loss: 1.3205 - val_accuracy: 0.5667 - 692ms/epoch - 6ms/step\n",
      "Epoch 39/10000\n",
      "113/113 - 1s - loss: 1.3123 - accuracy: 0.5885 - val_loss: 1.3167 - val_accuracy: 0.5859 - 675ms/epoch - 6ms/step\n",
      "Epoch 40/10000\n",
      "113/113 - 1s - loss: 1.2951 - accuracy: 0.5878 - val_loss: 1.2849 - val_accuracy: 0.5856 - 816ms/epoch - 7ms/step\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Iterazione 71/128 (03-11-2023_11-36-47)\n",
      "hidden_layer_size: 500\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 1\n",
      "activation_hidden: sigmoid\n",
      "activation_output: sigmoid\n",
      "batch_size: 1000\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 10\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 59.32%\n",
      "La recall di questo modello sul validation set è: 37.55%\n",
      "La f1 di questo modello sul validation set è: 41.67%\n",
      "La balanced accuracy di questo modello sul validation set è: 55.3%\n",
      "La precision di questo modello sul validation set è: 46.8%\n",
      "La AUC di questo modello sul validation set è: 56.76%\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.65    0.73      0.69  7,670.00\n",
      "1                  0.47    0.38      0.42  4,841.00\n",
      "accuracy           0.59    0.59      0.59      0.59\n",
      "macro avg          0.56    0.55      0.55 12,511.00\n",
      "weighted avg       0.58    0.59      0.58 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         5603         2067\n",
      "Actual 1         3023         1818\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 66.1% con il modello:\n",
      "66 - AC66.1%, L500, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA20, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_11-28-58).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 62.97% con il modello:\n",
      "1 - BA62.97%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 64.08% con il modello:\n",
      "13 - PR64.08%, L20, mV-10, MV10, P2, AHsigmoid, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-26-48).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "113/113 - 2s - loss: 13.3989 - accuracy: 0.4012 - val_loss: 6.6187 - val_accuracy: 0.4389 - 2s/epoch - 18ms/step\n",
      "Epoch 2/10000\n",
      "113/113 - 1s - loss: 5.2977 - accuracy: 0.4634 - val_loss: 4.2698 - val_accuracy: 0.4828 - 769ms/epoch - 7ms/step\n",
      "Epoch 3/10000\n",
      "113/113 - 1s - loss: 3.7261 - accuracy: 0.4866 - val_loss: 3.1344 - val_accuracy: 0.4980 - 809ms/epoch - 7ms/step\n",
      "Epoch 4/10000\n",
      "113/113 - 1s - loss: 2.8980 - accuracy: 0.5040 - val_loss: 2.6307 - val_accuracy: 0.5126 - 706ms/epoch - 6ms/step\n",
      "Epoch 5/10000\n",
      "113/113 - 1s - loss: 2.5100 - accuracy: 0.5140 - val_loss: 2.2977 - val_accuracy: 0.5176 - 765ms/epoch - 7ms/step\n",
      "Epoch 6/10000\n",
      "113/113 - 1s - loss: 2.1923 - accuracy: 0.5196 - val_loss: 2.0261 - val_accuracy: 0.5199 - 751ms/epoch - 7ms/step\n",
      "Epoch 7/10000\n",
      "113/113 - 1s - loss: 2.0093 - accuracy: 0.5219 - val_loss: 1.9187 - val_accuracy: 0.5273 - 706ms/epoch - 6ms/step\n",
      "Epoch 8/10000\n",
      "113/113 - 1s - loss: 1.9207 - accuracy: 0.5246 - val_loss: 1.8579 - val_accuracy: 0.5283 - 815ms/epoch - 7ms/step\n",
      "Epoch 9/10000\n",
      "113/113 - 1s - loss: 1.8523 - accuracy: 0.5275 - val_loss: 1.7856 - val_accuracy: 0.5330 - 838ms/epoch - 7ms/step\n",
      "Epoch 10/10000\n",
      "113/113 - 1s - loss: 1.7783 - accuracy: 0.5310 - val_loss: 1.7284 - val_accuracy: 0.5373 - 757ms/epoch - 7ms/step\n",
      "Epoch 11/10000\n",
      "113/113 - 1s - loss: 1.7316 - accuracy: 0.5359 - val_loss: 1.6667 - val_accuracy: 0.5346 - 617ms/epoch - 5ms/step\n",
      "Epoch 12/10000\n",
      "113/113 - 1s - loss: 1.6802 - accuracy: 0.5400 - val_loss: 1.6476 - val_accuracy: 0.5476 - 662ms/epoch - 6ms/step\n",
      "Epoch 13/10000\n",
      "113/113 - 1s - loss: 1.6603 - accuracy: 0.5412 - val_loss: 1.6144 - val_accuracy: 0.5462 - 623ms/epoch - 6ms/step\n",
      "Epoch 14/10000\n",
      "113/113 - 1s - loss: 1.6326 - accuracy: 0.5410 - val_loss: 1.5937 - val_accuracy: 0.5466 - 782ms/epoch - 7ms/step\n",
      "Epoch 15/10000\n",
      "113/113 - 1s - loss: 1.6365 - accuracy: 0.5355 - val_loss: 1.6070 - val_accuracy: 0.5434 - 628ms/epoch - 6ms/step\n",
      "Epoch 16/10000\n",
      "113/113 - 1s - loss: 1.6070 - accuracy: 0.5383 - val_loss: 1.5696 - val_accuracy: 0.5476 - 715ms/epoch - 6ms/step\n",
      "Epoch 17/10000\n",
      "113/113 - 1s - loss: 1.5818 - accuracy: 0.5397 - val_loss: 1.5667 - val_accuracy: 0.5465 - 625ms/epoch - 6ms/step\n",
      "Epoch 18/10000\n",
      "113/113 - 1s - loss: 1.5690 - accuracy: 0.5484 - val_loss: 1.5284 - val_accuracy: 0.5768 - 659ms/epoch - 6ms/step\n",
      "Epoch 19/10000\n",
      "113/113 - 1s - loss: 1.5534 - accuracy: 0.5620 - val_loss: 1.5423 - val_accuracy: 0.5456 - 650ms/epoch - 6ms/step\n",
      "Epoch 20/10000\n",
      "113/113 - 1s - loss: 1.5393 - accuracy: 0.5664 - val_loss: 1.5205 - val_accuracy: 0.5800 - 618ms/epoch - 5ms/step\n",
      "Epoch 21/10000\n",
      "113/113 - 1s - loss: 1.5272 - accuracy: 0.5588 - val_loss: 1.5271 - val_accuracy: 0.5461 - 688ms/epoch - 6ms/step\n",
      "Epoch 22/10000\n",
      "113/113 - 1s - loss: 1.5120 - accuracy: 0.5566 - val_loss: 1.4734 - val_accuracy: 0.5494 - 676ms/epoch - 6ms/step\n",
      "Epoch 23/10000\n",
      "113/113 - 1s - loss: 1.4954 - accuracy: 0.5473 - val_loss: 1.4716 - val_accuracy: 0.5537 - 701ms/epoch - 6ms/step\n",
      "Epoch 24/10000\n",
      "113/113 - 1s - loss: 1.4823 - accuracy: 0.5427 - val_loss: 1.4579 - val_accuracy: 0.5526 - 671ms/epoch - 6ms/step\n",
      "Epoch 25/10000\n",
      "113/113 - 1s - loss: 1.4727 - accuracy: 0.5404 - val_loss: 1.4623 - val_accuracy: 0.5307 - 632ms/epoch - 6ms/step\n",
      "Epoch 26/10000\n",
      "113/113 - 1s - loss: 1.4614 - accuracy: 0.5465 - val_loss: 1.4319 - val_accuracy: 0.5421 - 662ms/epoch - 6ms/step\n",
      "Epoch 27/10000\n",
      "113/113 - 1s - loss: 1.4443 - accuracy: 0.5501 - val_loss: 1.4126 - val_accuracy: 0.5855 - 664ms/epoch - 6ms/step\n",
      "Epoch 28/10000\n",
      "113/113 - 1s - loss: 1.4354 - accuracy: 0.5615 - val_loss: 1.4211 - val_accuracy: 0.5852 - 653ms/epoch - 6ms/step\n",
      "Epoch 29/10000\n",
      "113/113 - 1s - loss: 1.4277 - accuracy: 0.5784 - val_loss: 1.3987 - val_accuracy: 0.5904 - 703ms/epoch - 6ms/step\n",
      "Epoch 30/10000\n",
      "113/113 - 1s - loss: 1.4117 - accuracy: 0.5867 - val_loss: 1.3768 - val_accuracy: 0.5932 - 622ms/epoch - 6ms/step\n",
      "Epoch 31/10000\n",
      "113/113 - 1s - loss: 1.4074 - accuracy: 0.5809 - val_loss: 1.3765 - val_accuracy: 0.5840 - 722ms/epoch - 6ms/step\n",
      "Epoch 32/10000\n",
      "113/113 - 1s - loss: 1.3929 - accuracy: 0.5756 - val_loss: 1.3814 - val_accuracy: 0.5814 - 627ms/epoch - 6ms/step\n",
      "Epoch 33/10000\n",
      "113/113 - 1s - loss: 1.3795 - accuracy: 0.5760 - val_loss: 1.3682 - val_accuracy: 0.5835 - 620ms/epoch - 5ms/step\n",
      "Epoch 34/10000\n",
      "113/113 - 1s - loss: 1.3603 - accuracy: 0.5837 - val_loss: 1.3725 - val_accuracy: 0.5796 - 633ms/epoch - 6ms/step\n",
      "Epoch 35/10000\n",
      "113/113 - 1s - loss: 1.3503 - accuracy: 0.5859 - val_loss: 1.3515 - val_accuracy: 0.5879 - 750ms/epoch - 7ms/step\n",
      "Epoch 36/10000\n",
      "113/113 - 1s - loss: 1.3479 - accuracy: 0.5869 - val_loss: 1.3426 - val_accuracy: 0.5815 - 643ms/epoch - 6ms/step\n",
      "Epoch 37/10000\n",
      "113/113 - 1s - loss: 1.3356 - accuracy: 0.5867 - val_loss: 1.3242 - val_accuracy: 0.5912 - 633ms/epoch - 6ms/step\n",
      "Epoch 38/10000\n",
      "113/113 - 1s - loss: 1.3311 - accuracy: 0.5869 - val_loss: 1.3205 - val_accuracy: 0.5667 - 639ms/epoch - 6ms/step\n",
      "Epoch 39/10000\n",
      "113/113 - 1s - loss: 1.3123 - accuracy: 0.5885 - val_loss: 1.3167 - val_accuracy: 0.5859 - 699ms/epoch - 6ms/step\n",
      "Epoch 40/10000\n",
      "113/113 - 1s - loss: 1.2951 - accuracy: 0.5878 - val_loss: 1.2849 - val_accuracy: 0.5856 - 617ms/epoch - 5ms/step\n",
      "Epoch 41/10000\n",
      "113/113 - 1s - loss: 1.2856 - accuracy: 0.5883 - val_loss: 1.2795 - val_accuracy: 0.5886 - 634ms/epoch - 6ms/step\n",
      "Epoch 42/10000\n",
      "113/113 - 1s - loss: 1.2765 - accuracy: 0.5915 - val_loss: 1.2835 - val_accuracy: 0.5827 - 700ms/epoch - 6ms/step\n",
      "Epoch 43/10000\n",
      "113/113 - 1s - loss: 1.2693 - accuracy: 0.5889 - val_loss: 1.2636 - val_accuracy: 0.5862 - 683ms/epoch - 6ms/step\n",
      "Epoch 44/10000\n",
      "113/113 - 1s - loss: 1.2608 - accuracy: 0.5910 - val_loss: 1.2457 - val_accuracy: 0.5925 - 652ms/epoch - 6ms/step\n",
      "Epoch 45/10000\n",
      "113/113 - 1s - loss: 1.2555 - accuracy: 0.5914 - val_loss: 1.2353 - val_accuracy: 0.5881 - 630ms/epoch - 6ms/step\n",
      "Epoch 46/10000\n",
      "113/113 - 1s - loss: 1.2383 - accuracy: 0.5888 - val_loss: 1.2231 - val_accuracy: 0.5865 - 649ms/epoch - 6ms/step\n",
      "Epoch 47/10000\n",
      "113/113 - 1s - loss: 1.2262 - accuracy: 0.5876 - val_loss: 1.2254 - val_accuracy: 0.5872 - 716ms/epoch - 6ms/step\n",
      "Epoch 48/10000\n",
      "113/113 - 1s - loss: 1.2240 - accuracy: 0.5880 - val_loss: 1.2161 - val_accuracy: 0.5948 - 614ms/epoch - 5ms/step\n",
      "Epoch 49/10000\n",
      "113/113 - 1s - loss: 1.2131 - accuracy: 0.5882 - val_loss: 1.2197 - val_accuracy: 0.5920 - 633ms/epoch - 6ms/step\n",
      "Epoch 50/10000\n",
      "113/113 - 1s - loss: 1.2018 - accuracy: 0.5899 - val_loss: 1.1947 - val_accuracy: 0.5921 - 632ms/epoch - 6ms/step\n",
      "Epoch 51/10000\n",
      "113/113 - 1s - loss: 1.1883 - accuracy: 0.5890 - val_loss: 1.1954 - val_accuracy: 0.5865 - 704ms/epoch - 6ms/step\n",
      "Epoch 52/10000\n",
      "113/113 - 1s - loss: 1.1791 - accuracy: 0.5899 - val_loss: 1.1842 - val_accuracy: 0.5901 - 622ms/epoch - 6ms/step\n",
      "Epoch 53/10000\n",
      "113/113 - 1s - loss: 1.1671 - accuracy: 0.5879 - val_loss: 1.1660 - val_accuracy: 0.5916 - 700ms/epoch - 6ms/step\n",
      "Epoch 54/10000\n",
      "113/113 - 1s - loss: 1.1663 - accuracy: 0.5842 - val_loss: 1.1638 - val_accuracy: 0.5837 - 635ms/epoch - 6ms/step\n",
      "Epoch 55/10000\n",
      "113/113 - 1s - loss: 1.1575 - accuracy: 0.5874 - val_loss: 1.1739 - val_accuracy: 0.5825 - 698ms/epoch - 6ms/step\n",
      "Epoch 56/10000\n",
      "113/113 - 1s - loss: 1.1530 - accuracy: 0.5879 - val_loss: 1.1617 - val_accuracy: 0.5880 - 653ms/epoch - 6ms/step\n",
      "Epoch 57/10000\n",
      "113/113 - 1s - loss: 1.1538 - accuracy: 0.5866 - val_loss: 1.1579 - val_accuracy: 0.5807 - 636ms/epoch - 6ms/step\n",
      "Epoch 58/10000\n",
      "113/113 - 1s - loss: 1.1443 - accuracy: 0.5883 - val_loss: 1.1515 - val_accuracy: 0.5885 - 623ms/epoch - 6ms/step\n",
      "Epoch 59/10000\n",
      "113/113 - 1s - loss: 1.1410 - accuracy: 0.5889 - val_loss: 1.1305 - val_accuracy: 0.5864 - 688ms/epoch - 6ms/step\n",
      "Epoch 60/10000\n",
      "113/113 - 1s - loss: 1.1367 - accuracy: 0.5906 - val_loss: 1.1293 - val_accuracy: 0.5918 - 634ms/epoch - 6ms/step\n",
      "Epoch 61/10000\n",
      "113/113 - 1s - loss: 1.1283 - accuracy: 0.5945 - val_loss: 1.1364 - val_accuracy: 0.5944 - 649ms/epoch - 6ms/step\n",
      "Epoch 62/10000\n",
      "113/113 - 1s - loss: 1.1202 - accuracy: 0.5922 - val_loss: 1.1163 - val_accuracy: 0.5968 - 633ms/epoch - 6ms/step\n",
      "Epoch 63/10000\n",
      "113/113 - 1s - loss: 1.1009 - accuracy: 0.5946 - val_loss: 1.1104 - val_accuracy: 0.5956 - 693ms/epoch - 6ms/step\n",
      "Epoch 64/10000\n",
      "113/113 - 1s - loss: 1.0927 - accuracy: 0.5972 - val_loss: 1.0827 - val_accuracy: 0.5999 - 705ms/epoch - 6ms/step\n",
      "Epoch 65/10000\n",
      "113/113 - 1s - loss: 1.0914 - accuracy: 0.5960 - val_loss: 1.0822 - val_accuracy: 0.5963 - 656ms/epoch - 6ms/step\n",
      "Epoch 66/10000\n",
      "113/113 - 1s - loss: 1.0788 - accuracy: 0.5977 - val_loss: 1.0730 - val_accuracy: 0.6004 - 651ms/epoch - 6ms/step\n",
      "Epoch 67/10000\n",
      "113/113 - 1s - loss: 1.0701 - accuracy: 0.5995 - val_loss: 1.0780 - val_accuracy: 0.5980 - 713ms/epoch - 6ms/step\n",
      "Epoch 68/10000\n",
      "113/113 - 1s - loss: 1.0673 - accuracy: 0.5982 - val_loss: 1.0760 - val_accuracy: 0.5980 - 660ms/epoch - 6ms/step\n",
      "Epoch 69/10000\n",
      "113/113 - 1s - loss: 1.0707 - accuracy: 0.5937 - val_loss: 1.0808 - val_accuracy: 0.5992 - 605ms/epoch - 5ms/step\n",
      "Epoch 70/10000\n",
      "113/113 - 1s - loss: 1.0675 - accuracy: 0.5952 - val_loss: 1.0721 - val_accuracy: 0.5980 - 650ms/epoch - 6ms/step\n",
      "Epoch 71/10000\n",
      "113/113 - 1s - loss: 1.0542 - accuracy: 0.5969 - val_loss: 1.0477 - val_accuracy: 0.5912 - 750ms/epoch - 7ms/step\n",
      "Epoch 72/10000\n",
      "113/113 - 1s - loss: 1.0501 - accuracy: 0.5938 - val_loss: 1.0465 - val_accuracy: 0.5968 - 640ms/epoch - 6ms/step\n",
      "Epoch 73/10000\n",
      "113/113 - 1s - loss: 1.0423 - accuracy: 0.5963 - val_loss: 1.0581 - val_accuracy: 0.5965 - 632ms/epoch - 6ms/step\n",
      "Epoch 74/10000\n",
      "113/113 - 1s - loss: 1.0374 - accuracy: 0.5966 - val_loss: 1.0385 - val_accuracy: 0.5972 - 635ms/epoch - 6ms/step\n",
      "Epoch 75/10000\n",
      "113/113 - 1s - loss: 1.0322 - accuracy: 0.5977 - val_loss: 1.0442 - val_accuracy: 0.5989 - 680ms/epoch - 6ms/step\n",
      "Epoch 76/10000\n",
      "113/113 - 1s - loss: 1.0310 - accuracy: 0.5949 - val_loss: 1.0286 - val_accuracy: 0.5958 - 667ms/epoch - 6ms/step\n",
      "Epoch 77/10000\n",
      "113/113 - 1s - loss: 1.0274 - accuracy: 0.5950 - val_loss: 1.0275 - val_accuracy: 0.6023 - 669ms/epoch - 6ms/step\n",
      "Epoch 78/10000\n",
      "113/113 - 1s - loss: 1.0230 - accuracy: 0.5969 - val_loss: 1.0244 - val_accuracy: 0.5899 - 682ms/epoch - 6ms/step\n",
      "Epoch 79/10000\n",
      "113/113 - 1s - loss: 1.0211 - accuracy: 0.5953 - val_loss: 1.0142 - val_accuracy: 0.6000 - 700ms/epoch - 6ms/step\n",
      "Epoch 80/10000\n",
      "113/113 - 1s - loss: 1.0146 - accuracy: 0.5970 - val_loss: 1.0206 - val_accuracy: 0.6054 - 637ms/epoch - 6ms/step\n",
      "Epoch 81/10000\n",
      "113/113 - 1s - loss: 1.0100 - accuracy: 0.5968 - val_loss: 1.0297 - val_accuracy: 0.5958 - 635ms/epoch - 6ms/step\n",
      "Epoch 82/10000\n",
      "113/113 - 1s - loss: 1.0021 - accuracy: 0.5956 - val_loss: 1.0120 - val_accuracy: 0.6034 - 633ms/epoch - 6ms/step\n",
      "Epoch 83/10000\n",
      "113/113 - 1s - loss: 0.9988 - accuracy: 0.5978 - val_loss: 1.0137 - val_accuracy: 0.5948 - 761ms/epoch - 7ms/step\n",
      "Epoch 84/10000\n",
      "113/113 - 1s - loss: 0.9983 - accuracy: 0.5944 - val_loss: 1.0039 - val_accuracy: 0.5923 - 634ms/epoch - 6ms/step\n",
      "Epoch 85/10000\n",
      "113/113 - 1s - loss: 0.9933 - accuracy: 0.5949 - val_loss: 1.0036 - val_accuracy: 0.6004 - 655ms/epoch - 6ms/step\n",
      "Epoch 86/10000\n",
      "113/113 - 1s - loss: 0.9916 - accuracy: 0.5950 - val_loss: 1.0022 - val_accuracy: 0.6045 - 609ms/epoch - 5ms/step\n",
      "Epoch 87/10000\n",
      "113/113 - 1s - loss: 0.9858 - accuracy: 0.5967 - val_loss: 0.9787 - val_accuracy: 0.6029 - 711ms/epoch - 6ms/step\n",
      "Epoch 88/10000\n",
      "113/113 - 1s - loss: 0.9827 - accuracy: 0.5964 - val_loss: 0.9798 - val_accuracy: 0.6051 - 622ms/epoch - 6ms/step\n",
      "Epoch 89/10000\n",
      "113/113 - 1s - loss: 0.9786 - accuracy: 0.5987 - val_loss: 0.9821 - val_accuracy: 0.6013 - 628ms/epoch - 6ms/step\n",
      "Epoch 90/10000\n",
      "113/113 - 1s - loss: 0.9769 - accuracy: 0.5951 - val_loss: 0.9747 - val_accuracy: 0.6032 - 683ms/epoch - 6ms/step\n",
      "Epoch 91/10000\n",
      "113/113 - 1s - loss: 0.9685 - accuracy: 0.6004 - val_loss: 0.9768 - val_accuracy: 0.5911 - 703ms/epoch - 6ms/step\n",
      "Epoch 92/10000\n",
      "113/113 - 1s - loss: 0.9650 - accuracy: 0.6013 - val_loss: 0.9719 - val_accuracy: 0.5951 - 634ms/epoch - 6ms/step\n",
      "Epoch 93/10000\n",
      "113/113 - 1s - loss: 0.9622 - accuracy: 0.5996 - val_loss: 0.9704 - val_accuracy: 0.6043 - 628ms/epoch - 6ms/step\n",
      "Epoch 94/10000\n",
      "113/113 - 1s - loss: 0.9637 - accuracy: 0.5982 - val_loss: 0.9536 - val_accuracy: 0.6075 - 621ms/epoch - 5ms/step\n",
      "Epoch 95/10000\n",
      "113/113 - 1s - loss: 0.9609 - accuracy: 0.5993 - val_loss: 0.9560 - val_accuracy: 0.6084 - 685ms/epoch - 6ms/step\n",
      "Epoch 96/10000\n",
      "113/113 - 1s - loss: 0.9570 - accuracy: 0.6018 - val_loss: 0.9557 - val_accuracy: 0.6063 - 620ms/epoch - 5ms/step\n",
      "Epoch 97/10000\n",
      "113/113 - 1s - loss: 0.9565 - accuracy: 0.6018 - val_loss: 0.9580 - val_accuracy: 0.5983 - 710ms/epoch - 6ms/step\n",
      "Epoch 98/10000\n",
      "113/113 - 1s - loss: 0.9488 - accuracy: 0.5997 - val_loss: 0.9595 - val_accuracy: 0.6016 - 629ms/epoch - 6ms/step\n",
      "Epoch 99/10000\n",
      "113/113 - 1s - loss: 0.9462 - accuracy: 0.6019 - val_loss: 0.9569 - val_accuracy: 0.5976 - 689ms/epoch - 6ms/step\n",
      "Epoch 100/10000\n",
      "113/113 - 1s - loss: 0.9410 - accuracy: 0.5976 - val_loss: 0.9565 - val_accuracy: 0.5953 - 622ms/epoch - 6ms/step\n",
      "Epoch 101/10000\n",
      "113/113 - 1s - loss: 0.9448 - accuracy: 0.5966 - val_loss: 0.9477 - val_accuracy: 0.6051 - 630ms/epoch - 6ms/step\n",
      "Epoch 102/10000\n",
      "113/113 - 1s - loss: 0.9410 - accuracy: 0.5993 - val_loss: 0.9444 - val_accuracy: 0.5993 - 617ms/epoch - 5ms/step\n",
      "Epoch 103/10000\n",
      "113/113 - 1s - loss: 0.9410 - accuracy: 0.5968 - val_loss: 0.9449 - val_accuracy: 0.5935 - 739ms/epoch - 7ms/step\n",
      "Epoch 104/10000\n",
      "113/113 - 1s - loss: 0.9390 - accuracy: 0.5995 - val_loss: 0.9441 - val_accuracy: 0.6059 - 638ms/epoch - 6ms/step\n",
      "Epoch 105/10000\n",
      "113/113 - 1s - loss: 0.9365 - accuracy: 0.5991 - val_loss: 0.9266 - val_accuracy: 0.6079 - 716ms/epoch - 6ms/step\n",
      "Epoch 106/10000\n",
      "113/113 - 1s - loss: 0.9352 - accuracy: 0.5998 - val_loss: 0.9207 - val_accuracy: 0.6044 - 660ms/epoch - 6ms/step\n",
      "Epoch 107/10000\n",
      "113/113 - 1s - loss: 0.9243 - accuracy: 0.5973 - val_loss: 0.9259 - val_accuracy: 0.6041 - 701ms/epoch - 6ms/step\n",
      "Epoch 108/10000\n",
      "113/113 - 1s - loss: 0.9184 - accuracy: 0.5985 - val_loss: 0.9216 - val_accuracy: 0.6027 - 666ms/epoch - 6ms/step\n",
      "Epoch 109/10000\n",
      "113/113 - 1s - loss: 0.9192 - accuracy: 0.5981 - val_loss: 0.9332 - val_accuracy: 0.6026 - 651ms/epoch - 6ms/step\n",
      "Epoch 110/10000\n",
      "113/113 - 1s - loss: 0.9183 - accuracy: 0.6001 - val_loss: 0.9173 - val_accuracy: 0.5981 - 746ms/epoch - 7ms/step\n",
      "Epoch 111/10000\n",
      "113/113 - 1s - loss: 0.9159 - accuracy: 0.6003 - val_loss: 0.9170 - val_accuracy: 0.6043 - 696ms/epoch - 6ms/step\n",
      "Epoch 112/10000\n",
      "113/113 - 1s - loss: 0.9065 - accuracy: 0.5991 - val_loss: 0.9150 - val_accuracy: 0.5952 - 652ms/epoch - 6ms/step\n",
      "Epoch 113/10000\n",
      "113/113 - 1s - loss: 0.9018 - accuracy: 0.5990 - val_loss: 0.9124 - val_accuracy: 0.6032 - 618ms/epoch - 5ms/step\n",
      "Epoch 114/10000\n",
      "113/113 - 1s - loss: 0.9025 - accuracy: 0.5982 - val_loss: 0.9132 - val_accuracy: 0.6002 - 633ms/epoch - 6ms/step\n",
      "Epoch 115/10000\n",
      "113/113 - 1s - loss: 0.9064 - accuracy: 0.5998 - val_loss: 0.9204 - val_accuracy: 0.5932 - 705ms/epoch - 6ms/step\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Iterazione 72/128 (03-11-2023_11-38-08)\n",
      "hidden_layer_size: 500\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 1\n",
      "activation_hidden: sigmoid\n",
      "activation_output: sigmoid\n",
      "batch_size: 1000\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 20\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 60.84%\n",
      "La recall di questo modello sul validation set è: 38.73%\n",
      "La f1 di questo modello sul validation set è: 43.36%\n",
      "La balanced accuracy di questo modello sul validation set è: 56.76%\n",
      "La precision di questo modello sul validation set è: 49.24%\n",
      "La AUC di questo modello sul validation set è: 60.04%\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.66    0.75      0.70  7,670.00\n",
      "1                  0.49    0.39      0.43  4,841.00\n",
      "accuracy           0.61    0.61      0.61      0.61\n",
      "macro avg          0.58    0.57      0.57 12,511.00\n",
      "weighted avg       0.59    0.61      0.60 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         5737         1933\n",
      "Actual 1         2966         1875\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 66.1% con il modello:\n",
      "66 - AC66.1%, L500, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA20, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_11-28-58).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 62.97% con il modello:\n",
      "1 - BA62.97%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 64.08% con il modello:\n",
      "13 - PR64.08%, L20, mV-10, MV10, P2, AHsigmoid, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-26-48).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "1126/1126 - 24s - loss: 1156177.1250 - accuracy: 0.5725 - val_loss: 2096297.0000 - val_accuracy: 0.6126 - 24s/epoch - 21ms/step\n",
      "Epoch 2/10000\n",
      "1126/1126 - 8s - loss: 767425.6875 - accuracy: 0.5732 - val_loss: 581007.0625 - val_accuracy: 0.5099 - 8s/epoch - 7ms/step\n",
      "Epoch 3/10000\n",
      "1126/1126 - 7s - loss: 720477.6875 - accuracy: 0.5755 - val_loss: 990725.4375 - val_accuracy: 0.4399 - 7s/epoch - 6ms/step\n",
      "Epoch 4/10000\n",
      "1126/1126 - 12s - loss: 732470.5625 - accuracy: 0.5731 - val_loss: 359983.6875 - val_accuracy: 0.6349 - 12s/epoch - 11ms/step\n",
      "Epoch 5/10000\n",
      "1126/1126 - 13s - loss: 691178.8750 - accuracy: 0.5739 - val_loss: 220228.5938 - val_accuracy: 0.6362 - 13s/epoch - 12ms/step\n",
      "Epoch 6/10000\n",
      "1126/1126 - 10s - loss: 680929.4375 - accuracy: 0.5749 - val_loss: 412409.6875 - val_accuracy: 0.5125 - 10s/epoch - 9ms/step\n",
      "Epoch 7/10000\n",
      "1126/1126 - 8s - loss: 631347.4375 - accuracy: 0.5753 - val_loss: 173091.1406 - val_accuracy: 0.6307 - 8s/epoch - 7ms/step\n",
      "Epoch 8/10000\n",
      "1126/1126 - 8s - loss: 648597.5000 - accuracy: 0.5741 - val_loss: 320328.0312 - val_accuracy: 0.6305 - 8s/epoch - 7ms/step\n",
      "Epoch 9/10000\n",
      "1126/1126 - 7s - loss: 627513.6250 - accuracy: 0.5778 - val_loss: 1085492.7500 - val_accuracy: 0.6137 - 7s/epoch - 6ms/step\n",
      "Epoch 10/10000\n",
      "1126/1126 - 7s - loss: 679304.1250 - accuracy: 0.5769 - val_loss: 588152.8750 - val_accuracy: 0.6223 - 7s/epoch - 6ms/step\n",
      "Epoch 11/10000\n",
      "1126/1126 - 7s - loss: 665117.3750 - accuracy: 0.5786 - val_loss: 793047.1875 - val_accuracy: 0.5563 - 7s/epoch - 6ms/step\n",
      "Epoch 12/10000\n",
      "1126/1126 - 7s - loss: 619960.2500 - accuracy: 0.5792 - val_loss: 221046.3281 - val_accuracy: 0.6481 - 7s/epoch - 6ms/step\n",
      "Epoch 13/10000\n",
      "1126/1126 - 7s - loss: 632629.8750 - accuracy: 0.5787 - val_loss: 257013.8750 - val_accuracy: 0.6027 - 7s/epoch - 6ms/step\n",
      "Epoch 14/10000\n",
      "1126/1126 - 7s - loss: 703785.1875 - accuracy: 0.5787 - val_loss: 311863.5938 - val_accuracy: 0.5689 - 7s/epoch - 6ms/step\n",
      "Epoch 15/10000\n",
      "1126/1126 - 7s - loss: 708230.2500 - accuracy: 0.5795 - val_loss: 379014.5938 - val_accuracy: 0.6327 - 7s/epoch - 6ms/step\n",
      "Epoch 16/10000\n",
      "1126/1126 - 7s - loss: 735666.0625 - accuracy: 0.5804 - val_loss: 184386.5156 - val_accuracy: 0.6326 - 7s/epoch - 6ms/step\n",
      "Epoch 17/10000\n",
      "1126/1126 - 7s - loss: 619683.2500 - accuracy: 0.5818 - val_loss: 1312654.8750 - val_accuracy: 0.6199 - 7s/epoch - 6ms/step\n",
      "Epoch 18/10000\n",
      "1126/1126 - 7s - loss: 629990.1875 - accuracy: 0.5809 - val_loss: 441996.5625 - val_accuracy: 0.5972 - 7s/epoch - 6ms/step\n",
      "Epoch 19/10000\n",
      "1126/1126 - 7s - loss: 674066.8125 - accuracy: 0.5832 - val_loss: 160378.0625 - val_accuracy: 0.6223 - 7s/epoch - 6ms/step\n",
      "Epoch 20/10000\n",
      "1126/1126 - 7s - loss: 693422.6250 - accuracy: 0.5808 - val_loss: 400239.7812 - val_accuracy: 0.6287 - 7s/epoch - 6ms/step\n",
      "Epoch 21/10000\n",
      "1126/1126 - 7s - loss: 624594.9375 - accuracy: 0.5834 - val_loss: 832125.5000 - val_accuracy: 0.6227 - 7s/epoch - 6ms/step\n",
      "Epoch 22/10000\n",
      "1126/1126 - 7s - loss: 625678.2500 - accuracy: 0.5827 - val_loss: 384820.2812 - val_accuracy: 0.6087 - 7s/epoch - 6ms/step\n",
      "391/391 [==============================] - 2s 4ms/step\n",
      "\n",
      "Iterazione 73/128 (03-11-2023_11-41-19)\n",
      "hidden_layer_size: 500\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 2\n",
      "activation_hidden: relu\n",
      "activation_output: sigmoid\n",
      "batch_size: 100\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 10\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 64.81%\n",
      "La recall di questo modello sul validation set è: 32.6%\n",
      "La f1 di questo modello sul validation set è: 41.75%\n",
      "La balanced accuracy di questo modello sul validation set è: 58.87%\n",
      "La precision di questo modello sul validation set è: 58.06%\n",
      "La AUC di questo modello sul validation set è: 58.87%\n",
      "391/391 [==============================] - 1s 4ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.67    0.85      0.75  7,670.00\n",
      "1                  0.58    0.33      0.42  4,841.00\n",
      "accuracy           0.65    0.65      0.65      0.65\n",
      "macro avg          0.62    0.59      0.58 12,511.00\n",
      "weighted avg       0.63    0.65      0.62 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         6530         1140\n",
      "Actual 1         3263         1578\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 66.1% con il modello:\n",
      "66 - AC66.1%, L500, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA20, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_11-28-58).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 62.97% con il modello:\n",
      "1 - BA62.97%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 64.08% con il modello:\n",
      "13 - PR64.08%, L20, mV-10, MV10, P2, AHsigmoid, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-26-48).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "1126/1126 - 10s - loss: 1156177.1250 - accuracy: 0.5725 - val_loss: 2096297.0000 - val_accuracy: 0.6126 - 10s/epoch - 9ms/step\n",
      "Epoch 2/10000\n",
      "1126/1126 - 8s - loss: 767425.6875 - accuracy: 0.5732 - val_loss: 581007.0625 - val_accuracy: 0.5099 - 8s/epoch - 7ms/step\n",
      "Epoch 3/10000\n",
      "1126/1126 - 7s - loss: 720477.6875 - accuracy: 0.5755 - val_loss: 990725.4375 - val_accuracy: 0.4399 - 7s/epoch - 6ms/step\n",
      "Epoch 4/10000\n",
      "1126/1126 - 7s - loss: 732470.5625 - accuracy: 0.5731 - val_loss: 359983.6875 - val_accuracy: 0.6349 - 7s/epoch - 6ms/step\n",
      "Epoch 5/10000\n",
      "1126/1126 - 7s - loss: 691178.8750 - accuracy: 0.5739 - val_loss: 220228.5938 - val_accuracy: 0.6362 - 7s/epoch - 6ms/step\n",
      "Epoch 6/10000\n",
      "1126/1126 - 7s - loss: 680929.4375 - accuracy: 0.5749 - val_loss: 412409.6875 - val_accuracy: 0.5125 - 7s/epoch - 6ms/step\n",
      "Epoch 7/10000\n",
      "1126/1126 - 7s - loss: 631347.4375 - accuracy: 0.5753 - val_loss: 173091.1406 - val_accuracy: 0.6307 - 7s/epoch - 6ms/step\n",
      "Epoch 8/10000\n",
      "1126/1126 - 7s - loss: 648597.5000 - accuracy: 0.5741 - val_loss: 320328.0312 - val_accuracy: 0.6305 - 7s/epoch - 6ms/step\n",
      "Epoch 9/10000\n",
      "1126/1126 - 7s - loss: 627513.6250 - accuracy: 0.5778 - val_loss: 1085492.7500 - val_accuracy: 0.6137 - 7s/epoch - 6ms/step\n",
      "Epoch 10/10000\n",
      "1126/1126 - 7s - loss: 679304.1250 - accuracy: 0.5769 - val_loss: 588152.8750 - val_accuracy: 0.6223 - 7s/epoch - 7ms/step\n",
      "Epoch 11/10000\n",
      "1126/1126 - 7s - loss: 665117.3750 - accuracy: 0.5786 - val_loss: 793047.1875 - val_accuracy: 0.5563 - 7s/epoch - 6ms/step\n",
      "Epoch 12/10000\n",
      "1126/1126 - 6s - loss: 619960.2500 - accuracy: 0.5792 - val_loss: 221046.3281 - val_accuracy: 0.6481 - 6s/epoch - 6ms/step\n",
      "Epoch 13/10000\n",
      "1126/1126 - 6s - loss: 632629.8750 - accuracy: 0.5787 - val_loss: 257013.8750 - val_accuracy: 0.6027 - 6s/epoch - 6ms/step\n",
      "Epoch 14/10000\n",
      "1126/1126 - 7s - loss: 703785.1875 - accuracy: 0.5787 - val_loss: 311863.5938 - val_accuracy: 0.5689 - 7s/epoch - 6ms/step\n",
      "Epoch 15/10000\n",
      "1126/1126 - 7s - loss: 708230.2500 - accuracy: 0.5795 - val_loss: 379014.5938 - val_accuracy: 0.6327 - 7s/epoch - 6ms/step\n",
      "Epoch 16/10000\n",
      "1126/1126 - 7s - loss: 735666.0625 - accuracy: 0.5804 - val_loss: 184386.5156 - val_accuracy: 0.6326 - 7s/epoch - 6ms/step\n",
      "Epoch 17/10000\n",
      "1126/1126 - 7s - loss: 619683.2500 - accuracy: 0.5818 - val_loss: 1312654.8750 - val_accuracy: 0.6199 - 7s/epoch - 6ms/step\n",
      "Epoch 18/10000\n",
      "1126/1126 - 6s - loss: 629990.1875 - accuracy: 0.5809 - val_loss: 441996.5625 - val_accuracy: 0.5972 - 6s/epoch - 6ms/step\n",
      "Epoch 19/10000\n",
      "1126/1126 - 7s - loss: 674066.8125 - accuracy: 0.5832 - val_loss: 160378.0625 - val_accuracy: 0.6223 - 7s/epoch - 6ms/step\n",
      "Epoch 20/10000\n",
      "1126/1126 - 6s - loss: 693422.6250 - accuracy: 0.5808 - val_loss: 400239.7812 - val_accuracy: 0.6287 - 6s/epoch - 6ms/step\n",
      "Epoch 21/10000\n",
      "1126/1126 - 7s - loss: 624594.9375 - accuracy: 0.5834 - val_loss: 832125.5000 - val_accuracy: 0.6227 - 7s/epoch - 6ms/step\n",
      "Epoch 22/10000\n",
      "1126/1126 - 6s - loss: 625678.2500 - accuracy: 0.5827 - val_loss: 384820.2812 - val_accuracy: 0.6087 - 6s/epoch - 6ms/step\n",
      "Epoch 23/10000\n",
      "1126/1126 - 7s - loss: 662471.8750 - accuracy: 0.5809 - val_loss: 1154401.8750 - val_accuracy: 0.4727 - 7s/epoch - 6ms/step\n",
      "Epoch 24/10000\n",
      "1126/1126 - 7s - loss: 701382.6875 - accuracy: 0.5834 - val_loss: 677863.3125 - val_accuracy: 0.6262 - 7s/epoch - 6ms/step\n",
      "Epoch 25/10000\n",
      "1126/1126 - 7s - loss: 630954.2500 - accuracy: 0.5827 - val_loss: 174150.9688 - val_accuracy: 0.6431 - 7s/epoch - 6ms/step\n",
      "Epoch 26/10000\n",
      "1126/1126 - 7s - loss: 684994.1250 - accuracy: 0.5834 - val_loss: 352043.4062 - val_accuracy: 0.6333 - 7s/epoch - 6ms/step\n",
      "Epoch 27/10000\n",
      "1126/1126 - 7s - loss: 646557.7500 - accuracy: 0.5839 - val_loss: 635584.5000 - val_accuracy: 0.4724 - 7s/epoch - 6ms/step\n",
      "Epoch 28/10000\n",
      "1126/1126 - 6s - loss: 581158.4375 - accuracy: 0.5868 - val_loss: 323102.0312 - val_accuracy: 0.6306 - 6s/epoch - 6ms/step\n",
      "Epoch 29/10000\n",
      "1126/1126 - 6s - loss: 717705.1875 - accuracy: 0.5828 - val_loss: 341937.4688 - val_accuracy: 0.6367 - 6s/epoch - 6ms/step\n",
      "Epoch 30/10000\n",
      "1126/1126 - 6s - loss: 712857.8125 - accuracy: 0.5829 - val_loss: 591893.6250 - val_accuracy: 0.6243 - 6s/epoch - 6ms/step\n",
      "Epoch 31/10000\n",
      "1126/1126 - 7s - loss: 552586.6875 - accuracy: 0.5853 - val_loss: 2306475.0000 - val_accuracy: 0.4100 - 7s/epoch - 6ms/step\n",
      "Epoch 32/10000\n",
      "1126/1126 - 7s - loss: 575484.9375 - accuracy: 0.5878 - val_loss: 1524711.2500 - val_accuracy: 0.4200 - 7s/epoch - 6ms/step\n",
      "391/391 [==============================] - 2s 4ms/step\n",
      "\n",
      "Iterazione 74/128 (03-11-2023_11-45-00)\n",
      "hidden_layer_size: 500\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 2\n",
      "activation_hidden: relu\n",
      "activation_output: sigmoid\n",
      "batch_size: 100\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 20\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 64.81%\n",
      "La recall di questo modello sul validation set è: 32.6%\n",
      "La f1 di questo modello sul validation set è: 41.75%\n",
      "La balanced accuracy di questo modello sul validation set è: 58.87%\n",
      "La precision di questo modello sul validation set è: 58.06%\n",
      "La AUC di questo modello sul validation set è: 58.87%\n",
      "391/391 [==============================] - 1s 3ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.67    0.85      0.75  7,670.00\n",
      "1                  0.58    0.33      0.42  4,841.00\n",
      "accuracy           0.65    0.65      0.65      0.65\n",
      "macro avg          0.62    0.59      0.58 12,511.00\n",
      "weighted avg       0.63    0.65      0.62 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         6530         1140\n",
      "Actual 1         3263         1578\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 66.1% con il modello:\n",
      "66 - AC66.1%, L500, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA20, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_11-28-58).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 62.97% con il modello:\n",
      "1 - BA62.97%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 64.08% con il modello:\n",
      "13 - PR64.08%, L20, mV-10, MV10, P2, AHsigmoid, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-26-48).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "113/113 - 4s - loss: 4397611.0000 - accuracy: 0.5355 - val_loss: 599930.4375 - val_accuracy: 0.5683 - 4s/epoch - 37ms/step\n",
      "Epoch 2/10000\n",
      "113/113 - 2s - loss: 641767.9375 - accuracy: 0.5850 - val_loss: 576173.8750 - val_accuracy: 0.6087 - 2s/epoch - 22ms/step\n",
      "Epoch 3/10000\n",
      "113/113 - 3s - loss: 653875.2500 - accuracy: 0.5841 - val_loss: 532997.7500 - val_accuracy: 0.6181 - 3s/epoch - 22ms/step\n",
      "Epoch 4/10000\n",
      "113/113 - 2s - loss: 685082.6250 - accuracy: 0.5737 - val_loss: 592701.3125 - val_accuracy: 0.6162 - 2s/epoch - 22ms/step\n",
      "Epoch 5/10000\n",
      "113/113 - 2s - loss: 566239.4375 - accuracy: 0.5825 - val_loss: 1102273.5000 - val_accuracy: 0.6116 - 2s/epoch - 21ms/step\n",
      "Epoch 6/10000\n",
      "113/113 - 3s - loss: 518369.4375 - accuracy: 0.5835 - val_loss: 234187.0781 - val_accuracy: 0.6086 - 3s/epoch - 22ms/step\n",
      "Epoch 7/10000\n",
      "113/113 - 2s - loss: 432048.6250 - accuracy: 0.5817 - val_loss: 249694.0156 - val_accuracy: 0.6198 - 2s/epoch - 21ms/step\n",
      "Epoch 8/10000\n",
      "113/113 - 2s - loss: 816055.4375 - accuracy: 0.5734 - val_loss: 294692.7812 - val_accuracy: 0.5714 - 2s/epoch - 21ms/step\n",
      "Epoch 9/10000\n",
      "113/113 - 2s - loss: 405605.0938 - accuracy: 0.5776 - val_loss: 392121.9375 - val_accuracy: 0.6175 - 2s/epoch - 21ms/step\n",
      "Epoch 10/10000\n",
      "113/113 - 3s - loss: 688258.3750 - accuracy: 0.5728 - val_loss: 634032.5625 - val_accuracy: 0.4980 - 3s/epoch - 24ms/step\n",
      "Epoch 11/10000\n",
      "113/113 - 3s - loss: 700226.1875 - accuracy: 0.5742 - val_loss: 492535.9688 - val_accuracy: 0.6171 - 3s/epoch - 24ms/step\n",
      "Epoch 12/10000\n",
      "113/113 - 3s - loss: 682750.8125 - accuracy: 0.5769 - val_loss: 350288.9062 - val_accuracy: 0.5910 - 3s/epoch - 24ms/step\n",
      "Epoch 13/10000\n",
      "113/113 - 3s - loss: 533397.6250 - accuracy: 0.5797 - val_loss: 751919.5000 - val_accuracy: 0.6151 - 3s/epoch - 24ms/step\n",
      "Epoch 14/10000\n",
      "113/113 - 3s - loss: 655762.2500 - accuracy: 0.5765 - val_loss: 319805.7188 - val_accuracy: 0.6179 - 3s/epoch - 23ms/step\n",
      "Epoch 15/10000\n",
      "113/113 - 3s - loss: 764007.1875 - accuracy: 0.5670 - val_loss: 579320.7500 - val_accuracy: 0.6234 - 3s/epoch - 23ms/step\n",
      "Epoch 16/10000\n",
      "113/113 - 2s - loss: 664016.6250 - accuracy: 0.5751 - val_loss: 486140.1250 - val_accuracy: 0.6214 - 2s/epoch - 21ms/step\n",
      "Epoch 17/10000\n",
      "113/113 - 2s - loss: 524791.2500 - accuracy: 0.5801 - val_loss: 338803.5000 - val_accuracy: 0.5097 - 2s/epoch - 21ms/step\n",
      "Epoch 18/10000\n",
      "113/113 - 2s - loss: 645311.0625 - accuracy: 0.5679 - val_loss: 555812.7500 - val_accuracy: 0.6244 - 2s/epoch - 20ms/step\n",
      "Epoch 19/10000\n",
      "113/113 - 2s - loss: 693835.6250 - accuracy: 0.5724 - val_loss: 412998.9688 - val_accuracy: 0.6270 - 2s/epoch - 21ms/step\n",
      "Epoch 20/10000\n",
      "113/113 - 2s - loss: 813821.1875 - accuracy: 0.5684 - val_loss: 300370.0000 - val_accuracy: 0.5920 - 2s/epoch - 20ms/step\n",
      "Epoch 21/10000\n",
      "113/113 - 2s - loss: 907653.5625 - accuracy: 0.5681 - val_loss: 1069041.1250 - val_accuracy: 0.6175 - 2s/epoch - 20ms/step\n",
      "Epoch 22/10000\n",
      "113/113 - 2s - loss: 741815.9375 - accuracy: 0.5755 - val_loss: 687841.0625 - val_accuracy: 0.6209 - 2s/epoch - 21ms/step\n",
      "Epoch 23/10000\n",
      "113/113 - 3s - loss: 417703.7188 - accuracy: 0.5846 - val_loss: 154146.7188 - val_accuracy: 0.6033 - 3s/epoch - 23ms/step\n",
      "Epoch 24/10000\n",
      "113/113 - 3s - loss: 878995.0000 - accuracy: 0.5713 - val_loss: 1846750.2500 - val_accuracy: 0.4124 - 3s/epoch - 24ms/step\n",
      "Epoch 25/10000\n",
      "113/113 - 3s - loss: 725683.2500 - accuracy: 0.5747 - val_loss: 280408.9688 - val_accuracy: 0.5913 - 3s/epoch - 23ms/step\n",
      "Epoch 26/10000\n",
      "113/113 - 3s - loss: 535845.1875 - accuracy: 0.5726 - val_loss: 1402127.8750 - val_accuracy: 0.6184 - 3s/epoch - 24ms/step\n",
      "Epoch 27/10000\n",
      "113/113 - 3s - loss: 677184.0625 - accuracy: 0.5737 - val_loss: 434710.0938 - val_accuracy: 0.5339 - 3s/epoch - 23ms/step\n",
      "Epoch 28/10000\n",
      "113/113 - 3s - loss: 346653.4375 - accuracy: 0.5846 - val_loss: 506900.2188 - val_accuracy: 0.6178 - 3s/epoch - 24ms/step\n",
      "Epoch 29/10000\n",
      "113/113 - 3s - loss: 701746.3125 - accuracy: 0.5687 - val_loss: 924513.0000 - val_accuracy: 0.6126 - 3s/epoch - 23ms/step\n",
      "391/391 [==============================] - 2s 5ms/step\n",
      "\n",
      "Iterazione 75/128 (03-11-2023_11-46-19)\n",
      "hidden_layer_size: 500\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 2\n",
      "activation_hidden: relu\n",
      "activation_output: sigmoid\n",
      "batch_size: 1000\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 10\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 62.7%\n",
      "La recall di questo modello sul validation set è: 9.4%\n",
      "La f1 di questo modello sul validation set è: 16.32%\n",
      "La balanced accuracy di questo modello sul validation set è: 52.87%\n",
      "La precision di questo modello sul validation set è: 61.82%\n",
      "La AUC di questo modello sul validation set è: 52.87%\n",
      "391/391 [==============================] - 2s 5ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.63    0.96      0.76  7,670.00\n",
      "1                  0.62    0.09      0.16  4,841.00\n",
      "accuracy           0.63    0.63      0.63      0.63\n",
      "macro avg          0.62    0.53      0.46 12,511.00\n",
      "weighted avg       0.62    0.63      0.53 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         7389          281\n",
      "Actual 1         4386          455\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 66.1% con il modello:\n",
      "66 - AC66.1%, L500, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA20, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_11-28-58).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 62.97% con il modello:\n",
      "1 - BA62.97%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 64.08% con il modello:\n",
      "13 - PR64.08%, L20, mV-10, MV10, P2, AHsigmoid, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-26-48).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "113/113 - 5s - loss: 4397611.0000 - accuracy: 0.5355 - val_loss: 599930.4375 - val_accuracy: 0.5683 - 5s/epoch - 44ms/step\n",
      "Epoch 2/10000\n",
      "113/113 - 3s - loss: 641767.9375 - accuracy: 0.5850 - val_loss: 576173.8750 - val_accuracy: 0.6087 - 3s/epoch - 25ms/step\n",
      "Epoch 3/10000\n",
      "113/113 - 3s - loss: 653875.2500 - accuracy: 0.5841 - val_loss: 532997.7500 - val_accuracy: 0.6181 - 3s/epoch - 25ms/step\n",
      "Epoch 4/10000\n",
      "113/113 - 3s - loss: 685082.6250 - accuracy: 0.5737 - val_loss: 592701.3125 - val_accuracy: 0.6162 - 3s/epoch - 26ms/step\n",
      "Epoch 5/10000\n",
      "113/113 - 3s - loss: 566239.4375 - accuracy: 0.5825 - val_loss: 1102273.5000 - val_accuracy: 0.6116 - 3s/epoch - 27ms/step\n",
      "Epoch 6/10000\n",
      "113/113 - 3s - loss: 518369.4375 - accuracy: 0.5835 - val_loss: 234187.0781 - val_accuracy: 0.6086 - 3s/epoch - 25ms/step\n",
      "Epoch 7/10000\n",
      "113/113 - 3s - loss: 432048.6250 - accuracy: 0.5817 - val_loss: 249694.0156 - val_accuracy: 0.6198 - 3s/epoch - 24ms/step\n",
      "Epoch 8/10000\n",
      "113/113 - 3s - loss: 816055.4375 - accuracy: 0.5734 - val_loss: 294692.7812 - val_accuracy: 0.5714 - 3s/epoch - 25ms/step\n",
      "Epoch 9/10000\n",
      "113/113 - 3s - loss: 405605.0938 - accuracy: 0.5776 - val_loss: 392121.9375 - val_accuracy: 0.6175 - 3s/epoch - 25ms/step\n",
      "Epoch 10/10000\n",
      "113/113 - 3s - loss: 688258.3750 - accuracy: 0.5728 - val_loss: 634032.5625 - val_accuracy: 0.4980 - 3s/epoch - 24ms/step\n",
      "Epoch 11/10000\n",
      "113/113 - 3s - loss: 700226.1875 - accuracy: 0.5742 - val_loss: 492535.9688 - val_accuracy: 0.6171 - 3s/epoch - 25ms/step\n",
      "Epoch 12/10000\n",
      "113/113 - 3s - loss: 682750.8125 - accuracy: 0.5769 - val_loss: 350288.9062 - val_accuracy: 0.5910 - 3s/epoch - 24ms/step\n",
      "Epoch 13/10000\n",
      "113/113 - 3s - loss: 533397.6250 - accuracy: 0.5797 - val_loss: 751919.5000 - val_accuracy: 0.6151 - 3s/epoch - 25ms/step\n",
      "Epoch 14/10000\n",
      "113/113 - 3s - loss: 655762.2500 - accuracy: 0.5765 - val_loss: 319805.7188 - val_accuracy: 0.6179 - 3s/epoch - 24ms/step\n",
      "Epoch 15/10000\n",
      "113/113 - 3s - loss: 764007.1875 - accuracy: 0.5670 - val_loss: 579320.7500 - val_accuracy: 0.6234 - 3s/epoch - 24ms/step\n",
      "Epoch 16/10000\n",
      "113/113 - 3s - loss: 664016.6250 - accuracy: 0.5751 - val_loss: 486140.1250 - val_accuracy: 0.6214 - 3s/epoch - 24ms/step\n",
      "Epoch 17/10000\n",
      "113/113 - 3s - loss: 524791.2500 - accuracy: 0.5801 - val_loss: 338803.5000 - val_accuracy: 0.5097 - 3s/epoch - 24ms/step\n",
      "Epoch 18/10000\n",
      "113/113 - 3s - loss: 645311.0625 - accuracy: 0.5679 - val_loss: 555812.7500 - val_accuracy: 0.6244 - 3s/epoch - 25ms/step\n",
      "Epoch 19/10000\n",
      "113/113 - 3s - loss: 693835.6250 - accuracy: 0.5724 - val_loss: 412998.9688 - val_accuracy: 0.6270 - 3s/epoch - 25ms/step\n",
      "Epoch 20/10000\n",
      "113/113 - 3s - loss: 813821.1875 - accuracy: 0.5684 - val_loss: 300370.0000 - val_accuracy: 0.5920 - 3s/epoch - 24ms/step\n",
      "Epoch 21/10000\n",
      "113/113 - 3s - loss: 907653.5625 - accuracy: 0.5681 - val_loss: 1069041.1250 - val_accuracy: 0.6175 - 3s/epoch - 24ms/step\n",
      "Epoch 22/10000\n",
      "113/113 - 3s - loss: 741815.9375 - accuracy: 0.5755 - val_loss: 687841.0625 - val_accuracy: 0.6209 - 3s/epoch - 25ms/step\n",
      "Epoch 23/10000\n",
      "113/113 - 3s - loss: 417703.7188 - accuracy: 0.5846 - val_loss: 154146.7188 - val_accuracy: 0.6033 - 3s/epoch - 23ms/step\n",
      "Epoch 24/10000\n",
      "113/113 - 3s - loss: 878995.0000 - accuracy: 0.5713 - val_loss: 1846750.2500 - val_accuracy: 0.4124 - 3s/epoch - 23ms/step\n",
      "Epoch 25/10000\n",
      "113/113 - 3s - loss: 725683.2500 - accuracy: 0.5747 - val_loss: 280408.9688 - val_accuracy: 0.5913 - 3s/epoch - 22ms/step\n",
      "Epoch 26/10000\n",
      "113/113 - 3s - loss: 535845.1875 - accuracy: 0.5726 - val_loss: 1402127.8750 - val_accuracy: 0.6184 - 3s/epoch - 23ms/step\n",
      "Epoch 27/10000\n",
      "113/113 - 3s - loss: 677184.0625 - accuracy: 0.5737 - val_loss: 434710.0938 - val_accuracy: 0.5339 - 3s/epoch - 23ms/step\n",
      "Epoch 28/10000\n",
      "113/113 - 3s - loss: 346653.4375 - accuracy: 0.5846 - val_loss: 506900.2188 - val_accuracy: 0.6178 - 3s/epoch - 23ms/step\n",
      "Epoch 29/10000\n",
      "113/113 - 3s - loss: 701746.3125 - accuracy: 0.5687 - val_loss: 924513.0000 - val_accuracy: 0.6126 - 3s/epoch - 23ms/step\n",
      "Epoch 30/10000\n",
      "113/113 - 2s - loss: 493471.2812 - accuracy: 0.5746 - val_loss: 170067.7188 - val_accuracy: 0.6292 - 2s/epoch - 22ms/step\n",
      "Epoch 31/10000\n",
      "113/113 - 3s - loss: 524287.1250 - accuracy: 0.5809 - val_loss: 182774.5312 - val_accuracy: 0.6227 - 3s/epoch - 22ms/step\n",
      "Epoch 32/10000\n",
      "113/113 - 2s - loss: 731798.0000 - accuracy: 0.5715 - val_loss: 1319518.6250 - val_accuracy: 0.6129 - 2s/epoch - 22ms/step\n",
      "Epoch 33/10000\n",
      "113/113 - 3s - loss: 642368.8750 - accuracy: 0.5749 - val_loss: 181807.9688 - val_accuracy: 0.6370 - 3s/epoch - 22ms/step\n",
      "Epoch 34/10000\n",
      "113/113 - 3s - loss: 809470.1875 - accuracy: 0.5680 - val_loss: 635352.2500 - val_accuracy: 0.5237 - 3s/epoch - 22ms/step\n",
      "Epoch 35/10000\n",
      "113/113 - 3s - loss: 650032.6875 - accuracy: 0.5737 - val_loss: 609373.0000 - val_accuracy: 0.6183 - 3s/epoch - 22ms/step\n",
      "Epoch 36/10000\n",
      "113/113 - 3s - loss: 473982.4062 - accuracy: 0.5830 - val_loss: 207727.5938 - val_accuracy: 0.5766 - 3s/epoch - 23ms/step\n",
      "Epoch 37/10000\n",
      "113/113 - 3s - loss: 523327.5625 - accuracy: 0.5766 - val_loss: 177713.3750 - val_accuracy: 0.5953 - 3s/epoch - 23ms/step\n",
      "Epoch 38/10000\n",
      "113/113 - 2s - loss: 693883.8125 - accuracy: 0.5692 - val_loss: 556252.0625 - val_accuracy: 0.5357 - 2s/epoch - 22ms/step\n",
      "Epoch 39/10000\n",
      "113/113 - 2s - loss: 493808.4688 - accuracy: 0.5766 - val_loss: 236619.0312 - val_accuracy: 0.6215 - 2s/epoch - 22ms/step\n",
      "Epoch 40/10000\n",
      "113/113 - 3s - loss: 830943.5000 - accuracy: 0.5680 - val_loss: 1597914.2500 - val_accuracy: 0.4139 - 3s/epoch - 22ms/step\n",
      "Epoch 41/10000\n",
      "113/113 - 3s - loss: 807588.3750 - accuracy: 0.5762 - val_loss: 1883879.6250 - val_accuracy: 0.6134 - 3s/epoch - 23ms/step\n",
      "Epoch 42/10000\n",
      "113/113 - 3s - loss: 519983.2812 - accuracy: 0.5833 - val_loss: 159271.3750 - val_accuracy: 0.6184 - 3s/epoch - 22ms/step\n",
      "Epoch 43/10000\n",
      "113/113 - 3s - loss: 777162.3750 - accuracy: 0.5682 - val_loss: 274006.4688 - val_accuracy: 0.6430 - 3s/epoch - 23ms/step\n",
      "Epoch 44/10000\n",
      "113/113 - 2s - loss: 680940.3750 - accuracy: 0.5766 - val_loss: 1463809.3750 - val_accuracy: 0.6139 - 2s/epoch - 22ms/step\n",
      "Epoch 45/10000\n",
      "113/113 - 3s - loss: 650337.1875 - accuracy: 0.5794 - val_loss: 291420.7812 - val_accuracy: 0.6297 - 3s/epoch - 23ms/step\n",
      "Epoch 46/10000\n",
      "113/113 - 2s - loss: 587827.0625 - accuracy: 0.5777 - val_loss: 329811.9062 - val_accuracy: 0.5617 - 2s/epoch - 22ms/step\n",
      "Epoch 47/10000\n",
      "113/113 - 3s - loss: 576602.0000 - accuracy: 0.5784 - val_loss: 130687.7812 - val_accuracy: 0.6286 - 3s/epoch - 23ms/step\n",
      "Epoch 48/10000\n",
      "113/113 - 3s - loss: 875728.0000 - accuracy: 0.5694 - val_loss: 1713899.0000 - val_accuracy: 0.6204 - 3s/epoch - 22ms/step\n",
      "Epoch 49/10000\n",
      "113/113 - 3s - loss: 635555.0625 - accuracy: 0.5802 - val_loss: 200181.7969 - val_accuracy: 0.6381 - 3s/epoch - 23ms/step\n",
      "Epoch 50/10000\n",
      "113/113 - 3s - loss: 522143.0625 - accuracy: 0.5826 - val_loss: 1547758.2500 - val_accuracy: 0.6127 - 3s/epoch - 22ms/step\n",
      "Epoch 51/10000\n",
      "113/113 - 3s - loss: 612029.8750 - accuracy: 0.5800 - val_loss: 341164.8750 - val_accuracy: 0.6231 - 3s/epoch - 22ms/step\n",
      "Epoch 52/10000\n",
      "113/113 - 2s - loss: 643914.9375 - accuracy: 0.5760 - val_loss: 363443.8438 - val_accuracy: 0.5898 - 2s/epoch - 22ms/step\n",
      "Epoch 53/10000\n",
      "113/113 - 3s - loss: 548828.2500 - accuracy: 0.5791 - val_loss: 875622.0000 - val_accuracy: 0.6133 - 3s/epoch - 23ms/step\n",
      "Epoch 54/10000\n",
      "113/113 - 2s - loss: 690686.1875 - accuracy: 0.5732 - val_loss: 572082.2500 - val_accuracy: 0.6229 - 2s/epoch - 22ms/step\n",
      "Epoch 55/10000\n",
      "113/113 - 3s - loss: 782912.7500 - accuracy: 0.5778 - val_loss: 159155.6406 - val_accuracy: 0.6270 - 3s/epoch - 23ms/step\n",
      "Epoch 56/10000\n",
      "113/113 - 2s - loss: 638974.5000 - accuracy: 0.5733 - val_loss: 486987.0938 - val_accuracy: 0.4991 - 2s/epoch - 22ms/step\n",
      "Epoch 57/10000\n",
      "113/113 - 3s - loss: 602286.8125 - accuracy: 0.5798 - val_loss: 335414.5938 - val_accuracy: 0.5178 - 3s/epoch - 23ms/step\n",
      "Epoch 58/10000\n",
      "113/113 - 2s - loss: 575685.5000 - accuracy: 0.5785 - val_loss: 216212.5469 - val_accuracy: 0.5962 - 2s/epoch - 22ms/step\n",
      "Epoch 59/10000\n",
      "113/113 - 2s - loss: 599240.5000 - accuracy: 0.5827 - val_loss: 401499.3125 - val_accuracy: 0.4861 - 2s/epoch - 22ms/step\n",
      "Epoch 60/10000\n",
      "113/113 - 3s - loss: 323747.0000 - accuracy: 0.5856 - val_loss: 947163.1875 - val_accuracy: 0.6128 - 3s/epoch - 23ms/step\n",
      "Epoch 61/10000\n",
      "113/113 - 3s - loss: 522750.9688 - accuracy: 0.5825 - val_loss: 200207.5156 - val_accuracy: 0.6325 - 3s/epoch - 23ms/step\n",
      "Epoch 62/10000\n",
      "113/113 - 2s - loss: 396605.2500 - accuracy: 0.5819 - val_loss: 650012.0000 - val_accuracy: 0.6136 - 2s/epoch - 22ms/step\n",
      "Epoch 63/10000\n",
      "113/113 - 3s - loss: 669220.0000 - accuracy: 0.5751 - val_loss: 689437.5000 - val_accuracy: 0.5121 - 3s/epoch - 23ms/step\n",
      "391/391 [==============================] - 2s 5ms/step\n",
      "\n",
      "Iterazione 76/128 (03-11-2023_11-49-12)\n",
      "hidden_layer_size: 500\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 2\n",
      "activation_hidden: relu\n",
      "activation_output: sigmoid\n",
      "batch_size: 1000\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 20\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 64.3%\n",
      "La recall di questo modello sul validation set è: 25.28%\n",
      "La f1 di questo modello sul validation set è: 35.4%\n",
      "La balanced accuracy di questo modello sul validation set è: 57.1%\n",
      "La precision di questo modello sul validation set è: 59.02%\n",
      "La AUC di questo modello sul validation set è: 57.1%\n",
      "391/391 [==============================] - 2s 5ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.65    0.89      0.75  7,670.00\n",
      "1                  0.59    0.25      0.35  4,841.00\n",
      "accuracy           0.64    0.64      0.64      0.64\n",
      "macro avg          0.62    0.57      0.55 12,511.00\n",
      "weighted avg       0.63    0.64      0.60 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         6820          850\n",
      "Actual 1         3617         1224\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 66.1% con il modello:\n",
      "66 - AC66.1%, L500, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA20, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_11-28-58).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 62.97% con il modello:\n",
      "1 - BA62.97%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 64.08% con il modello:\n",
      "13 - PR64.08%, L20, mV-10, MV10, P2, AHsigmoid, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-26-48).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "1126/1126 - 12s - loss: 4.0388 - accuracy: 0.5590 - val_loss: 1.4924 - val_accuracy: 0.6127 - 12s/epoch - 10ms/step\n",
      "Epoch 2/10000\n",
      "1126/1126 - 9s - loss: 1.3965 - accuracy: 0.5728 - val_loss: 1.3069 - val_accuracy: 0.5806 - 9s/epoch - 8ms/step\n",
      "Epoch 3/10000\n",
      "1126/1126 - 10s - loss: 1.2796 - accuracy: 0.5792 - val_loss: 1.2277 - val_accuracy: 0.5614 - 10s/epoch - 9ms/step\n",
      "Epoch 4/10000\n",
      "1126/1126 - 9s - loss: 1.2141 - accuracy: 0.5810 - val_loss: 1.1365 - val_accuracy: 0.5924 - 9s/epoch - 8ms/step\n",
      "Epoch 5/10000\n",
      "1126/1126 - 8s - loss: 1.1442 - accuracy: 0.5828 - val_loss: 1.1648 - val_accuracy: 0.6011 - 8s/epoch - 8ms/step\n",
      "Epoch 6/10000\n",
      "1126/1126 - 9s - loss: 1.1143 - accuracy: 0.5857 - val_loss: 1.1335 - val_accuracy: 0.6060 - 9s/epoch - 8ms/step\n",
      "Epoch 7/10000\n",
      "1126/1126 - 8s - loss: 1.0820 - accuracy: 0.5855 - val_loss: 1.0991 - val_accuracy: 0.5697 - 8s/epoch - 8ms/step\n",
      "Epoch 8/10000\n",
      "1126/1126 - 8s - loss: 1.0797 - accuracy: 0.5857 - val_loss: 1.0682 - val_accuracy: 0.5792 - 8s/epoch - 7ms/step\n",
      "Epoch 9/10000\n",
      "1126/1126 - 9s - loss: 1.0656 - accuracy: 0.5855 - val_loss: 1.0857 - val_accuracy: 0.5843 - 9s/epoch - 8ms/step\n",
      "Epoch 10/10000\n",
      "1126/1126 - 9s - loss: 1.0355 - accuracy: 0.5876 - val_loss: 1.0325 - val_accuracy: 0.5867 - 9s/epoch - 8ms/step\n",
      "Epoch 11/10000\n",
      "1126/1126 - 8s - loss: 1.0460 - accuracy: 0.5863 - val_loss: 1.0187 - val_accuracy: 0.6021 - 8s/epoch - 7ms/step\n",
      "391/391 [==============================] - 2s 5ms/step\n",
      "\n",
      "Iterazione 77/128 (03-11-2023_11-50-57)\n",
      "hidden_layer_size: 500\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 2\n",
      "activation_hidden: sigmoid\n",
      "activation_output: sigmoid\n",
      "batch_size: 100\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 10\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 61.27%\n",
      "La recall di questo modello sul validation set è: 30.22%\n",
      "La f1 di questo modello sul validation set è: 37.65%\n",
      "La balanced accuracy di questo modello sul validation set è: 55.54%\n",
      "La precision di questo modello sul validation set è: 49.91%\n",
      "La AUC di questo modello sul validation set è: 58.96%\n",
      "391/391 [==============================] - 2s 5ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.65    0.81      0.72  7,670.00\n",
      "1                  0.50    0.30      0.38  4,841.00\n",
      "accuracy           0.61    0.61      0.61      0.61\n",
      "macro avg          0.57    0.56      0.55 12,511.00\n",
      "weighted avg       0.59    0.61      0.59 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         6202         1468\n",
      "Actual 1         3378         1463\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 66.1% con il modello:\n",
      "66 - AC66.1%, L500, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA20, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_11-28-58).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 62.97% con il modello:\n",
      "1 - BA62.97%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 64.08% con il modello:\n",
      "13 - PR64.08%, L20, mV-10, MV10, P2, AHsigmoid, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-26-48).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "1126/1126 - 12s - loss: 4.0388 - accuracy: 0.5590 - val_loss: 1.4924 - val_accuracy: 0.6127 - 12s/epoch - 10ms/step\n",
      "Epoch 2/10000\n",
      "1126/1126 - 9s - loss: 1.3965 - accuracy: 0.5728 - val_loss: 1.3069 - val_accuracy: 0.5806 - 9s/epoch - 8ms/step\n",
      "Epoch 3/10000\n",
      "1126/1126 - 9s - loss: 1.2796 - accuracy: 0.5792 - val_loss: 1.2277 - val_accuracy: 0.5614 - 9s/epoch - 8ms/step\n",
      "Epoch 4/10000\n",
      "1126/1126 - 9s - loss: 1.2141 - accuracy: 0.5810 - val_loss: 1.1365 - val_accuracy: 0.5924 - 9s/epoch - 8ms/step\n",
      "Epoch 5/10000\n",
      "1126/1126 - 9s - loss: 1.1442 - accuracy: 0.5828 - val_loss: 1.1648 - val_accuracy: 0.6011 - 9s/epoch - 8ms/step\n",
      "Epoch 6/10000\n",
      "1126/1126 - 8s - loss: 1.1143 - accuracy: 0.5857 - val_loss: 1.1335 - val_accuracy: 0.6060 - 8s/epoch - 8ms/step\n",
      "Epoch 7/10000\n",
      "1126/1126 - 9s - loss: 1.0820 - accuracy: 0.5855 - val_loss: 1.0991 - val_accuracy: 0.5697 - 9s/epoch - 8ms/step\n",
      "Epoch 8/10000\n",
      "1126/1126 - 9s - loss: 1.0797 - accuracy: 0.5857 - val_loss: 1.0682 - val_accuracy: 0.5792 - 9s/epoch - 8ms/step\n",
      "Epoch 9/10000\n",
      "1126/1126 - 9s - loss: 1.0656 - accuracy: 0.5855 - val_loss: 1.0857 - val_accuracy: 0.5843 - 9s/epoch - 8ms/step\n",
      "Epoch 10/10000\n",
      "1126/1126 - 9s - loss: 1.0355 - accuracy: 0.5876 - val_loss: 1.0325 - val_accuracy: 0.5867 - 9s/epoch - 8ms/step\n",
      "Epoch 11/10000\n",
      "1126/1126 - 8s - loss: 1.0460 - accuracy: 0.5863 - val_loss: 1.0187 - val_accuracy: 0.6021 - 8s/epoch - 7ms/step\n",
      "Epoch 12/10000\n",
      "1126/1126 - 8s - loss: 1.0122 - accuracy: 0.5874 - val_loss: 1.0137 - val_accuracy: 0.5751 - 8s/epoch - 7ms/step\n",
      "Epoch 13/10000\n",
      "1126/1126 - 9s - loss: 1.0256 - accuracy: 0.5844 - val_loss: 0.9949 - val_accuracy: 0.5721 - 9s/epoch - 8ms/step\n",
      "Epoch 14/10000\n",
      "1126/1126 - 8s - loss: 0.9922 - accuracy: 0.5873 - val_loss: 1.0757 - val_accuracy: 0.6013 - 8s/epoch - 7ms/step\n",
      "Epoch 15/10000\n",
      "1126/1126 - 8s - loss: 0.9990 - accuracy: 0.5880 - val_loss: 0.9801 - val_accuracy: 0.5876 - 8s/epoch - 7ms/step\n",
      "Epoch 16/10000\n",
      "1126/1126 - 9s - loss: 0.9899 - accuracy: 0.5872 - val_loss: 1.0271 - val_accuracy: 0.5533 - 9s/epoch - 8ms/step\n",
      "Epoch 17/10000\n",
      "1126/1126 - 9s - loss: 0.9559 - accuracy: 0.5893 - val_loss: 0.9427 - val_accuracy: 0.5790 - 9s/epoch - 8ms/step\n",
      "Epoch 18/10000\n",
      "1126/1126 - 9s - loss: 0.9629 - accuracy: 0.5856 - val_loss: 0.9536 - val_accuracy: 0.5969 - 9s/epoch - 8ms/step\n",
      "Epoch 19/10000\n",
      "1126/1126 - 8s - loss: 0.9498 - accuracy: 0.5874 - val_loss: 0.9640 - val_accuracy: 0.5673 - 8s/epoch - 7ms/step\n",
      "Epoch 20/10000\n",
      "1126/1126 - 8s - loss: 0.9536 - accuracy: 0.5864 - val_loss: 0.9376 - val_accuracy: 0.5944 - 8s/epoch - 7ms/step\n",
      "Epoch 21/10000\n",
      "1126/1126 - 8s - loss: 0.9415 - accuracy: 0.5903 - val_loss: 0.9290 - val_accuracy: 0.5807 - 8s/epoch - 7ms/step\n",
      "391/391 [==============================] - 2s 5ms/step\n",
      "\n",
      "Iterazione 78/128 (03-11-2023_11-54-06)\n",
      "hidden_layer_size: 500\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 2\n",
      "activation_hidden: sigmoid\n",
      "activation_output: sigmoid\n",
      "batch_size: 100\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 20\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 61.27%\n",
      "La recall di questo modello sul validation set è: 30.22%\n",
      "La f1 di questo modello sul validation set è: 37.65%\n",
      "La balanced accuracy di questo modello sul validation set è: 55.54%\n",
      "La precision di questo modello sul validation set è: 49.91%\n",
      "La AUC di questo modello sul validation set è: 58.96%\n",
      "391/391 [==============================] - 2s 4ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.65    0.81      0.72  7,670.00\n",
      "1                  0.50    0.30      0.38  4,841.00\n",
      "accuracy           0.61    0.61      0.61      0.61\n",
      "macro avg          0.57    0.56      0.55 12,511.00\n",
      "weighted avg       0.59    0.61      0.59 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         6202         1468\n",
      "Actual 1         3378         1463\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 66.1% con il modello:\n",
      "66 - AC66.1%, L500, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA20, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_11-28-58).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 62.97% con il modello:\n",
      "1 - BA62.97%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 64.08% con il modello:\n",
      "13 - PR64.08%, L20, mV-10, MV10, P2, AHsigmoid, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-26-48).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "113/113 - 5s - loss: 21.9787 - accuracy: 0.4843 - val_loss: 2.7319 - val_accuracy: 0.5454 - 5s/epoch - 44ms/step\n",
      "Epoch 2/10000\n",
      "113/113 - 3s - loss: 2.2325 - accuracy: 0.5740 - val_loss: 1.9592 - val_accuracy: 0.5701 - 3s/epoch - 24ms/step\n",
      "Epoch 3/10000\n",
      "113/113 - 3s - loss: 1.8029 - accuracy: 0.5822 - val_loss: 1.7195 - val_accuracy: 0.5728 - 3s/epoch - 23ms/step\n",
      "Epoch 4/10000\n",
      "113/113 - 3s - loss: 1.6117 - accuracy: 0.5888 - val_loss: 1.5748 - val_accuracy: 0.5718 - 3s/epoch - 24ms/step\n",
      "Epoch 5/10000\n",
      "113/113 - 3s - loss: 1.4812 - accuracy: 0.5915 - val_loss: 1.4805 - val_accuracy: 0.5812 - 3s/epoch - 24ms/step\n",
      "Epoch 6/10000\n",
      "113/113 - 3s - loss: 1.3939 - accuracy: 0.5981 - val_loss: 1.3933 - val_accuracy: 0.6036 - 3s/epoch - 25ms/step\n",
      "Epoch 7/10000\n",
      "113/113 - 3s - loss: 1.3372 - accuracy: 0.5967 - val_loss: 1.3544 - val_accuracy: 0.5886 - 3s/epoch - 25ms/step\n",
      "Epoch 8/10000\n",
      "113/113 - 3s - loss: 1.2798 - accuracy: 0.5968 - val_loss: 1.3149 - val_accuracy: 0.5910 - 3s/epoch - 23ms/step\n",
      "Epoch 9/10000\n",
      "113/113 - 3s - loss: 1.2442 - accuracy: 0.6004 - val_loss: 1.2793 - val_accuracy: 0.6101 - 3s/epoch - 23ms/step\n",
      "Epoch 10/10000\n",
      "113/113 - 3s - loss: 1.2116 - accuracy: 0.5974 - val_loss: 1.2199 - val_accuracy: 0.6109 - 3s/epoch - 23ms/step\n",
      "Epoch 11/10000\n",
      "113/113 - 3s - loss: 1.1763 - accuracy: 0.6031 - val_loss: 1.2025 - val_accuracy: 0.5944 - 3s/epoch - 23ms/step\n",
      "Epoch 12/10000\n",
      "113/113 - 3s - loss: 1.1519 - accuracy: 0.6009 - val_loss: 1.1861 - val_accuracy: 0.6136 - 3s/epoch - 23ms/step\n",
      "Epoch 13/10000\n",
      "113/113 - 3s - loss: 1.1312 - accuracy: 0.6037 - val_loss: 1.1723 - val_accuracy: 0.5856 - 3s/epoch - 23ms/step\n",
      "Epoch 14/10000\n",
      "113/113 - 3s - loss: 1.1102 - accuracy: 0.6020 - val_loss: 1.1248 - val_accuracy: 0.6066 - 3s/epoch - 23ms/step\n",
      "Epoch 15/10000\n",
      "113/113 - 3s - loss: 1.0903 - accuracy: 0.6028 - val_loss: 1.1276 - val_accuracy: 0.6059 - 3s/epoch - 23ms/step\n",
      "Epoch 16/10000\n",
      "113/113 - 3s - loss: 1.0685 - accuracy: 0.6064 - val_loss: 1.1160 - val_accuracy: 0.6097 - 3s/epoch - 25ms/step\n",
      "Epoch 17/10000\n",
      "113/113 - 3s - loss: 1.0560 - accuracy: 0.6041 - val_loss: 1.0869 - val_accuracy: 0.5995 - 3s/epoch - 22ms/step\n",
      "Epoch 18/10000\n",
      "113/113 - 2s - loss: 1.0623 - accuracy: 0.6040 - val_loss: 1.0666 - val_accuracy: 0.6127 - 2s/epoch - 21ms/step\n",
      "Epoch 19/10000\n",
      "113/113 - 3s - loss: 1.0422 - accuracy: 0.6070 - val_loss: 1.0593 - val_accuracy: 0.6052 - 3s/epoch - 23ms/step\n",
      "Epoch 20/10000\n",
      "113/113 - 3s - loss: 1.0294 - accuracy: 0.6036 - val_loss: 1.0745 - val_accuracy: 0.6009 - 3s/epoch - 22ms/step\n",
      "Epoch 21/10000\n",
      "113/113 - 2s - loss: 1.0270 - accuracy: 0.6007 - val_loss: 1.0711 - val_accuracy: 0.5872 - 2s/epoch - 22ms/step\n",
      "Epoch 22/10000\n",
      "113/113 - 3s - loss: 1.0024 - accuracy: 0.6064 - val_loss: 1.0842 - val_accuracy: 0.6011 - 3s/epoch - 22ms/step\n",
      "391/391 [==============================] - 2s 4ms/step\n",
      "\n",
      "Iterazione 79/128 (03-11-2023_11-55-11)\n",
      "hidden_layer_size: 500\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 2\n",
      "activation_hidden: sigmoid\n",
      "activation_output: sigmoid\n",
      "batch_size: 1000\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 10\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 61.36%\n",
      "La recall di questo modello sul validation set è: 21.38%\n",
      "La f1 di questo modello sul validation set è: 29.98%\n",
      "La balanced accuracy di questo modello sul validation set è: 53.99%\n",
      "La precision di questo modello sul validation set è: 50.17%\n",
      "La AUC di questo modello sul validation set è: 59.66%\n",
      "391/391 [==============================] - 2s 4ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.64    0.87      0.73  7,670.00\n",
      "1                  0.50    0.21      0.30  4,841.00\n",
      "accuracy           0.61    0.61      0.61      0.61\n",
      "macro avg          0.57    0.54      0.52 12,511.00\n",
      "weighted avg       0.58    0.61      0.57 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         6642         1028\n",
      "Actual 1         3806         1035\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 66.1% con il modello:\n",
      "66 - AC66.1%, L500, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA20, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_11-28-58).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 62.97% con il modello:\n",
      "1 - BA62.97%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 64.08% con il modello:\n",
      "13 - PR64.08%, L20, mV-10, MV10, P2, AHsigmoid, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-26-48).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "113/113 - 5s - loss: 21.9787 - accuracy: 0.4843 - val_loss: 2.7319 - val_accuracy: 0.5454 - 5s/epoch - 42ms/step\n",
      "Epoch 2/10000\n",
      "113/113 - 3s - loss: 2.2325 - accuracy: 0.5740 - val_loss: 1.9592 - val_accuracy: 0.5701 - 3s/epoch - 23ms/step\n",
      "Epoch 3/10000\n",
      "113/113 - 3s - loss: 1.8029 - accuracy: 0.5822 - val_loss: 1.7195 - val_accuracy: 0.5728 - 3s/epoch - 23ms/step\n",
      "Epoch 4/10000\n",
      "113/113 - 3s - loss: 1.6117 - accuracy: 0.5888 - val_loss: 1.5748 - val_accuracy: 0.5718 - 3s/epoch - 23ms/step\n",
      "Epoch 5/10000\n",
      "113/113 - 3s - loss: 1.4812 - accuracy: 0.5915 - val_loss: 1.4805 - val_accuracy: 0.5812 - 3s/epoch - 23ms/step\n",
      "Epoch 6/10000\n",
      "113/113 - 3s - loss: 1.3939 - accuracy: 0.5981 - val_loss: 1.3933 - val_accuracy: 0.6036 - 3s/epoch - 24ms/step\n",
      "Epoch 7/10000\n",
      "113/113 - 3s - loss: 1.3372 - accuracy: 0.5967 - val_loss: 1.3544 - val_accuracy: 0.5886 - 3s/epoch - 23ms/step\n",
      "Epoch 8/10000\n",
      "113/113 - 3s - loss: 1.2798 - accuracy: 0.5968 - val_loss: 1.3149 - val_accuracy: 0.5910 - 3s/epoch - 23ms/step\n",
      "Epoch 9/10000\n",
      "113/113 - 3s - loss: 1.2442 - accuracy: 0.6004 - val_loss: 1.2793 - val_accuracy: 0.6101 - 3s/epoch - 24ms/step\n",
      "Epoch 10/10000\n",
      "113/113 - 3s - loss: 1.2116 - accuracy: 0.5974 - val_loss: 1.2199 - val_accuracy: 0.6109 - 3s/epoch - 25ms/step\n",
      "Epoch 11/10000\n",
      "113/113 - 3s - loss: 1.1763 - accuracy: 0.6031 - val_loss: 1.2025 - val_accuracy: 0.5944 - 3s/epoch - 23ms/step\n",
      "Epoch 12/10000\n",
      "113/113 - 3s - loss: 1.1519 - accuracy: 0.6009 - val_loss: 1.1861 - val_accuracy: 0.6136 - 3s/epoch - 23ms/step\n",
      "Epoch 13/10000\n",
      "113/113 - 3s - loss: 1.1312 - accuracy: 0.6037 - val_loss: 1.1723 - val_accuracy: 0.5856 - 3s/epoch - 23ms/step\n",
      "Epoch 14/10000\n",
      "113/113 - 3s - loss: 1.1102 - accuracy: 0.6020 - val_loss: 1.1248 - val_accuracy: 0.6066 - 3s/epoch - 23ms/step\n",
      "Epoch 15/10000\n",
      "113/113 - 3s - loss: 1.0903 - accuracy: 0.6028 - val_loss: 1.1276 - val_accuracy: 0.6059 - 3s/epoch - 23ms/step\n",
      "Epoch 16/10000\n",
      "113/113 - 3s - loss: 1.0685 - accuracy: 0.6064 - val_loss: 1.1160 - val_accuracy: 0.6097 - 3s/epoch - 24ms/step\n",
      "Epoch 17/10000\n",
      "113/113 - 3s - loss: 1.0560 - accuracy: 0.6041 - val_loss: 1.0869 - val_accuracy: 0.5995 - 3s/epoch - 23ms/step\n",
      "Epoch 18/10000\n",
      "113/113 - 3s - loss: 1.0623 - accuracy: 0.6040 - val_loss: 1.0666 - val_accuracy: 0.6127 - 3s/epoch - 23ms/step\n",
      "Epoch 19/10000\n",
      "113/113 - 3s - loss: 1.0422 - accuracy: 0.6070 - val_loss: 1.0593 - val_accuracy: 0.6052 - 3s/epoch - 24ms/step\n",
      "Epoch 20/10000\n",
      "113/113 - 3s - loss: 1.0294 - accuracy: 0.6036 - val_loss: 1.0745 - val_accuracy: 0.6009 - 3s/epoch - 23ms/step\n",
      "Epoch 21/10000\n",
      "113/113 - 3s - loss: 1.0270 - accuracy: 0.6007 - val_loss: 1.0711 - val_accuracy: 0.5872 - 3s/epoch - 23ms/step\n",
      "Epoch 22/10000\n",
      "113/113 - 3s - loss: 1.0024 - accuracy: 0.6064 - val_loss: 1.0842 - val_accuracy: 0.6011 - 3s/epoch - 23ms/step\n",
      "Epoch 23/10000\n",
      "113/113 - 3s - loss: 1.0022 - accuracy: 0.6068 - val_loss: 1.0543 - val_accuracy: 0.6174 - 3s/epoch - 22ms/step\n",
      "Epoch 24/10000\n",
      "113/113 - 3s - loss: 1.0044 - accuracy: 0.6049 - val_loss: 1.0469 - val_accuracy: 0.5864 - 3s/epoch - 23ms/step\n",
      "Epoch 25/10000\n",
      "113/113 - 2s - loss: 0.9875 - accuracy: 0.6048 - val_loss: 1.0334 - val_accuracy: 0.6107 - 2s/epoch - 22ms/step\n",
      "Epoch 26/10000\n",
      "113/113 - 3s - loss: 0.9780 - accuracy: 0.6050 - val_loss: 1.0115 - val_accuracy: 0.6037 - 3s/epoch - 23ms/step\n",
      "Epoch 27/10000\n",
      "113/113 - 3s - loss: 0.9698 - accuracy: 0.6053 - val_loss: 1.0075 - val_accuracy: 0.6071 - 3s/epoch - 23ms/step\n",
      "Epoch 28/10000\n",
      "113/113 - 3s - loss: 0.9546 - accuracy: 0.6076 - val_loss: 1.0288 - val_accuracy: 0.5836 - 3s/epoch - 22ms/step\n",
      "Epoch 29/10000\n",
      "113/113 - 3s - loss: 0.9528 - accuracy: 0.6072 - val_loss: 0.9921 - val_accuracy: 0.5985 - 3s/epoch - 24ms/step\n",
      "Epoch 30/10000\n",
      "113/113 - 3s - loss: 0.9389 - accuracy: 0.6065 - val_loss: 1.0049 - val_accuracy: 0.5973 - 3s/epoch - 24ms/step\n",
      "Epoch 31/10000\n",
      "113/113 - 3s - loss: 0.9371 - accuracy: 0.6092 - val_loss: 0.9882 - val_accuracy: 0.6130 - 3s/epoch - 24ms/step\n",
      "Epoch 32/10000\n",
      "113/113 - 3s - loss: 0.9282 - accuracy: 0.6085 - val_loss: 0.9887 - val_accuracy: 0.5867 - 3s/epoch - 24ms/step\n",
      "Epoch 33/10000\n",
      "113/113 - 3s - loss: 0.9149 - accuracy: 0.6093 - val_loss: 0.9964 - val_accuracy: 0.5755 - 3s/epoch - 23ms/step\n",
      "Epoch 34/10000\n",
      "113/113 - 3s - loss: 0.9121 - accuracy: 0.6090 - val_loss: 0.9790 - val_accuracy: 0.6099 - 3s/epoch - 24ms/step\n",
      "Epoch 35/10000\n",
      "113/113 - 3s - loss: 0.9126 - accuracy: 0.6109 - val_loss: 0.9741 - val_accuracy: 0.5971 - 3s/epoch - 23ms/step\n",
      "Epoch 36/10000\n",
      "113/113 - 3s - loss: 0.9150 - accuracy: 0.6094 - val_loss: 0.9612 - val_accuracy: 0.6098 - 3s/epoch - 23ms/step\n",
      "Epoch 37/10000\n",
      "113/113 - 3s - loss: 0.9158 - accuracy: 0.6079 - val_loss: 0.9769 - val_accuracy: 0.5899 - 3s/epoch - 23ms/step\n",
      "Epoch 38/10000\n",
      "113/113 - 3s - loss: 0.9165 - accuracy: 0.6081 - val_loss: 0.9642 - val_accuracy: 0.6118 - 3s/epoch - 24ms/step\n",
      "Epoch 39/10000\n",
      "113/113 - 3s - loss: 0.9102 - accuracy: 0.6076 - val_loss: 0.9386 - val_accuracy: 0.6087 - 3s/epoch - 23ms/step\n",
      "Epoch 40/10000\n",
      "113/113 - 3s - loss: 0.8935 - accuracy: 0.6122 - val_loss: 0.9140 - val_accuracy: 0.6122 - 3s/epoch - 23ms/step\n",
      "Epoch 41/10000\n",
      "113/113 - 3s - loss: 0.8846 - accuracy: 0.6105 - val_loss: 0.9207 - val_accuracy: 0.6248 - 3s/epoch - 23ms/step\n",
      "Epoch 42/10000\n",
      "113/113 - 3s - loss: 0.8881 - accuracy: 0.6079 - val_loss: 0.9157 - val_accuracy: 0.6047 - 3s/epoch - 24ms/step\n",
      "Epoch 43/10000\n",
      "113/113 - 3s - loss: 0.8855 - accuracy: 0.6110 - val_loss: 0.9186 - val_accuracy: 0.6198 - 3s/epoch - 25ms/step\n",
      "Epoch 44/10000\n",
      "113/113 - 3s - loss: 0.8861 - accuracy: 0.6099 - val_loss: 0.8948 - val_accuracy: 0.6290 - 3s/epoch - 23ms/step\n",
      "Epoch 45/10000\n",
      "113/113 - 3s - loss: 0.8728 - accuracy: 0.6108 - val_loss: 0.9115 - val_accuracy: 0.6071 - 3s/epoch - 23ms/step\n",
      "Epoch 46/10000\n",
      "113/113 - 3s - loss: 0.8724 - accuracy: 0.6121 - val_loss: 0.8936 - val_accuracy: 0.6126 - 3s/epoch - 23ms/step\n",
      "Epoch 47/10000\n",
      "113/113 - 3s - loss: 0.8641 - accuracy: 0.6122 - val_loss: 0.8987 - val_accuracy: 0.6199 - 3s/epoch - 23ms/step\n",
      "Epoch 48/10000\n",
      "113/113 - 3s - loss: 0.8610 - accuracy: 0.6140 - val_loss: 0.9058 - val_accuracy: 0.6159 - 3s/epoch - 24ms/step\n",
      "Epoch 49/10000\n",
      "113/113 - 3s - loss: 0.8630 - accuracy: 0.6127 - val_loss: 0.9001 - val_accuracy: 0.6118 - 3s/epoch - 24ms/step\n",
      "Epoch 50/10000\n",
      "113/113 - 3s - loss: 0.8563 - accuracy: 0.6107 - val_loss: 0.9201 - val_accuracy: 0.5959 - 3s/epoch - 23ms/step\n",
      "Epoch 51/10000\n",
      "113/113 - 3s - loss: 0.8526 - accuracy: 0.6135 - val_loss: 0.9038 - val_accuracy: 0.6230 - 3s/epoch - 23ms/step\n",
      "Epoch 52/10000\n",
      "113/113 - 3s - loss: 0.8467 - accuracy: 0.6136 - val_loss: 0.8947 - val_accuracy: 0.6146 - 3s/epoch - 24ms/step\n",
      "Epoch 53/10000\n",
      "113/113 - 3s - loss: 0.8403 - accuracy: 0.6119 - val_loss: 0.9059 - val_accuracy: 0.5858 - 3s/epoch - 23ms/step\n",
      "Epoch 54/10000\n",
      "113/113 - 3s - loss: 0.8394 - accuracy: 0.6102 - val_loss: 0.8733 - val_accuracy: 0.6152 - 3s/epoch - 24ms/step\n",
      "Epoch 55/10000\n",
      "113/113 - 3s - loss: 0.8495 - accuracy: 0.6095 - val_loss: 0.8828 - val_accuracy: 0.6229 - 3s/epoch - 23ms/step\n",
      "Epoch 56/10000\n",
      "113/113 - 3s - loss: 0.8503 - accuracy: 0.6099 - val_loss: 0.8755 - val_accuracy: 0.6218 - 3s/epoch - 24ms/step\n",
      "Epoch 57/10000\n",
      "113/113 - 3s - loss: 0.8484 - accuracy: 0.6106 - val_loss: 0.8706 - val_accuracy: 0.6179 - 3s/epoch - 22ms/step\n",
      "Epoch 58/10000\n",
      "113/113 - 3s - loss: 0.8350 - accuracy: 0.6125 - val_loss: 0.8866 - val_accuracy: 0.6196 - 3s/epoch - 23ms/step\n",
      "Epoch 59/10000\n",
      "113/113 - 3s - loss: 0.8399 - accuracy: 0.6125 - val_loss: 0.8888 - val_accuracy: 0.6187 - 3s/epoch - 22ms/step\n",
      "Epoch 60/10000\n",
      "113/113 - 3s - loss: 0.8336 - accuracy: 0.6118 - val_loss: 0.8793 - val_accuracy: 0.5903 - 3s/epoch - 23ms/step\n",
      "Epoch 61/10000\n",
      "113/113 - 3s - loss: 0.8361 - accuracy: 0.6084 - val_loss: 0.8631 - val_accuracy: 0.6128 - 3s/epoch - 22ms/step\n",
      "Epoch 62/10000\n",
      "113/113 - 3s - loss: 0.8359 - accuracy: 0.6116 - val_loss: 0.8556 - val_accuracy: 0.6195 - 3s/epoch - 23ms/step\n",
      "Epoch 63/10000\n",
      "113/113 - 3s - loss: 0.8308 - accuracy: 0.6130 - val_loss: 0.8727 - val_accuracy: 0.6156 - 3s/epoch - 23ms/step\n",
      "Epoch 64/10000\n",
      "113/113 - 3s - loss: 0.8314 - accuracy: 0.6091 - val_loss: 0.8359 - val_accuracy: 0.6239 - 3s/epoch - 23ms/step\n",
      "391/391 [==============================] - 2s 5ms/step\n",
      "\n",
      "Iterazione 80/128 (03-11-2023_11-58-06)\n",
      "hidden_layer_size: 500\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 2\n",
      "activation_hidden: sigmoid\n",
      "activation_output: sigmoid\n",
      "batch_size: 1000\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 20\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 62.9%\n",
      "La recall di questo modello sul validation set è: 25.86%\n",
      "La f1 di questo modello sul validation set è: 35.04%\n",
      "La balanced accuracy di questo modello sul validation set è: 56.07%\n",
      "La precision di questo modello sul validation set è: 54.32%\n",
      "La AUC di questo modello sul validation set è: 62.07%\n",
      "391/391 [==============================] - 2s 4ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.65    0.86      0.74  7,670.00\n",
      "1                  0.54    0.26      0.35  4,841.00\n",
      "accuracy           0.63    0.63      0.63      0.63\n",
      "macro avg          0.60    0.56      0.55 12,511.00\n",
      "weighted avg       0.61    0.63      0.59 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         6617         1053\n",
      "Actual 1         3589         1252\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 66.1% con il modello:\n",
      "66 - AC66.1%, L500, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA20, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_11-28-58).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 62.97% con il modello:\n",
      "1 - BA62.97%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 64.08% con il modello:\n",
      "13 - PR64.08%, L20, mV-10, MV10, P2, AHsigmoid, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-26-48).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "1126/1126 - 15s - loss: 257706112.0000 - accuracy: 0.5549 - val_loss: 189085280.0000 - val_accuracy: 0.6113 - 15s/epoch - 14ms/step\n",
      "Epoch 2/10000\n",
      "1126/1126 - 13s - loss: 108991872.0000 - accuracy: 0.5664 - val_loss: 245870880.0000 - val_accuracy: 0.4257 - 13s/epoch - 12ms/step\n",
      "Epoch 3/10000\n",
      "1126/1126 - 13s - loss: 104706952.0000 - accuracy: 0.5710 - val_loss: 135633088.0000 - val_accuracy: 0.6143 - 13s/epoch - 12ms/step\n",
      "Epoch 4/10000\n",
      "1126/1126 - 12s - loss: 115633560.0000 - accuracy: 0.5714 - val_loss: 129247912.0000 - val_accuracy: 0.6137 - 12s/epoch - 11ms/step\n",
      "Epoch 5/10000\n",
      "1126/1126 - 12s - loss: 111149064.0000 - accuracy: 0.5712 - val_loss: 149168416.0000 - val_accuracy: 0.6127 - 12s/epoch - 10ms/step\n",
      "Epoch 6/10000\n",
      "1126/1126 - 12s - loss: 112124008.0000 - accuracy: 0.5722 - val_loss: 90229720.0000 - val_accuracy: 0.5005 - 12s/epoch - 11ms/step\n",
      "Epoch 7/10000\n",
      "1126/1126 - 11s - loss: 113051840.0000 - accuracy: 0.5725 - val_loss: 35305328.0000 - val_accuracy: 0.6414 - 11s/epoch - 10ms/step\n",
      "Epoch 8/10000\n",
      "1126/1126 - 12s - loss: 103913840.0000 - accuracy: 0.5738 - val_loss: 138153600.0000 - val_accuracy: 0.6218 - 12s/epoch - 10ms/step\n",
      "Epoch 9/10000\n",
      "1126/1126 - 11s - loss: 103491776.0000 - accuracy: 0.5729 - val_loss: 43905208.0000 - val_accuracy: 0.6228 - 11s/epoch - 10ms/step\n",
      "Epoch 10/10000\n",
      "1126/1126 - 13s - loss: 92944952.0000 - accuracy: 0.5743 - val_loss: 156685344.0000 - val_accuracy: 0.6209 - 13s/epoch - 11ms/step\n",
      "Epoch 11/10000\n",
      "1126/1126 - 12s - loss: 108218512.0000 - accuracy: 0.5744 - val_loss: 64204752.0000 - val_accuracy: 0.5612 - 12s/epoch - 10ms/step\n",
      "Epoch 12/10000\n",
      "1126/1126 - 12s - loss: 103784280.0000 - accuracy: 0.5748 - val_loss: 110850336.0000 - val_accuracy: 0.6175 - 12s/epoch - 10ms/step\n",
      "Epoch 13/10000\n",
      "1126/1126 - 12s - loss: 104496824.0000 - accuracy: 0.5753 - val_loss: 86284712.0000 - val_accuracy: 0.5830 - 12s/epoch - 11ms/step\n",
      "Epoch 14/10000\n",
      "1126/1126 - 12s - loss: 108287824.0000 - accuracy: 0.5760 - val_loss: 67570160.0000 - val_accuracy: 0.6365 - 12s/epoch - 10ms/step\n",
      "Epoch 15/10000\n",
      "1126/1126 - 12s - loss: 90208352.0000 - accuracy: 0.5783 - val_loss: 52753872.0000 - val_accuracy: 0.5940 - 12s/epoch - 11ms/step\n",
      "Epoch 16/10000\n",
      "1126/1126 - 12s - loss: 98736704.0000 - accuracy: 0.5741 - val_loss: 30222036.0000 - val_accuracy: 0.6426 - 12s/epoch - 10ms/step\n",
      "Epoch 17/10000\n",
      "1126/1126 - 11s - loss: 102573024.0000 - accuracy: 0.5764 - val_loss: 39425292.0000 - val_accuracy: 0.6329 - 11s/epoch - 10ms/step\n",
      "Epoch 18/10000\n",
      "1126/1126 - 12s - loss: 101102864.0000 - accuracy: 0.5767 - val_loss: 22006644.0000 - val_accuracy: 0.5945 - 12s/epoch - 10ms/step\n",
      "Epoch 19/10000\n",
      "1126/1126 - 12s - loss: 92309312.0000 - accuracy: 0.5772 - val_loss: 163157488.0000 - val_accuracy: 0.6147 - 12s/epoch - 10ms/step\n",
      "Epoch 20/10000\n",
      "1126/1126 - 12s - loss: 107906080.0000 - accuracy: 0.5796 - val_loss: 88697984.0000 - val_accuracy: 0.6340 - 12s/epoch - 11ms/step\n",
      "Epoch 21/10000\n",
      "1126/1126 - 13s - loss: 94297024.0000 - accuracy: 0.5781 - val_loss: 63971912.0000 - val_accuracy: 0.5940 - 13s/epoch - 11ms/step\n",
      "Epoch 22/10000\n",
      "1126/1126 - 12s - loss: 96363752.0000 - accuracy: 0.5785 - val_loss: 69054616.0000 - val_accuracy: 0.6216 - 12s/epoch - 10ms/step\n",
      "Epoch 23/10000\n",
      "1126/1126 - 12s - loss: 97781504.0000 - accuracy: 0.5781 - val_loss: 28317624.0000 - val_accuracy: 0.6508 - 12s/epoch - 11ms/step\n",
      "Epoch 24/10000\n",
      "1126/1126 - 12s - loss: 92806296.0000 - accuracy: 0.5793 - val_loss: 89448696.0000 - val_accuracy: 0.4627 - 12s/epoch - 11ms/step\n",
      "Epoch 25/10000\n",
      "1126/1126 - 12s - loss: 89001624.0000 - accuracy: 0.5797 - val_loss: 127794760.0000 - val_accuracy: 0.5240 - 12s/epoch - 11ms/step\n",
      "Epoch 26/10000\n",
      "1126/1126 - 12s - loss: 101849832.0000 - accuracy: 0.5799 - val_loss: 149959440.0000 - val_accuracy: 0.6196 - 12s/epoch - 11ms/step\n",
      "Epoch 27/10000\n",
      "1126/1126 - 12s - loss: 97314032.0000 - accuracy: 0.5772 - val_loss: 27809066.0000 - val_accuracy: 0.5752 - 12s/epoch - 10ms/step\n",
      "Epoch 28/10000\n",
      "1126/1126 - 15s - loss: 90661488.0000 - accuracy: 0.5800 - val_loss: 54333628.0000 - val_accuracy: 0.6379 - 15s/epoch - 13ms/step\n",
      "Epoch 29/10000\n",
      "1126/1126 - 12s - loss: 92537728.0000 - accuracy: 0.5805 - val_loss: 103333040.0000 - val_accuracy: 0.6183 - 12s/epoch - 11ms/step\n",
      "Epoch 30/10000\n",
      "1126/1126 - 13s - loss: 87922936.0000 - accuracy: 0.5782 - val_loss: 21587646.0000 - val_accuracy: 0.6553 - 13s/epoch - 11ms/step\n",
      "Epoch 31/10000\n",
      "1126/1126 - 11s - loss: 85895064.0000 - accuracy: 0.5814 - val_loss: 316145632.0000 - val_accuracy: 0.6131 - 11s/epoch - 10ms/step\n",
      "Epoch 32/10000\n",
      "1126/1126 - 12s - loss: 95639200.0000 - accuracy: 0.5804 - val_loss: 69853368.0000 - val_accuracy: 0.6273 - 12s/epoch - 10ms/step\n",
      "Epoch 33/10000\n",
      "1126/1126 - 11s - loss: 85385728.0000 - accuracy: 0.5818 - val_loss: 174282176.0000 - val_accuracy: 0.4092 - 11s/epoch - 10ms/step\n",
      "Epoch 34/10000\n",
      "1126/1126 - 11s - loss: 86380192.0000 - accuracy: 0.5822 - val_loss: 56316116.0000 - val_accuracy: 0.6047 - 11s/epoch - 10ms/step\n",
      "Epoch 35/10000\n",
      "1126/1126 - 11s - loss: 89662880.0000 - accuracy: 0.5811 - val_loss: 160106112.0000 - val_accuracy: 0.6240 - 11s/epoch - 10ms/step\n",
      "Epoch 36/10000\n",
      "1126/1126 - 11s - loss: 105774776.0000 - accuracy: 0.5810 - val_loss: 36863952.0000 - val_accuracy: 0.6420 - 11s/epoch - 10ms/step\n",
      "Epoch 37/10000\n",
      "1126/1126 - 11s - loss: 99719824.0000 - accuracy: 0.5812 - val_loss: 44375128.0000 - val_accuracy: 0.5904 - 11s/epoch - 10ms/step\n",
      "Epoch 38/10000\n",
      "1126/1126 - 11s - loss: 87255456.0000 - accuracy: 0.5788 - val_loss: 25573422.0000 - val_accuracy: 0.6512 - 11s/epoch - 10ms/step\n",
      "Epoch 39/10000\n",
      "1126/1126 - 11s - loss: 87463192.0000 - accuracy: 0.5834 - val_loss: 106632200.0000 - val_accuracy: 0.4766 - 11s/epoch - 10ms/step\n",
      "Epoch 40/10000\n",
      "1126/1126 - 11s - loss: 84601888.0000 - accuracy: 0.5808 - val_loss: 43732484.0000 - val_accuracy: 0.6100 - 11s/epoch - 10ms/step\n",
      "391/391 [==============================] - 2s 5ms/step\n",
      "\n",
      "Iterazione 81/128 (03-11-2023_12-06-07)\n",
      "hidden_layer_size: 500\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 3\n",
      "activation_hidden: relu\n",
      "activation_output: sigmoid\n",
      "batch_size: 100\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 10\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 65.53%\n",
      "La recall di questo modello sul validation set è: 42.31%\n",
      "La f1 di questo modello sul validation set è: 48.71%\n",
      "La balanced accuracy di questo modello sul validation set è: 61.24%\n",
      "La precision di questo modello sul validation set è: 57.4%\n",
      "La AUC di questo modello sul validation set è: 61.24%\n",
      "391/391 [==============================] - 2s 5ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.69    0.80      0.74  7,670.00\n",
      "1                  0.57    0.42      0.49  4,841.00\n",
      "accuracy           0.66    0.66      0.66      0.66\n",
      "macro avg          0.63    0.61      0.61 12,511.00\n",
      "weighted avg       0.64    0.66      0.64 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         6150         1520\n",
      "Actual 1         2793         2048\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 66.1% con il modello:\n",
      "66 - AC66.1%, L500, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA20, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_11-28-58).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 62.97% con il modello:\n",
      "1 - BA62.97%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 64.08% con il modello:\n",
      "13 - PR64.08%, L20, mV-10, MV10, P2, AHsigmoid, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-26-48).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "1126/1126 - 16s - loss: 257706112.0000 - accuracy: 0.5549 - val_loss: 189085280.0000 - val_accuracy: 0.6113 - 16s/epoch - 14ms/step\n",
      "Epoch 2/10000\n",
      "1126/1126 - 12s - loss: 108991872.0000 - accuracy: 0.5664 - val_loss: 245870880.0000 - val_accuracy: 0.4257 - 12s/epoch - 11ms/step\n",
      "Epoch 3/10000\n",
      "1126/1126 - 12s - loss: 104706952.0000 - accuracy: 0.5710 - val_loss: 135633088.0000 - val_accuracy: 0.6143 - 12s/epoch - 11ms/step\n",
      "Epoch 4/10000\n",
      "1126/1126 - 11s - loss: 115633560.0000 - accuracy: 0.5714 - val_loss: 129247912.0000 - val_accuracy: 0.6137 - 11s/epoch - 10ms/step\n",
      "Epoch 5/10000\n",
      "1126/1126 - 11s - loss: 111149064.0000 - accuracy: 0.5712 - val_loss: 149168416.0000 - val_accuracy: 0.6127 - 11s/epoch - 10ms/step\n",
      "Epoch 6/10000\n",
      "1126/1126 - 11s - loss: 112124008.0000 - accuracy: 0.5722 - val_loss: 90229720.0000 - val_accuracy: 0.5005 - 11s/epoch - 10ms/step\n",
      "Epoch 7/10000\n",
      "1126/1126 - 11s - loss: 113051840.0000 - accuracy: 0.5725 - val_loss: 35305328.0000 - val_accuracy: 0.6414 - 11s/epoch - 10ms/step\n",
      "Epoch 8/10000\n",
      "1126/1126 - 11s - loss: 103913840.0000 - accuracy: 0.5738 - val_loss: 138153600.0000 - val_accuracy: 0.6218 - 11s/epoch - 10ms/step\n",
      "Epoch 9/10000\n",
      "1126/1126 - 11s - loss: 103491776.0000 - accuracy: 0.5729 - val_loss: 43905208.0000 - val_accuracy: 0.6228 - 11s/epoch - 10ms/step\n",
      "Epoch 10/10000\n",
      "1126/1126 - 11s - loss: 92944952.0000 - accuracy: 0.5743 - val_loss: 156685344.0000 - val_accuracy: 0.6209 - 11s/epoch - 10ms/step\n",
      "Epoch 11/10000\n",
      "1126/1126 - 11s - loss: 108218512.0000 - accuracy: 0.5744 - val_loss: 64204752.0000 - val_accuracy: 0.5612 - 11s/epoch - 10ms/step\n",
      "Epoch 12/10000\n",
      "1126/1126 - 11s - loss: 103784280.0000 - accuracy: 0.5748 - val_loss: 110850336.0000 - val_accuracy: 0.6175 - 11s/epoch - 10ms/step\n",
      "Epoch 13/10000\n",
      "1126/1126 - 11s - loss: 104496824.0000 - accuracy: 0.5753 - val_loss: 86284712.0000 - val_accuracy: 0.5830 - 11s/epoch - 10ms/step\n",
      "Epoch 14/10000\n",
      "1126/1126 - 12s - loss: 108287824.0000 - accuracy: 0.5760 - val_loss: 67570160.0000 - val_accuracy: 0.6365 - 12s/epoch - 11ms/step\n",
      "Epoch 15/10000\n",
      "1126/1126 - 11s - loss: 90208352.0000 - accuracy: 0.5783 - val_loss: 52753872.0000 - val_accuracy: 0.5940 - 11s/epoch - 10ms/step\n",
      "Epoch 16/10000\n",
      "1126/1126 - 9s - loss: 98736704.0000 - accuracy: 0.5741 - val_loss: 30222036.0000 - val_accuracy: 0.6426 - 9s/epoch - 8ms/step\n",
      "Epoch 17/10000\n",
      "1126/1126 - 9s - loss: 102573024.0000 - accuracy: 0.5764 - val_loss: 39425292.0000 - val_accuracy: 0.6329 - 9s/epoch - 8ms/step\n",
      "Epoch 18/10000\n",
      "1126/1126 - 9s - loss: 101102864.0000 - accuracy: 0.5767 - val_loss: 22006644.0000 - val_accuracy: 0.5945 - 9s/epoch - 8ms/step\n",
      "Epoch 19/10000\n",
      "1126/1126 - 9s - loss: 92309312.0000 - accuracy: 0.5772 - val_loss: 163157488.0000 - val_accuracy: 0.6147 - 9s/epoch - 8ms/step\n",
      "Epoch 20/10000\n",
      "1126/1126 - 9s - loss: 107906080.0000 - accuracy: 0.5796 - val_loss: 88697984.0000 - val_accuracy: 0.6340 - 9s/epoch - 8ms/step\n",
      "Epoch 21/10000\n",
      "1126/1126 - 9s - loss: 94297024.0000 - accuracy: 0.5781 - val_loss: 63971912.0000 - val_accuracy: 0.5940 - 9s/epoch - 8ms/step\n",
      "Epoch 22/10000\n",
      "1126/1126 - 9s - loss: 96363752.0000 - accuracy: 0.5785 - val_loss: 69054616.0000 - val_accuracy: 0.6216 - 9s/epoch - 8ms/step\n",
      "Epoch 23/10000\n",
      "1126/1126 - 9s - loss: 97781504.0000 - accuracy: 0.5781 - val_loss: 28317624.0000 - val_accuracy: 0.6508 - 9s/epoch - 8ms/step\n",
      "Epoch 24/10000\n",
      "1126/1126 - 9s - loss: 92806296.0000 - accuracy: 0.5793 - val_loss: 89448696.0000 - val_accuracy: 0.4627 - 9s/epoch - 8ms/step\n",
      "Epoch 25/10000\n",
      "1126/1126 - 9s - loss: 89001624.0000 - accuracy: 0.5797 - val_loss: 127794760.0000 - val_accuracy: 0.5240 - 9s/epoch - 8ms/step\n",
      "Epoch 26/10000\n",
      "1126/1126 - 9s - loss: 101849832.0000 - accuracy: 0.5799 - val_loss: 149959440.0000 - val_accuracy: 0.6196 - 9s/epoch - 8ms/step\n",
      "Epoch 27/10000\n",
      "1126/1126 - 9s - loss: 97314032.0000 - accuracy: 0.5772 - val_loss: 27809066.0000 - val_accuracy: 0.5752 - 9s/epoch - 8ms/step\n",
      "Epoch 28/10000\n",
      "1126/1126 - 9s - loss: 90661488.0000 - accuracy: 0.5800 - val_loss: 54333628.0000 - val_accuracy: 0.6379 - 9s/epoch - 8ms/step\n",
      "Epoch 29/10000\n",
      "1126/1126 - 12s - loss: 92537728.0000 - accuracy: 0.5805 - val_loss: 103333040.0000 - val_accuracy: 0.6183 - 12s/epoch - 11ms/step\n",
      "Epoch 30/10000\n",
      "1126/1126 - 9s - loss: 87922936.0000 - accuracy: 0.5782 - val_loss: 21587646.0000 - val_accuracy: 0.6553 - 9s/epoch - 8ms/step\n",
      "Epoch 31/10000\n",
      "1126/1126 - 9s - loss: 85895064.0000 - accuracy: 0.5814 - val_loss: 316145632.0000 - val_accuracy: 0.6131 - 9s/epoch - 8ms/step\n",
      "Epoch 32/10000\n",
      "1126/1126 - 9s - loss: 95639200.0000 - accuracy: 0.5804 - val_loss: 69853368.0000 - val_accuracy: 0.6273 - 9s/epoch - 8ms/step\n",
      "Epoch 33/10000\n",
      "1126/1126 - 10s - loss: 85385728.0000 - accuracy: 0.5818 - val_loss: 174282176.0000 - val_accuracy: 0.4092 - 10s/epoch - 9ms/step\n",
      "Epoch 34/10000\n",
      "1126/1126 - 9s - loss: 86380192.0000 - accuracy: 0.5822 - val_loss: 56316116.0000 - val_accuracy: 0.6047 - 9s/epoch - 8ms/step\n",
      "Epoch 35/10000\n",
      "1126/1126 - 10s - loss: 89662880.0000 - accuracy: 0.5811 - val_loss: 160106112.0000 - val_accuracy: 0.6240 - 10s/epoch - 9ms/step\n",
      "Epoch 36/10000\n",
      "1126/1126 - 9s - loss: 105774776.0000 - accuracy: 0.5810 - val_loss: 36863952.0000 - val_accuracy: 0.6420 - 9s/epoch - 8ms/step\n",
      "Epoch 37/10000\n",
      "1126/1126 - 9s - loss: 99719824.0000 - accuracy: 0.5812 - val_loss: 44375128.0000 - val_accuracy: 0.5904 - 9s/epoch - 8ms/step\n",
      "Epoch 38/10000\n",
      "1126/1126 - 9s - loss: 87255456.0000 - accuracy: 0.5788 - val_loss: 25573422.0000 - val_accuracy: 0.6512 - 9s/epoch - 8ms/step\n",
      "Epoch 39/10000\n",
      "1126/1126 - 9s - loss: 87463192.0000 - accuracy: 0.5834 - val_loss: 106632200.0000 - val_accuracy: 0.4766 - 9s/epoch - 8ms/step\n",
      "Epoch 40/10000\n",
      "1126/1126 - 9s - loss: 84601888.0000 - accuracy: 0.5808 - val_loss: 43732484.0000 - val_accuracy: 0.6100 - 9s/epoch - 8ms/step\n",
      "Epoch 41/10000\n",
      "1126/1126 - 9s - loss: 88992112.0000 - accuracy: 0.5838 - val_loss: 145212528.0000 - val_accuracy: 0.6144 - 9s/epoch - 8ms/step\n",
      "Epoch 42/10000\n",
      "1126/1126 - 9s - loss: 92139432.0000 - accuracy: 0.5818 - val_loss: 235725744.0000 - val_accuracy: 0.6186 - 9s/epoch - 8ms/step\n",
      "Epoch 43/10000\n",
      "1126/1126 - 9s - loss: 90514520.0000 - accuracy: 0.5836 - val_loss: 32587702.0000 - val_accuracy: 0.5739 - 9s/epoch - 8ms/step\n",
      "Epoch 44/10000\n",
      "1126/1126 - 9s - loss: 100783664.0000 - accuracy: 0.5821 - val_loss: 47700540.0000 - val_accuracy: 0.6257 - 9s/epoch - 8ms/step\n",
      "Epoch 45/10000\n",
      "1126/1126 - 9s - loss: 85719184.0000 - accuracy: 0.5831 - val_loss: 82211744.0000 - val_accuracy: 0.4720 - 9s/epoch - 8ms/step\n",
      "Epoch 46/10000\n",
      "1126/1126 - 9s - loss: 88294776.0000 - accuracy: 0.5824 - val_loss: 212197808.0000 - val_accuracy: 0.6139 - 9s/epoch - 8ms/step\n",
      "Epoch 47/10000\n",
      "1126/1126 - 9s - loss: 90640032.0000 - accuracy: 0.5829 - val_loss: 46742316.0000 - val_accuracy: 0.6390 - 9s/epoch - 8ms/step\n",
      "Epoch 48/10000\n",
      "1126/1126 - 9s - loss: 86086912.0000 - accuracy: 0.5859 - val_loss: 45224764.0000 - val_accuracy: 0.5752 - 9s/epoch - 8ms/step\n",
      "Epoch 49/10000\n",
      "1126/1126 - 9s - loss: 79307472.0000 - accuracy: 0.5844 - val_loss: 25455344.0000 - val_accuracy: 0.6113 - 9s/epoch - 8ms/step\n",
      "Epoch 50/10000\n",
      "1126/1126 - 9s - loss: 80674264.0000 - accuracy: 0.5857 - val_loss: 170070080.0000 - val_accuracy: 0.4293 - 9s/epoch - 8ms/step\n",
      "391/391 [==============================] - 2s 4ms/step\n",
      "\n",
      "Iterazione 82/128 (03-11-2023_12-14-30)\n",
      "hidden_layer_size: 500\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 3\n",
      "activation_hidden: relu\n",
      "activation_output: sigmoid\n",
      "batch_size: 100\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 20\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 65.53%\n",
      "La recall di questo modello sul validation set è: 42.31%\n",
      "La f1 di questo modello sul validation set è: 48.71%\n",
      "La balanced accuracy di questo modello sul validation set è: 61.24%\n",
      "La precision di questo modello sul validation set è: 57.4%\n",
      "La AUC di questo modello sul validation set è: 61.24%\n",
      "391/391 [==============================] - 2s 4ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.69    0.80      0.74  7,670.00\n",
      "1                  0.57    0.42      0.49  4,841.00\n",
      "accuracy           0.66    0.66      0.66      0.66\n",
      "macro avg          0.63    0.61      0.61 12,511.00\n",
      "weighted avg       0.64    0.66      0.64 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         6150         1520\n",
      "Actual 1         2793         2048\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 66.1% con il modello:\n",
      "66 - AC66.1%, L500, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA20, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_11-28-58).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 62.97% con il modello:\n",
      "1 - BA62.97%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 64.08% con il modello:\n",
      "13 - PR64.08%, L20, mV-10, MV10, P2, AHsigmoid, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-26-48).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "113/113 - 5s - loss: 1425117184.0000 - accuracy: 0.5134 - val_loss: 97160800.0000 - val_accuracy: 0.5202 - 5s/epoch - 43ms/step\n",
      "Epoch 2/10000\n",
      "113/113 - 3s - loss: 103819888.0000 - accuracy: 0.5460 - val_loss: 122434448.0000 - val_accuracy: 0.4768 - 3s/epoch - 29ms/step\n",
      "Epoch 3/10000\n",
      "113/113 - 3s - loss: 101648088.0000 - accuracy: 0.5607 - val_loss: 56018436.0000 - val_accuracy: 0.6020 - 3s/epoch - 29ms/step\n",
      "Epoch 4/10000\n",
      "113/113 - 3s - loss: 108519320.0000 - accuracy: 0.5616 - val_loss: 46834532.0000 - val_accuracy: 0.6163 - 3s/epoch - 29ms/step\n",
      "Epoch 5/10000\n",
      "113/113 - 3s - loss: 129173200.0000 - accuracy: 0.5609 - val_loss: 228735824.0000 - val_accuracy: 0.4653 - 3s/epoch - 28ms/step\n",
      "Epoch 6/10000\n",
      "113/113 - 3s - loss: 104053056.0000 - accuracy: 0.5657 - val_loss: 171144960.0000 - val_accuracy: 0.6174 - 3s/epoch - 29ms/step\n",
      "Epoch 7/10000\n",
      "113/113 - 3s - loss: 85058264.0000 - accuracy: 0.5667 - val_loss: 242615456.0000 - val_accuracy: 0.6129 - 3s/epoch - 29ms/step\n",
      "Epoch 8/10000\n",
      "113/113 - 3s - loss: 76586768.0000 - accuracy: 0.5706 - val_loss: 46571068.0000 - val_accuracy: 0.6265 - 3s/epoch - 30ms/step\n",
      "Epoch 9/10000\n",
      "113/113 - 3s - loss: 115434520.0000 - accuracy: 0.5649 - val_loss: 138570560.0000 - val_accuracy: 0.6127 - 3s/epoch - 28ms/step\n",
      "Epoch 10/10000\n",
      "113/113 - 3s - loss: 107376112.0000 - accuracy: 0.5712 - val_loss: 30207406.0000 - val_accuracy: 0.6161 - 3s/epoch - 27ms/step\n",
      "Epoch 11/10000\n",
      "113/113 - 3s - loss: 130445272.0000 - accuracy: 0.5652 - val_loss: 83236432.0000 - val_accuracy: 0.6243 - 3s/epoch - 28ms/step\n",
      "Epoch 12/10000\n",
      "113/113 - 3s - loss: 135836752.0000 - accuracy: 0.5649 - val_loss: 86419840.0000 - val_accuracy: 0.6264 - 3s/epoch - 28ms/step\n",
      "Epoch 13/10000\n",
      "113/113 - 3s - loss: 73910776.0000 - accuracy: 0.5726 - val_loss: 41768508.0000 - val_accuracy: 0.6311 - 3s/epoch - 27ms/step\n",
      "Epoch 14/10000\n",
      "113/113 - 3s - loss: 103521872.0000 - accuracy: 0.5722 - val_loss: 116303480.0000 - val_accuracy: 0.6171 - 3s/epoch - 27ms/step\n",
      "Epoch 15/10000\n",
      "113/113 - 3s - loss: 78683432.0000 - accuracy: 0.5699 - val_loss: 56065688.0000 - val_accuracy: 0.5299 - 3s/epoch - 28ms/step\n",
      "Epoch 16/10000\n",
      "113/113 - 3s - loss: 85099664.0000 - accuracy: 0.5691 - val_loss: 191930896.0000 - val_accuracy: 0.6170 - 3s/epoch - 27ms/step\n",
      "Epoch 17/10000\n",
      "113/113 - 3s - loss: 96946280.0000 - accuracy: 0.5717 - val_loss: 84017816.0000 - val_accuracy: 0.4746 - 3s/epoch - 28ms/step\n",
      "Epoch 18/10000\n",
      "113/113 - 3s - loss: 122568704.0000 - accuracy: 0.5669 - val_loss: 224829488.0000 - val_accuracy: 0.6132 - 3s/epoch - 28ms/step\n",
      "Epoch 19/10000\n",
      "113/113 - 4s - loss: 117250024.0000 - accuracy: 0.5691 - val_loss: 43310120.0000 - val_accuracy: 0.6437 - 4s/epoch - 31ms/step\n",
      "Epoch 20/10000\n",
      "113/113 - 3s - loss: 91536552.0000 - accuracy: 0.5731 - val_loss: 38831144.0000 - val_accuracy: 0.6410 - 3s/epoch - 29ms/step\n",
      "Epoch 21/10000\n",
      "113/113 - 3s - loss: 168901008.0000 - accuracy: 0.5643 - val_loss: 196796928.0000 - val_accuracy: 0.4838 - 3s/epoch - 28ms/step\n",
      "Epoch 22/10000\n",
      "113/113 - 3s - loss: 85439176.0000 - accuracy: 0.5725 - val_loss: 246483296.0000 - val_accuracy: 0.6131 - 3s/epoch - 28ms/step\n",
      "Epoch 23/10000\n",
      "113/113 - 3s - loss: 106079352.0000 - accuracy: 0.5715 - val_loss: 29186204.0000 - val_accuracy: 0.6333 - 3s/epoch - 27ms/step\n",
      "Epoch 24/10000\n",
      "113/113 - 3s - loss: 91985064.0000 - accuracy: 0.5691 - val_loss: 111339960.0000 - val_accuracy: 0.4708 - 3s/epoch - 28ms/step\n",
      "Epoch 25/10000\n",
      "113/113 - 3s - loss: 88349480.0000 - accuracy: 0.5713 - val_loss: 195518288.0000 - val_accuracy: 0.4453 - 3s/epoch - 28ms/step\n",
      "Epoch 26/10000\n",
      "113/113 - 3s - loss: 111923600.0000 - accuracy: 0.5699 - val_loss: 190608464.0000 - val_accuracy: 0.6210 - 3s/epoch - 27ms/step\n",
      "Epoch 27/10000\n",
      "113/113 - 3s - loss: 120803952.0000 - accuracy: 0.5694 - val_loss: 41510528.0000 - val_accuracy: 0.6358 - 3s/epoch - 27ms/step\n",
      "Epoch 28/10000\n",
      "113/113 - 3s - loss: 98041584.0000 - accuracy: 0.5731 - val_loss: 43448116.0000 - val_accuracy: 0.6107 - 3s/epoch - 27ms/step\n",
      "Epoch 29/10000\n",
      "113/113 - 3s - loss: 97985264.0000 - accuracy: 0.5715 - val_loss: 173977808.0000 - val_accuracy: 0.6133 - 3s/epoch - 28ms/step\n",
      "391/391 [==============================] - 2s 4ms/step\n",
      "\n",
      "Iterazione 83/128 (03-11-2023_12-16-08)\n",
      "hidden_layer_size: 500\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 3\n",
      "activation_hidden: relu\n",
      "activation_output: sigmoid\n",
      "batch_size: 1000\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 10\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 64.36%\n",
      "La recall di questo modello sul validation set è: 25.57%\n",
      "La f1 di questo modello sul validation set è: 35.7%\n",
      "La balanced accuracy di questo modello sul validation set è: 57.21%\n",
      "La precision di questo modello sul validation set è: 59.12%\n",
      "La AUC di questo modello sul validation set è: 57.21%\n",
      "391/391 [==============================] - 2s 4ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.65    0.89      0.75  7,670.00\n",
      "1                  0.59    0.26      0.36  4,841.00\n",
      "accuracy           0.64    0.64      0.64      0.64\n",
      "macro avg          0.62    0.57      0.56 12,511.00\n",
      "weighted avg       0.63    0.64      0.60 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         6814          856\n",
      "Actual 1         3603         1238\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 66.1% con il modello:\n",
      "66 - AC66.1%, L500, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA20, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_11-28-58).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 62.97% con il modello:\n",
      "1 - BA62.97%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 64.08% con il modello:\n",
      "13 - PR64.08%, L20, mV-10, MV10, P2, AHsigmoid, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-26-48).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "113/113 - 5s - loss: 1425117184.0000 - accuracy: 0.5134 - val_loss: 97160800.0000 - val_accuracy: 0.5202 - 5s/epoch - 43ms/step\n",
      "Epoch 2/10000\n",
      "113/113 - 3s - loss: 103819888.0000 - accuracy: 0.5460 - val_loss: 122434448.0000 - val_accuracy: 0.4768 - 3s/epoch - 28ms/step\n",
      "Epoch 3/10000\n",
      "113/113 - 3s - loss: 101648088.0000 - accuracy: 0.5607 - val_loss: 56018436.0000 - val_accuracy: 0.6020 - 3s/epoch - 30ms/step\n",
      "Epoch 4/10000\n",
      "113/113 - 4s - loss: 108519320.0000 - accuracy: 0.5616 - val_loss: 46834532.0000 - val_accuracy: 0.6163 - 4s/epoch - 35ms/step\n",
      "Epoch 5/10000\n",
      "113/113 - 4s - loss: 129173200.0000 - accuracy: 0.5609 - val_loss: 228735824.0000 - val_accuracy: 0.4653 - 4s/epoch - 32ms/step\n",
      "Epoch 6/10000\n",
      "113/113 - 4s - loss: 104053056.0000 - accuracy: 0.5657 - val_loss: 171144960.0000 - val_accuracy: 0.6174 - 4s/epoch - 39ms/step\n",
      "Epoch 7/10000\n",
      "113/113 - 4s - loss: 85058264.0000 - accuracy: 0.5667 - val_loss: 242615456.0000 - val_accuracy: 0.6129 - 4s/epoch - 39ms/step\n",
      "Epoch 8/10000\n",
      "113/113 - 4s - loss: 76586768.0000 - accuracy: 0.5706 - val_loss: 46571068.0000 - val_accuracy: 0.6265 - 4s/epoch - 39ms/step\n",
      "Epoch 9/10000\n",
      "113/113 - 4s - loss: 115434520.0000 - accuracy: 0.5649 - val_loss: 138570560.0000 - val_accuracy: 0.6127 - 4s/epoch - 39ms/step\n",
      "Epoch 10/10000\n",
      "113/113 - 5s - loss: 107376112.0000 - accuracy: 0.5712 - val_loss: 30207406.0000 - val_accuracy: 0.6161 - 5s/epoch - 42ms/step\n",
      "Epoch 11/10000\n",
      "113/113 - 4s - loss: 130445272.0000 - accuracy: 0.5652 - val_loss: 83236432.0000 - val_accuracy: 0.6243 - 4s/epoch - 36ms/step\n",
      "Epoch 12/10000\n",
      "113/113 - 4s - loss: 135836752.0000 - accuracy: 0.5649 - val_loss: 86419840.0000 - val_accuracy: 0.6264 - 4s/epoch - 34ms/step\n",
      "Epoch 13/10000\n",
      "113/113 - 4s - loss: 73910776.0000 - accuracy: 0.5726 - val_loss: 41768508.0000 - val_accuracy: 0.6311 - 4s/epoch - 38ms/step\n",
      "Epoch 14/10000\n",
      "113/113 - 4s - loss: 103521872.0000 - accuracy: 0.5722 - val_loss: 116303480.0000 - val_accuracy: 0.6171 - 4s/epoch - 38ms/step\n",
      "Epoch 15/10000\n",
      "113/113 - 5s - loss: 78683432.0000 - accuracy: 0.5699 - val_loss: 56065688.0000 - val_accuracy: 0.5299 - 5s/epoch - 43ms/step\n",
      "Epoch 16/10000\n",
      "113/113 - 5s - loss: 85099664.0000 - accuracy: 0.5691 - val_loss: 191930896.0000 - val_accuracy: 0.6170 - 5s/epoch - 43ms/step\n",
      "Epoch 17/10000\n",
      "113/113 - 4s - loss: 96946280.0000 - accuracy: 0.5717 - val_loss: 84017816.0000 - val_accuracy: 0.4746 - 4s/epoch - 38ms/step\n",
      "Epoch 18/10000\n",
      "113/113 - 4s - loss: 122568704.0000 - accuracy: 0.5669 - val_loss: 224829488.0000 - val_accuracy: 0.6132 - 4s/epoch - 35ms/step\n",
      "Epoch 19/10000\n",
      "113/113 - 4s - loss: 117250024.0000 - accuracy: 0.5691 - val_loss: 43310120.0000 - val_accuracy: 0.6437 - 4s/epoch - 33ms/step\n",
      "Epoch 20/10000\n",
      "113/113 - 4s - loss: 91536552.0000 - accuracy: 0.5731 - val_loss: 38831144.0000 - val_accuracy: 0.6410 - 4s/epoch - 33ms/step\n",
      "Epoch 21/10000\n",
      "113/113 - 4s - loss: 168901008.0000 - accuracy: 0.5643 - val_loss: 196796928.0000 - val_accuracy: 0.4838 - 4s/epoch - 35ms/step\n",
      "Epoch 22/10000\n",
      "113/113 - 4s - loss: 85439176.0000 - accuracy: 0.5725 - val_loss: 246483296.0000 - val_accuracy: 0.6131 - 4s/epoch - 33ms/step\n",
      "Epoch 23/10000\n",
      "113/113 - 4s - loss: 106079352.0000 - accuracy: 0.5715 - val_loss: 29186204.0000 - val_accuracy: 0.6333 - 4s/epoch - 33ms/step\n",
      "Epoch 24/10000\n",
      "113/113 - 4s - loss: 91985064.0000 - accuracy: 0.5691 - val_loss: 111339960.0000 - val_accuracy: 0.4708 - 4s/epoch - 33ms/step\n",
      "Epoch 25/10000\n",
      "113/113 - 4s - loss: 88349480.0000 - accuracy: 0.5713 - val_loss: 195518288.0000 - val_accuracy: 0.4453 - 4s/epoch - 33ms/step\n",
      "Epoch 26/10000\n",
      "113/113 - 4s - loss: 111923600.0000 - accuracy: 0.5699 - val_loss: 190608464.0000 - val_accuracy: 0.6210 - 4s/epoch - 34ms/step\n",
      "Epoch 27/10000\n",
      "113/113 - 4s - loss: 120803952.0000 - accuracy: 0.5694 - val_loss: 41510528.0000 - val_accuracy: 0.6358 - 4s/epoch - 32ms/step\n",
      "Epoch 28/10000\n",
      "113/113 - 4s - loss: 98041584.0000 - accuracy: 0.5731 - val_loss: 43448116.0000 - val_accuracy: 0.6107 - 4s/epoch - 33ms/step\n",
      "Epoch 29/10000\n",
      "113/113 - 4s - loss: 97985264.0000 - accuracy: 0.5715 - val_loss: 173977808.0000 - val_accuracy: 0.6133 - 4s/epoch - 32ms/step\n",
      "Epoch 30/10000\n",
      "113/113 - 4s - loss: 109930872.0000 - accuracy: 0.5685 - val_loss: 31329288.0000 - val_accuracy: 0.6179 - 4s/epoch - 33ms/step\n",
      "Epoch 31/10000\n",
      "113/113 - 4s - loss: 111494816.0000 - accuracy: 0.5706 - val_loss: 97709208.0000 - val_accuracy: 0.5323 - 4s/epoch - 33ms/step\n",
      "Epoch 32/10000\n",
      "113/113 - 4s - loss: 86315176.0000 - accuracy: 0.5704 - val_loss: 59487788.0000 - val_accuracy: 0.6219 - 4s/epoch - 32ms/step\n",
      "Epoch 33/10000\n",
      "113/113 - 4s - loss: 75923584.0000 - accuracy: 0.5723 - val_loss: 50535420.0000 - val_accuracy: 0.6310 - 4s/epoch - 33ms/step\n",
      "Epoch 34/10000\n",
      "113/113 - 4s - loss: 99347880.0000 - accuracy: 0.5729 - val_loss: 91666952.0000 - val_accuracy: 0.6261 - 4s/epoch - 33ms/step\n",
      "Epoch 35/10000\n",
      "113/113 - 4s - loss: 100390784.0000 - accuracy: 0.5713 - val_loss: 32497312.0000 - val_accuracy: 0.6435 - 4s/epoch - 32ms/step\n",
      "Epoch 36/10000\n",
      "113/113 - 4s - loss: 95414400.0000 - accuracy: 0.5727 - val_loss: 46260960.0000 - val_accuracy: 0.6336 - 4s/epoch - 33ms/step\n",
      "Epoch 37/10000\n",
      "113/113 - 4s - loss: 94889928.0000 - accuracy: 0.5704 - val_loss: 81331184.0000 - val_accuracy: 0.6310 - 4s/epoch - 33ms/step\n",
      "Epoch 38/10000\n",
      "113/113 - 5s - loss: 98766160.0000 - accuracy: 0.5740 - val_loss: 137424432.0000 - val_accuracy: 0.6203 - 5s/epoch - 41ms/step\n",
      "Epoch 39/10000\n",
      "113/113 - 7s - loss: 100591960.0000 - accuracy: 0.5730 - val_loss: 234666400.0000 - val_accuracy: 0.4080 - 7s/epoch - 60ms/step\n",
      "391/391 [==============================] - 3s 8ms/step\n",
      "\n",
      "Iterazione 84/128 (03-11-2023_12-18-52)\n",
      "hidden_layer_size: 500\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 3\n",
      "activation_hidden: relu\n",
      "activation_output: sigmoid\n",
      "batch_size: 1000\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 20\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 64.36%\n",
      "La recall di questo modello sul validation set è: 25.57%\n",
      "La f1 di questo modello sul validation set è: 35.7%\n",
      "La balanced accuracy di questo modello sul validation set è: 57.21%\n",
      "La precision di questo modello sul validation set è: 59.12%\n",
      "La AUC di questo modello sul validation set è: 57.21%\n",
      "391/391 [==============================] - 3s 8ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.65    0.89      0.75  7,670.00\n",
      "1                  0.59    0.26      0.36  4,841.00\n",
      "accuracy           0.64    0.64      0.64      0.64\n",
      "macro avg          0.62    0.57      0.56 12,511.00\n",
      "weighted avg       0.63    0.64      0.60 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         6814          856\n",
      "Actual 1         3603         1238\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 66.1% con il modello:\n",
      "66 - AC66.1%, L500, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA20, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_11-28-58).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 62.97% con il modello:\n",
      "1 - BA62.97%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 64.08% con il modello:\n",
      "13 - PR64.08%, L20, mV-10, MV10, P2, AHsigmoid, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-26-48).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "1126/1126 - 16s - loss: 2.2979 - accuracy: 0.5662 - val_loss: 1.8393 - val_accuracy: 0.5434 - 16s/epoch - 14ms/step\n",
      "Epoch 2/10000\n",
      "1126/1126 - 13s - loss: 1.7936 - accuracy: 0.5700 - val_loss: 1.6622 - val_accuracy: 0.5793 - 13s/epoch - 11ms/step\n",
      "Epoch 3/10000\n",
      "1126/1126 - 13s - loss: 1.6492 - accuracy: 0.5730 - val_loss: 1.4893 - val_accuracy: 0.5668 - 13s/epoch - 11ms/step\n",
      "Epoch 4/10000\n",
      "1126/1126 - 13s - loss: 1.5516 - accuracy: 0.5728 - val_loss: 1.4573 - val_accuracy: 0.5757 - 13s/epoch - 11ms/step\n",
      "Epoch 5/10000\n",
      "1126/1126 - 12s - loss: 1.5240 - accuracy: 0.5740 - val_loss: 1.4065 - val_accuracy: 0.5975 - 12s/epoch - 11ms/step\n",
      "Epoch 6/10000\n",
      "1126/1126 - 12s - loss: 1.4804 - accuracy: 0.5770 - val_loss: 1.3753 - val_accuracy: 0.5846 - 12s/epoch - 11ms/step\n",
      "Epoch 7/10000\n",
      "1126/1126 - 14s - loss: 1.3993 - accuracy: 0.5790 - val_loss: 1.3859 - val_accuracy: 0.5775 - 14s/epoch - 12ms/step\n",
      "Epoch 8/10000\n",
      "1126/1126 - 11s - loss: 1.3926 - accuracy: 0.5805 - val_loss: 1.3685 - val_accuracy: 0.5870 - 11s/epoch - 10ms/step\n",
      "Epoch 9/10000\n",
      "1126/1126 - 11s - loss: 1.3911 - accuracy: 0.5769 - val_loss: 1.3833 - val_accuracy: 0.6029 - 11s/epoch - 10ms/step\n",
      "Epoch 10/10000\n",
      "1126/1126 - 12s - loss: 1.3554 - accuracy: 0.5798 - val_loss: 1.3825 - val_accuracy: 0.5868 - 12s/epoch - 10ms/step\n",
      "Epoch 11/10000\n",
      "1126/1126 - 12s - loss: 1.3042 - accuracy: 0.5799 - val_loss: 1.1935 - val_accuracy: 0.6101 - 12s/epoch - 11ms/step\n",
      "Epoch 12/10000\n",
      "1126/1126 - 12s - loss: 1.2490 - accuracy: 0.5799 - val_loss: 1.2077 - val_accuracy: 0.6098 - 12s/epoch - 11ms/step\n",
      "Epoch 13/10000\n",
      "1126/1126 - 13s - loss: 1.2223 - accuracy: 0.5814 - val_loss: 1.2184 - val_accuracy: 0.5892 - 13s/epoch - 11ms/step\n",
      "Epoch 14/10000\n",
      "1126/1126 - 12s - loss: 1.2118 - accuracy: 0.5825 - val_loss: 1.1697 - val_accuracy: 0.5930 - 12s/epoch - 11ms/step\n",
      "Epoch 15/10000\n",
      "1126/1126 - 12s - loss: 1.1862 - accuracy: 0.5782 - val_loss: 1.2456 - val_accuracy: 0.5853 - 12s/epoch - 10ms/step\n",
      "Epoch 16/10000\n",
      "1126/1126 - 12s - loss: 1.1836 - accuracy: 0.5798 - val_loss: 1.1359 - val_accuracy: 0.5796 - 12s/epoch - 11ms/step\n",
      "Epoch 17/10000\n",
      "1126/1126 - 13s - loss: 1.1401 - accuracy: 0.5804 - val_loss: 1.0720 - val_accuracy: 0.5962 - 13s/epoch - 11ms/step\n",
      "Epoch 18/10000\n",
      "1126/1126 - 12s - loss: 1.1128 - accuracy: 0.5852 - val_loss: 1.0764 - val_accuracy: 0.6007 - 12s/epoch - 11ms/step\n",
      "Epoch 19/10000\n",
      "1126/1126 - 11s - loss: 1.1083 - accuracy: 0.5830 - val_loss: 1.1500 - val_accuracy: 0.5776 - 11s/epoch - 10ms/step\n",
      "Epoch 20/10000\n",
      "1126/1126 - 11s - loss: 1.0848 - accuracy: 0.5857 - val_loss: 1.0524 - val_accuracy: 0.5820 - 11s/epoch - 10ms/step\n",
      "Epoch 21/10000\n",
      "1126/1126 - 11s - loss: 1.0707 - accuracy: 0.5868 - val_loss: 1.1407 - val_accuracy: 0.5935 - 11s/epoch - 10ms/step\n",
      "391/391 [==============================] - 2s 6ms/step\n",
      "\n",
      "Iterazione 85/128 (03-11-2023_12-23-18)\n",
      "hidden_layer_size: 500\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 3\n",
      "activation_hidden: sigmoid\n",
      "activation_output: sigmoid\n",
      "batch_size: 100\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 10\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 61.01%\n",
      "La recall di questo modello sul validation set è: 39.6%\n",
      "La f1 di questo modello sul validation set è: 44.01%\n",
      "La balanced accuracy di questo modello sul validation set è: 57.06%\n",
      "La precision di questo modello sul validation set è: 49.52%\n",
      "La AUC di questo modello sul validation set è: 60.79%\n",
      "391/391 [==============================] - 2s 5ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.66    0.75      0.70  7,670.00\n",
      "1                  0.50    0.40      0.44  4,841.00\n",
      "accuracy           0.61    0.61      0.61      0.61\n",
      "macro avg          0.58    0.57      0.57 12,511.00\n",
      "weighted avg       0.60    0.61      0.60 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         5716         1954\n",
      "Actual 1         2924         1917\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 66.1% con il modello:\n",
      "66 - AC66.1%, L500, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA20, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_11-28-58).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 62.97% con il modello:\n",
      "1 - BA62.97%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 64.08% con il modello:\n",
      "13 - PR64.08%, L20, mV-10, MV10, P2, AHsigmoid, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-26-48).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "1126/1126 - 15s - loss: 2.2979 - accuracy: 0.5662 - val_loss: 1.8393 - val_accuracy: 0.5434 - 15s/epoch - 13ms/step\n",
      "Epoch 2/10000\n",
      "1126/1126 - 13s - loss: 1.7936 - accuracy: 0.5700 - val_loss: 1.6622 - val_accuracy: 0.5793 - 13s/epoch - 12ms/step\n",
      "Epoch 3/10000\n",
      "1126/1126 - 12s - loss: 1.6492 - accuracy: 0.5730 - val_loss: 1.4893 - val_accuracy: 0.5668 - 12s/epoch - 11ms/step\n",
      "Epoch 4/10000\n",
      "1126/1126 - 12s - loss: 1.5516 - accuracy: 0.5728 - val_loss: 1.4573 - val_accuracy: 0.5757 - 12s/epoch - 11ms/step\n",
      "Epoch 5/10000\n",
      "1126/1126 - 13s - loss: 1.5240 - accuracy: 0.5740 - val_loss: 1.4065 - val_accuracy: 0.5975 - 13s/epoch - 11ms/step\n",
      "Epoch 6/10000\n",
      "1126/1126 - 13s - loss: 1.4804 - accuracy: 0.5770 - val_loss: 1.3753 - val_accuracy: 0.5846 - 13s/epoch - 12ms/step\n",
      "Epoch 7/10000\n",
      "1126/1126 - 13s - loss: 1.3993 - accuracy: 0.5790 - val_loss: 1.3859 - val_accuracy: 0.5775 - 13s/epoch - 11ms/step\n",
      "Epoch 8/10000\n",
      "1126/1126 - 11s - loss: 1.3926 - accuracy: 0.5805 - val_loss: 1.3685 - val_accuracy: 0.5870 - 11s/epoch - 10ms/step\n",
      "Epoch 9/10000\n",
      "1126/1126 - 11s - loss: 1.3911 - accuracy: 0.5769 - val_loss: 1.3833 - val_accuracy: 0.6029 - 11s/epoch - 10ms/step\n",
      "Epoch 10/10000\n",
      "1126/1126 - 11s - loss: 1.3554 - accuracy: 0.5798 - val_loss: 1.3825 - val_accuracy: 0.5868 - 11s/epoch - 10ms/step\n",
      "Epoch 11/10000\n",
      "1126/1126 - 11s - loss: 1.3042 - accuracy: 0.5799 - val_loss: 1.1935 - val_accuracy: 0.6101 - 11s/epoch - 10ms/step\n",
      "Epoch 12/10000\n",
      "1126/1126 - 11s - loss: 1.2490 - accuracy: 0.5799 - val_loss: 1.2077 - val_accuracy: 0.6098 - 11s/epoch - 10ms/step\n",
      "Epoch 13/10000\n",
      "1126/1126 - 12s - loss: 1.2223 - accuracy: 0.5814 - val_loss: 1.2184 - val_accuracy: 0.5892 - 12s/epoch - 11ms/step\n",
      "Epoch 14/10000\n",
      "1126/1126 - 12s - loss: 1.2118 - accuracy: 0.5825 - val_loss: 1.1697 - val_accuracy: 0.5930 - 12s/epoch - 11ms/step\n",
      "Epoch 15/10000\n",
      "1126/1126 - 11s - loss: 1.1862 - accuracy: 0.5782 - val_loss: 1.2456 - val_accuracy: 0.5853 - 11s/epoch - 10ms/step\n",
      "Epoch 16/10000\n",
      "1126/1126 - 11s - loss: 1.1836 - accuracy: 0.5798 - val_loss: 1.1359 - val_accuracy: 0.5796 - 11s/epoch - 10ms/step\n",
      "Epoch 17/10000\n",
      "1126/1126 - 11s - loss: 1.1401 - accuracy: 0.5804 - val_loss: 1.0720 - val_accuracy: 0.5962 - 11s/epoch - 10ms/step\n",
      "Epoch 18/10000\n",
      "1126/1126 - 11s - loss: 1.1128 - accuracy: 0.5852 - val_loss: 1.0764 - val_accuracy: 0.6007 - 11s/epoch - 10ms/step\n",
      "Epoch 19/10000\n",
      "1126/1126 - 11s - loss: 1.1083 - accuracy: 0.5830 - val_loss: 1.1500 - val_accuracy: 0.5776 - 11s/epoch - 10ms/step\n",
      "Epoch 20/10000\n",
      "1126/1126 - 11s - loss: 1.0848 - accuracy: 0.5857 - val_loss: 1.0524 - val_accuracy: 0.5820 - 11s/epoch - 10ms/step\n",
      "Epoch 21/10000\n",
      "1126/1126 - 11s - loss: 1.0707 - accuracy: 0.5868 - val_loss: 1.1407 - val_accuracy: 0.5935 - 11s/epoch - 10ms/step\n",
      "Epoch 22/10000\n",
      "1126/1126 - 11s - loss: 1.0831 - accuracy: 0.5863 - val_loss: 1.0430 - val_accuracy: 0.6014 - 11s/epoch - 10ms/step\n",
      "Epoch 23/10000\n",
      "1126/1126 - 11s - loss: 1.0398 - accuracy: 0.5900 - val_loss: 1.0568 - val_accuracy: 0.5802 - 11s/epoch - 10ms/step\n",
      "Epoch 24/10000\n",
      "1126/1126 - 12s - loss: 1.0347 - accuracy: 0.5875 - val_loss: 1.0094 - val_accuracy: 0.5911 - 12s/epoch - 10ms/step\n",
      "Epoch 25/10000\n",
      "1126/1126 - 11s - loss: 1.0389 - accuracy: 0.5887 - val_loss: 0.9902 - val_accuracy: 0.5980 - 11s/epoch - 10ms/step\n",
      "Epoch 26/10000\n",
      "1126/1126 - 11s - loss: 0.9983 - accuracy: 0.5898 - val_loss: 0.9675 - val_accuracy: 0.5902 - 11s/epoch - 10ms/step\n",
      "Epoch 27/10000\n",
      "1126/1126 - 12s - loss: 0.9854 - accuracy: 0.5879 - val_loss: 0.9879 - val_accuracy: 0.5987 - 12s/epoch - 10ms/step\n",
      "Epoch 28/10000\n",
      "1126/1126 - 11s - loss: 0.9691 - accuracy: 0.5919 - val_loss: 0.9718 - val_accuracy: 0.5780 - 11s/epoch - 10ms/step\n",
      "Epoch 29/10000\n",
      "1126/1126 - 12s - loss: 0.9592 - accuracy: 0.5899 - val_loss: 0.9924 - val_accuracy: 0.5915 - 12s/epoch - 10ms/step\n",
      "Epoch 30/10000\n",
      "1126/1126 - 14s - loss: 0.9579 - accuracy: 0.5946 - val_loss: 0.9367 - val_accuracy: 0.6064 - 14s/epoch - 13ms/step\n",
      "Epoch 31/10000\n",
      "1126/1126 - 18s - loss: 0.9733 - accuracy: 0.5930 - val_loss: 0.9672 - val_accuracy: 0.5714 - 18s/epoch - 16ms/step\n",
      "391/391 [==============================] - 3s 7ms/step\n",
      "\n",
      "Iterazione 86/128 (03-11-2023_12-29-37)\n",
      "hidden_layer_size: 500\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 3\n",
      "activation_hidden: sigmoid\n",
      "activation_output: sigmoid\n",
      "batch_size: 100\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 20\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 61.01%\n",
      "La recall di questo modello sul validation set è: 39.6%\n",
      "La f1 di questo modello sul validation set è: 44.01%\n",
      "La balanced accuracy di questo modello sul validation set è: 57.06%\n",
      "La precision di questo modello sul validation set è: 49.52%\n",
      "La AUC di questo modello sul validation set è: 60.79%\n",
      "391/391 [==============================] - 2s 6ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.66    0.75      0.70  7,670.00\n",
      "1                  0.50    0.40      0.44  4,841.00\n",
      "accuracy           0.61    0.61      0.61      0.61\n",
      "macro avg          0.58    0.57      0.57 12,511.00\n",
      "weighted avg       0.60    0.61      0.60 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         5716         1954\n",
      "Actual 1         2924         1917\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 66.1% con il modello:\n",
      "66 - AC66.1%, L500, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA20, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_11-28-58).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 62.97% con il modello:\n",
      "1 - BA62.97%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 64.08% con il modello:\n",
      "13 - PR64.08%, L20, mV-10, MV10, P2, AHsigmoid, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-26-48).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "113/113 - 7s - loss: 3.5115 - accuracy: 0.5656 - val_loss: 2.0017 - val_accuracy: 0.6060 - 7s/epoch - 58ms/step\n",
      "Epoch 2/10000\n",
      "113/113 - 4s - loss: 1.7925 - accuracy: 0.5873 - val_loss: 1.7438 - val_accuracy: 0.5928 - 4s/epoch - 39ms/step\n",
      "Epoch 3/10000\n",
      "113/113 - 5s - loss: 1.5536 - accuracy: 0.5902 - val_loss: 1.5654 - val_accuracy: 0.5523 - 5s/epoch - 40ms/step\n",
      "Epoch 4/10000\n",
      "113/113 - 5s - loss: 1.4296 - accuracy: 0.5918 - val_loss: 1.4862 - val_accuracy: 0.5848 - 5s/epoch - 42ms/step\n",
      "Epoch 5/10000\n",
      "113/113 - 4s - loss: 1.4051 - accuracy: 0.5953 - val_loss: 1.4436 - val_accuracy: 0.5977 - 4s/epoch - 39ms/step\n",
      "Epoch 6/10000\n",
      "113/113 - 4s - loss: 1.3052 - accuracy: 0.5971 - val_loss: 1.3746 - val_accuracy: 0.6028 - 4s/epoch - 39ms/step\n",
      "Epoch 7/10000\n",
      "113/113 - 4s - loss: 1.2582 - accuracy: 0.5964 - val_loss: 1.3480 - val_accuracy: 0.5729 - 4s/epoch - 39ms/step\n",
      "Epoch 8/10000\n",
      "113/113 - 4s - loss: 1.2396 - accuracy: 0.5939 - val_loss: 1.3094 - val_accuracy: 0.5741 - 4s/epoch - 39ms/step\n",
      "Epoch 9/10000\n",
      "113/113 - 4s - loss: 1.2211 - accuracy: 0.5983 - val_loss: 1.2300 - val_accuracy: 0.5915 - 4s/epoch - 37ms/step\n",
      "Epoch 10/10000\n",
      "113/113 - 5s - loss: 1.2192 - accuracy: 0.5995 - val_loss: 1.2844 - val_accuracy: 0.5800 - 5s/epoch - 42ms/step\n",
      "Epoch 11/10000\n",
      "113/113 - 5s - loss: 1.1967 - accuracy: 0.5971 - val_loss: 1.2171 - val_accuracy: 0.6054 - 5s/epoch - 40ms/step\n",
      "391/391 [==============================] - 2s 6ms/step\n",
      "\n",
      "Iterazione 87/128 (03-11-2023_12-30-35)\n",
      "hidden_layer_size: 500\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 3\n",
      "activation_hidden: sigmoid\n",
      "activation_output: sigmoid\n",
      "batch_size: 1000\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 10\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 60.6%\n",
      "La recall di questo modello sul validation set è: 36.05%\n",
      "La f1 di questo modello sul validation set è: 41.45%\n",
      "La balanced accuracy di questo modello sul validation set è: 56.07%\n",
      "La precision di questo modello sul validation set è: 48.77%\n",
      "La AUC di questo modello sul validation set è: 58.9%\n",
      "391/391 [==============================] - 2s 5ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.65    0.76      0.70  7,670.00\n",
      "1                  0.49    0.36      0.41  4,841.00\n",
      "accuracy           0.61    0.61      0.61      0.61\n",
      "macro avg          0.57    0.56      0.56 12,511.00\n",
      "weighted avg       0.59    0.61      0.59 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         5837         1833\n",
      "Actual 1         3096         1745\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 66.1% con il modello:\n",
      "66 - AC66.1%, L500, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA20, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_11-28-58).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 62.97% con il modello:\n",
      "1 - BA62.97%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 64.08% con il modello:\n",
      "13 - PR64.08%, L20, mV-10, MV10, P2, AHsigmoid, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-26-48).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "113/113 - 6s - loss: 3.5115 - accuracy: 0.5656 - val_loss: 2.0017 - val_accuracy: 0.6060 - 6s/epoch - 55ms/step\n",
      "Epoch 2/10000\n",
      "113/113 - 5s - loss: 1.7925 - accuracy: 0.5873 - val_loss: 1.7438 - val_accuracy: 0.5928 - 5s/epoch - 40ms/step\n",
      "Epoch 3/10000\n",
      "113/113 - 5s - loss: 1.5536 - accuracy: 0.5902 - val_loss: 1.5654 - val_accuracy: 0.5523 - 5s/epoch - 42ms/step\n",
      "Epoch 4/10000\n",
      "113/113 - 5s - loss: 1.4296 - accuracy: 0.5918 - val_loss: 1.4862 - val_accuracy: 0.5848 - 5s/epoch - 41ms/step\n",
      "Epoch 5/10000\n",
      "113/113 - 5s - loss: 1.4051 - accuracy: 0.5953 - val_loss: 1.4436 - val_accuracy: 0.5977 - 5s/epoch - 41ms/step\n",
      "Epoch 6/10000\n",
      "113/113 - 5s - loss: 1.3052 - accuracy: 0.5971 - val_loss: 1.3746 - val_accuracy: 0.6028 - 5s/epoch - 42ms/step\n",
      "Epoch 7/10000\n",
      "113/113 - 4s - loss: 1.2582 - accuracy: 0.5964 - val_loss: 1.3480 - val_accuracy: 0.5729 - 4s/epoch - 39ms/step\n",
      "Epoch 8/10000\n",
      "113/113 - 5s - loss: 1.2396 - accuracy: 0.5939 - val_loss: 1.3094 - val_accuracy: 0.5741 - 5s/epoch - 43ms/step\n",
      "Epoch 9/10000\n",
      "113/113 - 4s - loss: 1.2211 - accuracy: 0.5983 - val_loss: 1.2300 - val_accuracy: 0.5915 - 4s/epoch - 37ms/step\n",
      "Epoch 10/10000\n",
      "113/113 - 4s - loss: 1.2192 - accuracy: 0.5995 - val_loss: 1.2844 - val_accuracy: 0.5800 - 4s/epoch - 38ms/step\n",
      "Epoch 11/10000\n",
      "113/113 - 5s - loss: 1.1967 - accuracy: 0.5971 - val_loss: 1.2171 - val_accuracy: 0.6054 - 5s/epoch - 43ms/step\n",
      "Epoch 12/10000\n",
      "113/113 - 4s - loss: 1.1938 - accuracy: 0.5980 - val_loss: 1.2284 - val_accuracy: 0.5932 - 4s/epoch - 39ms/step\n",
      "Epoch 13/10000\n",
      "113/113 - 5s - loss: 1.1823 - accuracy: 0.5996 - val_loss: 1.2719 - val_accuracy: 0.5888 - 5s/epoch - 40ms/step\n",
      "Epoch 14/10000\n",
      "113/113 - 4s - loss: 1.1909 - accuracy: 0.5985 - val_loss: 1.2093 - val_accuracy: 0.6082 - 4s/epoch - 39ms/step\n",
      "Epoch 15/10000\n",
      "113/113 - 4s - loss: 1.1530 - accuracy: 0.6000 - val_loss: 1.2310 - val_accuracy: 0.6020 - 4s/epoch - 39ms/step\n",
      "Epoch 16/10000\n",
      "113/113 - 4s - loss: 1.1424 - accuracy: 0.6027 - val_loss: 1.1927 - val_accuracy: 0.6043 - 4s/epoch - 38ms/step\n",
      "Epoch 17/10000\n",
      "113/113 - 4s - loss: 1.1505 - accuracy: 0.5969 - val_loss: 1.2176 - val_accuracy: 0.6079 - 4s/epoch - 37ms/step\n",
      "Epoch 18/10000\n",
      "113/113 - 4s - loss: 1.1480 - accuracy: 0.6006 - val_loss: 1.1980 - val_accuracy: 0.6031 - 4s/epoch - 38ms/step\n",
      "Epoch 19/10000\n",
      "113/113 - 4s - loss: 1.1255 - accuracy: 0.6006 - val_loss: 1.1734 - val_accuracy: 0.6052 - 4s/epoch - 39ms/step\n",
      "Epoch 20/10000\n",
      "113/113 - 4s - loss: 1.1196 - accuracy: 0.6032 - val_loss: 1.1855 - val_accuracy: 0.6080 - 4s/epoch - 37ms/step\n",
      "Epoch 21/10000\n",
      "113/113 - 4s - loss: 1.1107 - accuracy: 0.5985 - val_loss: 1.1613 - val_accuracy: 0.5998 - 4s/epoch - 36ms/step\n",
      "Epoch 22/10000\n",
      "113/113 - 4s - loss: 1.0795 - accuracy: 0.6025 - val_loss: 1.1530 - val_accuracy: 0.5932 - 4s/epoch - 37ms/step\n",
      "Epoch 23/10000\n",
      "113/113 - 4s - loss: 1.0847 - accuracy: 0.6053 - val_loss: 1.1732 - val_accuracy: 0.6165 - 4s/epoch - 39ms/step\n",
      "Epoch 24/10000\n",
      "113/113 - 4s - loss: 1.0947 - accuracy: 0.6026 - val_loss: 1.1435 - val_accuracy: 0.5892 - 4s/epoch - 37ms/step\n",
      "Epoch 25/10000\n",
      "113/113 - 4s - loss: 1.0985 - accuracy: 0.6024 - val_loss: 1.1377 - val_accuracy: 0.6226 - 4s/epoch - 38ms/step\n",
      "Epoch 26/10000\n",
      "113/113 - 4s - loss: 1.0780 - accuracy: 0.6062 - val_loss: 1.1638 - val_accuracy: 0.6079 - 4s/epoch - 37ms/step\n",
      "Epoch 27/10000\n",
      "113/113 - 4s - loss: 1.0959 - accuracy: 0.6015 - val_loss: 1.1410 - val_accuracy: 0.6208 - 4s/epoch - 37ms/step\n",
      "Epoch 28/10000\n",
      "113/113 - 4s - loss: 1.0600 - accuracy: 0.6050 - val_loss: 1.1143 - val_accuracy: 0.5880 - 4s/epoch - 37ms/step\n",
      "Epoch 29/10000\n",
      "113/113 - 4s - loss: 1.0511 - accuracy: 0.6042 - val_loss: 1.1191 - val_accuracy: 0.5722 - 4s/epoch - 37ms/step\n",
      "Epoch 30/10000\n",
      "113/113 - 4s - loss: 1.0406 - accuracy: 0.6045 - val_loss: 1.1006 - val_accuracy: 0.6049 - 4s/epoch - 34ms/step\n",
      "Epoch 31/10000\n",
      "113/113 - 4s - loss: 1.0160 - accuracy: 0.6049 - val_loss: 1.0750 - val_accuracy: 0.5957 - 4s/epoch - 35ms/step\n",
      "Epoch 32/10000\n",
      "113/113 - 4s - loss: 1.0130 - accuracy: 0.6054 - val_loss: 1.0918 - val_accuracy: 0.6016 - 4s/epoch - 38ms/step\n",
      "Epoch 33/10000\n",
      "113/113 - 4s - loss: 1.0208 - accuracy: 0.6062 - val_loss: 1.0632 - val_accuracy: 0.5811 - 4s/epoch - 38ms/step\n",
      "Epoch 34/10000\n",
      "113/113 - 4s - loss: 1.0183 - accuracy: 0.6046 - val_loss: 1.0421 - val_accuracy: 0.5950 - 4s/epoch - 37ms/step\n",
      "Epoch 35/10000\n",
      "113/113 - 4s - loss: 1.0024 - accuracy: 0.6068 - val_loss: 1.0690 - val_accuracy: 0.6259 - 4s/epoch - 37ms/step\n",
      "Epoch 36/10000\n",
      "113/113 - 4s - loss: 0.9960 - accuracy: 0.6058 - val_loss: 1.0709 - val_accuracy: 0.6068 - 4s/epoch - 38ms/step\n",
      "Epoch 37/10000\n",
      "113/113 - 4s - loss: 0.9905 - accuracy: 0.6079 - val_loss: 1.0442 - val_accuracy: 0.6041 - 4s/epoch - 35ms/step\n",
      "Epoch 38/10000\n",
      "113/113 - 4s - loss: 0.9934 - accuracy: 0.6074 - val_loss: 1.0798 - val_accuracy: 0.5801 - 4s/epoch - 35ms/step\n",
      "Epoch 39/10000\n",
      "113/113 - 5s - loss: 1.0132 - accuracy: 0.6054 - val_loss: 1.0499 - val_accuracy: 0.5928 - 5s/epoch - 44ms/step\n",
      "Epoch 40/10000\n",
      "113/113 - 5s - loss: 0.9984 - accuracy: 0.6079 - val_loss: 1.0421 - val_accuracy: 0.5969 - 5s/epoch - 41ms/step\n",
      "Epoch 41/10000\n",
      "113/113 - 5s - loss: 1.0031 - accuracy: 0.6052 - val_loss: 1.0706 - val_accuracy: 0.6119 - 5s/epoch - 41ms/step\n",
      "Epoch 42/10000\n",
      "113/113 - 4s - loss: 0.9852 - accuracy: 0.6064 - val_loss: 1.0494 - val_accuracy: 0.6086 - 4s/epoch - 38ms/step\n",
      "Epoch 43/10000\n",
      "113/113 - 4s - loss: 0.9955 - accuracy: 0.6076 - val_loss: 1.0241 - val_accuracy: 0.6168 - 4s/epoch - 38ms/step\n",
      "Epoch 44/10000\n",
      "113/113 - 4s - loss: 0.9809 - accuracy: 0.6086 - val_loss: 1.0023 - val_accuracy: 0.6016 - 4s/epoch - 36ms/step\n",
      "Epoch 45/10000\n",
      "113/113 - 4s - loss: 0.9664 - accuracy: 0.6086 - val_loss: 1.0192 - val_accuracy: 0.6246 - 4s/epoch - 37ms/step\n",
      "Epoch 46/10000\n",
      "113/113 - 4s - loss: 0.9933 - accuracy: 0.6040 - val_loss: 1.0509 - val_accuracy: 0.5925 - 4s/epoch - 35ms/step\n",
      "Epoch 47/10000\n",
      "113/113 - 4s - loss: 0.9819 - accuracy: 0.6077 - val_loss: 1.0231 - val_accuracy: 0.5964 - 4s/epoch - 38ms/step\n",
      "Epoch 48/10000\n",
      "113/113 - 4s - loss: 0.9696 - accuracy: 0.6084 - val_loss: 1.0268 - val_accuracy: 0.6011 - 4s/epoch - 35ms/step\n",
      "Epoch 49/10000\n",
      "113/113 - 4s - loss: 0.9846 - accuracy: 0.6083 - val_loss: 1.0198 - val_accuracy: 0.6073 - 4s/epoch - 38ms/step\n",
      "Epoch 50/10000\n",
      "113/113 - 4s - loss: 0.9861 - accuracy: 0.6054 - val_loss: 1.0157 - val_accuracy: 0.6100 - 4s/epoch - 37ms/step\n",
      "Epoch 51/10000\n",
      "113/113 - 4s - loss: 0.9832 - accuracy: 0.6074 - val_loss: 1.0264 - val_accuracy: 0.6148 - 4s/epoch - 38ms/step\n",
      "Epoch 52/10000\n",
      "113/113 - 4s - loss: 0.9922 - accuracy: 0.6073 - val_loss: 1.0195 - val_accuracy: 0.6051 - 4s/epoch - 36ms/step\n",
      "Epoch 53/10000\n",
      "113/113 - 4s - loss: 0.9763 - accuracy: 0.6079 - val_loss: 1.0236 - val_accuracy: 0.6143 - 4s/epoch - 36ms/step\n",
      "Epoch 54/10000\n",
      "113/113 - 4s - loss: 0.9722 - accuracy: 0.6081 - val_loss: 1.0070 - val_accuracy: 0.5972 - 4s/epoch - 37ms/step\n",
      "Epoch 55/10000\n",
      "113/113 - 4s - loss: 0.9714 - accuracy: 0.6069 - val_loss: 0.9622 - val_accuracy: 0.6025 - 4s/epoch - 38ms/step\n",
      "391/391 [==============================] - 2s 5ms/step\n",
      "\n",
      "Iterazione 88/128 (03-11-2023_12-34-39)\n",
      "hidden_layer_size: 500\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 3\n",
      "activation_hidden: sigmoid\n",
      "activation_output: sigmoid\n",
      "batch_size: 1000\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 20\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 62.59%\n",
      "La recall di questo modello sul validation set è: 29.6%\n",
      "La f1 di questo modello sul validation set è: 37.98%\n",
      "La balanced accuracy di questo modello sul validation set è: 56.51%\n",
      "La precision di questo modello sul validation set è: 52.98%\n",
      "La AUC di questo modello sul validation set è: 61.85%\n",
      "391/391 [==============================] - 2s 6ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.65    0.83      0.73  7,670.00\n",
      "1                  0.53    0.30      0.38  4,841.00\n",
      "accuracy           0.63    0.63      0.63      0.63\n",
      "macro avg          0.59    0.57      0.56 12,511.00\n",
      "weighted avg       0.60    0.63      0.60 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         6398         1272\n",
      "Actual 1         3408         1433\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 66.1% con il modello:\n",
      "66 - AC66.1%, L500, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA20, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_11-28-58).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 62.97% con il modello:\n",
      "1 - BA62.97%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 64.08% con il modello:\n",
      "13 - PR64.08%, L20, mV-10, MV10, P2, AHsigmoid, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-26-48).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "1126/1126 - 20s - loss: 28386269184.0000 - accuracy: 0.5641 - val_loss: 19654017024.0000 - val_accuracy: 0.4245 - 20s/epoch - 18ms/step\n",
      "Epoch 2/10000\n",
      "1126/1126 - 23s - loss: 15180363776.0000 - accuracy: 0.5722 - val_loss: 19238273024.0000 - val_accuracy: 0.4267 - 23s/epoch - 21ms/step\n",
      "Epoch 3/10000\n",
      "1126/1126 - 19s - loss: 14191074304.0000 - accuracy: 0.5725 - val_loss: 2541434368.0000 - val_accuracy: 0.5809 - 19s/epoch - 17ms/step\n",
      "Epoch 4/10000\n",
      "1126/1126 - 18s - loss: 12900164608.0000 - accuracy: 0.5736 - val_loss: 13706691584.0000 - val_accuracy: 0.6265 - 18s/epoch - 16ms/step\n",
      "Epoch 5/10000\n",
      "1126/1126 - 18s - loss: 12062548992.0000 - accuracy: 0.5753 - val_loss: 6175849984.0000 - val_accuracy: 0.4841 - 18s/epoch - 16ms/step\n",
      "Epoch 6/10000\n",
      "1126/1126 - 19s - loss: 13785888768.0000 - accuracy: 0.5745 - val_loss: 6936081408.0000 - val_accuracy: 0.5566 - 19s/epoch - 16ms/step\n",
      "Epoch 7/10000\n",
      "1126/1126 - 18s - loss: 12235842560.0000 - accuracy: 0.5746 - val_loss: 8821578752.0000 - val_accuracy: 0.5844 - 18s/epoch - 16ms/step\n",
      "Epoch 8/10000\n",
      "1126/1126 - 17s - loss: 11939108864.0000 - accuracy: 0.5746 - val_loss: 43832958976.0000 - val_accuracy: 0.6130 - 17s/epoch - 15ms/step\n",
      "Epoch 9/10000\n",
      "1126/1126 - 17s - loss: 12639883264.0000 - accuracy: 0.5745 - val_loss: 9933900800.0000 - val_accuracy: 0.6306 - 17s/epoch - 15ms/step\n",
      "Epoch 10/10000\n",
      "1126/1126 - 16s - loss: 12444967936.0000 - accuracy: 0.5743 - val_loss: 13471995904.0000 - val_accuracy: 0.6170 - 16s/epoch - 14ms/step\n",
      "Epoch 11/10000\n",
      "1126/1126 - 16s - loss: 11555759104.0000 - accuracy: 0.5790 - val_loss: 9834803200.0000 - val_accuracy: 0.5611 - 16s/epoch - 14ms/step\n",
      "Epoch 12/10000\n",
      "1126/1126 - 16s - loss: 11424171008.0000 - accuracy: 0.5739 - val_loss: 11124342784.0000 - val_accuracy: 0.6167 - 16s/epoch - 14ms/step\n",
      "Epoch 13/10000\n",
      "1126/1126 - 15s - loss: 12881332224.0000 - accuracy: 0.5753 - val_loss: 13805959168.0000 - val_accuracy: 0.4868 - 15s/epoch - 14ms/step\n",
      "Epoch 14/10000\n",
      "1126/1126 - 17s - loss: 11467361280.0000 - accuracy: 0.5756 - val_loss: 4767666688.0000 - val_accuracy: 0.6339 - 17s/epoch - 15ms/step\n",
      "Epoch 15/10000\n",
      "1126/1126 - 17s - loss: 12157396992.0000 - accuracy: 0.5767 - val_loss: 9876939776.0000 - val_accuracy: 0.5954 - 17s/epoch - 16ms/step\n",
      "Epoch 16/10000\n",
      "1126/1126 - 17s - loss: 12600434688.0000 - accuracy: 0.5756 - val_loss: 5065830400.0000 - val_accuracy: 0.6374 - 17s/epoch - 15ms/step\n",
      "Epoch 17/10000\n",
      "1126/1126 - 16s - loss: 12113143808.0000 - accuracy: 0.5762 - val_loss: 54323785728.0000 - val_accuracy: 0.6131 - 16s/epoch - 14ms/step\n",
      "Epoch 18/10000\n",
      "1126/1126 - 15s - loss: 11441118208.0000 - accuracy: 0.5795 - val_loss: 4879596032.0000 - val_accuracy: 0.4808 - 15s/epoch - 14ms/step\n",
      "Epoch 19/10000\n",
      "1126/1126 - 16s - loss: 11790027776.0000 - accuracy: 0.5756 - val_loss: 11506666496.0000 - val_accuracy: 0.4542 - 16s/epoch - 14ms/step\n",
      "Epoch 20/10000\n",
      "1126/1126 - 18s - loss: 10373872640.0000 - accuracy: 0.5784 - val_loss: 34577182720.0000 - val_accuracy: 0.6135 - 18s/epoch - 16ms/step\n",
      "Epoch 21/10000\n",
      "1126/1126 - 20s - loss: 11305498624.0000 - accuracy: 0.5764 - val_loss: 18988709888.0000 - val_accuracy: 0.5272 - 20s/epoch - 18ms/step\n",
      "Epoch 22/10000\n",
      "1126/1126 - 16s - loss: 12548332544.0000 - accuracy: 0.5763 - val_loss: 18958995456.0000 - val_accuracy: 0.4546 - 16s/epoch - 14ms/step\n",
      "Epoch 23/10000\n",
      "1126/1126 - 17s - loss: 10744470528.0000 - accuracy: 0.5783 - val_loss: 6048973312.0000 - val_accuracy: 0.5437 - 17s/epoch - 15ms/step\n",
      "Epoch 24/10000\n",
      "1126/1126 - 16s - loss: 12052358144.0000 - accuracy: 0.5778 - val_loss: 8478069248.0000 - val_accuracy: 0.5916 - 16s/epoch - 14ms/step\n",
      "Epoch 25/10000\n",
      "1126/1126 - 15s - loss: 10421815296.0000 - accuracy: 0.5810 - val_loss: 4141528832.0000 - val_accuracy: 0.6414 - 15s/epoch - 14ms/step\n",
      "Epoch 26/10000\n",
      "1126/1126 - 16s - loss: 10760091648.0000 - accuracy: 0.5791 - val_loss: 4531566592.0000 - val_accuracy: 0.6505 - 16s/epoch - 14ms/step\n",
      "Epoch 27/10000\n",
      "1126/1126 - 17s - loss: 10444268544.0000 - accuracy: 0.5814 - val_loss: 12934874112.0000 - val_accuracy: 0.6231 - 17s/epoch - 15ms/step\n",
      "Epoch 28/10000\n",
      "1126/1126 - 17s - loss: 10725879808.0000 - accuracy: 0.5800 - val_loss: 6687379456.0000 - val_accuracy: 0.6375 - 17s/epoch - 16ms/step\n",
      "Epoch 29/10000\n",
      "1126/1126 - 18s - loss: 10376128512.0000 - accuracy: 0.5825 - val_loss: 11270326272.0000 - val_accuracy: 0.4952 - 18s/epoch - 16ms/step\n",
      "Epoch 30/10000\n",
      "1126/1126 - 16s - loss: 10244190208.0000 - accuracy: 0.5812 - val_loss: 8140467712.0000 - val_accuracy: 0.6211 - 16s/epoch - 15ms/step\n",
      "Epoch 31/10000\n",
      "1126/1126 - 16s - loss: 10292782080.0000 - accuracy: 0.5817 - val_loss: 9842778112.0000 - val_accuracy: 0.4888 - 16s/epoch - 15ms/step\n",
      "Epoch 32/10000\n",
      "1126/1126 - 15s - loss: 9721461760.0000 - accuracy: 0.5817 - val_loss: 11679488000.0000 - val_accuracy: 0.6269 - 15s/epoch - 13ms/step\n",
      "Epoch 33/10000\n",
      "1126/1126 - 15s - loss: 9128749056.0000 - accuracy: 0.5801 - val_loss: 11287499776.0000 - val_accuracy: 0.6247 - 15s/epoch - 13ms/step\n",
      "Epoch 34/10000\n",
      "1126/1126 - 17s - loss: 9633731584.0000 - accuracy: 0.5818 - val_loss: 14047023104.0000 - val_accuracy: 0.6223 - 17s/epoch - 15ms/step\n",
      "Epoch 35/10000\n",
      "1126/1126 - 23s - loss: 10163117056.0000 - accuracy: 0.5823 - val_loss: 9318713344.0000 - val_accuracy: 0.6168 - 23s/epoch - 20ms/step\n",
      "Epoch 36/10000\n",
      "1126/1126 - 18s - loss: 10328397824.0000 - accuracy: 0.5814 - val_loss: 5303664640.0000 - val_accuracy: 0.5820 - 18s/epoch - 16ms/step\n",
      "391/391 [==============================] - 2s 6ms/step\n",
      "\n",
      "Iterazione 89/128 (03-11-2023_12-45-05)\n",
      "hidden_layer_size: 500\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 4\n",
      "activation_hidden: relu\n",
      "activation_output: sigmoid\n",
      "batch_size: 100\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 10\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 65.05%\n",
      "La recall di questo modello sul validation set è: 43.32%\n",
      "La f1 di questo modello sul validation set è: 48.96%\n",
      "La balanced accuracy di questo modello sul validation set è: 61.05%\n",
      "La precision di questo modello sul validation set è: 56.3%\n",
      "La AUC di questo modello sul validation set è: 61.05%\n",
      "391/391 [==============================] - 2s 6ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.69    0.79      0.73  7,670.00\n",
      "1                  0.56    0.43      0.49  4,841.00\n",
      "accuracy           0.65    0.65      0.65      0.65\n",
      "macro avg          0.63    0.61      0.61 12,511.00\n",
      "weighted avg       0.64    0.65      0.64 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         6042         1628\n",
      "Actual 1         2744         2097\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 66.1% con il modello:\n",
      "66 - AC66.1%, L500, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA20, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_11-28-58).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 62.97% con il modello:\n",
      "1 - BA62.97%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 64.08% con il modello:\n",
      "13 - PR64.08%, L20, mV-10, MV10, P2, AHsigmoid, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-26-48).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "1126/1126 - 22s - loss: 28386269184.0000 - accuracy: 0.5641 - val_loss: 19654017024.0000 - val_accuracy: 0.4245 - 22s/epoch - 20ms/step\n",
      "Epoch 2/10000\n",
      "1126/1126 - 18s - loss: 15180363776.0000 - accuracy: 0.5722 - val_loss: 19238273024.0000 - val_accuracy: 0.4267 - 18s/epoch - 16ms/step\n",
      "Epoch 3/10000\n",
      "1126/1126 - 16s - loss: 14191074304.0000 - accuracy: 0.5725 - val_loss: 2541434368.0000 - val_accuracy: 0.5809 - 16s/epoch - 15ms/step\n",
      "Epoch 4/10000\n",
      "1126/1126 - 18s - loss: 12900164608.0000 - accuracy: 0.5736 - val_loss: 13706691584.0000 - val_accuracy: 0.6265 - 18s/epoch - 16ms/step\n",
      "Epoch 5/10000\n",
      "1126/1126 - 17s - loss: 12062548992.0000 - accuracy: 0.5753 - val_loss: 6175849984.0000 - val_accuracy: 0.4841 - 17s/epoch - 15ms/step\n",
      "Epoch 6/10000\n",
      "1126/1126 - 16s - loss: 13785888768.0000 - accuracy: 0.5745 - val_loss: 6936081408.0000 - val_accuracy: 0.5566 - 16s/epoch - 14ms/step\n",
      "Epoch 7/10000\n",
      "1126/1126 - 15s - loss: 12235842560.0000 - accuracy: 0.5746 - val_loss: 8821578752.0000 - val_accuracy: 0.5844 - 15s/epoch - 13ms/step\n",
      "Epoch 8/10000\n",
      "1126/1126 - 16s - loss: 11939108864.0000 - accuracy: 0.5746 - val_loss: 43832958976.0000 - val_accuracy: 0.6130 - 16s/epoch - 14ms/step\n",
      "Epoch 9/10000\n",
      "1126/1126 - 16s - loss: 12639883264.0000 - accuracy: 0.5745 - val_loss: 9933900800.0000 - val_accuracy: 0.6306 - 16s/epoch - 14ms/step\n",
      "Epoch 10/10000\n",
      "1126/1126 - 15s - loss: 12444967936.0000 - accuracy: 0.5743 - val_loss: 13471995904.0000 - val_accuracy: 0.6170 - 15s/epoch - 14ms/step\n",
      "Epoch 11/10000\n",
      "1126/1126 - 15s - loss: 11555759104.0000 - accuracy: 0.5790 - val_loss: 9834803200.0000 - val_accuracy: 0.5611 - 15s/epoch - 13ms/step\n",
      "Epoch 12/10000\n",
      "1126/1126 - 18s - loss: 11424171008.0000 - accuracy: 0.5739 - val_loss: 11124342784.0000 - val_accuracy: 0.6167 - 18s/epoch - 16ms/step\n",
      "Epoch 13/10000\n",
      "1126/1126 - 20s - loss: 12881332224.0000 - accuracy: 0.5753 - val_loss: 13805959168.0000 - val_accuracy: 0.4868 - 20s/epoch - 18ms/step\n",
      "Epoch 14/10000\n",
      "1126/1126 - 20s - loss: 11467361280.0000 - accuracy: 0.5756 - val_loss: 4767666688.0000 - val_accuracy: 0.6339 - 20s/epoch - 18ms/step\n",
      "Epoch 15/10000\n",
      "1126/1126 - 21s - loss: 12157396992.0000 - accuracy: 0.5767 - val_loss: 9876939776.0000 - val_accuracy: 0.5954 - 21s/epoch - 19ms/step\n",
      "Epoch 16/10000\n",
      "1126/1126 - 23s - loss: 12600434688.0000 - accuracy: 0.5756 - val_loss: 5065830400.0000 - val_accuracy: 0.6374 - 23s/epoch - 20ms/step\n",
      "Epoch 17/10000\n",
      "1126/1126 - 25s - loss: 12113143808.0000 - accuracy: 0.5762 - val_loss: 54323785728.0000 - val_accuracy: 0.6131 - 25s/epoch - 22ms/step\n",
      "Epoch 18/10000\n",
      "1126/1126 - 27s - loss: 11441118208.0000 - accuracy: 0.5795 - val_loss: 4879596032.0000 - val_accuracy: 0.4808 - 27s/epoch - 24ms/step\n",
      "Epoch 19/10000\n",
      "1126/1126 - 28s - loss: 11790027776.0000 - accuracy: 0.5756 - val_loss: 11506666496.0000 - val_accuracy: 0.4542 - 28s/epoch - 25ms/step\n",
      "Epoch 20/10000\n",
      "1126/1126 - 18s - loss: 10373872640.0000 - accuracy: 0.5784 - val_loss: 34577182720.0000 - val_accuracy: 0.6135 - 18s/epoch - 16ms/step\n",
      "Epoch 21/10000\n",
      "1126/1126 - 19s - loss: 11305498624.0000 - accuracy: 0.5764 - val_loss: 18988709888.0000 - val_accuracy: 0.5272 - 19s/epoch - 17ms/step\n",
      "Epoch 22/10000\n",
      "1126/1126 - 17s - loss: 12548332544.0000 - accuracy: 0.5763 - val_loss: 18958995456.0000 - val_accuracy: 0.4546 - 17s/epoch - 15ms/step\n",
      "Epoch 23/10000\n",
      "1126/1126 - 16s - loss: 10744470528.0000 - accuracy: 0.5783 - val_loss: 6048973312.0000 - val_accuracy: 0.5437 - 16s/epoch - 14ms/step\n",
      "Epoch 24/10000\n",
      "1126/1126 - 17s - loss: 12052358144.0000 - accuracy: 0.5778 - val_loss: 8478069248.0000 - val_accuracy: 0.5916 - 17s/epoch - 15ms/step\n",
      "Epoch 25/10000\n",
      "1126/1126 - 17s - loss: 10421815296.0000 - accuracy: 0.5810 - val_loss: 4141528832.0000 - val_accuracy: 0.6414 - 17s/epoch - 15ms/step\n",
      "Epoch 26/10000\n",
      "1126/1126 - 16s - loss: 10760091648.0000 - accuracy: 0.5791 - val_loss: 4531566592.0000 - val_accuracy: 0.6505 - 16s/epoch - 14ms/step\n",
      "Epoch 27/10000\n",
      "1126/1126 - 19s - loss: 10444268544.0000 - accuracy: 0.5814 - val_loss: 12934874112.0000 - val_accuracy: 0.6231 - 19s/epoch - 17ms/step\n",
      "Epoch 28/10000\n",
      "1126/1126 - 18s - loss: 10725879808.0000 - accuracy: 0.5800 - val_loss: 6687379456.0000 - val_accuracy: 0.6375 - 18s/epoch - 16ms/step\n",
      "Epoch 29/10000\n",
      "1126/1126 - 18s - loss: 10376128512.0000 - accuracy: 0.5825 - val_loss: 11270326272.0000 - val_accuracy: 0.4952 - 18s/epoch - 16ms/step\n",
      "Epoch 30/10000\n",
      "1126/1126 - 19s - loss: 10244190208.0000 - accuracy: 0.5812 - val_loss: 8140467712.0000 - val_accuracy: 0.6211 - 19s/epoch - 17ms/step\n",
      "Epoch 31/10000\n",
      "1126/1126 - 16s - loss: 10292782080.0000 - accuracy: 0.5817 - val_loss: 9842778112.0000 - val_accuracy: 0.4888 - 16s/epoch - 15ms/step\n",
      "Epoch 32/10000\n",
      "1126/1126 - 15s - loss: 9721461760.0000 - accuracy: 0.5817 - val_loss: 11679488000.0000 - val_accuracy: 0.6269 - 15s/epoch - 14ms/step\n",
      "Epoch 33/10000\n",
      "1126/1126 - 16s - loss: 9128749056.0000 - accuracy: 0.5801 - val_loss: 11287499776.0000 - val_accuracy: 0.6247 - 16s/epoch - 14ms/step\n",
      "Epoch 34/10000\n",
      "1126/1126 - 15s - loss: 9633731584.0000 - accuracy: 0.5818 - val_loss: 14047023104.0000 - val_accuracy: 0.6223 - 15s/epoch - 14ms/step\n",
      "Epoch 35/10000\n",
      "1126/1126 - 15s - loss: 10163117056.0000 - accuracy: 0.5823 - val_loss: 9318713344.0000 - val_accuracy: 0.6168 - 15s/epoch - 14ms/step\n",
      "Epoch 36/10000\n",
      "1126/1126 - 16s - loss: 10328397824.0000 - accuracy: 0.5814 - val_loss: 5303664640.0000 - val_accuracy: 0.5820 - 16s/epoch - 14ms/step\n",
      "Epoch 37/10000\n",
      "1126/1126 - 17s - loss: 9242669056.0000 - accuracy: 0.5812 - val_loss: 3950729728.0000 - val_accuracy: 0.6217 - 17s/epoch - 15ms/step\n",
      "Epoch 38/10000\n",
      "1126/1126 - 16s - loss: 9531672576.0000 - accuracy: 0.5789 - val_loss: 12380234752.0000 - val_accuracy: 0.6266 - 16s/epoch - 14ms/step\n",
      "Epoch 39/10000\n",
      "1126/1126 - 18s - loss: 9450611712.0000 - accuracy: 0.5798 - val_loss: 12824571904.0000 - val_accuracy: 0.4624 - 18s/epoch - 16ms/step\n",
      "Epoch 40/10000\n",
      "1126/1126 - 16s - loss: 9067112448.0000 - accuracy: 0.5824 - val_loss: 3977676032.0000 - val_accuracy: 0.6236 - 16s/epoch - 14ms/step\n",
      "Epoch 41/10000\n",
      "1126/1126 - 15s - loss: 9337001984.0000 - accuracy: 0.5809 - val_loss: 5219824640.0000 - val_accuracy: 0.6415 - 15s/epoch - 14ms/step\n",
      "Epoch 42/10000\n",
      "1126/1126 - 16s - loss: 9353129984.0000 - accuracy: 0.5823 - val_loss: 16218816512.0000 - val_accuracy: 0.6142 - 16s/epoch - 14ms/step\n",
      "Epoch 43/10000\n",
      "1126/1126 - 15s - loss: 10468444160.0000 - accuracy: 0.5814 - val_loss: 9897080832.0000 - val_accuracy: 0.5583 - 15s/epoch - 14ms/step\n",
      "Epoch 44/10000\n",
      "1126/1126 - 15s - loss: 10148638720.0000 - accuracy: 0.5849 - val_loss: 3823491840.0000 - val_accuracy: 0.5443 - 15s/epoch - 14ms/step\n",
      "Epoch 45/10000\n",
      "1126/1126 - 16s - loss: 9722237952.0000 - accuracy: 0.5812 - val_loss: 16011356160.0000 - val_accuracy: 0.4419 - 16s/epoch - 14ms/step\n",
      "Epoch 46/10000\n",
      "1126/1126 - 17s - loss: 8764288000.0000 - accuracy: 0.5829 - val_loss: 5150474752.0000 - val_accuracy: 0.6417 - 17s/epoch - 15ms/step\n",
      "391/391 [==============================] - 2s 6ms/step\n",
      "\n",
      "Iterazione 90/128 (03-11-2023_12-58-48)\n",
      "hidden_layer_size: 500\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 4\n",
      "activation_hidden: relu\n",
      "activation_output: sigmoid\n",
      "batch_size: 100\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 20\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 65.05%\n",
      "La recall di questo modello sul validation set è: 43.32%\n",
      "La f1 di questo modello sul validation set è: 48.96%\n",
      "La balanced accuracy di questo modello sul validation set è: 61.05%\n",
      "La precision di questo modello sul validation set è: 56.3%\n",
      "La AUC di questo modello sul validation set è: 61.05%\n",
      "391/391 [==============================] - 3s 8ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.69    0.79      0.73  7,670.00\n",
      "1                  0.56    0.43      0.49  4,841.00\n",
      "accuracy           0.65    0.65      0.65      0.65\n",
      "macro avg          0.63    0.61      0.61 12,511.00\n",
      "weighted avg       0.64    0.65      0.64 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         6042         1628\n",
      "Actual 1         2744         2097\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 66.1% con il modello:\n",
      "66 - AC66.1%, L500, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA20, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_11-28-58).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 62.97% con il modello:\n",
      "1 - BA62.97%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 64.08% con il modello:\n",
      "13 - PR64.08%, L20, mV-10, MV10, P2, AHsigmoid, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-26-48).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "113/113 - 10s - loss: 152803606528.0000 - accuracy: 0.5503 - val_loss: 10382693376.0000 - val_accuracy: 0.6120 - 10s/epoch - 91ms/step\n",
      "Epoch 2/10000\n",
      "113/113 - 7s - loss: 11380007936.0000 - accuracy: 0.5609 - val_loss: 13784928256.0000 - val_accuracy: 0.4635 - 7s/epoch - 63ms/step\n",
      "Epoch 3/10000\n",
      "113/113 - 7s - loss: 11735233536.0000 - accuracy: 0.5631 - val_loss: 7447442944.0000 - val_accuracy: 0.6219 - 7s/epoch - 61ms/step\n",
      "Epoch 4/10000\n",
      "113/113 - 6s - loss: 11498073088.0000 - accuracy: 0.5690 - val_loss: 14260723712.0000 - val_accuracy: 0.4367 - 6s/epoch - 51ms/step\n",
      "Epoch 5/10000\n",
      "113/113 - 5s - loss: 13140692992.0000 - accuracy: 0.5707 - val_loss: 2836141824.0000 - val_accuracy: 0.5838 - 5s/epoch - 48ms/step\n",
      "Epoch 6/10000\n",
      "113/113 - 5s - loss: 15831453696.0000 - accuracy: 0.5681 - val_loss: 11250761728.0000 - val_accuracy: 0.6164 - 5s/epoch - 47ms/step\n",
      "Epoch 7/10000\n",
      "113/113 - 6s - loss: 14599333888.0000 - accuracy: 0.5662 - val_loss: 21351489536.0000 - val_accuracy: 0.4426 - 6s/epoch - 49ms/step\n",
      "Epoch 8/10000\n",
      "113/113 - 5s - loss: 9780789248.0000 - accuracy: 0.5736 - val_loss: 5089283584.0000 - val_accuracy: 0.6312 - 5s/epoch - 49ms/step\n",
      "Epoch 9/10000\n",
      "113/113 - 5s - loss: 11629027328.0000 - accuracy: 0.5721 - val_loss: 18236211200.0000 - val_accuracy: 0.6136 - 5s/epoch - 48ms/step\n",
      "Epoch 10/10000\n",
      "113/113 - 5s - loss: 14443730944.0000 - accuracy: 0.5686 - val_loss: 17756299264.0000 - val_accuracy: 0.5147 - 5s/epoch - 47ms/step\n",
      "Epoch 11/10000\n",
      "113/113 - 6s - loss: 11506596864.0000 - accuracy: 0.5748 - val_loss: 11089052672.0000 - val_accuracy: 0.4488 - 6s/epoch - 49ms/step\n",
      "Epoch 12/10000\n",
      "113/113 - 5s - loss: 8746659840.0000 - accuracy: 0.5762 - val_loss: 4668965888.0000 - val_accuracy: 0.6250 - 5s/epoch - 48ms/step\n",
      "Epoch 13/10000\n",
      "113/113 - 5s - loss: 12345679872.0000 - accuracy: 0.5696 - val_loss: 24589713408.0000 - val_accuracy: 0.6147 - 5s/epoch - 49ms/step\n",
      "Epoch 14/10000\n",
      "113/113 - 6s - loss: 18940717056.0000 - accuracy: 0.5691 - val_loss: 42787217408.0000 - val_accuracy: 0.6130 - 6s/epoch - 56ms/step\n",
      "Epoch 15/10000\n",
      "113/113 - 7s - loss: 14498996224.0000 - accuracy: 0.5729 - val_loss: 4905914368.0000 - val_accuracy: 0.6370 - 7s/epoch - 65ms/step\n",
      "Epoch 16/10000\n",
      "113/113 - 6s - loss: 14214106112.0000 - accuracy: 0.5721 - val_loss: 9662444544.0000 - val_accuracy: 0.5121 - 6s/epoch - 52ms/step\n",
      "Epoch 17/10000\n",
      "113/113 - 6s - loss: 10925375488.0000 - accuracy: 0.5712 - val_loss: 5227463680.0000 - val_accuracy: 0.6379 - 6s/epoch - 51ms/step\n",
      "Epoch 18/10000\n",
      "113/113 - 5s - loss: 17237065728.0000 - accuracy: 0.5737 - val_loss: 12892308480.0000 - val_accuracy: 0.4488 - 5s/epoch - 47ms/step\n",
      "Epoch 19/10000\n",
      "113/113 - 7s - loss: 9622815744.0000 - accuracy: 0.5754 - val_loss: 2012492288.0000 - val_accuracy: 0.6171 - 7s/epoch - 62ms/step\n",
      "Epoch 20/10000\n",
      "113/113 - 5s - loss: 10608697344.0000 - accuracy: 0.5702 - val_loss: 8960110592.0000 - val_accuracy: 0.6276 - 5s/epoch - 48ms/step\n",
      "Epoch 21/10000\n",
      "113/113 - 5s - loss: 14958169088.0000 - accuracy: 0.5692 - val_loss: 31034660864.0000 - val_accuracy: 0.6131 - 5s/epoch - 46ms/step\n",
      "Epoch 22/10000\n",
      "113/113 - 5s - loss: 15678025728.0000 - accuracy: 0.5743 - val_loss: 16931493888.0000 - val_accuracy: 0.4832 - 5s/epoch - 46ms/step\n",
      "Epoch 23/10000\n",
      "113/113 - 5s - loss: 11673191424.0000 - accuracy: 0.5731 - val_loss: 12438820864.0000 - val_accuracy: 0.5958 - 5s/epoch - 45ms/step\n",
      "Epoch 24/10000\n",
      "113/113 - 5s - loss: 11737605120.0000 - accuracy: 0.5738 - val_loss: 34818621440.0000 - val_accuracy: 0.4046 - 5s/epoch - 47ms/step\n",
      "Epoch 25/10000\n",
      "113/113 - 5s - loss: 13772085248.0000 - accuracy: 0.5758 - val_loss: 2575952128.0000 - val_accuracy: 0.6051 - 5s/epoch - 45ms/step\n",
      "Epoch 26/10000\n",
      "113/113 - 5s - loss: 11337387008.0000 - accuracy: 0.5696 - val_loss: 9246274560.0000 - val_accuracy: 0.6307 - 5s/epoch - 48ms/step\n",
      "Epoch 27/10000\n",
      "113/113 - 6s - loss: 9308232704.0000 - accuracy: 0.5786 - val_loss: 9040282624.0000 - val_accuracy: 0.6187 - 6s/epoch - 49ms/step\n",
      "391/391 [==============================] - 2s 5ms/step\n",
      "\n",
      "Iterazione 91/128 (03-11-2023_13-01-34)\n",
      "hidden_layer_size: 500\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 4\n",
      "activation_hidden: relu\n",
      "activation_output: sigmoid\n",
      "batch_size: 1000\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 10\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 63.78%\n",
      "La recall di questo modello sul validation set è: 15.97%\n",
      "La f1 di questo modello sul validation set è: 25.44%\n",
      "La balanced accuracy di questo modello sul validation set è: 54.97%\n",
      "La precision di questo modello sul validation set è: 62.54%\n",
      "La AUC di questo modello sul validation set è: 54.97%\n",
      "391/391 [==============================] - 2s 6ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.64    0.94      0.76  7,670.00\n",
      "1                  0.63    0.16      0.25  4,841.00\n",
      "accuracy           0.64    0.64      0.64      0.64\n",
      "macro avg          0.63    0.55      0.51 12,511.00\n",
      "weighted avg       0.63    0.64      0.56 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         7207          463\n",
      "Actual 1         4068          773\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 66.1% con il modello:\n",
      "66 - AC66.1%, L500, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA20, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_11-28-58).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 62.97% con il modello:\n",
      "1 - BA62.97%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 64.08% con il modello:\n",
      "13 - PR64.08%, L20, mV-10, MV10, P2, AHsigmoid, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-26-48).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "113/113 - 8s - loss: 152803606528.0000 - accuracy: 0.5503 - val_loss: 10382693376.0000 - val_accuracy: 0.6120 - 8s/epoch - 69ms/step\n",
      "Epoch 2/10000\n",
      "113/113 - 5s - loss: 11380007936.0000 - accuracy: 0.5609 - val_loss: 13784928256.0000 - val_accuracy: 0.4635 - 5s/epoch - 47ms/step\n",
      "Epoch 3/10000\n",
      "113/113 - 5s - loss: 11735233536.0000 - accuracy: 0.5631 - val_loss: 7447442944.0000 - val_accuracy: 0.6219 - 5s/epoch - 47ms/step\n",
      "Epoch 4/10000\n",
      "113/113 - 6s - loss: 11498073088.0000 - accuracy: 0.5690 - val_loss: 14260723712.0000 - val_accuracy: 0.4367 - 6s/epoch - 50ms/step\n",
      "Epoch 5/10000\n",
      "113/113 - 7s - loss: 13140692992.0000 - accuracy: 0.5707 - val_loss: 2836141824.0000 - val_accuracy: 0.5838 - 7s/epoch - 64ms/step\n",
      "Epoch 6/10000\n",
      "113/113 - 7s - loss: 15831453696.0000 - accuracy: 0.5681 - val_loss: 11250761728.0000 - val_accuracy: 0.6164 - 7s/epoch - 61ms/step\n",
      "Epoch 7/10000\n",
      "113/113 - 5s - loss: 14599333888.0000 - accuracy: 0.5662 - val_loss: 21351489536.0000 - val_accuracy: 0.4426 - 5s/epoch - 47ms/step\n",
      "Epoch 8/10000\n",
      "113/113 - 6s - loss: 9780789248.0000 - accuracy: 0.5736 - val_loss: 5089283584.0000 - val_accuracy: 0.6312 - 6s/epoch - 51ms/step\n",
      "Epoch 9/10000\n",
      "113/113 - 6s - loss: 11629027328.0000 - accuracy: 0.5721 - val_loss: 18236211200.0000 - val_accuracy: 0.6136 - 6s/epoch - 50ms/step\n",
      "Epoch 10/10000\n",
      "113/113 - 6s - loss: 14443730944.0000 - accuracy: 0.5686 - val_loss: 17756299264.0000 - val_accuracy: 0.5147 - 6s/epoch - 50ms/step\n",
      "Epoch 11/10000\n",
      "113/113 - 6s - loss: 11506596864.0000 - accuracy: 0.5748 - val_loss: 11089052672.0000 - val_accuracy: 0.4488 - 6s/epoch - 50ms/step\n",
      "Epoch 12/10000\n",
      "113/113 - 6s - loss: 8746659840.0000 - accuracy: 0.5762 - val_loss: 4668965888.0000 - val_accuracy: 0.6250 - 6s/epoch - 50ms/step\n",
      "Epoch 13/10000\n",
      "113/113 - 5s - loss: 12345679872.0000 - accuracy: 0.5696 - val_loss: 24589713408.0000 - val_accuracy: 0.6147 - 5s/epoch - 48ms/step\n",
      "Epoch 14/10000\n",
      "113/113 - 6s - loss: 18940717056.0000 - accuracy: 0.5691 - val_loss: 42787217408.0000 - val_accuracy: 0.6130 - 6s/epoch - 49ms/step\n",
      "Epoch 15/10000\n",
      "113/113 - 6s - loss: 14498996224.0000 - accuracy: 0.5729 - val_loss: 4905914368.0000 - val_accuracy: 0.6370 - 6s/epoch - 49ms/step\n",
      "Epoch 16/10000\n",
      "113/113 - 5s - loss: 14214106112.0000 - accuracy: 0.5721 - val_loss: 9662444544.0000 - val_accuracy: 0.5121 - 5s/epoch - 48ms/step\n",
      "Epoch 17/10000\n",
      "113/113 - 5s - loss: 10925375488.0000 - accuracy: 0.5712 - val_loss: 5227463680.0000 - val_accuracy: 0.6379 - 5s/epoch - 48ms/step\n",
      "Epoch 18/10000\n",
      "113/113 - 6s - loss: 17237065728.0000 - accuracy: 0.5737 - val_loss: 12892308480.0000 - val_accuracy: 0.4488 - 6s/epoch - 49ms/step\n",
      "Epoch 19/10000\n",
      "113/113 - 5s - loss: 9622815744.0000 - accuracy: 0.5754 - val_loss: 2012492288.0000 - val_accuracy: 0.6171 - 5s/epoch - 44ms/step\n",
      "Epoch 20/10000\n",
      "113/113 - 5s - loss: 10608697344.0000 - accuracy: 0.5702 - val_loss: 8960110592.0000 - val_accuracy: 0.6276 - 5s/epoch - 45ms/step\n",
      "Epoch 21/10000\n",
      "113/113 - 5s - loss: 14958169088.0000 - accuracy: 0.5692 - val_loss: 31034660864.0000 - val_accuracy: 0.6131 - 5s/epoch - 46ms/step\n",
      "Epoch 22/10000\n",
      "113/113 - 5s - loss: 15678025728.0000 - accuracy: 0.5743 - val_loss: 16931493888.0000 - val_accuracy: 0.4832 - 5s/epoch - 46ms/step\n",
      "Epoch 23/10000\n",
      "113/113 - 5s - loss: 11673191424.0000 - accuracy: 0.5731 - val_loss: 12438820864.0000 - val_accuracy: 0.5958 - 5s/epoch - 48ms/step\n",
      "Epoch 24/10000\n",
      "113/113 - 5s - loss: 11737605120.0000 - accuracy: 0.5738 - val_loss: 34818621440.0000 - val_accuracy: 0.4046 - 5s/epoch - 46ms/step\n",
      "Epoch 25/10000\n",
      "113/113 - 5s - loss: 13772085248.0000 - accuracy: 0.5758 - val_loss: 2575952128.0000 - val_accuracy: 0.6051 - 5s/epoch - 46ms/step\n",
      "Epoch 26/10000\n",
      "113/113 - 5s - loss: 11337387008.0000 - accuracy: 0.5696 - val_loss: 9246274560.0000 - val_accuracy: 0.6307 - 5s/epoch - 46ms/step\n",
      "Epoch 27/10000\n",
      "113/113 - 5s - loss: 9308232704.0000 - accuracy: 0.5786 - val_loss: 9040282624.0000 - val_accuracy: 0.6187 - 5s/epoch - 44ms/step\n",
      "Epoch 28/10000\n",
      "113/113 - 5s - loss: 17494368256.0000 - accuracy: 0.5653 - val_loss: 24907712512.0000 - val_accuracy: 0.4368 - 5s/epoch - 45ms/step\n",
      "Epoch 29/10000\n",
      "113/113 - 5s - loss: 13592723456.0000 - accuracy: 0.5735 - val_loss: 13846830080.0000 - val_accuracy: 0.6145 - 5s/epoch - 45ms/step\n",
      "Epoch 30/10000\n",
      "113/113 - 5s - loss: 10744779776.0000 - accuracy: 0.5749 - val_loss: 18090696704.0000 - val_accuracy: 0.6191 - 5s/epoch - 45ms/step\n",
      "Epoch 31/10000\n",
      "113/113 - 5s - loss: 12255377408.0000 - accuracy: 0.5718 - val_loss: 12238114816.0000 - val_accuracy: 0.5414 - 5s/epoch - 45ms/step\n",
      "Epoch 32/10000\n",
      "113/113 - 5s - loss: 12251967488.0000 - accuracy: 0.5736 - val_loss: 5640708096.0000 - val_accuracy: 0.6286 - 5s/epoch - 46ms/step\n",
      "Epoch 33/10000\n",
      "113/113 - 5s - loss: 11748364288.0000 - accuracy: 0.5749 - val_loss: 15518569472.0000 - val_accuracy: 0.4541 - 5s/epoch - 45ms/step\n",
      "Epoch 34/10000\n",
      "113/113 - 5s - loss: 7007339520.0000 - accuracy: 0.5792 - val_loss: 4180800256.0000 - val_accuracy: 0.5454 - 5s/epoch - 46ms/step\n",
      "Epoch 35/10000\n",
      "113/113 - 5s - loss: 16667352064.0000 - accuracy: 0.5652 - val_loss: 10567195648.0000 - val_accuracy: 0.6003 - 5s/epoch - 46ms/step\n",
      "Epoch 36/10000\n",
      "113/113 - 5s - loss: 12151325696.0000 - accuracy: 0.5794 - val_loss: 5297886720.0000 - val_accuracy: 0.6334 - 5s/epoch - 44ms/step\n",
      "Epoch 37/10000\n",
      "113/113 - 5s - loss: 11404053504.0000 - accuracy: 0.5701 - val_loss: 12727276544.0000 - val_accuracy: 0.6281 - 5s/epoch - 46ms/step\n",
      "391/391 [==============================] - 2s 5ms/step\n",
      "\n",
      "Iterazione 92/128 (03-11-2023_13-05-02)\n",
      "hidden_layer_size: 500\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 4\n",
      "activation_hidden: relu\n",
      "activation_output: sigmoid\n",
      "batch_size: 1000\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 20\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 63.78%\n",
      "La recall di questo modello sul validation set è: 15.97%\n",
      "La f1 di questo modello sul validation set è: 25.44%\n",
      "La balanced accuracy di questo modello sul validation set è: 54.97%\n",
      "La precision di questo modello sul validation set è: 62.54%\n",
      "La AUC di questo modello sul validation set è: 54.97%\n",
      "391/391 [==============================] - 2s 6ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.64    0.94      0.76  7,670.00\n",
      "1                  0.63    0.16      0.25  4,841.00\n",
      "accuracy           0.64    0.64      0.64      0.64\n",
      "macro avg          0.63    0.55      0.51 12,511.00\n",
      "weighted avg       0.63    0.64      0.56 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         7207          463\n",
      "Actual 1         4068          773\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 66.1% con il modello:\n",
      "66 - AC66.1%, L500, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA20, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_11-28-58).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 62.97% con il modello:\n",
      "1 - BA62.97%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 64.08% con il modello:\n",
      "13 - PR64.08%, L20, mV-10, MV10, P2, AHsigmoid, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-26-48).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "1126/1126 - 20s - loss: 3.4418 - accuracy: 0.5558 - val_loss: 2.7455 - val_accuracy: 0.5282 - 20s/epoch - 18ms/step\n",
      "Epoch 2/10000\n",
      "1126/1126 - 18s - loss: 2.7472 - accuracy: 0.5588 - val_loss: 2.5473 - val_accuracy: 0.5651 - 18s/epoch - 16ms/step\n",
      "Epoch 3/10000\n",
      "1126/1126 - 17s - loss: 2.6061 - accuracy: 0.5593 - val_loss: 2.5452 - val_accuracy: 0.5625 - 17s/epoch - 15ms/step\n",
      "Epoch 4/10000\n",
      "1126/1126 - 20s - loss: 2.3646 - accuracy: 0.5595 - val_loss: 2.3520 - val_accuracy: 0.5557 - 20s/epoch - 17ms/step\n",
      "Epoch 5/10000\n",
      "1126/1126 - 18s - loss: 2.2316 - accuracy: 0.5604 - val_loss: 2.0439 - val_accuracy: 0.5800 - 18s/epoch - 16ms/step\n",
      "Epoch 6/10000\n",
      "1126/1126 - 16s - loss: 2.0585 - accuracy: 0.5623 - val_loss: 1.8948 - val_accuracy: 0.5837 - 16s/epoch - 14ms/step\n",
      "Epoch 7/10000\n",
      "1126/1126 - 20s - loss: 1.8427 - accuracy: 0.5632 - val_loss: 1.6752 - val_accuracy: 0.5743 - 20s/epoch - 18ms/step\n",
      "Epoch 8/10000\n",
      "1126/1126 - 17s - loss: 1.7437 - accuracy: 0.5648 - val_loss: 1.8379 - val_accuracy: 0.5550 - 17s/epoch - 15ms/step\n",
      "Epoch 9/10000\n",
      "1126/1126 - 19s - loss: 1.6626 - accuracy: 0.5680 - val_loss: 1.6795 - val_accuracy: 0.5685 - 19s/epoch - 16ms/step\n",
      "Epoch 10/10000\n",
      "1126/1126 - 22s - loss: 1.5778 - accuracy: 0.5663 - val_loss: 1.4226 - val_accuracy: 0.5794 - 22s/epoch - 20ms/step\n",
      "Epoch 11/10000\n",
      "1126/1126 - 17s - loss: 1.4649 - accuracy: 0.5681 - val_loss: 1.3819 - val_accuracy: 0.5598 - 17s/epoch - 15ms/step\n",
      "Epoch 12/10000\n",
      "1126/1126 - 17s - loss: 1.3857 - accuracy: 0.5670 - val_loss: 1.2354 - val_accuracy: 0.5960 - 17s/epoch - 15ms/step\n",
      "Epoch 13/10000\n",
      "1126/1126 - 18s - loss: 1.3293 - accuracy: 0.5712 - val_loss: 1.3655 - val_accuracy: 0.5770 - 18s/epoch - 16ms/step\n",
      "Epoch 14/10000\n",
      "1126/1126 - 20s - loss: 1.2689 - accuracy: 0.5682 - val_loss: 1.1890 - val_accuracy: 0.5772 - 20s/epoch - 18ms/step\n",
      "Epoch 15/10000\n",
      "1126/1126 - 21s - loss: 1.1903 - accuracy: 0.5702 - val_loss: 1.1259 - val_accuracy: 0.5733 - 21s/epoch - 19ms/step\n",
      "Epoch 16/10000\n",
      "1126/1126 - 17s - loss: 1.1319 - accuracy: 0.5735 - val_loss: 1.0589 - val_accuracy: 0.5916 - 17s/epoch - 15ms/step\n",
      "Epoch 17/10000\n",
      "1126/1126 - 19s - loss: 1.1263 - accuracy: 0.5707 - val_loss: 1.0767 - val_accuracy: 0.5965 - 19s/epoch - 17ms/step\n",
      "Epoch 18/10000\n",
      "1126/1126 - 15s - loss: 1.0881 - accuracy: 0.5736 - val_loss: 1.0418 - val_accuracy: 0.5972 - 15s/epoch - 14ms/step\n",
      "Epoch 19/10000\n",
      "1126/1126 - 17s - loss: 1.0451 - accuracy: 0.5718 - val_loss: 0.9786 - val_accuracy: 0.5848 - 17s/epoch - 15ms/step\n",
      "Epoch 20/10000\n",
      "1126/1126 - 16s - loss: 0.9892 - accuracy: 0.5783 - val_loss: 0.9422 - val_accuracy: 0.5984 - 16s/epoch - 14ms/step\n",
      "Epoch 21/10000\n",
      "1126/1126 - 17s - loss: 0.9559 - accuracy: 0.5808 - val_loss: 0.9532 - val_accuracy: 0.5988 - 17s/epoch - 15ms/step\n",
      "Epoch 22/10000\n",
      "1126/1126 - 18s - loss: 0.9402 - accuracy: 0.5780 - val_loss: 0.8802 - val_accuracy: 0.5777 - 18s/epoch - 16ms/step\n",
      "Epoch 23/10000\n",
      "1126/1126 - 18s - loss: 0.9110 - accuracy: 0.5790 - val_loss: 0.8718 - val_accuracy: 0.5722 - 18s/epoch - 16ms/step\n",
      "Epoch 24/10000\n",
      "1126/1126 - 16s - loss: 0.8672 - accuracy: 0.5820 - val_loss: 0.8598 - val_accuracy: 0.5869 - 16s/epoch - 14ms/step\n",
      "Epoch 25/10000\n",
      "1126/1126 - 15s - loss: 0.8579 - accuracy: 0.5865 - val_loss: 0.8104 - val_accuracy: 0.5948 - 15s/epoch - 13ms/step\n",
      "Epoch 26/10000\n",
      "1126/1126 - 16s - loss: 0.8294 - accuracy: 0.5885 - val_loss: 0.8040 - val_accuracy: 0.5958 - 16s/epoch - 14ms/step\n",
      "Epoch 27/10000\n",
      "1126/1126 - 15s - loss: 0.8101 - accuracy: 0.5889 - val_loss: 0.8061 - val_accuracy: 0.5862 - 15s/epoch - 14ms/step\n",
      "Epoch 28/10000\n",
      "1126/1126 - 17s - loss: 0.7934 - accuracy: 0.5914 - val_loss: 0.7733 - val_accuracy: 0.6075 - 17s/epoch - 15ms/step\n",
      "Epoch 29/10000\n",
      "1126/1126 - 16s - loss: 0.7776 - accuracy: 0.6016 - val_loss: 0.7616 - val_accuracy: 0.5992 - 16s/epoch - 14ms/step\n",
      "Epoch 30/10000\n",
      "1126/1126 - 13s - loss: 0.7614 - accuracy: 0.5989 - val_loss: 0.7480 - val_accuracy: 0.6049 - 13s/epoch - 11ms/step\n",
      "Epoch 31/10000\n",
      "1126/1126 - 13s - loss: 0.7626 - accuracy: 0.6003 - val_loss: 0.7247 - val_accuracy: 0.6107 - 13s/epoch - 12ms/step\n",
      "Epoch 32/10000\n",
      "1126/1126 - 15s - loss: 0.7429 - accuracy: 0.6069 - val_loss: 0.7177 - val_accuracy: 0.6178 - 15s/epoch - 13ms/step\n",
      "Epoch 33/10000\n",
      "1126/1126 - 19s - loss: 0.7331 - accuracy: 0.6050 - val_loss: 0.7248 - val_accuracy: 0.6064 - 19s/epoch - 17ms/step\n",
      "Epoch 34/10000\n",
      "1126/1126 - 13s - loss: 0.7255 - accuracy: 0.6050 - val_loss: 0.7431 - val_accuracy: 0.6071 - 13s/epoch - 12ms/step\n",
      "Epoch 35/10000\n",
      "1126/1126 - 13s - loss: 0.7449 - accuracy: 0.6026 - val_loss: 0.7541 - val_accuracy: 0.5900 - 13s/epoch - 12ms/step\n",
      "Epoch 36/10000\n",
      "1126/1126 - 13s - loss: 0.7367 - accuracy: 0.6098 - val_loss: 0.7167 - val_accuracy: 0.6047 - 13s/epoch - 12ms/step\n",
      "Epoch 37/10000\n",
      "1126/1126 - 13s - loss: 0.7151 - accuracy: 0.6061 - val_loss: 0.7053 - val_accuracy: 0.6150 - 13s/epoch - 12ms/step\n",
      "Epoch 38/10000\n",
      "1126/1126 - 16s - loss: 0.7190 - accuracy: 0.6102 - val_loss: 0.7030 - val_accuracy: 0.6030 - 16s/epoch - 14ms/step\n",
      "Epoch 39/10000\n",
      "1126/1126 - 14s - loss: 0.6993 - accuracy: 0.6106 - val_loss: 0.6855 - val_accuracy: 0.6147 - 14s/epoch - 12ms/step\n",
      "Epoch 40/10000\n",
      "1126/1126 - 13s - loss: 0.6935 - accuracy: 0.6135 - val_loss: 0.6973 - val_accuracy: 0.6176 - 13s/epoch - 12ms/step\n",
      "Epoch 41/10000\n",
      "1126/1126 - 13s - loss: 0.6929 - accuracy: 0.6147 - val_loss: 0.6860 - val_accuracy: 0.6191 - 13s/epoch - 12ms/step\n",
      "Epoch 42/10000\n",
      "1126/1126 - 14s - loss: 0.6833 - accuracy: 0.6144 - val_loss: 0.6779 - val_accuracy: 0.6231 - 14s/epoch - 12ms/step\n",
      "Epoch 43/10000\n",
      "1126/1126 - 14s - loss: 0.6862 - accuracy: 0.6137 - val_loss: 0.6861 - val_accuracy: 0.6188 - 14s/epoch - 12ms/step\n",
      "Epoch 44/10000\n",
      "1126/1126 - 13s - loss: 0.6885 - accuracy: 0.6141 - val_loss: 0.6682 - val_accuracy: 0.6189 - 13s/epoch - 11ms/step\n",
      "Epoch 45/10000\n",
      "1126/1126 - 13s - loss: 0.6857 - accuracy: 0.6170 - val_loss: 0.6928 - val_accuracy: 0.6153 - 13s/epoch - 12ms/step\n",
      "Epoch 46/10000\n",
      "1126/1126 - 14s - loss: 0.6954 - accuracy: 0.6160 - val_loss: 0.7164 - val_accuracy: 0.6015 - 14s/epoch - 12ms/step\n",
      "Epoch 47/10000\n",
      "1126/1126 - 13s - loss: 0.6880 - accuracy: 0.6135 - val_loss: 0.6845 - val_accuracy: 0.6240 - 13s/epoch - 11ms/step\n",
      "Epoch 48/10000\n",
      "1126/1126 - 14s - loss: 0.6760 - accuracy: 0.6166 - val_loss: 0.6788 - val_accuracy: 0.6212 - 14s/epoch - 12ms/step\n",
      "Epoch 49/10000\n",
      "1126/1126 - 13s - loss: 0.6743 - accuracy: 0.6203 - val_loss: 0.7118 - val_accuracy: 0.6117 - 13s/epoch - 12ms/step\n",
      "Epoch 50/10000\n",
      "1126/1126 - 14s - loss: 0.6758 - accuracy: 0.6189 - val_loss: 0.6650 - val_accuracy: 0.6275 - 14s/epoch - 13ms/step\n",
      "Epoch 51/10000\n",
      "1126/1126 - 13s - loss: 0.6793 - accuracy: 0.6166 - val_loss: 0.6752 - val_accuracy: 0.6119 - 13s/epoch - 12ms/step\n",
      "Epoch 52/10000\n",
      "1126/1126 - 19s - loss: 0.6762 - accuracy: 0.6157 - val_loss: 0.6699 - val_accuracy: 0.6192 - 19s/epoch - 17ms/step\n",
      "Epoch 53/10000\n",
      "1126/1126 - 21s - loss: 0.6752 - accuracy: 0.6177 - val_loss: 0.6794 - val_accuracy: 0.6215 - 21s/epoch - 19ms/step\n",
      "Epoch 54/10000\n",
      "1126/1126 - 19s - loss: 0.6754 - accuracy: 0.6170 - val_loss: 0.6543 - val_accuracy: 0.6298 - 19s/epoch - 17ms/step\n",
      "Epoch 55/10000\n",
      "1126/1126 - 17s - loss: 0.6728 - accuracy: 0.6174 - val_loss: 0.6737 - val_accuracy: 0.6151 - 17s/epoch - 15ms/step\n",
      "Epoch 56/10000\n",
      "1126/1126 - 21s - loss: 0.6664 - accuracy: 0.6187 - val_loss: 0.6661 - val_accuracy: 0.6226 - 21s/epoch - 19ms/step\n",
      "Epoch 57/10000\n",
      "1126/1126 - 17s - loss: 0.6729 - accuracy: 0.6163 - val_loss: 0.6668 - val_accuracy: 0.6155 - 17s/epoch - 15ms/step\n",
      "Epoch 58/10000\n",
      "1126/1126 - 17s - loss: 0.6697 - accuracy: 0.6145 - val_loss: 0.6653 - val_accuracy: 0.6134 - 17s/epoch - 16ms/step\n",
      "Epoch 59/10000\n",
      "1126/1126 - 16s - loss: 0.6711 - accuracy: 0.6110 - val_loss: 0.6655 - val_accuracy: 0.6159 - 16s/epoch - 14ms/step\n",
      "Epoch 60/10000\n",
      "1126/1126 - 16s - loss: 0.6656 - accuracy: 0.6184 - val_loss: 0.6713 - val_accuracy: 0.6223 - 16s/epoch - 14ms/step\n",
      "Epoch 61/10000\n",
      "1126/1126 - 16s - loss: 0.6685 - accuracy: 0.6193 - val_loss: 0.6624 - val_accuracy: 0.6233 - 16s/epoch - 14ms/step\n",
      "Epoch 62/10000\n",
      "1126/1126 - 16s - loss: 0.6720 - accuracy: 0.6176 - val_loss: 0.6710 - val_accuracy: 0.6124 - 16s/epoch - 14ms/step\n",
      "Epoch 63/10000\n",
      "1126/1126 - 16s - loss: 0.6746 - accuracy: 0.6176 - val_loss: 0.6787 - val_accuracy: 0.6123 - 16s/epoch - 14ms/step\n",
      "Epoch 64/10000\n",
      "1126/1126 - 16s - loss: 0.6678 - accuracy: 0.6199 - val_loss: 0.6651 - val_accuracy: 0.6168 - 16s/epoch - 14ms/step\n",
      "391/391 [==============================] - 3s 6ms/step\n",
      "\n",
      "Iterazione 93/128 (03-11-2023_13-22-32)\n",
      "hidden_layer_size: 500\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 4\n",
      "activation_hidden: sigmoid\n",
      "activation_output: sigmoid\n",
      "batch_size: 100\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 10\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 62.98%\n",
      "La recall di questo modello sul validation set è: 14.89%\n",
      "La f1 di questo modello sul validation set è: 23.74%\n",
      "La balanced accuracy di questo modello sul validation set è: 54.11%\n",
      "La precision di questo modello sul validation set è: 58.48%\n",
      "La AUC di questo modello sul validation set è: 63.03%\n",
      "391/391 [==============================] - 3s 7ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.63    0.93      0.76  7,670.00\n",
      "1                  0.58    0.15      0.24  4,841.00\n",
      "accuracy           0.63    0.63      0.63      0.63\n",
      "macro avg          0.61    0.54      0.50 12,511.00\n",
      "weighted avg       0.62    0.63      0.56 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         7158          512\n",
      "Actual 1         4120          721\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 66.1% con il modello:\n",
      "66 - AC66.1%, L500, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA20, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_11-28-58).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 62.97% con il modello:\n",
      "1 - BA62.97%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 64.08% con il modello:\n",
      "13 - PR64.08%, L20, mV-10, MV10, P2, AHsigmoid, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-26-48).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "1126/1126 - 22s - loss: 3.4418 - accuracy: 0.5558 - val_loss: 2.7455 - val_accuracy: 0.5282 - 22s/epoch - 19ms/step\n",
      "Epoch 2/10000\n",
      "1126/1126 - 17s - loss: 2.7472 - accuracy: 0.5588 - val_loss: 2.5473 - val_accuracy: 0.5651 - 17s/epoch - 15ms/step\n",
      "Epoch 3/10000\n",
      "1126/1126 - 17s - loss: 2.6061 - accuracy: 0.5593 - val_loss: 2.5452 - val_accuracy: 0.5625 - 17s/epoch - 15ms/step\n",
      "Epoch 4/10000\n",
      "1126/1126 - 16s - loss: 2.3646 - accuracy: 0.5595 - val_loss: 2.3520 - val_accuracy: 0.5557 - 16s/epoch - 14ms/step\n",
      "Epoch 5/10000\n",
      "1126/1126 - 16s - loss: 2.2316 - accuracy: 0.5604 - val_loss: 2.0439 - val_accuracy: 0.5800 - 16s/epoch - 14ms/step\n",
      "Epoch 6/10000\n",
      "1126/1126 - 16s - loss: 2.0585 - accuracy: 0.5623 - val_loss: 1.8948 - val_accuracy: 0.5837 - 16s/epoch - 14ms/step\n",
      "Epoch 7/10000\n",
      "1126/1126 - 15s - loss: 1.8427 - accuracy: 0.5632 - val_loss: 1.6752 - val_accuracy: 0.5743 - 15s/epoch - 14ms/step\n",
      "Epoch 8/10000\n",
      "1126/1126 - 16s - loss: 1.7437 - accuracy: 0.5648 - val_loss: 1.8379 - val_accuracy: 0.5550 - 16s/epoch - 14ms/step\n",
      "Epoch 9/10000\n",
      "1126/1126 - 17s - loss: 1.6626 - accuracy: 0.5680 - val_loss: 1.6795 - val_accuracy: 0.5685 - 17s/epoch - 15ms/step\n",
      "Epoch 10/10000\n",
      "1126/1126 - 15s - loss: 1.5778 - accuracy: 0.5663 - val_loss: 1.4226 - val_accuracy: 0.5794 - 15s/epoch - 14ms/step\n",
      "Epoch 11/10000\n",
      "1126/1126 - 16s - loss: 1.4649 - accuracy: 0.5681 - val_loss: 1.3819 - val_accuracy: 0.5598 - 16s/epoch - 14ms/step\n",
      "Epoch 12/10000\n",
      "1126/1126 - 16s - loss: 1.3857 - accuracy: 0.5670 - val_loss: 1.2354 - val_accuracy: 0.5960 - 16s/epoch - 14ms/step\n",
      "Epoch 13/10000\n",
      "1126/1126 - 16s - loss: 1.3293 - accuracy: 0.5712 - val_loss: 1.3655 - val_accuracy: 0.5770 - 16s/epoch - 14ms/step\n",
      "Epoch 14/10000\n",
      "1126/1126 - 16s - loss: 1.2689 - accuracy: 0.5682 - val_loss: 1.1890 - val_accuracy: 0.5772 - 16s/epoch - 15ms/step\n",
      "Epoch 15/10000\n",
      "1126/1126 - 17s - loss: 1.1903 - accuracy: 0.5702 - val_loss: 1.1259 - val_accuracy: 0.5733 - 17s/epoch - 15ms/step\n",
      "Epoch 16/10000\n",
      "1126/1126 - 17s - loss: 1.1319 - accuracy: 0.5735 - val_loss: 1.0589 - val_accuracy: 0.5916 - 17s/epoch - 15ms/step\n",
      "Epoch 17/10000\n",
      "1126/1126 - 16s - loss: 1.1263 - accuracy: 0.5707 - val_loss: 1.0767 - val_accuracy: 0.5965 - 16s/epoch - 14ms/step\n",
      "Epoch 18/10000\n",
      "1126/1126 - 16s - loss: 1.0881 - accuracy: 0.5736 - val_loss: 1.0418 - val_accuracy: 0.5972 - 16s/epoch - 14ms/step\n",
      "Epoch 19/10000\n",
      "1126/1126 - 16s - loss: 1.0451 - accuracy: 0.5718 - val_loss: 0.9786 - val_accuracy: 0.5848 - 16s/epoch - 14ms/step\n",
      "Epoch 20/10000\n",
      "1126/1126 - 15s - loss: 0.9892 - accuracy: 0.5783 - val_loss: 0.9422 - val_accuracy: 0.5984 - 15s/epoch - 14ms/step\n",
      "Epoch 21/10000\n",
      "1126/1126 - 15s - loss: 0.9559 - accuracy: 0.5808 - val_loss: 0.9532 - val_accuracy: 0.5988 - 15s/epoch - 13ms/step\n",
      "Epoch 22/10000\n",
      "1126/1126 - 15s - loss: 0.9402 - accuracy: 0.5780 - val_loss: 0.8802 - val_accuracy: 0.5777 - 15s/epoch - 14ms/step\n",
      "Epoch 23/10000\n",
      "1126/1126 - 15s - loss: 0.9110 - accuracy: 0.5790 - val_loss: 0.8718 - val_accuracy: 0.5722 - 15s/epoch - 13ms/step\n",
      "Epoch 24/10000\n",
      "1126/1126 - 15s - loss: 0.8672 - accuracy: 0.5820 - val_loss: 0.8598 - val_accuracy: 0.5869 - 15s/epoch - 14ms/step\n",
      "Epoch 25/10000\n",
      "1126/1126 - 15s - loss: 0.8579 - accuracy: 0.5865 - val_loss: 0.8104 - val_accuracy: 0.5948 - 15s/epoch - 13ms/step\n",
      "Epoch 26/10000\n",
      "1126/1126 - 15s - loss: 0.8294 - accuracy: 0.5885 - val_loss: 0.8040 - val_accuracy: 0.5958 - 15s/epoch - 13ms/step\n",
      "Epoch 27/10000\n",
      "1126/1126 - 15s - loss: 0.8101 - accuracy: 0.5889 - val_loss: 0.8061 - val_accuracy: 0.5862 - 15s/epoch - 13ms/step\n",
      "Epoch 28/10000\n",
      "1126/1126 - 15s - loss: 0.7934 - accuracy: 0.5914 - val_loss: 0.7733 - val_accuracy: 0.6075 - 15s/epoch - 14ms/step\n",
      "Epoch 29/10000\n",
      "1126/1126 - 15s - loss: 0.7776 - accuracy: 0.6016 - val_loss: 0.7616 - val_accuracy: 0.5992 - 15s/epoch - 14ms/step\n",
      "Epoch 30/10000\n",
      "1126/1126 - 15s - loss: 0.7614 - accuracy: 0.5989 - val_loss: 0.7480 - val_accuracy: 0.6049 - 15s/epoch - 13ms/step\n",
      "Epoch 31/10000\n",
      "1126/1126 - 15s - loss: 0.7626 - accuracy: 0.6003 - val_loss: 0.7247 - val_accuracy: 0.6107 - 15s/epoch - 14ms/step\n",
      "Epoch 32/10000\n",
      "1126/1126 - 15s - loss: 0.7429 - accuracy: 0.6069 - val_loss: 0.7177 - val_accuracy: 0.6178 - 15s/epoch - 14ms/step\n",
      "Epoch 33/10000\n",
      "1126/1126 - 16s - loss: 0.7331 - accuracy: 0.6050 - val_loss: 0.7248 - val_accuracy: 0.6064 - 16s/epoch - 14ms/step\n",
      "Epoch 34/10000\n",
      "1126/1126 - 15s - loss: 0.7255 - accuracy: 0.6050 - val_loss: 0.7431 - val_accuracy: 0.6071 - 15s/epoch - 14ms/step\n",
      "Epoch 35/10000\n",
      "1126/1126 - 16s - loss: 0.7449 - accuracy: 0.6026 - val_loss: 0.7541 - val_accuracy: 0.5900 - 16s/epoch - 14ms/step\n",
      "Epoch 36/10000\n",
      "1126/1126 - 16s - loss: 0.7367 - accuracy: 0.6098 - val_loss: 0.7167 - val_accuracy: 0.6047 - 16s/epoch - 14ms/step\n",
      "Epoch 37/10000\n",
      "1126/1126 - 16s - loss: 0.7151 - accuracy: 0.6061 - val_loss: 0.7053 - val_accuracy: 0.6150 - 16s/epoch - 14ms/step\n",
      "Epoch 38/10000\n",
      "1126/1126 - 16s - loss: 0.7190 - accuracy: 0.6102 - val_loss: 0.7030 - val_accuracy: 0.6030 - 16s/epoch - 14ms/step\n",
      "Epoch 39/10000\n",
      "1126/1126 - 16s - loss: 0.6993 - accuracy: 0.6106 - val_loss: 0.6855 - val_accuracy: 0.6147 - 16s/epoch - 14ms/step\n",
      "Epoch 40/10000\n",
      "1126/1126 - 16s - loss: 0.6935 - accuracy: 0.6135 - val_loss: 0.6973 - val_accuracy: 0.6176 - 16s/epoch - 14ms/step\n",
      "Epoch 41/10000\n",
      "1126/1126 - 16s - loss: 0.6929 - accuracy: 0.6147 - val_loss: 0.6860 - val_accuracy: 0.6191 - 16s/epoch - 14ms/step\n",
      "Epoch 42/10000\n",
      "1126/1126 - 16s - loss: 0.6833 - accuracy: 0.6144 - val_loss: 0.6779 - val_accuracy: 0.6231 - 16s/epoch - 14ms/step\n",
      "Epoch 43/10000\n",
      "1126/1126 - 15s - loss: 0.6862 - accuracy: 0.6137 - val_loss: 0.6861 - val_accuracy: 0.6188 - 15s/epoch - 14ms/step\n",
      "Epoch 44/10000\n",
      "1126/1126 - 15s - loss: 0.6885 - accuracy: 0.6141 - val_loss: 0.6682 - val_accuracy: 0.6189 - 15s/epoch - 14ms/step\n",
      "Epoch 45/10000\n",
      "1126/1126 - 15s - loss: 0.6857 - accuracy: 0.6170 - val_loss: 0.6928 - val_accuracy: 0.6153 - 15s/epoch - 13ms/step\n",
      "Epoch 46/10000\n",
      "1126/1126 - 15s - loss: 0.6954 - accuracy: 0.6160 - val_loss: 0.7164 - val_accuracy: 0.6015 - 15s/epoch - 14ms/step\n",
      "Epoch 47/10000\n",
      "1126/1126 - 15s - loss: 0.6880 - accuracy: 0.6135 - val_loss: 0.6845 - val_accuracy: 0.6240 - 15s/epoch - 14ms/step\n",
      "Epoch 48/10000\n",
      "1126/1126 - 15s - loss: 0.6760 - accuracy: 0.6166 - val_loss: 0.6788 - val_accuracy: 0.6212 - 15s/epoch - 13ms/step\n",
      "Epoch 49/10000\n",
      "1126/1126 - 15s - loss: 0.6743 - accuracy: 0.6203 - val_loss: 0.7118 - val_accuracy: 0.6117 - 15s/epoch - 13ms/step\n",
      "Epoch 50/10000\n",
      "1126/1126 - 15s - loss: 0.6758 - accuracy: 0.6189 - val_loss: 0.6650 - val_accuracy: 0.6275 - 15s/epoch - 13ms/step\n",
      "Epoch 51/10000\n",
      "1126/1126 - 15s - loss: 0.6793 - accuracy: 0.6166 - val_loss: 0.6752 - val_accuracy: 0.6119 - 15s/epoch - 13ms/step\n",
      "Epoch 52/10000\n",
      "1126/1126 - 15s - loss: 0.6762 - accuracy: 0.6157 - val_loss: 0.6699 - val_accuracy: 0.6192 - 15s/epoch - 14ms/step\n",
      "Epoch 53/10000\n",
      "1126/1126 - 15s - loss: 0.6752 - accuracy: 0.6177 - val_loss: 0.6794 - val_accuracy: 0.6215 - 15s/epoch - 14ms/step\n",
      "Epoch 54/10000\n",
      "1126/1126 - 16s - loss: 0.6754 - accuracy: 0.6170 - val_loss: 0.6543 - val_accuracy: 0.6298 - 16s/epoch - 14ms/step\n",
      "Epoch 55/10000\n",
      "1126/1126 - 16s - loss: 0.6728 - accuracy: 0.6174 - val_loss: 0.6737 - val_accuracy: 0.6151 - 16s/epoch - 14ms/step\n",
      "Epoch 56/10000\n",
      "1126/1126 - 16s - loss: 0.6664 - accuracy: 0.6187 - val_loss: 0.6661 - val_accuracy: 0.6226 - 16s/epoch - 14ms/step\n",
      "Epoch 57/10000\n",
      "1126/1126 - 16s - loss: 0.6729 - accuracy: 0.6163 - val_loss: 0.6668 - val_accuracy: 0.6155 - 16s/epoch - 14ms/step\n",
      "Epoch 58/10000\n",
      "1126/1126 - 16s - loss: 0.6697 - accuracy: 0.6145 - val_loss: 0.6653 - val_accuracy: 0.6134 - 16s/epoch - 14ms/step\n",
      "Epoch 59/10000\n",
      "1126/1126 - 16s - loss: 0.6711 - accuracy: 0.6110 - val_loss: 0.6655 - val_accuracy: 0.6159 - 16s/epoch - 14ms/step\n",
      "Epoch 60/10000\n",
      "1126/1126 - 16s - loss: 0.6656 - accuracy: 0.6184 - val_loss: 0.6713 - val_accuracy: 0.6223 - 16s/epoch - 14ms/step\n",
      "Epoch 61/10000\n",
      "1126/1126 - 16s - loss: 0.6685 - accuracy: 0.6193 - val_loss: 0.6624 - val_accuracy: 0.6233 - 16s/epoch - 14ms/step\n",
      "Epoch 62/10000\n",
      "1126/1126 - 15s - loss: 0.6720 - accuracy: 0.6176 - val_loss: 0.6710 - val_accuracy: 0.6124 - 15s/epoch - 14ms/step\n",
      "Epoch 63/10000\n",
      "1126/1126 - 16s - loss: 0.6746 - accuracy: 0.6176 - val_loss: 0.6787 - val_accuracy: 0.6123 - 16s/epoch - 15ms/step\n",
      "Epoch 64/10000\n",
      "1126/1126 - 16s - loss: 0.6678 - accuracy: 0.6199 - val_loss: 0.6651 - val_accuracy: 0.6168 - 16s/epoch - 14ms/step\n",
      "Epoch 65/10000\n",
      "1126/1126 - 15s - loss: 0.6651 - accuracy: 0.6197 - val_loss: 0.6592 - val_accuracy: 0.6314 - 15s/epoch - 13ms/step\n",
      "Epoch 66/10000\n",
      "1126/1126 - 15s - loss: 0.6657 - accuracy: 0.6210 - val_loss: 0.6633 - val_accuracy: 0.6270 - 15s/epoch - 13ms/step\n",
      "Epoch 67/10000\n",
      "1126/1126 - 19s - loss: 0.6643 - accuracy: 0.6195 - val_loss: 0.6650 - val_accuracy: 0.6262 - 19s/epoch - 17ms/step\n",
      "Epoch 68/10000\n",
      "1126/1126 - 22s - loss: 0.6680 - accuracy: 0.6190 - val_loss: 0.6580 - val_accuracy: 0.6298 - 22s/epoch - 19ms/step\n",
      "Epoch 69/10000\n",
      "1126/1126 - 17s - loss: 0.6665 - accuracy: 0.6216 - val_loss: 0.6636 - val_accuracy: 0.6287 - 17s/epoch - 15ms/step\n",
      "Epoch 70/10000\n",
      "1126/1126 - 16s - loss: 0.6658 - accuracy: 0.6198 - val_loss: 0.6586 - val_accuracy: 0.6148 - 16s/epoch - 14ms/step\n",
      "Epoch 71/10000\n",
      "1126/1126 - 16s - loss: 0.6634 - accuracy: 0.6206 - val_loss: 0.6665 - val_accuracy: 0.6288 - 16s/epoch - 14ms/step\n",
      "Epoch 72/10000\n",
      "1126/1126 - 16s - loss: 0.6621 - accuracy: 0.6219 - val_loss: 0.6589 - val_accuracy: 0.6219 - 16s/epoch - 14ms/step\n",
      "Epoch 73/10000\n",
      "1126/1126 - 16s - loss: 0.6665 - accuracy: 0.6187 - val_loss: 0.6637 - val_accuracy: 0.6143 - 16s/epoch - 14ms/step\n",
      "Epoch 74/10000\n",
      "1126/1126 - 16s - loss: 0.6643 - accuracy: 0.6216 - val_loss: 0.6581 - val_accuracy: 0.6250 - 16s/epoch - 14ms/step\n",
      "Epoch 75/10000\n",
      "1126/1126 - 16s - loss: 0.6660 - accuracy: 0.6158 - val_loss: 0.6607 - val_accuracy: 0.6305 - 16s/epoch - 14ms/step\n",
      "Epoch 76/10000\n",
      "1126/1126 - 16s - loss: 0.6582 - accuracy: 0.6225 - val_loss: 0.6552 - val_accuracy: 0.6290 - 16s/epoch - 14ms/step\n",
      "Epoch 77/10000\n",
      "1126/1126 - 16s - loss: 0.6669 - accuracy: 0.6167 - val_loss: 0.6709 - val_accuracy: 0.6161 - 16s/epoch - 14ms/step\n",
      "Epoch 78/10000\n",
      "1126/1126 - 16s - loss: 0.6642 - accuracy: 0.6172 - val_loss: 0.6596 - val_accuracy: 0.6265 - 16s/epoch - 15ms/step\n",
      "Epoch 79/10000\n",
      "1126/1126 - 16s - loss: 0.6591 - accuracy: 0.6218 - val_loss: 0.6491 - val_accuracy: 0.6307 - 16s/epoch - 15ms/step\n",
      "Epoch 80/10000\n",
      "1126/1126 - 17s - loss: 0.6621 - accuracy: 0.6169 - val_loss: 0.6698 - val_accuracy: 0.6148 - 17s/epoch - 15ms/step\n",
      "Epoch 81/10000\n",
      "1126/1126 - 16s - loss: 0.6653 - accuracy: 0.6175 - val_loss: 0.6689 - val_accuracy: 0.5968 - 16s/epoch - 14ms/step\n",
      "Epoch 82/10000\n",
      "1126/1126 - 15s - loss: 0.6646 - accuracy: 0.6210 - val_loss: 0.6669 - val_accuracy: 0.6250 - 15s/epoch - 13ms/step\n",
      "Epoch 83/10000\n",
      "1126/1126 - 15s - loss: 0.6662 - accuracy: 0.6180 - val_loss: 0.6712 - val_accuracy: 0.6125 - 15s/epoch - 14ms/step\n",
      "Epoch 84/10000\n",
      "1126/1126 - 15s - loss: 0.6638 - accuracy: 0.6224 - val_loss: 0.6635 - val_accuracy: 0.6095 - 15s/epoch - 13ms/step\n",
      "Epoch 85/10000\n",
      "1126/1126 - 15s - loss: 0.6603 - accuracy: 0.6196 - val_loss: 0.6668 - val_accuracy: 0.6135 - 15s/epoch - 14ms/step\n",
      "391/391 [==============================] - 3s 7ms/step\n",
      "\n",
      "Iterazione 94/128 (03-11-2023_13-45-12)\n",
      "hidden_layer_size: 500\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 4\n",
      "activation_hidden: sigmoid\n",
      "activation_output: sigmoid\n",
      "batch_size: 100\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 20\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 63.14%\n",
      "La recall di questo modello sul validation set è: 20.78%\n",
      "La f1 di questo modello sul validation set è: 30.37%\n",
      "La balanced accuracy di questo modello sul validation set è: 55.33%\n",
      "La precision di questo modello sul validation set è: 56.42%\n",
      "La AUC di questo modello sul validation set è: 63.22%\n",
      "391/391 [==============================] - 2s 6ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.64    0.90      0.75  7,670.00\n",
      "1                  0.56    0.21      0.30  4,841.00\n",
      "accuracy           0.63    0.63      0.63      0.63\n",
      "macro avg          0.60    0.55      0.53 12,511.00\n",
      "weighted avg       0.61    0.63      0.58 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         6893          777\n",
      "Actual 1         3835         1006\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 66.1% con il modello:\n",
      "66 - AC66.1%, L500, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA20, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_11-28-58).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 62.97% con il modello:\n",
      "1 - BA62.97%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 64.08% con il modello:\n",
      "13 - PR64.08%, L20, mV-10, MV10, P2, AHsigmoid, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-26-48).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "113/113 - 9s - loss: 3.7884 - accuracy: 0.5557 - val_loss: 2.3715 - val_accuracy: 0.5919 - 9s/epoch - 79ms/step\n",
      "Epoch 2/10000\n",
      "113/113 - 6s - loss: 2.2179 - accuracy: 0.5742 - val_loss: 2.2861 - val_accuracy: 0.5559 - 6s/epoch - 50ms/step\n",
      "Epoch 3/10000\n",
      "113/113 - 6s - loss: 2.0668 - accuracy: 0.5732 - val_loss: 1.9902 - val_accuracy: 0.5595 - 6s/epoch - 51ms/step\n",
      "Epoch 4/10000\n",
      "113/113 - 6s - loss: 1.9444 - accuracy: 0.5712 - val_loss: 2.0881 - val_accuracy: 0.5512 - 6s/epoch - 52ms/step\n",
      "Epoch 5/10000\n",
      "113/113 - 6s - loss: 1.8812 - accuracy: 0.5734 - val_loss: 1.8733 - val_accuracy: 0.5607 - 6s/epoch - 51ms/step\n",
      "Epoch 6/10000\n",
      "113/113 - 6s - loss: 1.8546 - accuracy: 0.5756 - val_loss: 1.8571 - val_accuracy: 0.5744 - 6s/epoch - 50ms/step\n",
      "Epoch 7/10000\n",
      "113/113 - 6s - loss: 1.8478 - accuracy: 0.5731 - val_loss: 1.7763 - val_accuracy: 0.5718 - 6s/epoch - 51ms/step\n",
      "Epoch 8/10000\n",
      "113/113 - 6s - loss: 1.8373 - accuracy: 0.5760 - val_loss: 1.7425 - val_accuracy: 0.5977 - 6s/epoch - 51ms/step\n",
      "Epoch 9/10000\n",
      "113/113 - 6s - loss: 1.7273 - accuracy: 0.5776 - val_loss: 1.7309 - val_accuracy: 0.5894 - 6s/epoch - 50ms/step\n",
      "Epoch 10/10000\n",
      "113/113 - 6s - loss: 1.6907 - accuracy: 0.5814 - val_loss: 1.6879 - val_accuracy: 0.5706 - 6s/epoch - 51ms/step\n",
      "Epoch 11/10000\n",
      "113/113 - 6s - loss: 1.7089 - accuracy: 0.5764 - val_loss: 1.7714 - val_accuracy: 0.5728 - 6s/epoch - 50ms/step\n",
      "Epoch 12/10000\n",
      "113/113 - 6s - loss: 1.7062 - accuracy: 0.5801 - val_loss: 1.6514 - val_accuracy: 0.5807 - 6s/epoch - 50ms/step\n",
      "Epoch 13/10000\n",
      "113/113 - 6s - loss: 1.6725 - accuracy: 0.5800 - val_loss: 1.7894 - val_accuracy: 0.5751 - 6s/epoch - 52ms/step\n",
      "Epoch 14/10000\n",
      "113/113 - 5s - loss: 1.6235 - accuracy: 0.5816 - val_loss: 1.6122 - val_accuracy: 0.5928 - 5s/epoch - 47ms/step\n",
      "Epoch 15/10000\n",
      "113/113 - 5s - loss: 1.6235 - accuracy: 0.5818 - val_loss: 1.6542 - val_accuracy: 0.5710 - 5s/epoch - 47ms/step\n",
      "Epoch 16/10000\n",
      "113/113 - 5s - loss: 1.6764 - accuracy: 0.5776 - val_loss: 1.6970 - val_accuracy: 0.5725 - 5s/epoch - 48ms/step\n",
      "Epoch 17/10000\n",
      "113/113 - 5s - loss: 1.6852 - accuracy: 0.5785 - val_loss: 1.8334 - val_accuracy: 0.5673 - 5s/epoch - 48ms/step\n",
      "Epoch 18/10000\n",
      "113/113 - 5s - loss: 1.7830 - accuracy: 0.5735 - val_loss: 1.8021 - val_accuracy: 0.5522 - 5s/epoch - 48ms/step\n",
      "391/391 [==============================] - 3s 7ms/step\n",
      "\n",
      "Iterazione 95/128 (03-11-2023_13-47-03)\n",
      "hidden_layer_size: 500\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 4\n",
      "activation_hidden: sigmoid\n",
      "activation_output: sigmoid\n",
      "batch_size: 1000\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 10\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 59.77%\n",
      "La recall di questo modello sul validation set è: 40.9%\n",
      "La f1 di questo modello sul validation set è: 44.03%\n",
      "La balanced accuracy di questo modello sul validation set è: 56.29%\n",
      "La precision di questo modello sul validation set è: 47.69%\n",
      "La AUC di questo modello sul validation set è: 58.85%\n",
      "391/391 [==============================] - 2s 6ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.66    0.72      0.69  7,670.00\n",
      "1                  0.48    0.41      0.44  4,841.00\n",
      "accuracy           0.60    0.60      0.60      0.60\n",
      "macro avg          0.57    0.56      0.56 12,511.00\n",
      "weighted avg       0.59    0.60      0.59 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         5498         2172\n",
      "Actual 1         2861         1980\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 66.1% con il modello:\n",
      "66 - AC66.1%, L500, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA20, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_11-28-58).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 62.97% con il modello:\n",
      "1 - BA62.97%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 64.08% con il modello:\n",
      "13 - PR64.08%, L20, mV-10, MV10, P2, AHsigmoid, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-26-48).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "113/113 - 8s - loss: 3.7884 - accuracy: 0.5557 - val_loss: 2.3715 - val_accuracy: 0.5919 - 8s/epoch - 71ms/step\n",
      "Epoch 2/10000\n",
      "113/113 - 6s - loss: 2.2179 - accuracy: 0.5742 - val_loss: 2.2861 - val_accuracy: 0.5559 - 6s/epoch - 50ms/step\n",
      "Epoch 3/10000\n",
      "113/113 - 6s - loss: 2.0668 - accuracy: 0.5732 - val_loss: 1.9902 - val_accuracy: 0.5595 - 6s/epoch - 52ms/step\n",
      "Epoch 4/10000\n",
      "113/113 - 6s - loss: 1.9444 - accuracy: 0.5712 - val_loss: 2.0881 - val_accuracy: 0.5512 - 6s/epoch - 50ms/step\n",
      "Epoch 5/10000\n",
      "113/113 - 6s - loss: 1.8812 - accuracy: 0.5734 - val_loss: 1.8733 - val_accuracy: 0.5607 - 6s/epoch - 52ms/step\n",
      "Epoch 6/10000\n",
      "113/113 - 6s - loss: 1.8546 - accuracy: 0.5756 - val_loss: 1.8571 - val_accuracy: 0.5744 - 6s/epoch - 51ms/step\n",
      "Epoch 7/10000\n",
      "113/113 - 6s - loss: 1.8478 - accuracy: 0.5731 - val_loss: 1.7763 - val_accuracy: 0.5718 - 6s/epoch - 56ms/step\n",
      "Epoch 8/10000\n",
      "113/113 - 6s - loss: 1.8373 - accuracy: 0.5760 - val_loss: 1.7425 - val_accuracy: 0.5977 - 6s/epoch - 55ms/step\n",
      "Epoch 9/10000\n",
      "113/113 - 6s - loss: 1.7273 - accuracy: 0.5776 - val_loss: 1.7309 - val_accuracy: 0.5894 - 6s/epoch - 54ms/step\n",
      "Epoch 10/10000\n",
      "113/113 - 6s - loss: 1.6907 - accuracy: 0.5814 - val_loss: 1.6879 - val_accuracy: 0.5706 - 6s/epoch - 53ms/step\n",
      "Epoch 11/10000\n",
      "113/113 - 6s - loss: 1.7089 - accuracy: 0.5764 - val_loss: 1.7714 - val_accuracy: 0.5728 - 6s/epoch - 55ms/step\n",
      "Epoch 12/10000\n",
      "113/113 - 6s - loss: 1.7062 - accuracy: 0.5801 - val_loss: 1.6514 - val_accuracy: 0.5807 - 6s/epoch - 53ms/step\n",
      "Epoch 13/10000\n",
      "113/113 - 6s - loss: 1.6725 - accuracy: 0.5800 - val_loss: 1.7894 - val_accuracy: 0.5751 - 6s/epoch - 54ms/step\n",
      "Epoch 14/10000\n",
      "113/113 - 6s - loss: 1.6235 - accuracy: 0.5816 - val_loss: 1.6122 - val_accuracy: 0.5928 - 6s/epoch - 54ms/step\n",
      "Epoch 15/10000\n",
      "113/113 - 6s - loss: 1.6235 - accuracy: 0.5818 - val_loss: 1.6542 - val_accuracy: 0.5710 - 6s/epoch - 51ms/step\n",
      "Epoch 16/10000\n",
      "113/113 - 6s - loss: 1.6764 - accuracy: 0.5776 - val_loss: 1.6970 - val_accuracy: 0.5725 - 6s/epoch - 49ms/step\n",
      "Epoch 17/10000\n",
      "113/113 - 6s - loss: 1.6852 - accuracy: 0.5785 - val_loss: 1.8334 - val_accuracy: 0.5673 - 6s/epoch - 49ms/step\n",
      "Epoch 18/10000\n",
      "113/113 - 6s - loss: 1.7830 - accuracy: 0.5735 - val_loss: 1.8021 - val_accuracy: 0.5522 - 6s/epoch - 50ms/step\n",
      "Epoch 19/10000\n",
      "113/113 - 6s - loss: 1.6794 - accuracy: 0.5824 - val_loss: 1.7343 - val_accuracy: 0.5821 - 6s/epoch - 49ms/step\n",
      "Epoch 20/10000\n",
      "113/113 - 6s - loss: 1.6469 - accuracy: 0.5758 - val_loss: 1.6171 - val_accuracy: 0.5866 - 6s/epoch - 52ms/step\n",
      "Epoch 21/10000\n",
      "113/113 - 6s - loss: 1.6164 - accuracy: 0.5798 - val_loss: 1.6584 - val_accuracy: 0.5824 - 6s/epoch - 49ms/step\n",
      "Epoch 22/10000\n",
      "113/113 - 6s - loss: 1.5938 - accuracy: 0.5827 - val_loss: 1.5736 - val_accuracy: 0.5674 - 6s/epoch - 51ms/step\n",
      "Epoch 23/10000\n",
      "113/113 - 6s - loss: 1.5612 - accuracy: 0.5811 - val_loss: 1.5650 - val_accuracy: 0.5755 - 6s/epoch - 50ms/step\n",
      "Epoch 24/10000\n",
      "113/113 - 6s - loss: 1.5186 - accuracy: 0.5804 - val_loss: 1.5624 - val_accuracy: 0.5786 - 6s/epoch - 50ms/step\n",
      "Epoch 25/10000\n",
      "113/113 - 6s - loss: 1.5479 - accuracy: 0.5800 - val_loss: 1.5844 - val_accuracy: 0.5976 - 6s/epoch - 54ms/step\n",
      "Epoch 26/10000\n",
      "113/113 - 6s - loss: 1.5882 - accuracy: 0.5811 - val_loss: 1.5466 - val_accuracy: 0.5988 - 6s/epoch - 53ms/step\n",
      "Epoch 27/10000\n",
      "113/113 - 6s - loss: 1.5486 - accuracy: 0.5807 - val_loss: 1.5002 - val_accuracy: 0.5789 - 6s/epoch - 55ms/step\n",
      "Epoch 28/10000\n",
      "113/113 - 6s - loss: 1.5392 - accuracy: 0.5827 - val_loss: 1.5775 - val_accuracy: 0.5698 - 6s/epoch - 49ms/step\n",
      "Epoch 29/10000\n",
      "113/113 - 6s - loss: 1.4675 - accuracy: 0.5838 - val_loss: 1.4679 - val_accuracy: 0.5734 - 6s/epoch - 49ms/step\n",
      "Epoch 30/10000\n",
      "113/113 - 6s - loss: 1.4420 - accuracy: 0.5869 - val_loss: 1.4109 - val_accuracy: 0.5940 - 6s/epoch - 50ms/step\n",
      "Epoch 31/10000\n",
      "113/113 - 6s - loss: 1.4671 - accuracy: 0.5868 - val_loss: 1.5430 - val_accuracy: 0.6002 - 6s/epoch - 53ms/step\n",
      "Epoch 32/10000\n",
      "113/113 - 6s - loss: 1.5149 - accuracy: 0.5840 - val_loss: 1.4755 - val_accuracy: 0.5840 - 6s/epoch - 53ms/step\n",
      "Epoch 33/10000\n",
      "113/113 - 6s - loss: 1.5075 - accuracy: 0.5852 - val_loss: 1.5468 - val_accuracy: 0.5868 - 6s/epoch - 53ms/step\n",
      "Epoch 34/10000\n",
      "113/113 - 6s - loss: 1.4664 - accuracy: 0.5883 - val_loss: 1.5008 - val_accuracy: 0.5750 - 6s/epoch - 53ms/step\n",
      "Epoch 35/10000\n",
      "113/113 - 6s - loss: 1.5144 - accuracy: 0.5850 - val_loss: 1.5187 - val_accuracy: 0.5779 - 6s/epoch - 53ms/step\n",
      "Epoch 36/10000\n",
      "113/113 - 6s - loss: 1.5372 - accuracy: 0.5815 - val_loss: 1.5473 - val_accuracy: 0.5844 - 6s/epoch - 55ms/step\n",
      "Epoch 37/10000\n",
      "113/113 - 6s - loss: 1.4675 - accuracy: 0.5868 - val_loss: 1.5400 - val_accuracy: 0.5913 - 6s/epoch - 53ms/step\n",
      "Epoch 38/10000\n",
      "113/113 - 6s - loss: 1.5210 - accuracy: 0.5818 - val_loss: 1.5966 - val_accuracy: 0.5875 - 6s/epoch - 53ms/step\n",
      "Epoch 39/10000\n",
      "113/113 - 6s - loss: 1.5352 - accuracy: 0.5814 - val_loss: 1.6302 - val_accuracy: 0.5683 - 6s/epoch - 53ms/step\n",
      "Epoch 40/10000\n",
      "113/113 - 6s - loss: 1.5458 - accuracy: 0.5776 - val_loss: 1.5307 - val_accuracy: 0.5789 - 6s/epoch - 54ms/step\n",
      "Epoch 41/10000\n",
      "113/113 - 6s - loss: 1.4834 - accuracy: 0.5847 - val_loss: 1.5444 - val_accuracy: 0.5724 - 6s/epoch - 55ms/step\n",
      "Epoch 42/10000\n",
      "113/113 - 6s - loss: 1.4680 - accuracy: 0.5857 - val_loss: 1.5117 - val_accuracy: 0.5948 - 6s/epoch - 54ms/step\n",
      "Epoch 43/10000\n",
      "113/113 - 6s - loss: 1.4711 - accuracy: 0.5808 - val_loss: 1.5187 - val_accuracy: 0.5758 - 6s/epoch - 56ms/step\n",
      "Epoch 44/10000\n",
      "113/113 - 6s - loss: 1.4482 - accuracy: 0.5812 - val_loss: 1.4574 - val_accuracy: 0.5820 - 6s/epoch - 53ms/step\n",
      "Epoch 45/10000\n",
      "113/113 - 6s - loss: 1.4542 - accuracy: 0.5852 - val_loss: 1.4580 - val_accuracy: 0.5940 - 6s/epoch - 53ms/step\n",
      "Epoch 46/10000\n",
      "113/113 - 6s - loss: 1.5131 - accuracy: 0.5793 - val_loss: 1.5106 - val_accuracy: 0.5692 - 6s/epoch - 53ms/step\n",
      "Epoch 47/10000\n",
      "113/113 - 6s - loss: 1.4762 - accuracy: 0.5825 - val_loss: 1.4437 - val_accuracy: 0.6016 - 6s/epoch - 53ms/step\n",
      "Epoch 48/10000\n",
      "113/113 - 6s - loss: 1.4158 - accuracy: 0.5858 - val_loss: 1.4473 - val_accuracy: 0.5963 - 6s/epoch - 52ms/step\n",
      "Epoch 49/10000\n",
      "113/113 - 6s - loss: 1.3897 - accuracy: 0.5854 - val_loss: 1.3242 - val_accuracy: 0.5916 - 6s/epoch - 53ms/step\n",
      "Epoch 50/10000\n",
      "113/113 - 6s - loss: 1.3653 - accuracy: 0.5902 - val_loss: 1.3477 - val_accuracy: 0.5870 - 6s/epoch - 54ms/step\n",
      "Epoch 51/10000\n",
      "113/113 - 6s - loss: 1.3615 - accuracy: 0.5854 - val_loss: 1.3234 - val_accuracy: 0.5880 - 6s/epoch - 53ms/step\n",
      "Epoch 52/10000\n",
      "113/113 - 6s - loss: 1.3624 - accuracy: 0.5877 - val_loss: 1.3914 - val_accuracy: 0.5798 - 6s/epoch - 54ms/step\n",
      "Epoch 53/10000\n",
      "113/113 - 6s - loss: 1.3529 - accuracy: 0.5903 - val_loss: 1.3811 - val_accuracy: 0.5776 - 6s/epoch - 52ms/step\n",
      "Epoch 54/10000\n",
      "113/113 - 6s - loss: 1.3603 - accuracy: 0.5875 - val_loss: 1.4056 - val_accuracy: 0.5951 - 6s/epoch - 53ms/step\n",
      "Epoch 55/10000\n",
      "113/113 - 6s - loss: 1.3873 - accuracy: 0.5877 - val_loss: 1.4438 - val_accuracy: 0.5999 - 6s/epoch - 52ms/step\n",
      "Epoch 56/10000\n",
      "113/113 - 6s - loss: 1.3761 - accuracy: 0.5896 - val_loss: 1.4172 - val_accuracy: 0.5899 - 6s/epoch - 54ms/step\n",
      "Epoch 57/10000\n",
      "113/113 - 6s - loss: 1.4016 - accuracy: 0.5887 - val_loss: 1.5953 - val_accuracy: 0.5616 - 6s/epoch - 52ms/step\n",
      "Epoch 58/10000\n",
      "113/113 - 6s - loss: 1.4419 - accuracy: 0.5829 - val_loss: 1.4600 - val_accuracy: 0.5797 - 6s/epoch - 53ms/step\n",
      "Epoch 59/10000\n",
      "113/113 - 6s - loss: 1.4488 - accuracy: 0.5831 - val_loss: 1.4190 - val_accuracy: 0.5935 - 6s/epoch - 53ms/step\n",
      "Epoch 60/10000\n",
      "113/113 - 6s - loss: 1.3753 - accuracy: 0.5871 - val_loss: 1.3387 - val_accuracy: 0.5818 - 6s/epoch - 53ms/step\n",
      "Epoch 61/10000\n",
      "113/113 - 6s - loss: 1.3459 - accuracy: 0.5880 - val_loss: 1.3429 - val_accuracy: 0.6024 - 6s/epoch - 52ms/step\n",
      "Epoch 62/10000\n",
      "113/113 - 6s - loss: 1.4059 - accuracy: 0.5834 - val_loss: 1.4396 - val_accuracy: 0.5880 - 6s/epoch - 53ms/step\n",
      "Epoch 63/10000\n",
      "113/113 - 6s - loss: 1.3660 - accuracy: 0.5857 - val_loss: 1.3495 - val_accuracy: 0.5719 - 6s/epoch - 52ms/step\n",
      "Epoch 64/10000\n",
      "113/113 - 6s - loss: 1.3552 - accuracy: 0.5876 - val_loss: 1.3936 - val_accuracy: 0.5764 - 6s/epoch - 53ms/step\n",
      "Epoch 65/10000\n",
      "113/113 - 6s - loss: 1.3326 - accuracy: 0.5876 - val_loss: 1.3571 - val_accuracy: 0.5874 - 6s/epoch - 52ms/step\n",
      "Epoch 66/10000\n",
      "113/113 - 6s - loss: 1.3270 - accuracy: 0.5902 - val_loss: 1.3600 - val_accuracy: 0.5873 - 6s/epoch - 53ms/step\n",
      "Epoch 67/10000\n",
      "113/113 - 6s - loss: 1.3129 - accuracy: 0.5874 - val_loss: 1.3230 - val_accuracy: 0.6052 - 6s/epoch - 53ms/step\n",
      "Epoch 68/10000\n",
      "113/113 - 6s - loss: 1.3486 - accuracy: 0.5867 - val_loss: 1.3516 - val_accuracy: 0.5924 - 6s/epoch - 54ms/step\n",
      "Epoch 69/10000\n",
      "113/113 - 6s - loss: 1.3524 - accuracy: 0.5851 - val_loss: 1.3905 - val_accuracy: 0.5772 - 6s/epoch - 55ms/step\n",
      "Epoch 70/10000\n",
      "113/113 - 6s - loss: 1.3372 - accuracy: 0.5870 - val_loss: 1.3225 - val_accuracy: 0.5861 - 6s/epoch - 53ms/step\n",
      "Epoch 71/10000\n",
      "113/113 - 6s - loss: 1.3230 - accuracy: 0.5857 - val_loss: 1.2945 - val_accuracy: 0.5785 - 6s/epoch - 53ms/step\n",
      "Epoch 72/10000\n",
      "113/113 - 6s - loss: 1.3118 - accuracy: 0.5899 - val_loss: 1.2734 - val_accuracy: 0.5812 - 6s/epoch - 55ms/step\n",
      "Epoch 73/10000\n",
      "113/113 - 6s - loss: 1.2871 - accuracy: 0.5898 - val_loss: 1.2675 - val_accuracy: 0.5972 - 6s/epoch - 54ms/step\n",
      "Epoch 74/10000\n",
      "113/113 - 6s - loss: 1.2928 - accuracy: 0.5889 - val_loss: 1.3390 - val_accuracy: 0.5671 - 6s/epoch - 57ms/step\n",
      "Epoch 75/10000\n",
      "113/113 - 6s - loss: 1.3111 - accuracy: 0.5858 - val_loss: 1.3461 - val_accuracy: 0.5770 - 6s/epoch - 57ms/step\n",
      "Epoch 76/10000\n",
      "113/113 - 6s - loss: 1.2988 - accuracy: 0.5872 - val_loss: 1.3041 - val_accuracy: 0.5946 - 6s/epoch - 55ms/step\n",
      "Epoch 77/10000\n",
      "113/113 - 6s - loss: 1.2947 - accuracy: 0.5864 - val_loss: 1.2706 - val_accuracy: 0.5894 - 6s/epoch - 55ms/step\n",
      "Epoch 78/10000\n",
      "113/113 - 6s - loss: 1.2826 - accuracy: 0.5884 - val_loss: 1.3251 - val_accuracy: 0.5789 - 6s/epoch - 53ms/step\n",
      "Epoch 79/10000\n",
      "113/113 - 6s - loss: 1.2877 - accuracy: 0.5871 - val_loss: 1.3043 - val_accuracy: 0.5974 - 6s/epoch - 53ms/step\n",
      "Epoch 80/10000\n",
      "113/113 - 6s - loss: 1.2744 - accuracy: 0.5868 - val_loss: 1.2809 - val_accuracy: 0.5816 - 6s/epoch - 55ms/step\n",
      "Epoch 81/10000\n",
      "113/113 - 6s - loss: 1.2784 - accuracy: 0.5881 - val_loss: 1.3310 - val_accuracy: 0.5928 - 6s/epoch - 55ms/step\n",
      "Epoch 82/10000\n",
      "113/113 - 6s - loss: 1.3015 - accuracy: 0.5895 - val_loss: 1.2947 - val_accuracy: 0.6013 - 6s/epoch - 56ms/step\n",
      "Epoch 83/10000\n",
      "113/113 - 6s - loss: 1.3241 - accuracy: 0.5886 - val_loss: 1.3653 - val_accuracy: 0.5954 - 6s/epoch - 53ms/step\n",
      "Epoch 84/10000\n",
      "113/113 - 6s - loss: 1.3297 - accuracy: 0.5828 - val_loss: 1.3017 - val_accuracy: 0.5948 - 6s/epoch - 53ms/step\n",
      "Epoch 85/10000\n",
      "113/113 - 6s - loss: 1.2930 - accuracy: 0.5851 - val_loss: 1.3700 - val_accuracy: 0.5839 - 6s/epoch - 53ms/step\n",
      "Epoch 86/10000\n",
      "113/113 - 6s - loss: 1.2999 - accuracy: 0.5864 - val_loss: 1.3300 - val_accuracy: 0.5835 - 6s/epoch - 54ms/step\n",
      "Epoch 87/10000\n",
      "113/113 - 6s - loss: 1.2691 - accuracy: 0.5867 - val_loss: 1.3511 - val_accuracy: 0.5780 - 6s/epoch - 53ms/step\n",
      "391/391 [==============================] - 4s 9ms/step\n",
      "\n",
      "Iterazione 96/128 (03-11-2023_13-55-53)\n",
      "hidden_layer_size: 500\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 4\n",
      "activation_hidden: sigmoid\n",
      "activation_output: sigmoid\n",
      "batch_size: 1000\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 20\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 60.52%\n",
      "La recall di questo modello sul validation set è: 38.01%\n",
      "La f1 di questo modello sul validation set è: 42.7%\n",
      "La balanced accuracy di questo modello sul validation set è: 56.37%\n",
      "La precision di questo modello sul validation set è: 48.7%\n",
      "La AUC di questo modello sul validation set è: 59.42%\n",
      "391/391 [==============================] - 3s 7ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.66    0.75      0.70  7,670.00\n",
      "1                  0.49    0.38      0.43  4,841.00\n",
      "accuracy           0.61    0.61      0.61      0.61\n",
      "macro avg          0.57    0.56      0.56 12,511.00\n",
      "weighted avg       0.59    0.61      0.59 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         5732         1938\n",
      "Actual 1         3001         1840\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 66.1% con il modello:\n",
      "66 - AC66.1%, L500, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA20, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_11-28-58).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 62.97% con il modello:\n",
      "1 - BA62.97%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 64.08% con il modello:\n",
      "13 - PR64.08%, L20, mV-10, MV10, P2, AHsigmoid, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-26-48).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "1126/1126 - 7s - loss: 148601.1250 - accuracy: 0.5631 - val_loss: 4740.9858 - val_accuracy: 0.5092 - 7s/epoch - 6ms/step\n",
      "Epoch 2/10000\n",
      "1126/1126 - 4s - loss: 3092.2383 - accuracy: 0.5818 - val_loss: 6479.7788 - val_accuracy: 0.6155 - 4s/epoch - 4ms/step\n",
      "Epoch 3/10000\n",
      "1126/1126 - 4s - loss: 2772.9141 - accuracy: 0.5852 - val_loss: 1354.4009 - val_accuracy: 0.6176 - 4s/epoch - 4ms/step\n",
      "Epoch 4/10000\n",
      "1126/1126 - 4s - loss: 2359.1726 - accuracy: 0.5884 - val_loss: 1408.4114 - val_accuracy: 0.5913 - 4s/epoch - 4ms/step\n",
      "Epoch 5/10000\n",
      "1126/1126 - 4s - loss: 2023.0587 - accuracy: 0.5884 - val_loss: 1228.8959 - val_accuracy: 0.6018 - 4s/epoch - 4ms/step\n",
      "Epoch 6/10000\n",
      "1126/1126 - 4s - loss: 2063.6357 - accuracy: 0.5882 - val_loss: 2016.3541 - val_accuracy: 0.6239 - 4s/epoch - 4ms/step\n",
      "Epoch 7/10000\n",
      "1126/1126 - 4s - loss: 1868.8718 - accuracy: 0.5904 - val_loss: 2988.7449 - val_accuracy: 0.6200 - 4s/epoch - 4ms/step\n",
      "Epoch 8/10000\n",
      "1126/1126 - 4s - loss: 1896.6969 - accuracy: 0.5857 - val_loss: 957.6741 - val_accuracy: 0.6176 - 4s/epoch - 4ms/step\n",
      "Epoch 9/10000\n",
      "1126/1126 - 4s - loss: 1914.7317 - accuracy: 0.5895 - val_loss: 881.0008 - val_accuracy: 0.6247 - 4s/epoch - 3ms/step\n",
      "Epoch 10/10000\n",
      "1126/1126 - 4s - loss: 1857.9177 - accuracy: 0.5892 - val_loss: 909.8686 - val_accuracy: 0.6109 - 4s/epoch - 3ms/step\n",
      "Epoch 11/10000\n",
      "1126/1126 - 4s - loss: 1904.2859 - accuracy: 0.5904 - val_loss: 1618.2716 - val_accuracy: 0.6233 - 4s/epoch - 3ms/step\n",
      "Epoch 12/10000\n",
      "1126/1126 - 4s - loss: 1887.5233 - accuracy: 0.5897 - val_loss: 1675.6566 - val_accuracy: 0.5912 - 4s/epoch - 3ms/step\n",
      "Epoch 13/10000\n",
      "1126/1126 - 4s - loss: 1804.8159 - accuracy: 0.5904 - val_loss: 1239.6241 - val_accuracy: 0.6253 - 4s/epoch - 4ms/step\n",
      "Epoch 14/10000\n",
      "1126/1126 - 3s - loss: 1782.4404 - accuracy: 0.5924 - val_loss: 2460.1868 - val_accuracy: 0.4950 - 3s/epoch - 3ms/step\n",
      "Epoch 15/10000\n",
      "1126/1126 - 3s - loss: 1680.7009 - accuracy: 0.5925 - val_loss: 1034.0255 - val_accuracy: 0.6235 - 3s/epoch - 3ms/step\n",
      "Epoch 16/10000\n",
      "1126/1126 - 4s - loss: 1933.7882 - accuracy: 0.5927 - val_loss: 1124.5833 - val_accuracy: 0.5930 - 4s/epoch - 3ms/step\n",
      "Epoch 17/10000\n",
      "1126/1126 - 4s - loss: 1769.7650 - accuracy: 0.5904 - val_loss: 1904.6187 - val_accuracy: 0.6246 - 4s/epoch - 3ms/step\n",
      "Epoch 18/10000\n",
      "1126/1126 - 4s - loss: 1728.5879 - accuracy: 0.5931 - val_loss: 723.4359 - val_accuracy: 0.6387 - 4s/epoch - 3ms/step\n",
      "Epoch 19/10000\n",
      "1126/1126 - 4s - loss: 1700.9005 - accuracy: 0.5944 - val_loss: 796.0588 - val_accuracy: 0.6059 - 4s/epoch - 3ms/step\n",
      "Epoch 20/10000\n",
      "1126/1126 - 3s - loss: 1679.5227 - accuracy: 0.5947 - val_loss: 2083.8528 - val_accuracy: 0.6213 - 3s/epoch - 3ms/step\n",
      "Epoch 21/10000\n",
      "1126/1126 - 4s - loss: 1652.6720 - accuracy: 0.5949 - val_loss: 1386.6460 - val_accuracy: 0.5628 - 4s/epoch - 3ms/step\n",
      "Epoch 22/10000\n",
      "1126/1126 - 4s - loss: 1817.7806 - accuracy: 0.5940 - val_loss: 1685.3617 - val_accuracy: 0.6354 - 4s/epoch - 3ms/step\n",
      "Epoch 23/10000\n",
      "1126/1126 - 4s - loss: 1859.9755 - accuracy: 0.5936 - val_loss: 936.4122 - val_accuracy: 0.6043 - 4s/epoch - 3ms/step\n",
      "Epoch 24/10000\n",
      "1126/1126 - 4s - loss: 1782.0253 - accuracy: 0.5950 - val_loss: 1692.4679 - val_accuracy: 0.6268 - 4s/epoch - 3ms/step\n",
      "Epoch 25/10000\n",
      "1126/1126 - 4s - loss: 1695.5944 - accuracy: 0.5950 - val_loss: 1063.0203 - val_accuracy: 0.6282 - 4s/epoch - 3ms/step\n",
      "Epoch 26/10000\n",
      "1126/1126 - 4s - loss: 1743.2472 - accuracy: 0.5938 - val_loss: 674.2551 - val_accuracy: 0.6472 - 4s/epoch - 3ms/step\n",
      "Epoch 27/10000\n",
      "1126/1126 - 3s - loss: 1522.6774 - accuracy: 0.5978 - val_loss: 1505.2384 - val_accuracy: 0.5145 - 3s/epoch - 3ms/step\n",
      "Epoch 28/10000\n",
      "1126/1126 - 4s - loss: 1761.7415 - accuracy: 0.5936 - val_loss: 817.9446 - val_accuracy: 0.6342 - 4s/epoch - 3ms/step\n",
      "Epoch 29/10000\n",
      "1126/1126 - 4s - loss: 1767.9116 - accuracy: 0.5956 - val_loss: 721.0356 - val_accuracy: 0.6383 - 4s/epoch - 3ms/step\n",
      "Epoch 30/10000\n",
      "1126/1126 - 4s - loss: 1562.5975 - accuracy: 0.5961 - val_loss: 816.0841 - val_accuracy: 0.5970 - 4s/epoch - 3ms/step\n",
      "Epoch 31/10000\n",
      "1126/1126 - 4s - loss: 1707.2017 - accuracy: 0.5951 - val_loss: 1935.1853 - val_accuracy: 0.5577 - 4s/epoch - 3ms/step\n",
      "Epoch 32/10000\n",
      "1126/1126 - 4s - loss: 1668.9862 - accuracy: 0.5976 - val_loss: 3876.7202 - val_accuracy: 0.6214 - 4s/epoch - 3ms/step\n",
      "Epoch 33/10000\n",
      "1126/1126 - 4s - loss: 1814.1217 - accuracy: 0.5981 - val_loss: 644.8721 - val_accuracy: 0.6043 - 4s/epoch - 3ms/step\n",
      "Epoch 34/10000\n",
      "1126/1126 - 4s - loss: 1697.4038 - accuracy: 0.5977 - val_loss: 1085.9869 - val_accuracy: 0.5749 - 4s/epoch - 3ms/step\n",
      "Epoch 35/10000\n",
      "1126/1126 - 4s - loss: 1845.3250 - accuracy: 0.5953 - val_loss: 885.0867 - val_accuracy: 0.6306 - 4s/epoch - 3ms/step\n",
      "Epoch 36/10000\n",
      "1126/1126 - 4s - loss: 1738.0580 - accuracy: 0.5974 - val_loss: 1482.4009 - val_accuracy: 0.5577 - 4s/epoch - 3ms/step\n",
      "391/391 [==============================] - 2s 4ms/step\n",
      "\n",
      "Iterazione 97/128 (03-11-2023_13-58-17)\n",
      "hidden_layer_size: 1000\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 1\n",
      "activation_hidden: relu\n",
      "activation_output: sigmoid\n",
      "batch_size: 100\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 10\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 64.72%\n",
      "La recall di questo modello sul validation set è: 33.94%\n",
      "La f1 di questo modello sul validation set è: 42.68%\n",
      "La balanced accuracy di questo modello sul validation set è: 59.04%\n",
      "La precision di questo modello sul validation set è: 57.47%\n",
      "La AUC di questo modello sul validation set è: 59.63%\n",
      "391/391 [==============================] - 1s 3ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.67    0.84      0.75  7,670.00\n",
      "1                  0.57    0.34      0.43  4,841.00\n",
      "accuracy           0.65    0.65      0.65      0.65\n",
      "macro avg          0.62    0.59      0.59 12,511.00\n",
      "weighted avg       0.63    0.65      0.62 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         6454         1216\n",
      "Actual 1         3198         1643\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 66.1% con il modello:\n",
      "66 - AC66.1%, L500, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA20, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_11-28-58).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 62.97% con il modello:\n",
      "1 - BA62.97%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 64.08% con il modello:\n",
      "13 - PR64.08%, L20, mV-10, MV10, P2, AHsigmoid, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-26-48).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "1126/1126 - 6s - loss: 148601.1250 - accuracy: 0.5631 - val_loss: 4740.9858 - val_accuracy: 0.5092 - 6s/epoch - 6ms/step\n",
      "Epoch 2/10000\n",
      "1126/1126 - 4s - loss: 3092.2383 - accuracy: 0.5818 - val_loss: 6479.7788 - val_accuracy: 0.6155 - 4s/epoch - 4ms/step\n",
      "Epoch 3/10000\n",
      "1126/1126 - 4s - loss: 2772.9141 - accuracy: 0.5852 - val_loss: 1354.4009 - val_accuracy: 0.6176 - 4s/epoch - 4ms/step\n",
      "Epoch 4/10000\n",
      "1126/1126 - 4s - loss: 2359.1726 - accuracy: 0.5884 - val_loss: 1408.4114 - val_accuracy: 0.5913 - 4s/epoch - 3ms/step\n",
      "Epoch 5/10000\n",
      "1126/1126 - 4s - loss: 2023.0587 - accuracy: 0.5884 - val_loss: 1228.8959 - val_accuracy: 0.6018 - 4s/epoch - 3ms/step\n",
      "Epoch 6/10000\n",
      "1126/1126 - 4s - loss: 2063.6357 - accuracy: 0.5882 - val_loss: 2016.3541 - val_accuracy: 0.6239 - 4s/epoch - 3ms/step\n",
      "Epoch 7/10000\n",
      "1126/1126 - 4s - loss: 1868.8718 - accuracy: 0.5904 - val_loss: 2988.7449 - val_accuracy: 0.6200 - 4s/epoch - 3ms/step\n",
      "Epoch 8/10000\n",
      "1126/1126 - 4s - loss: 1896.6969 - accuracy: 0.5857 - val_loss: 957.6741 - val_accuracy: 0.6176 - 4s/epoch - 3ms/step\n",
      "Epoch 9/10000\n",
      "1126/1126 - 4s - loss: 1914.7317 - accuracy: 0.5895 - val_loss: 881.0008 - val_accuracy: 0.6247 - 4s/epoch - 3ms/step\n",
      "Epoch 10/10000\n",
      "1126/1126 - 4s - loss: 1857.9177 - accuracy: 0.5892 - val_loss: 909.8686 - val_accuracy: 0.6109 - 4s/epoch - 3ms/step\n",
      "Epoch 11/10000\n",
      "1126/1126 - 4s - loss: 1904.2859 - accuracy: 0.5904 - val_loss: 1618.2716 - val_accuracy: 0.6233 - 4s/epoch - 3ms/step\n",
      "Epoch 12/10000\n",
      "1126/1126 - 3s - loss: 1887.5233 - accuracy: 0.5897 - val_loss: 1675.6566 - val_accuracy: 0.5912 - 3s/epoch - 3ms/step\n",
      "Epoch 13/10000\n",
      "1126/1126 - 4s - loss: 1804.8159 - accuracy: 0.5904 - val_loss: 1239.6241 - val_accuracy: 0.6253 - 4s/epoch - 4ms/step\n",
      "Epoch 14/10000\n",
      "1126/1126 - 4s - loss: 1782.4404 - accuracy: 0.5924 - val_loss: 2460.1868 - val_accuracy: 0.4950 - 4s/epoch - 3ms/step\n",
      "Epoch 15/10000\n",
      "1126/1126 - 4s - loss: 1680.7009 - accuracy: 0.5925 - val_loss: 1034.0255 - val_accuracy: 0.6235 - 4s/epoch - 3ms/step\n",
      "Epoch 16/10000\n",
      "1126/1126 - 4s - loss: 1933.7882 - accuracy: 0.5927 - val_loss: 1124.5833 - val_accuracy: 0.5930 - 4s/epoch - 3ms/step\n",
      "Epoch 17/10000\n",
      "1126/1126 - 4s - loss: 1769.7650 - accuracy: 0.5904 - val_loss: 1904.6187 - val_accuracy: 0.6246 - 4s/epoch - 3ms/step\n",
      "Epoch 18/10000\n",
      "1126/1126 - 4s - loss: 1728.5879 - accuracy: 0.5931 - val_loss: 723.4359 - val_accuracy: 0.6387 - 4s/epoch - 4ms/step\n",
      "Epoch 19/10000\n",
      "1126/1126 - 4s - loss: 1700.9005 - accuracy: 0.5944 - val_loss: 796.0588 - val_accuracy: 0.6059 - 4s/epoch - 3ms/step\n",
      "Epoch 20/10000\n",
      "1126/1126 - 3s - loss: 1679.5227 - accuracy: 0.5947 - val_loss: 2083.8528 - val_accuracy: 0.6213 - 3s/epoch - 3ms/step\n",
      "Epoch 21/10000\n",
      "1126/1126 - 3s - loss: 1652.6720 - accuracy: 0.5949 - val_loss: 1386.6460 - val_accuracy: 0.5628 - 3s/epoch - 3ms/step\n",
      "Epoch 22/10000\n",
      "1126/1126 - 3s - loss: 1817.7806 - accuracy: 0.5940 - val_loss: 1685.3617 - val_accuracy: 0.6354 - 3s/epoch - 3ms/step\n",
      "Epoch 23/10000\n",
      "1126/1126 - 3s - loss: 1859.9755 - accuracy: 0.5936 - val_loss: 936.4122 - val_accuracy: 0.6043 - 3s/epoch - 3ms/step\n",
      "Epoch 24/10000\n",
      "1126/1126 - 3s - loss: 1782.0253 - accuracy: 0.5950 - val_loss: 1692.4679 - val_accuracy: 0.6268 - 3s/epoch - 3ms/step\n",
      "Epoch 25/10000\n",
      "1126/1126 - 3s - loss: 1695.5944 - accuracy: 0.5950 - val_loss: 1063.0203 - val_accuracy: 0.6282 - 3s/epoch - 3ms/step\n",
      "Epoch 26/10000\n",
      "1126/1126 - 8s - loss: 1743.2472 - accuracy: 0.5938 - val_loss: 674.2551 - val_accuracy: 0.6472 - 8s/epoch - 7ms/step\n",
      "Epoch 27/10000\n",
      "1126/1126 - 7s - loss: 1522.6774 - accuracy: 0.5978 - val_loss: 1505.2384 - val_accuracy: 0.5145 - 7s/epoch - 6ms/step\n",
      "Epoch 28/10000\n",
      "1126/1126 - 4s - loss: 1761.7415 - accuracy: 0.5936 - val_loss: 817.9446 - val_accuracy: 0.6342 - 4s/epoch - 3ms/step\n",
      "Epoch 29/10000\n",
      "1126/1126 - 4s - loss: 1767.9116 - accuracy: 0.5956 - val_loss: 721.0356 - val_accuracy: 0.6383 - 4s/epoch - 3ms/step\n",
      "Epoch 30/10000\n",
      "1126/1126 - 3s - loss: 1562.5975 - accuracy: 0.5961 - val_loss: 816.0841 - val_accuracy: 0.5970 - 3s/epoch - 3ms/step\n",
      "Epoch 31/10000\n",
      "1126/1126 - 3s - loss: 1707.2017 - accuracy: 0.5951 - val_loss: 1935.1853 - val_accuracy: 0.5577 - 3s/epoch - 3ms/step\n",
      "Epoch 32/10000\n",
      "1126/1126 - 3s - loss: 1668.9862 - accuracy: 0.5976 - val_loss: 3876.7202 - val_accuracy: 0.6214 - 3s/epoch - 3ms/step\n",
      "Epoch 33/10000\n",
      "1126/1126 - 3s - loss: 1814.1217 - accuracy: 0.5981 - val_loss: 644.8721 - val_accuracy: 0.6043 - 3s/epoch - 3ms/step\n",
      "Epoch 34/10000\n",
      "1126/1126 - 3s - loss: 1697.4038 - accuracy: 0.5977 - val_loss: 1085.9869 - val_accuracy: 0.5749 - 3s/epoch - 3ms/step\n",
      "Epoch 35/10000\n",
      "1126/1126 - 4s - loss: 1845.3250 - accuracy: 0.5953 - val_loss: 885.0867 - val_accuracy: 0.6306 - 4s/epoch - 3ms/step\n",
      "Epoch 36/10000\n",
      "1126/1126 - 4s - loss: 1738.0580 - accuracy: 0.5974 - val_loss: 1482.4009 - val_accuracy: 0.5577 - 4s/epoch - 3ms/step\n",
      "Epoch 37/10000\n",
      "1126/1126 - 3s - loss: 1791.3091 - accuracy: 0.5969 - val_loss: 695.9594 - val_accuracy: 0.6589 - 3s/epoch - 3ms/step\n",
      "Epoch 38/10000\n",
      "1126/1126 - 3s - loss: 1573.7135 - accuracy: 0.5991 - val_loss: 1052.6718 - val_accuracy: 0.6366 - 3s/epoch - 3ms/step\n",
      "Epoch 39/10000\n",
      "1126/1126 - 3s - loss: 1684.8319 - accuracy: 0.5961 - val_loss: 1536.2960 - val_accuracy: 0.6291 - 3s/epoch - 3ms/step\n",
      "Epoch 40/10000\n",
      "1126/1126 - 3s - loss: 1585.1587 - accuracy: 0.5983 - val_loss: 690.1497 - val_accuracy: 0.6088 - 3s/epoch - 3ms/step\n",
      "Epoch 41/10000\n",
      "1126/1126 - 3s - loss: 1710.8571 - accuracy: 0.5977 - val_loss: 1093.7108 - val_accuracy: 0.6357 - 3s/epoch - 3ms/step\n",
      "Epoch 42/10000\n",
      "1126/1126 - 3s - loss: 1702.1394 - accuracy: 0.5976 - val_loss: 1109.1263 - val_accuracy: 0.6319 - 3s/epoch - 3ms/step\n",
      "Epoch 43/10000\n",
      "1126/1126 - 3s - loss: 1732.1859 - accuracy: 0.5981 - val_loss: 7345.8608 - val_accuracy: 0.6155 - 3s/epoch - 3ms/step\n",
      "Epoch 44/10000\n",
      "1126/1126 - 3s - loss: 1737.1234 - accuracy: 0.5973 - val_loss: 1229.6124 - val_accuracy: 0.6153 - 3s/epoch - 3ms/step\n",
      "Epoch 45/10000\n",
      "1126/1126 - 3s - loss: 1821.8280 - accuracy: 0.5961 - val_loss: 2120.4995 - val_accuracy: 0.6277 - 3s/epoch - 3ms/step\n",
      "Epoch 46/10000\n",
      "1126/1126 - 3s - loss: 1711.0764 - accuracy: 0.5995 - val_loss: 758.7823 - val_accuracy: 0.6614 - 3s/epoch - 3ms/step\n",
      "Epoch 47/10000\n",
      "1126/1126 - 3s - loss: 1873.1580 - accuracy: 0.5945 - val_loss: 856.5043 - val_accuracy: 0.6394 - 3s/epoch - 3ms/step\n",
      "Epoch 48/10000\n",
      "1126/1126 - 3s - loss: 1784.9980 - accuracy: 0.5981 - val_loss: 548.4175 - val_accuracy: 0.6443 - 3s/epoch - 3ms/step\n",
      "Epoch 49/10000\n",
      "1126/1126 - 3s - loss: 1729.0133 - accuracy: 0.5962 - val_loss: 3159.7456 - val_accuracy: 0.4805 - 3s/epoch - 3ms/step\n",
      "Epoch 50/10000\n",
      "1126/1126 - 3s - loss: 1723.9747 - accuracy: 0.5979 - val_loss: 5851.4326 - val_accuracy: 0.4399 - 3s/epoch - 3ms/step\n",
      "Epoch 51/10000\n",
      "1126/1126 - 3s - loss: 1648.4939 - accuracy: 0.5988 - val_loss: 767.9803 - val_accuracy: 0.5982 - 3s/epoch - 3ms/step\n",
      "Epoch 52/10000\n",
      "1126/1126 - 3s - loss: 1703.1779 - accuracy: 0.5969 - val_loss: 809.3837 - val_accuracy: 0.6466 - 3s/epoch - 3ms/step\n",
      "Epoch 53/10000\n",
      "1126/1126 - 3s - loss: 1596.0470 - accuracy: 0.5992 - val_loss: 2079.3123 - val_accuracy: 0.6276 - 3s/epoch - 3ms/step\n",
      "Epoch 54/10000\n",
      "1126/1126 - 3s - loss: 1678.0715 - accuracy: 0.5966 - val_loss: 2686.5981 - val_accuracy: 0.6230 - 3s/epoch - 3ms/step\n",
      "Epoch 55/10000\n",
      "1126/1126 - 3s - loss: 1707.5386 - accuracy: 0.5985 - val_loss: 3464.1135 - val_accuracy: 0.6185 - 3s/epoch - 3ms/step\n",
      "Epoch 56/10000\n",
      "1126/1126 - 3s - loss: 1657.5922 - accuracy: 0.5985 - val_loss: 685.1996 - val_accuracy: 0.6563 - 3s/epoch - 3ms/step\n",
      "Epoch 57/10000\n",
      "1126/1126 - 3s - loss: 1698.1305 - accuracy: 0.5982 - val_loss: 1302.7448 - val_accuracy: 0.5754 - 3s/epoch - 3ms/step\n",
      "Epoch 58/10000\n",
      "1126/1126 - 3s - loss: 1562.0341 - accuracy: 0.5977 - val_loss: 935.7616 - val_accuracy: 0.6327 - 3s/epoch - 3ms/step\n",
      "Epoch 59/10000\n",
      "1126/1126 - 3s - loss: 1808.3368 - accuracy: 0.5952 - val_loss: 656.4517 - val_accuracy: 0.6488 - 3s/epoch - 3ms/step\n",
      "Epoch 60/10000\n",
      "1126/1126 - 3s - loss: 1682.7994 - accuracy: 0.5987 - val_loss: 1195.6857 - val_accuracy: 0.5517 - 3s/epoch - 3ms/step\n",
      "Epoch 61/10000\n",
      "1126/1126 - 3s - loss: 1547.6145 - accuracy: 0.6003 - val_loss: 988.6342 - val_accuracy: 0.6361 - 3s/epoch - 3ms/step\n",
      "Epoch 62/10000\n",
      "1126/1126 - 3s - loss: 1554.1599 - accuracy: 0.6013 - val_loss: 1142.8881 - val_accuracy: 0.6310 - 3s/epoch - 3ms/step\n",
      "Epoch 63/10000\n",
      "1126/1126 - 3s - loss: 1536.6776 - accuracy: 0.6002 - val_loss: 651.2352 - val_accuracy: 0.6611 - 3s/epoch - 3ms/step\n",
      "Epoch 64/10000\n",
      "1126/1126 - 3s - loss: 1673.4629 - accuracy: 0.5972 - val_loss: 897.1331 - val_accuracy: 0.6105 - 3s/epoch - 3ms/step\n",
      "Epoch 65/10000\n",
      "1126/1126 - 3s - loss: 1566.7456 - accuracy: 0.6006 - val_loss: 2320.1914 - val_accuracy: 0.5038 - 3s/epoch - 3ms/step\n",
      "Epoch 66/10000\n",
      "1126/1126 - 3s - loss: 1609.9066 - accuracy: 0.5992 - val_loss: 667.8636 - val_accuracy: 0.6331 - 3s/epoch - 3ms/step\n",
      "391/391 [==============================] - 1s 3ms/step\n",
      "\n",
      "Iterazione 98/128 (03-11-2023_14-02-17)\n",
      "hidden_layer_size: 1000\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 1\n",
      "activation_hidden: relu\n",
      "activation_output: sigmoid\n",
      "batch_size: 100\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 20\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 66.14%\n",
      "La recall di questo modello sul validation set è: 49.64%\n",
      "La f1 di questo modello sul validation set è: 53.15%\n",
      "La balanced accuracy di questo modello sul validation set è: 63.1%\n",
      "La precision di questo modello sul validation set è: 57.2%\n",
      "La AUC di questo modello sul validation set è: 63.45%\n",
      "391/391 [==============================] - 1s 3ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.71    0.77      0.73  7,670.00\n",
      "1                  0.57    0.50      0.53  4,841.00\n",
      "accuracy           0.66    0.66      0.66      0.66\n",
      "macro avg          0.64    0.63      0.63 12,511.00\n",
      "weighted avg       0.65    0.66      0.66 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         5872         1798\n",
      "Actual 1         2438         2403\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 66.14% con il modello:\n",
      "98 - AC66.14%, L1000, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA20, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_14-02-18).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 63.1% con il modello:\n",
      "98 - BA63.1%, L1000, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA20, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_14-02-18).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 64.08% con il modello:\n",
      "13 - PR64.08%, L20, mV-10, MV10, P2, AHsigmoid, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-26-48).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "113/113 - 4s - loss: 1144040.6250 - accuracy: 0.4648 - val_loss: 196535.8281 - val_accuracy: 0.5955 - 4s/epoch - 31ms/step\n",
      "Epoch 2/10000\n",
      "113/113 - 1s - loss: 159862.8125 - accuracy: 0.5949 - val_loss: 122967.1875 - val_accuracy: 0.6008 - 1s/epoch - 12ms/step\n",
      "Epoch 3/10000\n",
      "113/113 - 1s - loss: 85815.0781 - accuracy: 0.6066 - val_loss: 48700.3359 - val_accuracy: 0.6151 - 1s/epoch - 11ms/step\n",
      "Epoch 4/10000\n",
      "113/113 - 1s - loss: 28927.5449 - accuracy: 0.5988 - val_loss: 19912.1660 - val_accuracy: 0.5832 - 1s/epoch - 13ms/step\n",
      "Epoch 5/10000\n",
      "113/113 - 1s - loss: 14985.1240 - accuracy: 0.5723 - val_loss: 11098.0566 - val_accuracy: 0.5542 - 1s/epoch - 12ms/step\n",
      "Epoch 6/10000\n",
      "113/113 - 1s - loss: 9033.8535 - accuracy: 0.5560 - val_loss: 7728.3457 - val_accuracy: 0.5673 - 1s/epoch - 13ms/step\n",
      "Epoch 7/10000\n",
      "113/113 - 1s - loss: 6328.0278 - accuracy: 0.5548 - val_loss: 5384.3848 - val_accuracy: 0.5675 - 1s/epoch - 13ms/step\n",
      "Epoch 8/10000\n",
      "113/113 - 1s - loss: 4270.8955 - accuracy: 0.5561 - val_loss: 3768.0566 - val_accuracy: 0.5848 - 1s/epoch - 13ms/step\n",
      "Epoch 9/10000\n",
      "113/113 - 1s - loss: 3195.0276 - accuracy: 0.5709 - val_loss: 2386.2163 - val_accuracy: 0.5693 - 1s/epoch - 12ms/step\n",
      "Epoch 10/10000\n",
      "113/113 - 1s - loss: 3215.7747 - accuracy: 0.5778 - val_loss: 2045.6320 - val_accuracy: 0.5856 - 1s/epoch - 13ms/step\n",
      "Epoch 11/10000\n",
      "113/113 - 1s - loss: 3172.0605 - accuracy: 0.5795 - val_loss: 2331.7664 - val_accuracy: 0.6263 - 1s/epoch - 11ms/step\n",
      "Epoch 12/10000\n",
      "113/113 - 2s - loss: 2479.4441 - accuracy: 0.5890 - val_loss: 2985.9231 - val_accuracy: 0.6144 - 2s/epoch - 14ms/step\n",
      "Epoch 13/10000\n",
      "113/113 - 1s - loss: 3140.2849 - accuracy: 0.5837 - val_loss: 2790.8628 - val_accuracy: 0.5350 - 1s/epoch - 13ms/step\n",
      "Epoch 14/10000\n",
      "113/113 - 1s - loss: 2360.6628 - accuracy: 0.5868 - val_loss: 1550.5624 - val_accuracy: 0.6074 - 1s/epoch - 10ms/step\n",
      "Epoch 15/10000\n",
      "113/113 - 1s - loss: 1886.1445 - accuracy: 0.5907 - val_loss: 1647.1967 - val_accuracy: 0.5773 - 1s/epoch - 11ms/step\n",
      "Epoch 16/10000\n",
      "113/113 - 1s - loss: 1608.4697 - accuracy: 0.5918 - val_loss: 1578.7922 - val_accuracy: 0.5692 - 1s/epoch - 11ms/step\n",
      "Epoch 17/10000\n",
      "113/113 - 1s - loss: 1549.2162 - accuracy: 0.5926 - val_loss: 1319.9717 - val_accuracy: 0.6040 - 1s/epoch - 10ms/step\n",
      "Epoch 18/10000\n",
      "113/113 - 1s - loss: 1828.9415 - accuracy: 0.5898 - val_loss: 1685.6648 - val_accuracy: 0.6142 - 1s/epoch - 12ms/step\n",
      "Epoch 19/10000\n",
      "113/113 - 1s - loss: 1598.5906 - accuracy: 0.5908 - val_loss: 1235.0170 - val_accuracy: 0.6016 - 1s/epoch - 10ms/step\n",
      "Epoch 20/10000\n",
      "113/113 - 1s - loss: 2053.1746 - accuracy: 0.5885 - val_loss: 1266.0593 - val_accuracy: 0.6187 - 1s/epoch - 11ms/step\n",
      "Epoch 21/10000\n",
      "113/113 - 1s - loss: 1770.1659 - accuracy: 0.5902 - val_loss: 1189.9194 - val_accuracy: 0.6211 - 1s/epoch - 12ms/step\n",
      "391/391 [==============================] - 1s 3ms/step\n",
      "\n",
      "Iterazione 99/128 (03-11-2023_14-02-51)\n",
      "hidden_layer_size: 1000\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 1\n",
      "activation_hidden: relu\n",
      "activation_output: sigmoid\n",
      "batch_size: 1000\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 10\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 62.63%\n",
      "La recall di questo modello sul validation set è: 23.45%\n",
      "La f1 di questo modello sul validation set è: 32.69%\n",
      "La balanced accuracy di questo modello sul validation set è: 55.41%\n",
      "La precision di questo modello sul validation set è: 53.94%\n",
      "La AUC di questo modello sul validation set è: 55.37%\n",
      "391/391 [==============================] - 1s 3ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.64    0.87      0.74  7,670.00\n",
      "1                  0.54    0.23      0.33  4,841.00\n",
      "accuracy           0.63    0.63      0.63      0.63\n",
      "macro avg          0.59    0.55      0.53 12,511.00\n",
      "weighted avg       0.60    0.63      0.58 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         6701          969\n",
      "Actual 1         3706         1135\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 66.14% con il modello:\n",
      "98 - AC66.14%, L1000, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA20, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_14-02-18).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 63.1% con il modello:\n",
      "98 - BA63.1%, L1000, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA20, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_14-02-18).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 64.08% con il modello:\n",
      "13 - PR64.08%, L20, mV-10, MV10, P2, AHsigmoid, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-26-48).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "113/113 - 3s - loss: 1144040.6250 - accuracy: 0.4648 - val_loss: 196535.8281 - val_accuracy: 0.5955 - 3s/epoch - 29ms/step\n",
      "Epoch 2/10000\n",
      "113/113 - 1s - loss: 159862.8125 - accuracy: 0.5949 - val_loss: 122967.1875 - val_accuracy: 0.6008 - 1s/epoch - 12ms/step\n",
      "Epoch 3/10000\n",
      "113/113 - 1s - loss: 85815.0781 - accuracy: 0.6066 - val_loss: 48700.3359 - val_accuracy: 0.6151 - 1s/epoch - 12ms/step\n",
      "Epoch 4/10000\n",
      "113/113 - 1s - loss: 28927.5449 - accuracy: 0.5988 - val_loss: 19912.1660 - val_accuracy: 0.5832 - 1s/epoch - 13ms/step\n",
      "Epoch 5/10000\n",
      "113/113 - 1s - loss: 14985.1240 - accuracy: 0.5723 - val_loss: 11098.0566 - val_accuracy: 0.5542 - 1s/epoch - 13ms/step\n",
      "Epoch 6/10000\n",
      "113/113 - 1s - loss: 9033.8535 - accuracy: 0.5560 - val_loss: 7728.3457 - val_accuracy: 0.5673 - 1s/epoch - 12ms/step\n",
      "Epoch 7/10000\n",
      "113/113 - 1s - loss: 6328.0278 - accuracy: 0.5548 - val_loss: 5384.3848 - val_accuracy: 0.5675 - 1s/epoch - 13ms/step\n",
      "Epoch 8/10000\n",
      "113/113 - 1s - loss: 4270.8955 - accuracy: 0.5561 - val_loss: 3768.0566 - val_accuracy: 0.5848 - 1s/epoch - 11ms/step\n",
      "Epoch 9/10000\n",
      "113/113 - 2s - loss: 3195.0276 - accuracy: 0.5709 - val_loss: 2386.2163 - val_accuracy: 0.5693 - 2s/epoch - 13ms/step\n",
      "Epoch 10/10000\n",
      "113/113 - 1s - loss: 3215.7747 - accuracy: 0.5778 - val_loss: 2045.6320 - val_accuracy: 0.5856 - 1s/epoch - 12ms/step\n",
      "Epoch 11/10000\n",
      "113/113 - 1s - loss: 3172.0605 - accuracy: 0.5795 - val_loss: 2331.7664 - val_accuracy: 0.6263 - 1s/epoch - 13ms/step\n",
      "Epoch 12/10000\n",
      "113/113 - 1s - loss: 2479.4441 - accuracy: 0.5890 - val_loss: 2985.9231 - val_accuracy: 0.6144 - 1s/epoch - 12ms/step\n",
      "Epoch 13/10000\n",
      "113/113 - 1s - loss: 3140.2849 - accuracy: 0.5837 - val_loss: 2790.8628 - val_accuracy: 0.5350 - 1s/epoch - 10ms/step\n",
      "Epoch 14/10000\n",
      "113/113 - 1s - loss: 2360.6628 - accuracy: 0.5868 - val_loss: 1550.5624 - val_accuracy: 0.6074 - 1s/epoch - 12ms/step\n",
      "Epoch 15/10000\n",
      "113/113 - 1s - loss: 1886.1445 - accuracy: 0.5907 - val_loss: 1647.1967 - val_accuracy: 0.5773 - 1s/epoch - 11ms/step\n",
      "Epoch 16/10000\n",
      "113/113 - 1s - loss: 1608.4697 - accuracy: 0.5918 - val_loss: 1578.7922 - val_accuracy: 0.5692 - 1s/epoch - 12ms/step\n",
      "Epoch 17/10000\n",
      "113/113 - 1s - loss: 1549.2162 - accuracy: 0.5926 - val_loss: 1319.9717 - val_accuracy: 0.6040 - 1s/epoch - 12ms/step\n",
      "Epoch 18/10000\n",
      "113/113 - 2s - loss: 1828.9415 - accuracy: 0.5898 - val_loss: 1685.6648 - val_accuracy: 0.6142 - 2s/epoch - 20ms/step\n",
      "Epoch 19/10000\n",
      "113/113 - 1s - loss: 1598.5906 - accuracy: 0.5908 - val_loss: 1235.0170 - val_accuracy: 0.6016 - 1s/epoch - 12ms/step\n",
      "Epoch 20/10000\n",
      "113/113 - 1s - loss: 2053.1746 - accuracy: 0.5885 - val_loss: 1266.0593 - val_accuracy: 0.6187 - 1s/epoch - 11ms/step\n",
      "Epoch 21/10000\n",
      "113/113 - 1s - loss: 1770.1659 - accuracy: 0.5902 - val_loss: 1189.9194 - val_accuracy: 0.6211 - 1s/epoch - 13ms/step\n",
      "Epoch 22/10000\n",
      "113/113 - 1s - loss: 1936.8053 - accuracy: 0.5921 - val_loss: 1180.8651 - val_accuracy: 0.6126 - 1s/epoch - 11ms/step\n",
      "Epoch 23/10000\n",
      "113/113 - 1s - loss: 1632.6888 - accuracy: 0.5915 - val_loss: 2016.2501 - val_accuracy: 0.6195 - 1s/epoch - 12ms/step\n",
      "Epoch 24/10000\n",
      "113/113 - 1s - loss: 1853.2499 - accuracy: 0.5899 - val_loss: 1067.6509 - val_accuracy: 0.6098 - 1s/epoch - 12ms/step\n",
      "Epoch 25/10000\n",
      "113/113 - 1s - loss: 1726.0088 - accuracy: 0.5897 - val_loss: 1256.7438 - val_accuracy: 0.5717 - 1s/epoch - 11ms/step\n",
      "Epoch 26/10000\n",
      "113/113 - 1s - loss: 2197.1387 - accuracy: 0.5831 - val_loss: 5698.6270 - val_accuracy: 0.6167 - 1s/epoch - 11ms/step\n",
      "Epoch 27/10000\n",
      "113/113 - 1s - loss: 1693.9371 - accuracy: 0.5951 - val_loss: 1330.8805 - val_accuracy: 0.6108 - 1s/epoch - 13ms/step\n",
      "Epoch 28/10000\n",
      "113/113 - 1s - loss: 1604.0314 - accuracy: 0.5877 - val_loss: 1935.0116 - val_accuracy: 0.6197 - 1s/epoch - 11ms/step\n",
      "Epoch 29/10000\n",
      "113/113 - 1s - loss: 2074.4045 - accuracy: 0.5870 - val_loss: 1479.3416 - val_accuracy: 0.5714 - 1s/epoch - 12ms/step\n",
      "Epoch 30/10000\n",
      "113/113 - 1s - loss: 1784.2889 - accuracy: 0.5878 - val_loss: 1268.5352 - val_accuracy: 0.5737 - 1s/epoch - 13ms/step\n",
      "Epoch 31/10000\n",
      "113/113 - 1s - loss: 1272.5856 - accuracy: 0.5934 - val_loss: 940.8676 - val_accuracy: 0.6095 - 1s/epoch - 12ms/step\n",
      "391/391 [==============================] - 1s 3ms/step\n",
      "\n",
      "Iterazione 100/128 (03-11-2023_14-03-39)\n",
      "hidden_layer_size: 1000\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 1\n",
      "activation_hidden: relu\n",
      "activation_output: sigmoid\n",
      "batch_size: 1000\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 20\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 62.63%\n",
      "La recall di questo modello sul validation set è: 23.45%\n",
      "La f1 di questo modello sul validation set è: 32.69%\n",
      "La balanced accuracy di questo modello sul validation set è: 55.41%\n",
      "La precision di questo modello sul validation set è: 53.94%\n",
      "La AUC di questo modello sul validation set è: 55.37%\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.64    0.87      0.74  7,670.00\n",
      "1                  0.54    0.23      0.33  4,841.00\n",
      "accuracy           0.63    0.63      0.63      0.63\n",
      "macro avg          0.59    0.55      0.53 12,511.00\n",
      "weighted avg       0.60    0.63      0.58 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         6701          969\n",
      "Actual 1         3706         1135\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 66.14% con il modello:\n",
      "98 - AC66.14%, L1000, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA20, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_14-02-18).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 63.1% con il modello:\n",
      "98 - BA63.1%, L1000, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA20, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_14-02-18).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 64.08% con il modello:\n",
      "13 - PR64.08%, L20, mV-10, MV10, P2, AHsigmoid, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-26-48).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "1126/1126 - 6s - loss: 4.8035 - accuracy: 0.5740 - val_loss: 3.3964 - val_accuracy: 0.5875 - 6s/epoch - 5ms/step\n",
      "Epoch 2/10000\n",
      "1126/1126 - 4s - loss: 2.8891 - accuracy: 0.5729 - val_loss: 2.5225 - val_accuracy: 0.5692 - 4s/epoch - 4ms/step\n",
      "Epoch 3/10000\n",
      "1126/1126 - 4s - loss: 2.3444 - accuracy: 0.5750 - val_loss: 2.2945 - val_accuracy: 0.5684 - 4s/epoch - 3ms/step\n",
      "Epoch 4/10000\n",
      "1126/1126 - 3s - loss: 2.1852 - accuracy: 0.5684 - val_loss: 2.0756 - val_accuracy: 0.5665 - 3s/epoch - 3ms/step\n",
      "Epoch 5/10000\n",
      "1126/1126 - 3s - loss: 2.0484 - accuracy: 0.5700 - val_loss: 1.9618 - val_accuracy: 0.5764 - 3s/epoch - 3ms/step\n",
      "Epoch 6/10000\n",
      "1126/1126 - 3s - loss: 1.9058 - accuracy: 0.5721 - val_loss: 1.8303 - val_accuracy: 0.5716 - 3s/epoch - 3ms/step\n",
      "Epoch 7/10000\n",
      "1126/1126 - 3s - loss: 1.8427 - accuracy: 0.5702 - val_loss: 1.8215 - val_accuracy: 0.5578 - 3s/epoch - 3ms/step\n",
      "Epoch 8/10000\n",
      "1126/1126 - 4s - loss: 1.7484 - accuracy: 0.5765 - val_loss: 1.7177 - val_accuracy: 0.5692 - 4s/epoch - 3ms/step\n",
      "Epoch 9/10000\n",
      "1126/1126 - 4s - loss: 1.6653 - accuracy: 0.5788 - val_loss: 1.6338 - val_accuracy: 0.5860 - 4s/epoch - 3ms/step\n",
      "Epoch 10/10000\n",
      "1126/1126 - 3s - loss: 1.5836 - accuracy: 0.5825 - val_loss: 1.5688 - val_accuracy: 0.5960 - 3s/epoch - 3ms/step\n",
      "Epoch 11/10000\n",
      "1126/1126 - 3s - loss: 1.5776 - accuracy: 0.5853 - val_loss: 1.5795 - val_accuracy: 0.5899 - 3s/epoch - 3ms/step\n",
      "Epoch 12/10000\n",
      "1126/1126 - 3s - loss: 1.4894 - accuracy: 0.5831 - val_loss: 1.4798 - val_accuracy: 0.5816 - 3s/epoch - 3ms/step\n",
      "Epoch 13/10000\n",
      "1126/1126 - 3s - loss: 1.4682 - accuracy: 0.5778 - val_loss: 1.4663 - val_accuracy: 0.5750 - 3s/epoch - 3ms/step\n",
      "Epoch 14/10000\n",
      "1126/1126 - 3s - loss: 1.4513 - accuracy: 0.5785 - val_loss: 1.4132 - val_accuracy: 0.5908 - 3s/epoch - 3ms/step\n",
      "Epoch 15/10000\n",
      "1126/1126 - 3s - loss: 1.4094 - accuracy: 0.5782 - val_loss: 1.4186 - val_accuracy: 0.5833 - 3s/epoch - 3ms/step\n",
      "Epoch 16/10000\n",
      "1126/1126 - 3s - loss: 1.3926 - accuracy: 0.5824 - val_loss: 1.4367 - val_accuracy: 0.5856 - 3s/epoch - 3ms/step\n",
      "Epoch 17/10000\n",
      "1126/1126 - 3s - loss: 1.3849 - accuracy: 0.5809 - val_loss: 1.3503 - val_accuracy: 0.5885 - 3s/epoch - 3ms/step\n",
      "Epoch 18/10000\n",
      "1126/1126 - 3s - loss: 1.3548 - accuracy: 0.5850 - val_loss: 1.3280 - val_accuracy: 0.6004 - 3s/epoch - 3ms/step\n",
      "Epoch 19/10000\n",
      "1126/1126 - 3s - loss: 1.2842 - accuracy: 0.5860 - val_loss: 1.3058 - val_accuracy: 0.5785 - 3s/epoch - 3ms/step\n",
      "Epoch 20/10000\n",
      "1126/1126 - 3s - loss: 1.2698 - accuracy: 0.5872 - val_loss: 1.2877 - val_accuracy: 0.5944 - 3s/epoch - 3ms/step\n",
      "Epoch 21/10000\n",
      "1126/1126 - 3s - loss: 1.2651 - accuracy: 0.5822 - val_loss: 1.2807 - val_accuracy: 0.5699 - 3s/epoch - 3ms/step\n",
      "Epoch 22/10000\n",
      "1126/1126 - 3s - loss: 1.2529 - accuracy: 0.5791 - val_loss: 1.2907 - val_accuracy: 0.5872 - 3s/epoch - 3ms/step\n",
      "Epoch 23/10000\n",
      "1126/1126 - 3s - loss: 1.2245 - accuracy: 0.5811 - val_loss: 1.2271 - val_accuracy: 0.5881 - 3s/epoch - 3ms/step\n",
      "Epoch 24/10000\n",
      "1126/1126 - 3s - loss: 1.1998 - accuracy: 0.5819 - val_loss: 1.2342 - val_accuracy: 0.5922 - 3s/epoch - 3ms/step\n",
      "Epoch 25/10000\n",
      "1126/1126 - 3s - loss: 1.2110 - accuracy: 0.5835 - val_loss: 1.1934 - val_accuracy: 0.5976 - 3s/epoch - 3ms/step\n",
      "Epoch 26/10000\n",
      "1126/1126 - 3s - loss: 1.1790 - accuracy: 0.5910 - val_loss: 1.1709 - val_accuracy: 0.5932 - 3s/epoch - 3ms/step\n",
      "Epoch 27/10000\n",
      "1126/1126 - 3s - loss: 1.1765 - accuracy: 0.5876 - val_loss: 1.1611 - val_accuracy: 0.5965 - 3s/epoch - 3ms/step\n",
      "Epoch 28/10000\n",
      "1126/1126 - 3s - loss: 1.1560 - accuracy: 0.5900 - val_loss: 1.1708 - val_accuracy: 0.5836 - 3s/epoch - 3ms/step\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Iterazione 101/128 (03-11-2023_14-05-14)\n",
      "hidden_layer_size: 1000\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 1\n",
      "activation_hidden: sigmoid\n",
      "activation_output: sigmoid\n",
      "batch_size: 100\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 10\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 60.04%\n",
      "La recall di questo modello sul validation set è: 29.23%\n",
      "La f1 di questo modello sul validation set è: 36.15%\n",
      "La balanced accuracy di questo modello sul validation set è: 54.36%\n",
      "La precision di questo modello sul validation set è: 47.36%\n",
      "La AUC di questo modello sul validation set è: 59.22%\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.64    0.79      0.71  7,670.00\n",
      "1                  0.47    0.29      0.36  4,841.00\n",
      "accuracy           0.60    0.60      0.60      0.60\n",
      "macro avg          0.56    0.54      0.54 12,511.00\n",
      "weighted avg       0.58    0.60      0.57 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         6097         1573\n",
      "Actual 1         3426         1415\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 66.14% con il modello:\n",
      "98 - AC66.14%, L1000, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA20, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_14-02-18).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 63.1% con il modello:\n",
      "98 - BA63.1%, L1000, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA20, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_14-02-18).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 64.08% con il modello:\n",
      "13 - PR64.08%, L20, mV-10, MV10, P2, AHsigmoid, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-26-48).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "1126/1126 - 5s - loss: 4.8035 - accuracy: 0.5740 - val_loss: 3.3964 - val_accuracy: 0.5875 - 5s/epoch - 4ms/step\n",
      "Epoch 2/10000\n",
      "1126/1126 - 3s - loss: 2.8891 - accuracy: 0.5729 - val_loss: 2.5225 - val_accuracy: 0.5692 - 3s/epoch - 3ms/step\n",
      "Epoch 3/10000\n",
      "1126/1126 - 3s - loss: 2.3444 - accuracy: 0.5750 - val_loss: 2.2945 - val_accuracy: 0.5684 - 3s/epoch - 3ms/step\n",
      "Epoch 4/10000\n",
      "1126/1126 - 3s - loss: 2.1852 - accuracy: 0.5684 - val_loss: 2.0756 - val_accuracy: 0.5665 - 3s/epoch - 3ms/step\n",
      "Epoch 5/10000\n",
      "1126/1126 - 3s - loss: 2.0484 - accuracy: 0.5700 - val_loss: 1.9618 - val_accuracy: 0.5764 - 3s/epoch - 3ms/step\n",
      "Epoch 6/10000\n",
      "1126/1126 - 3s - loss: 1.9058 - accuracy: 0.5721 - val_loss: 1.8303 - val_accuracy: 0.5716 - 3s/epoch - 2ms/step\n",
      "Epoch 7/10000\n",
      "1126/1126 - 3s - loss: 1.8427 - accuracy: 0.5702 - val_loss: 1.8215 - val_accuracy: 0.5578 - 3s/epoch - 3ms/step\n",
      "Epoch 8/10000\n",
      "1126/1126 - 3s - loss: 1.7484 - accuracy: 0.5765 - val_loss: 1.7177 - val_accuracy: 0.5692 - 3s/epoch - 3ms/step\n",
      "Epoch 9/10000\n",
      "1126/1126 - 3s - loss: 1.6653 - accuracy: 0.5788 - val_loss: 1.6338 - val_accuracy: 0.5860 - 3s/epoch - 3ms/step\n",
      "Epoch 10/10000\n",
      "1126/1126 - 3s - loss: 1.5836 - accuracy: 0.5825 - val_loss: 1.5688 - val_accuracy: 0.5960 - 3s/epoch - 3ms/step\n",
      "Epoch 11/10000\n",
      "1126/1126 - 3s - loss: 1.5776 - accuracy: 0.5853 - val_loss: 1.5795 - val_accuracy: 0.5899 - 3s/epoch - 3ms/step\n",
      "Epoch 12/10000\n",
      "1126/1126 - 3s - loss: 1.4894 - accuracy: 0.5831 - val_loss: 1.4798 - val_accuracy: 0.5816 - 3s/epoch - 3ms/step\n",
      "Epoch 13/10000\n",
      "1126/1126 - 3s - loss: 1.4682 - accuracy: 0.5778 - val_loss: 1.4663 - val_accuracy: 0.5750 - 3s/epoch - 3ms/step\n",
      "Epoch 14/10000\n",
      "1126/1126 - 3s - loss: 1.4513 - accuracy: 0.5785 - val_loss: 1.4132 - val_accuracy: 0.5908 - 3s/epoch - 3ms/step\n",
      "Epoch 15/10000\n",
      "1126/1126 - 3s - loss: 1.4094 - accuracy: 0.5782 - val_loss: 1.4186 - val_accuracy: 0.5833 - 3s/epoch - 3ms/step\n",
      "Epoch 16/10000\n",
      "1126/1126 - 3s - loss: 1.3926 - accuracy: 0.5824 - val_loss: 1.4367 - val_accuracy: 0.5856 - 3s/epoch - 3ms/step\n",
      "Epoch 17/10000\n",
      "1126/1126 - 3s - loss: 1.3849 - accuracy: 0.5809 - val_loss: 1.3503 - val_accuracy: 0.5885 - 3s/epoch - 3ms/step\n",
      "Epoch 18/10000\n",
      "1126/1126 - 3s - loss: 1.3548 - accuracy: 0.5850 - val_loss: 1.3280 - val_accuracy: 0.6004 - 3s/epoch - 2ms/step\n",
      "Epoch 19/10000\n",
      "1126/1126 - 3s - loss: 1.2842 - accuracy: 0.5860 - val_loss: 1.3058 - val_accuracy: 0.5785 - 3s/epoch - 2ms/step\n",
      "Epoch 20/10000\n",
      "1126/1126 - 3s - loss: 1.2698 - accuracy: 0.5872 - val_loss: 1.2877 - val_accuracy: 0.5944 - 3s/epoch - 3ms/step\n",
      "Epoch 21/10000\n",
      "1126/1126 - 3s - loss: 1.2651 - accuracy: 0.5822 - val_loss: 1.2807 - val_accuracy: 0.5699 - 3s/epoch - 3ms/step\n",
      "Epoch 22/10000\n",
      "1126/1126 - 3s - loss: 1.2529 - accuracy: 0.5791 - val_loss: 1.2907 - val_accuracy: 0.5872 - 3s/epoch - 2ms/step\n",
      "Epoch 23/10000\n",
      "1126/1126 - 3s - loss: 1.2245 - accuracy: 0.5811 - val_loss: 1.2271 - val_accuracy: 0.5881 - 3s/epoch - 3ms/step\n",
      "Epoch 24/10000\n",
      "1126/1126 - 3s - loss: 1.1998 - accuracy: 0.5819 - val_loss: 1.2342 - val_accuracy: 0.5922 - 3s/epoch - 3ms/step\n",
      "Epoch 25/10000\n",
      "1126/1126 - 3s - loss: 1.2110 - accuracy: 0.5835 - val_loss: 1.1934 - val_accuracy: 0.5976 - 3s/epoch - 3ms/step\n",
      "Epoch 26/10000\n",
      "1126/1126 - 3s - loss: 1.1790 - accuracy: 0.5910 - val_loss: 1.1709 - val_accuracy: 0.5932 - 3s/epoch - 2ms/step\n",
      "Epoch 27/10000\n",
      "1126/1126 - 3s - loss: 1.1765 - accuracy: 0.5876 - val_loss: 1.1611 - val_accuracy: 0.5965 - 3s/epoch - 2ms/step\n",
      "Epoch 28/10000\n",
      "1126/1126 - 3s - loss: 1.1560 - accuracy: 0.5900 - val_loss: 1.1708 - val_accuracy: 0.5836 - 3s/epoch - 2ms/step\n",
      "Epoch 29/10000\n",
      "1126/1126 - 3s - loss: 1.1445 - accuracy: 0.5884 - val_loss: 1.1437 - val_accuracy: 0.5987 - 3s/epoch - 2ms/step\n",
      "Epoch 30/10000\n",
      "1126/1126 - 3s - loss: 1.1277 - accuracy: 0.5880 - val_loss: 1.1312 - val_accuracy: 0.5984 - 3s/epoch - 2ms/step\n",
      "Epoch 31/10000\n",
      "1126/1126 - 3s - loss: 1.1224 - accuracy: 0.5924 - val_loss: 1.1642 - val_accuracy: 0.5483 - 3s/epoch - 2ms/step\n",
      "Epoch 32/10000\n",
      "1126/1126 - 3s - loss: 1.1292 - accuracy: 0.5955 - val_loss: 1.1233 - val_accuracy: 0.5990 - 3s/epoch - 3ms/step\n",
      "Epoch 33/10000\n",
      "1126/1126 - 3s - loss: 1.1367 - accuracy: 0.5940 - val_loss: 1.1170 - val_accuracy: 0.5979 - 3s/epoch - 3ms/step\n",
      "Epoch 34/10000\n",
      "1126/1126 - 3s - loss: 1.1189 - accuracy: 0.5947 - val_loss: 1.1371 - val_accuracy: 0.5962 - 3s/epoch - 3ms/step\n",
      "Epoch 35/10000\n",
      "1126/1126 - 3s - loss: 1.1130 - accuracy: 0.5989 - val_loss: 1.0780 - val_accuracy: 0.6026 - 3s/epoch - 3ms/step\n",
      "Epoch 36/10000\n",
      "1126/1126 - 3s - loss: 1.0882 - accuracy: 0.5991 - val_loss: 1.0735 - val_accuracy: 0.6057 - 3s/epoch - 2ms/step\n",
      "Epoch 37/10000\n",
      "1126/1126 - 3s - loss: 1.0877 - accuracy: 0.5962 - val_loss: 1.1069 - val_accuracy: 0.5871 - 3s/epoch - 2ms/step\n",
      "Epoch 38/10000\n",
      "1126/1126 - 3s - loss: 1.0906 - accuracy: 0.5940 - val_loss: 1.0941 - val_accuracy: 0.5845 - 3s/epoch - 3ms/step\n",
      "Epoch 39/10000\n",
      "1126/1126 - 3s - loss: 1.0673 - accuracy: 0.5903 - val_loss: 1.0908 - val_accuracy: 0.5895 - 3s/epoch - 3ms/step\n",
      "Epoch 40/10000\n",
      "1126/1126 - 3s - loss: 1.0659 - accuracy: 0.5885 - val_loss: 1.0755 - val_accuracy: 0.5977 - 3s/epoch - 3ms/step\n",
      "Epoch 41/10000\n",
      "1126/1126 - 3s - loss: 1.0569 - accuracy: 0.5895 - val_loss: 1.0440 - val_accuracy: 0.5980 - 3s/epoch - 3ms/step\n",
      "Epoch 42/10000\n",
      "1126/1126 - 3s - loss: 1.0514 - accuracy: 0.5924 - val_loss: 1.0271 - val_accuracy: 0.5975 - 3s/epoch - 3ms/step\n",
      "Epoch 43/10000\n",
      "1126/1126 - 3s - loss: 1.0372 - accuracy: 0.5916 - val_loss: 1.0406 - val_accuracy: 0.5906 - 3s/epoch - 3ms/step\n",
      "Epoch 44/10000\n",
      "1126/1126 - 3s - loss: 1.0241 - accuracy: 0.5914 - val_loss: 1.0781 - val_accuracy: 0.5962 - 3s/epoch - 3ms/step\n",
      "Epoch 45/10000\n",
      "1126/1126 - 3s - loss: 1.0320 - accuracy: 0.5887 - val_loss: 1.0592 - val_accuracy: 0.5928 - 3s/epoch - 3ms/step\n",
      "Epoch 46/10000\n",
      "1126/1126 - 3s - loss: 1.0365 - accuracy: 0.5909 - val_loss: 1.0330 - val_accuracy: 0.5954 - 3s/epoch - 3ms/step\n",
      "Epoch 47/10000\n",
      "1126/1126 - 3s - loss: 1.0202 - accuracy: 0.5913 - val_loss: 1.0441 - val_accuracy: 0.5989 - 3s/epoch - 3ms/step\n",
      "Epoch 48/10000\n",
      "1126/1126 - 3s - loss: 1.0085 - accuracy: 0.5913 - val_loss: 1.0217 - val_accuracy: 0.5917 - 3s/epoch - 3ms/step\n",
      "Epoch 49/10000\n",
      "1126/1126 - 3s - loss: 1.0130 - accuracy: 0.5918 - val_loss: 1.0267 - val_accuracy: 0.5910 - 3s/epoch - 3ms/step\n",
      "Epoch 50/10000\n",
      "1126/1126 - 3s - loss: 1.0105 - accuracy: 0.5951 - val_loss: 1.0051 - val_accuracy: 0.6063 - 3s/epoch - 3ms/step\n",
      "Epoch 51/10000\n",
      "1126/1126 - 3s - loss: 0.9966 - accuracy: 0.5955 - val_loss: 1.0102 - val_accuracy: 0.5442 - 3s/epoch - 3ms/step\n",
      "Epoch 52/10000\n",
      "1126/1126 - 3s - loss: 0.9829 - accuracy: 0.5940 - val_loss: 0.9647 - val_accuracy: 0.6051 - 3s/epoch - 3ms/step\n",
      "Epoch 53/10000\n",
      "1126/1126 - 3s - loss: 0.9659 - accuracy: 0.5950 - val_loss: 0.9709 - val_accuracy: 0.5987 - 3s/epoch - 3ms/step\n",
      "Epoch 54/10000\n",
      "1126/1126 - 3s - loss: 0.9599 - accuracy: 0.5976 - val_loss: 0.9668 - val_accuracy: 0.6069 - 3s/epoch - 3ms/step\n",
      "Epoch 55/10000\n",
      "1126/1126 - 3s - loss: 0.9597 - accuracy: 0.5981 - val_loss: 0.9602 - val_accuracy: 0.5994 - 3s/epoch - 3ms/step\n",
      "Epoch 56/10000\n",
      "1126/1126 - 3s - loss: 0.9583 - accuracy: 0.5922 - val_loss: 0.9765 - val_accuracy: 0.5943 - 3s/epoch - 3ms/step\n",
      "Epoch 57/10000\n",
      "1126/1126 - 3s - loss: 0.9604 - accuracy: 0.5933 - val_loss: 0.9345 - val_accuracy: 0.6155 - 3s/epoch - 3ms/step\n",
      "Epoch 58/10000\n",
      "1126/1126 - 3s - loss: 0.9483 - accuracy: 0.5972 - val_loss: 0.9343 - val_accuracy: 0.6057 - 3s/epoch - 3ms/step\n",
      "Epoch 59/10000\n",
      "1126/1126 - 3s - loss: 0.9553 - accuracy: 0.5949 - val_loss: 0.9468 - val_accuracy: 0.5896 - 3s/epoch - 3ms/step\n",
      "Epoch 60/10000\n",
      "1126/1126 - 3s - loss: 0.9415 - accuracy: 0.5957 - val_loss: 0.9364 - val_accuracy: 0.5927 - 3s/epoch - 3ms/step\n",
      "Epoch 61/10000\n",
      "1126/1126 - 3s - loss: 0.9365 - accuracy: 0.5946 - val_loss: 0.9455 - val_accuracy: 0.6136 - 3s/epoch - 2ms/step\n",
      "Epoch 62/10000\n",
      "1126/1126 - 3s - loss: 0.9244 - accuracy: 0.5911 - val_loss: 0.9191 - val_accuracy: 0.5896 - 3s/epoch - 3ms/step\n",
      "Epoch 63/10000\n",
      "1126/1126 - 3s - loss: 0.9226 - accuracy: 0.5932 - val_loss: 0.9515 - val_accuracy: 0.6107 - 3s/epoch - 3ms/step\n",
      "Epoch 64/10000\n",
      "1126/1126 - 3s - loss: 0.9160 - accuracy: 0.5957 - val_loss: 0.9207 - val_accuracy: 0.6022 - 3s/epoch - 3ms/step\n",
      "Epoch 65/10000\n",
      "1126/1126 - 3s - loss: 0.9161 - accuracy: 0.5974 - val_loss: 0.9356 - val_accuracy: 0.5887 - 3s/epoch - 3ms/step\n",
      "Epoch 66/10000\n",
      "1126/1126 - 3s - loss: 0.9165 - accuracy: 0.5912 - val_loss: 0.9216 - val_accuracy: 0.5944 - 3s/epoch - 3ms/step\n",
      "Epoch 67/10000\n",
      "1126/1126 - 3s - loss: 0.9091 - accuracy: 0.5934 - val_loss: 0.9007 - val_accuracy: 0.5930 - 3s/epoch - 3ms/step\n",
      "Epoch 68/10000\n",
      "1126/1126 - 3s - loss: 0.9104 - accuracy: 0.5932 - val_loss: 0.9252 - val_accuracy: 0.5826 - 3s/epoch - 3ms/step\n",
      "Epoch 69/10000\n",
      "1126/1126 - 3s - loss: 0.8986 - accuracy: 0.5976 - val_loss: 0.9160 - val_accuracy: 0.6011 - 3s/epoch - 3ms/step\n",
      "Epoch 70/10000\n",
      "1126/1126 - 3s - loss: 0.8920 - accuracy: 0.6010 - val_loss: 0.9103 - val_accuracy: 0.6155 - 3s/epoch - 3ms/step\n",
      "Epoch 71/10000\n",
      "1126/1126 - 3s - loss: 0.8870 - accuracy: 0.6032 - val_loss: 0.8910 - val_accuracy: 0.6089 - 3s/epoch - 3ms/step\n",
      "Epoch 72/10000\n",
      "1126/1126 - 3s - loss: 0.8856 - accuracy: 0.6027 - val_loss: 0.9270 - val_accuracy: 0.6054 - 3s/epoch - 3ms/step\n",
      "Epoch 73/10000\n",
      "1126/1126 - 3s - loss: 0.8792 - accuracy: 0.6029 - val_loss: 0.8960 - val_accuracy: 0.6095 - 3s/epoch - 3ms/step\n",
      "Epoch 74/10000\n",
      "1126/1126 - 3s - loss: 0.8695 - accuracy: 0.6051 - val_loss: 0.8913 - val_accuracy: 0.6116 - 3s/epoch - 3ms/step\n",
      "Epoch 75/10000\n",
      "1126/1126 - 3s - loss: 0.8750 - accuracy: 0.6030 - val_loss: 0.8924 - val_accuracy: 0.6080 - 3s/epoch - 2ms/step\n",
      "Epoch 76/10000\n",
      "1126/1126 - 3s - loss: 0.8699 - accuracy: 0.6047 - val_loss: 0.9098 - val_accuracy: 0.6004 - 3s/epoch - 2ms/step\n",
      "Epoch 77/10000\n",
      "1126/1126 - 3s - loss: 0.8707 - accuracy: 0.6027 - val_loss: 0.8825 - val_accuracy: 0.6103 - 3s/epoch - 3ms/step\n",
      "391/391 [==============================] - 1s 3ms/step\n",
      "\n",
      "Iterazione 102/128 (03-11-2023_14-09-01)\n",
      "hidden_layer_size: 1000\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 1\n",
      "activation_hidden: sigmoid\n",
      "activation_output: sigmoid\n",
      "batch_size: 100\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 20\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 61.55%\n",
      "La recall di questo modello sul validation set è: 32.12%\n",
      "La f1 di questo modello sul validation set è: 39.27%\n",
      "La balanced accuracy di questo modello sul validation set è: 56.13%\n",
      "La precision di questo modello sul validation set è: 50.5%\n",
      "La AUC di questo modello sul validation set è: 61.07%\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.65    0.80      0.72  7,670.00\n",
      "1                  0.51    0.32      0.39  4,841.00\n",
      "accuracy           0.62    0.62      0.62      0.62\n",
      "macro avg          0.58    0.56      0.56 12,511.00\n",
      "weighted avg       0.59    0.62      0.59 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         6146         1524\n",
      "Actual 1         3286         1555\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 66.14% con il modello:\n",
      "98 - AC66.14%, L1000, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA20, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_14-02-18).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 63.1% con il modello:\n",
      "98 - BA63.1%, L1000, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA20, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_14-02-18).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 64.08% con il modello:\n",
      "13 - PR64.08%, L20, mV-10, MV10, P2, AHsigmoid, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-26-48).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "113/113 - 3s - loss: 7.9162 - accuracy: 0.5571 - val_loss: 5.8380 - val_accuracy: 0.5828 - 3s/epoch - 23ms/step\n",
      "Epoch 2/10000\n",
      "113/113 - 1s - loss: 5.2580 - accuracy: 0.5876 - val_loss: 4.7398 - val_accuracy: 0.5916 - 1s/epoch - 11ms/step\n",
      "Epoch 3/10000\n",
      "113/113 - 1s - loss: 4.2785 - accuracy: 0.5897 - val_loss: 3.9919 - val_accuracy: 0.5787 - 1s/epoch - 13ms/step\n",
      "Epoch 4/10000\n",
      "113/113 - 1s - loss: 3.7158 - accuracy: 0.5797 - val_loss: 3.5241 - val_accuracy: 0.5764 - 1s/epoch - 11ms/step\n",
      "Epoch 5/10000\n",
      "113/113 - 1s - loss: 3.3540 - accuracy: 0.5776 - val_loss: 3.2068 - val_accuracy: 0.5845 - 1s/epoch - 13ms/step\n",
      "Epoch 6/10000\n",
      "113/113 - 1s - loss: 3.0630 - accuracy: 0.5769 - val_loss: 2.9569 - val_accuracy: 0.5781 - 1s/epoch - 10ms/step\n",
      "Epoch 7/10000\n",
      "113/113 - 1s - loss: 2.8700 - accuracy: 0.5767 - val_loss: 2.8061 - val_accuracy: 0.5721 - 1s/epoch - 13ms/step\n",
      "Epoch 8/10000\n",
      "113/113 - 1s - loss: 2.7470 - accuracy: 0.5764 - val_loss: 2.6847 - val_accuracy: 0.5775 - 1s/epoch - 11ms/step\n",
      "Epoch 9/10000\n",
      "113/113 - 1s - loss: 2.6446 - accuracy: 0.5802 - val_loss: 2.5685 - val_accuracy: 0.5777 - 1s/epoch - 13ms/step\n",
      "Epoch 10/10000\n",
      "113/113 - 1s - loss: 2.5504 - accuracy: 0.5832 - val_loss: 2.5053 - val_accuracy: 0.5748 - 1s/epoch - 13ms/step\n",
      "Epoch 11/10000\n",
      "113/113 - 2s - loss: 2.4607 - accuracy: 0.5820 - val_loss: 2.4489 - val_accuracy: 0.5877 - 2s/epoch - 14ms/step\n",
      "Epoch 12/10000\n",
      "113/113 - 1s - loss: 2.4000 - accuracy: 0.5868 - val_loss: 2.3985 - val_accuracy: 0.5839 - 1s/epoch - 10ms/step\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Iterazione 103/128 (03-11-2023_14-09-21)\n",
      "hidden_layer_size: 1000\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 1\n",
      "activation_hidden: sigmoid\n",
      "activation_output: sigmoid\n",
      "batch_size: 1000\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 10\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 59.16%\n",
      "La recall di questo modello sul validation set è: 43.34%\n",
      "La f1 di questo modello sul validation set è: 45.09%\n",
      "La balanced accuracy di questo modello sul validation set è: 56.25%\n",
      "La precision di questo modello sul validation set è: 47.0%\n",
      "La AUC di questo modello sul validation set è: 59.62%\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.66    0.69      0.67  7,670.00\n",
      "1                  0.47    0.43      0.45  4,841.00\n",
      "accuracy           0.59    0.59      0.59      0.59\n",
      "macro avg          0.56    0.56      0.56 12,511.00\n",
      "weighted avg       0.59    0.59      0.59 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         5304         2366\n",
      "Actual 1         2743         2098\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 66.14% con il modello:\n",
      "98 - AC66.14%, L1000, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA20, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_14-02-18).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 63.1% con il modello:\n",
      "98 - BA63.1%, L1000, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA20, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_14-02-18).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 64.08% con il modello:\n",
      "13 - PR64.08%, L20, mV-10, MV10, P2, AHsigmoid, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-26-48).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "113/113 - 2s - loss: 7.9162 - accuracy: 0.5571 - val_loss: 5.8380 - val_accuracy: 0.5828 - 2s/epoch - 21ms/step\n",
      "Epoch 2/10000\n",
      "113/113 - 1s - loss: 5.2580 - accuracy: 0.5876 - val_loss: 4.7398 - val_accuracy: 0.5916 - 1s/epoch - 10ms/step\n",
      "Epoch 3/10000\n",
      "113/113 - 1s - loss: 4.2785 - accuracy: 0.5897 - val_loss: 3.9919 - val_accuracy: 0.5787 - 1s/epoch - 12ms/step\n",
      "Epoch 4/10000\n",
      "113/113 - 1s - loss: 3.7158 - accuracy: 0.5797 - val_loss: 3.5241 - val_accuracy: 0.5764 - 1s/epoch - 12ms/step\n",
      "Epoch 5/10000\n",
      "113/113 - 1s - loss: 3.3540 - accuracy: 0.5776 - val_loss: 3.2068 - val_accuracy: 0.5845 - 1s/epoch - 11ms/step\n",
      "Epoch 6/10000\n",
      "113/113 - 1s - loss: 3.0630 - accuracy: 0.5769 - val_loss: 2.9569 - val_accuracy: 0.5781 - 1s/epoch - 12ms/step\n",
      "Epoch 7/10000\n",
      "113/113 - 1s - loss: 2.8700 - accuracy: 0.5767 - val_loss: 2.8061 - val_accuracy: 0.5721 - 1s/epoch - 12ms/step\n",
      "Epoch 8/10000\n",
      "113/113 - 1s - loss: 2.7470 - accuracy: 0.5764 - val_loss: 2.6847 - val_accuracy: 0.5775 - 1s/epoch - 12ms/step\n",
      "Epoch 9/10000\n",
      "113/113 - 1s - loss: 2.6446 - accuracy: 0.5802 - val_loss: 2.5685 - val_accuracy: 0.5777 - 1s/epoch - 11ms/step\n",
      "Epoch 10/10000\n",
      "113/113 - 1s - loss: 2.5504 - accuracy: 0.5832 - val_loss: 2.5053 - val_accuracy: 0.5748 - 1s/epoch - 13ms/step\n",
      "Epoch 11/10000\n",
      "113/113 - 1s - loss: 2.4607 - accuracy: 0.5820 - val_loss: 2.4489 - val_accuracy: 0.5877 - 1s/epoch - 11ms/step\n",
      "Epoch 12/10000\n",
      "113/113 - 1s - loss: 2.4000 - accuracy: 0.5868 - val_loss: 2.3985 - val_accuracy: 0.5839 - 1s/epoch - 9ms/step\n",
      "Epoch 13/10000\n",
      "113/113 - 1s - loss: 2.3181 - accuracy: 0.5821 - val_loss: 2.3567 - val_accuracy: 0.5871 - 1s/epoch - 11ms/step\n",
      "Epoch 14/10000\n",
      "113/113 - 1s - loss: 2.2657 - accuracy: 0.5826 - val_loss: 2.2904 - val_accuracy: 0.5893 - 1s/epoch - 12ms/step\n",
      "Epoch 15/10000\n",
      "113/113 - 1s - loss: 2.2230 - accuracy: 0.5813 - val_loss: 2.2424 - val_accuracy: 0.5631 - 1s/epoch - 10ms/step\n",
      "Epoch 16/10000\n",
      "113/113 - 1s - loss: 2.1657 - accuracy: 0.5872 - val_loss: 2.2025 - val_accuracy: 0.5668 - 1s/epoch - 10ms/step\n",
      "Epoch 17/10000\n",
      "113/113 - 1s - loss: 2.1239 - accuracy: 0.5829 - val_loss: 2.1650 - val_accuracy: 0.5990 - 1s/epoch - 11ms/step\n",
      "Epoch 18/10000\n",
      "113/113 - 1s - loss: 2.0847 - accuracy: 0.5853 - val_loss: 2.1494 - val_accuracy: 0.5885 - 1s/epoch - 11ms/step\n",
      "Epoch 19/10000\n",
      "113/113 - 1s - loss: 2.0563 - accuracy: 0.5835 - val_loss: 2.1388 - val_accuracy: 0.5523 - 1s/epoch - 11ms/step\n",
      "Epoch 20/10000\n",
      "113/113 - 1s - loss: 2.0320 - accuracy: 0.5822 - val_loss: 2.0960 - val_accuracy: 0.5804 - 1s/epoch - 11ms/step\n",
      "Epoch 21/10000\n",
      "113/113 - 1s - loss: 1.9975 - accuracy: 0.5838 - val_loss: 2.0564 - val_accuracy: 0.5834 - 1s/epoch - 10ms/step\n",
      "Epoch 22/10000\n",
      "113/113 - 1s - loss: 1.9709 - accuracy: 0.5825 - val_loss: 2.0457 - val_accuracy: 0.5654 - 1s/epoch - 11ms/step\n",
      "Epoch 23/10000\n",
      "113/113 - 1s - loss: 1.9563 - accuracy: 0.5777 - val_loss: 2.0297 - val_accuracy: 0.5888 - 1s/epoch - 10ms/step\n",
      "Epoch 24/10000\n",
      "113/113 - 1s - loss: 1.9212 - accuracy: 0.5815 - val_loss: 1.9824 - val_accuracy: 0.5662 - 1s/epoch - 11ms/step\n",
      "Epoch 25/10000\n",
      "113/113 - 1s - loss: 1.8974 - accuracy: 0.5799 - val_loss: 1.9764 - val_accuracy: 0.5815 - 1s/epoch - 11ms/step\n",
      "Epoch 26/10000\n",
      "113/113 - 1s - loss: 1.8763 - accuracy: 0.5826 - val_loss: 1.9545 - val_accuracy: 0.5785 - 1s/epoch - 11ms/step\n",
      "Epoch 27/10000\n",
      "113/113 - 1s - loss: 1.8539 - accuracy: 0.5852 - val_loss: 1.9133 - val_accuracy: 0.5813 - 1s/epoch - 11ms/step\n",
      "Epoch 28/10000\n",
      "113/113 - 1s - loss: 1.8281 - accuracy: 0.5856 - val_loss: 1.8860 - val_accuracy: 0.5743 - 1s/epoch - 12ms/step\n",
      "Epoch 29/10000\n",
      "113/113 - 1s - loss: 1.8199 - accuracy: 0.5827 - val_loss: 1.8515 - val_accuracy: 0.5777 - 1s/epoch - 11ms/step\n",
      "Epoch 30/10000\n",
      "113/113 - 1s - loss: 1.7996 - accuracy: 0.5858 - val_loss: 1.8374 - val_accuracy: 0.5835 - 1s/epoch - 10ms/step\n",
      "Epoch 31/10000\n",
      "113/113 - 1s - loss: 1.7776 - accuracy: 0.5870 - val_loss: 1.7957 - val_accuracy: 0.5829 - 1s/epoch - 12ms/step\n",
      "Epoch 32/10000\n",
      "113/113 - 1s - loss: 1.7664 - accuracy: 0.5853 - val_loss: 1.7947 - val_accuracy: 0.5845 - 1s/epoch - 11ms/step\n",
      "Epoch 33/10000\n",
      "113/113 - 1s - loss: 1.7473 - accuracy: 0.5889 - val_loss: 1.7867 - val_accuracy: 0.5924 - 1s/epoch - 11ms/step\n",
      "Epoch 34/10000\n",
      "113/113 - 1s - loss: 1.7347 - accuracy: 0.5887 - val_loss: 1.8068 - val_accuracy: 0.5618 - 1s/epoch - 11ms/step\n",
      "Epoch 35/10000\n",
      "113/113 - 1s - loss: 1.7203 - accuracy: 0.5895 - val_loss: 1.7886 - val_accuracy: 0.5777 - 1s/epoch - 11ms/step\n",
      "Epoch 36/10000\n",
      "113/113 - 1s - loss: 1.7036 - accuracy: 0.5878 - val_loss: 1.7358 - val_accuracy: 0.5777 - 1s/epoch - 10ms/step\n",
      "Epoch 37/10000\n",
      "113/113 - 1s - loss: 1.6781 - accuracy: 0.5849 - val_loss: 1.7302 - val_accuracy: 0.5860 - 1s/epoch - 12ms/step\n",
      "391/391 [==============================] - 1s 3ms/step\n",
      "\n",
      "Iterazione 104/128 (03-11-2023_14-10-12)\n",
      "hidden_layer_size: 1000\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 1\n",
      "activation_hidden: sigmoid\n",
      "activation_output: sigmoid\n",
      "batch_size: 1000\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 20\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 59.9%\n",
      "La recall di questo modello sul validation set è: 38.15%\n",
      "La f1 di questo modello sul validation set è: 42.41%\n",
      "La balanced accuracy di questo modello sul validation set è: 55.89%\n",
      "La precision di questo modello sul validation set è: 47.73%\n",
      "La AUC di questo modello sul validation set è: 58.67%\n",
      "391/391 [==============================] - 1s 2ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.65    0.74      0.69  7,670.00\n",
      "1                  0.48    0.38      0.42  4,841.00\n",
      "accuracy           0.60    0.60      0.60      0.60\n",
      "macro avg          0.57    0.56      0.56 12,511.00\n",
      "weighted avg       0.59    0.60      0.59 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         5647         2023\n",
      "Actual 1         2994         1847\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 66.14% con il modello:\n",
      "98 - AC66.14%, L1000, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA20, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_14-02-18).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 63.1% con il modello:\n",
      "98 - BA63.1%, L1000, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA20, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_14-02-18).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 64.08% con il modello:\n",
      "13 - PR64.08%, L20, mV-10, MV10, P2, AHsigmoid, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-26-48).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "1126/1126 - 30s - loss: 4741544.5000 - accuracy: 0.5729 - val_loss: 3495835.0000 - val_accuracy: 0.4452 - 30s/epoch - 27ms/step\n",
      "Epoch 2/10000\n",
      "1126/1126 - 26s - loss: 2090307.0000 - accuracy: 0.5753 - val_loss: 1934292.2500 - val_accuracy: 0.6246 - 26s/epoch - 23ms/step\n",
      "Epoch 3/10000\n",
      "1126/1126 - 26s - loss: 2433806.5000 - accuracy: 0.5720 - val_loss: 849906.5625 - val_accuracy: 0.5329 - 26s/epoch - 23ms/step\n",
      "Epoch 4/10000\n",
      "1126/1126 - 27s - loss: 2040013.8750 - accuracy: 0.5717 - val_loss: 616095.5000 - val_accuracy: 0.6296 - 27s/epoch - 24ms/step\n",
      "Epoch 5/10000\n",
      "1126/1126 - 25s - loss: 2315941.2500 - accuracy: 0.5729 - val_loss: 1039747.0000 - val_accuracy: 0.6286 - 25s/epoch - 23ms/step\n",
      "Epoch 6/10000\n",
      "1126/1126 - 25s - loss: 2023093.8750 - accuracy: 0.5749 - val_loss: 538962.6250 - val_accuracy: 0.6356 - 25s/epoch - 22ms/step\n",
      "Epoch 7/10000\n",
      "1126/1126 - 26s - loss: 2138365.0000 - accuracy: 0.5752 - val_loss: 4104034.5000 - val_accuracy: 0.6133 - 26s/epoch - 23ms/step\n",
      "Epoch 8/10000\n",
      "1126/1126 - 25s - loss: 2255870.0000 - accuracy: 0.5769 - val_loss: 1696417.8750 - val_accuracy: 0.6189 - 25s/epoch - 23ms/step\n",
      "Epoch 9/10000\n",
      "1126/1126 - 26s - loss: 2292719.5000 - accuracy: 0.5757 - val_loss: 1690367.1250 - val_accuracy: 0.5904 - 26s/epoch - 23ms/step\n",
      "Epoch 10/10000\n",
      "1126/1126 - 26s - loss: 2067137.5000 - accuracy: 0.5778 - val_loss: 1059667.3750 - val_accuracy: 0.4940 - 26s/epoch - 23ms/step\n",
      "Epoch 11/10000\n",
      "1126/1126 - 27s - loss: 2091699.5000 - accuracy: 0.5780 - val_loss: 840675.0000 - val_accuracy: 0.6437 - 27s/epoch - 24ms/step\n",
      "Epoch 12/10000\n",
      "1126/1126 - 27s - loss: 2065642.7500 - accuracy: 0.5806 - val_loss: 1784938.0000 - val_accuracy: 0.6187 - 27s/epoch - 24ms/step\n",
      "Epoch 13/10000\n",
      "1126/1126 - 26s - loss: 2165981.5000 - accuracy: 0.5779 - val_loss: 2377244.0000 - val_accuracy: 0.6231 - 26s/epoch - 23ms/step\n",
      "Epoch 14/10000\n",
      "1126/1126 - 26s - loss: 1917099.7500 - accuracy: 0.5799 - val_loss: 2180535.0000 - val_accuracy: 0.6189 - 26s/epoch - 23ms/step\n",
      "Epoch 15/10000\n",
      "1126/1126 - 25s - loss: 1996879.3750 - accuracy: 0.5800 - val_loss: 6973116.5000 - val_accuracy: 0.6180 - 25s/epoch - 22ms/step\n",
      "Epoch 16/10000\n",
      "1126/1126 - 27s - loss: 1894071.7500 - accuracy: 0.5822 - val_loss: 921807.8125 - val_accuracy: 0.6270 - 27s/epoch - 24ms/step\n",
      "Epoch 17/10000\n",
      "1126/1126 - 26s - loss: 1852580.3750 - accuracy: 0.5826 - val_loss: 5900749.0000 - val_accuracy: 0.6135 - 26s/epoch - 23ms/step\n",
      "Epoch 18/10000\n",
      "1126/1126 - 26s - loss: 1820925.5000 - accuracy: 0.5827 - val_loss: 1800991.5000 - val_accuracy: 0.6251 - 26s/epoch - 23ms/step\n",
      "Epoch 19/10000\n",
      "1126/1126 - 25s - loss: 2039584.5000 - accuracy: 0.5805 - val_loss: 832913.3125 - val_accuracy: 0.6359 - 25s/epoch - 23ms/step\n",
      "Epoch 20/10000\n",
      "1126/1126 - 25s - loss: 1989167.8750 - accuracy: 0.5829 - val_loss: 3589649.2500 - val_accuracy: 0.6146 - 25s/epoch - 22ms/step\n",
      "Epoch 21/10000\n",
      "1126/1126 - 25s - loss: 2004429.6250 - accuracy: 0.5808 - val_loss: 2564537.2500 - val_accuracy: 0.6235 - 25s/epoch - 23ms/step\n",
      "391/391 [==============================] - 2s 5ms/step\n",
      "\n",
      "Iterazione 105/128 (03-11-2023_14-19-23)\n",
      "hidden_layer_size: 1000\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 2\n",
      "activation_hidden: relu\n",
      "activation_output: sigmoid\n",
      "batch_size: 100\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 10\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 64.37%\n",
      "La recall di questo modello sul validation set è: 15.76%\n",
      "La f1 di questo modello sul validation set è: 25.5%\n",
      "La balanced accuracy di questo modello sul validation set è: 55.4%\n",
      "La precision di questo modello sul validation set è: 66.75%\n",
      "La AUC di questo modello sul validation set è: 55.4%\n",
      "391/391 [==============================] - 2s 5ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.64    0.95      0.77  7,670.00\n",
      "1                  0.67    0.16      0.26  4,841.00\n",
      "accuracy           0.64    0.64      0.64      0.64\n",
      "macro avg          0.65    0.55      0.51 12,511.00\n",
      "weighted avg       0.65    0.64      0.57 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         7290          380\n",
      "Actual 1         4078          763\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 66.14% con il modello:\n",
      "98 - AC66.14%, L1000, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA20, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_14-02-18).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 63.1% con il modello:\n",
      "98 - BA63.1%, L1000, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA20, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_14-02-18).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 66.75% con il modello:\n",
      "105 - PR66.75%, L1000, mV-10, MV10, P2, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_14-19-26).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "1126/1126 - 29s - loss: 4741544.5000 - accuracy: 0.5729 - val_loss: 3495835.0000 - val_accuracy: 0.4452 - 29s/epoch - 26ms/step\n",
      "Epoch 2/10000\n",
      "1126/1126 - 28s - loss: 2090307.0000 - accuracy: 0.5753 - val_loss: 1934292.2500 - val_accuracy: 0.6246 - 28s/epoch - 25ms/step\n",
      "Epoch 3/10000\n",
      "1126/1126 - 26s - loss: 2433806.5000 - accuracy: 0.5720 - val_loss: 849906.5625 - val_accuracy: 0.5329 - 26s/epoch - 23ms/step\n",
      "Epoch 4/10000\n",
      "1126/1126 - 26s - loss: 2040013.8750 - accuracy: 0.5717 - val_loss: 616095.5000 - val_accuracy: 0.6296 - 26s/epoch - 23ms/step\n",
      "Epoch 5/10000\n",
      "1126/1126 - 26s - loss: 2315941.2500 - accuracy: 0.5729 - val_loss: 1039747.0000 - val_accuracy: 0.6286 - 26s/epoch - 23ms/step\n",
      "Epoch 6/10000\n",
      "1126/1126 - 26s - loss: 2023093.8750 - accuracy: 0.5749 - val_loss: 538962.6250 - val_accuracy: 0.6356 - 26s/epoch - 23ms/step\n",
      "Epoch 7/10000\n",
      "1126/1126 - 27s - loss: 2138365.0000 - accuracy: 0.5752 - val_loss: 4104034.5000 - val_accuracy: 0.6133 - 27s/epoch - 24ms/step\n",
      "Epoch 8/10000\n",
      "1126/1126 - 26s - loss: 2255870.0000 - accuracy: 0.5769 - val_loss: 1696417.8750 - val_accuracy: 0.6189 - 26s/epoch - 23ms/step\n",
      "Epoch 9/10000\n",
      "1126/1126 - 26s - loss: 2292719.5000 - accuracy: 0.5757 - val_loss: 1690367.1250 - val_accuracy: 0.5904 - 26s/epoch - 23ms/step\n",
      "Epoch 10/10000\n",
      "1126/1126 - 26s - loss: 2067137.5000 - accuracy: 0.5778 - val_loss: 1059667.3750 - val_accuracy: 0.4940 - 26s/epoch - 23ms/step\n",
      "Epoch 11/10000\n",
      "1126/1126 - 25s - loss: 2091699.5000 - accuracy: 0.5780 - val_loss: 840675.0000 - val_accuracy: 0.6437 - 25s/epoch - 23ms/step\n",
      "Epoch 12/10000\n",
      "1126/1126 - 25s - loss: 2065642.7500 - accuracy: 0.5806 - val_loss: 1784938.0000 - val_accuracy: 0.6187 - 25s/epoch - 22ms/step\n",
      "Epoch 13/10000\n",
      "1126/1126 - 25s - loss: 2165981.5000 - accuracy: 0.5779 - val_loss: 2377244.0000 - val_accuracy: 0.6231 - 25s/epoch - 23ms/step\n",
      "Epoch 14/10000\n",
      "1126/1126 - 25s - loss: 1917099.7500 - accuracy: 0.5799 - val_loss: 2180535.0000 - val_accuracy: 0.6189 - 25s/epoch - 22ms/step\n",
      "Epoch 15/10000\n",
      "1126/1126 - 26s - loss: 1996879.3750 - accuracy: 0.5800 - val_loss: 6973116.5000 - val_accuracy: 0.6180 - 26s/epoch - 23ms/step\n",
      "Epoch 16/10000\n",
      "1126/1126 - 26s - loss: 1894071.7500 - accuracy: 0.5822 - val_loss: 921807.8125 - val_accuracy: 0.6270 - 26s/epoch - 23ms/step\n",
      "Epoch 17/10000\n",
      "1126/1126 - 26s - loss: 1852580.3750 - accuracy: 0.5826 - val_loss: 5900749.0000 - val_accuracy: 0.6135 - 26s/epoch - 23ms/step\n",
      "Epoch 18/10000\n",
      "1126/1126 - 27s - loss: 1820925.5000 - accuracy: 0.5827 - val_loss: 1800991.5000 - val_accuracy: 0.6251 - 27s/epoch - 24ms/step\n",
      "Epoch 19/10000\n",
      "1126/1126 - 26s - loss: 2039584.5000 - accuracy: 0.5805 - val_loss: 832913.3125 - val_accuracy: 0.6359 - 26s/epoch - 23ms/step\n",
      "Epoch 20/10000\n",
      "1126/1126 - 25s - loss: 1989167.8750 - accuracy: 0.5829 - val_loss: 3589649.2500 - val_accuracy: 0.6146 - 25s/epoch - 22ms/step\n",
      "Epoch 21/10000\n",
      "1126/1126 - 25s - loss: 2004429.6250 - accuracy: 0.5808 - val_loss: 2564537.2500 - val_accuracy: 0.6235 - 25s/epoch - 22ms/step\n",
      "Epoch 22/10000\n",
      "1126/1126 - 25s - loss: 1805204.1250 - accuracy: 0.5831 - val_loss: 1071786.0000 - val_accuracy: 0.6127 - 25s/epoch - 22ms/step\n",
      "Epoch 23/10000\n",
      "1126/1126 - 26s - loss: 2032343.2500 - accuracy: 0.5808 - val_loss: 994668.1250 - val_accuracy: 0.5112 - 26s/epoch - 23ms/step\n",
      "Epoch 24/10000\n",
      "1126/1126 - 25s - loss: 2151310.5000 - accuracy: 0.5809 - val_loss: 1075645.1250 - val_accuracy: 0.5481 - 25s/epoch - 23ms/step\n",
      "Epoch 25/10000\n",
      "1126/1126 - 25s - loss: 1921276.1250 - accuracy: 0.5798 - val_loss: 2322500.5000 - val_accuracy: 0.6213 - 25s/epoch - 23ms/step\n",
      "Epoch 26/10000\n",
      "1126/1126 - 26s - loss: 2111129.5000 - accuracy: 0.5827 - val_loss: 770065.7500 - val_accuracy: 0.6354 - 26s/epoch - 23ms/step\n",
      "Epoch 27/10000\n",
      "1126/1126 - 26s - loss: 2144264.2500 - accuracy: 0.5818 - val_loss: 1100954.0000 - val_accuracy: 0.5218 - 26s/epoch - 23ms/step\n",
      "Epoch 28/10000\n",
      "1126/1126 - 25s - loss: 1974989.6250 - accuracy: 0.5856 - val_loss: 1146095.1250 - val_accuracy: 0.6386 - 25s/epoch - 23ms/step\n",
      "Epoch 29/10000\n",
      "1126/1126 - 26s - loss: 1952817.6250 - accuracy: 0.5847 - val_loss: 901489.6250 - val_accuracy: 0.6420 - 26s/epoch - 23ms/step\n",
      "Epoch 30/10000\n",
      "1126/1126 - 27s - loss: 1876682.6250 - accuracy: 0.5812 - val_loss: 1128784.2500 - val_accuracy: 0.6314 - 27s/epoch - 24ms/step\n",
      "Epoch 31/10000\n",
      "1126/1126 - 28s - loss: 1956449.6250 - accuracy: 0.5856 - val_loss: 873776.1250 - val_accuracy: 0.6425 - 28s/epoch - 25ms/step\n",
      "391/391 [==============================] - 2s 6ms/step\n",
      "\n",
      "Iterazione 106/128 (03-11-2023_14-32-57)\n",
      "hidden_layer_size: 1000\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 2\n",
      "activation_hidden: relu\n",
      "activation_output: sigmoid\n",
      "batch_size: 100\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 20\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 64.37%\n",
      "La recall di questo modello sul validation set è: 15.76%\n",
      "La f1 di questo modello sul validation set è: 25.5%\n",
      "La balanced accuracy di questo modello sul validation set è: 55.4%\n",
      "La precision di questo modello sul validation set è: 66.75%\n",
      "La AUC di questo modello sul validation set è: 55.4%\n",
      "391/391 [==============================] - 2s 6ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.64    0.95      0.77  7,670.00\n",
      "1                  0.67    0.16      0.26  4,841.00\n",
      "accuracy           0.64    0.64      0.64      0.64\n",
      "macro avg          0.65    0.55      0.51 12,511.00\n",
      "weighted avg       0.65    0.64      0.57 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         7290          380\n",
      "Actual 1         4078          763\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 66.14% con il modello:\n",
      "98 - AC66.14%, L1000, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA20, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_14-02-18).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 63.1% con il modello:\n",
      "98 - BA63.1%, L1000, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA20, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_14-02-18).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 66.75% con il modello:\n",
      "105 - PR66.75%, L1000, mV-10, MV10, P2, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_14-19-26).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "113/113 - 9s - loss: 26026204.0000 - accuracy: 0.5718 - val_loss: 1262691.1250 - val_accuracy: 0.6367 - 9s/epoch - 80ms/step\n",
      "Epoch 2/10000\n",
      "113/113 - 8s - loss: 2459376.0000 - accuracy: 0.5760 - val_loss: 1397907.1250 - val_accuracy: 0.6285 - 8s/epoch - 67ms/step\n",
      "Epoch 3/10000\n",
      "113/113 - 7s - loss: 2353494.7500 - accuracy: 0.5747 - val_loss: 743355.6875 - val_accuracy: 0.6216 - 7s/epoch - 65ms/step\n",
      "Epoch 4/10000\n",
      "113/113 - 7s - loss: 2359426.5000 - accuracy: 0.5740 - val_loss: 1359598.8750 - val_accuracy: 0.5299 - 7s/epoch - 65ms/step\n",
      "Epoch 5/10000\n",
      "113/113 - 7s - loss: 1960101.8750 - accuracy: 0.5775 - val_loss: 4740158.0000 - val_accuracy: 0.4302 - 7s/epoch - 66ms/step\n",
      "Epoch 6/10000\n",
      "113/113 - 7s - loss: 2632451.2500 - accuracy: 0.5651 - val_loss: 1192694.1250 - val_accuracy: 0.6213 - 7s/epoch - 65ms/step\n",
      "Epoch 7/10000\n",
      "113/113 - 8s - loss: 2560594.2500 - accuracy: 0.5731 - val_loss: 1202510.1250 - val_accuracy: 0.5883 - 8s/epoch - 68ms/step\n",
      "Epoch 8/10000\n",
      "113/113 - 7s - loss: 1917518.1250 - accuracy: 0.5796 - val_loss: 908326.0625 - val_accuracy: 0.6310 - 7s/epoch - 65ms/step\n",
      "Epoch 9/10000\n",
      "113/113 - 7s - loss: 1482873.1250 - accuracy: 0.5787 - val_loss: 895458.7500 - val_accuracy: 0.4991 - 7s/epoch - 64ms/step\n",
      "Epoch 10/10000\n",
      "113/113 - 7s - loss: 2311680.5000 - accuracy: 0.5704 - val_loss: 890429.6875 - val_accuracy: 0.6317 - 7s/epoch - 64ms/step\n",
      "Epoch 11/10000\n",
      "113/113 - 7s - loss: 2350205.0000 - accuracy: 0.5684 - val_loss: 1759007.7500 - val_accuracy: 0.6240 - 7s/epoch - 64ms/step\n",
      "391/391 [==============================] - 3s 6ms/step\n",
      "\n",
      "Iterazione 107/128 (03-11-2023_14-34-25)\n",
      "hidden_layer_size: 1000\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 2\n",
      "activation_hidden: relu\n",
      "activation_output: sigmoid\n",
      "batch_size: 1000\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 10\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 63.67%\n",
      "La recall di questo modello sul validation set è: 20.86%\n",
      "La f1 di questo modello sul validation set è: 30.77%\n",
      "La balanced accuracy di questo modello sul validation set è: 55.78%\n",
      "La precision di questo modello sul validation set è: 58.58%\n",
      "La AUC di questo modello sul validation set è: 55.78%\n",
      "391/391 [==============================] - 2s 6ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.64    0.91      0.75  7,670.00\n",
      "1                  0.59    0.21      0.31  4,841.00\n",
      "accuracy           0.64    0.64      0.64      0.64\n",
      "macro avg          0.62    0.56      0.53 12,511.00\n",
      "weighted avg       0.62    0.64      0.58 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         6956          714\n",
      "Actual 1         3831         1010\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 66.14% con il modello:\n",
      "98 - AC66.14%, L1000, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA20, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_14-02-18).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 63.1% con il modello:\n",
      "98 - BA63.1%, L1000, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA20, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_14-02-18).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 66.75% con il modello:\n",
      "105 - PR66.75%, L1000, mV-10, MV10, P2, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_14-19-26).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "113/113 - 8s - loss: 26026204.0000 - accuracy: 0.5718 - val_loss: 1262691.1250 - val_accuracy: 0.6367 - 8s/epoch - 75ms/step\n",
      "Epoch 2/10000\n",
      "113/113 - 7s - loss: 2459376.0000 - accuracy: 0.5760 - val_loss: 1397907.1250 - val_accuracy: 0.6285 - 7s/epoch - 66ms/step\n",
      "Epoch 3/10000\n",
      "113/113 - 7s - loss: 2353494.7500 - accuracy: 0.5747 - val_loss: 743355.6875 - val_accuracy: 0.6216 - 7s/epoch - 64ms/step\n",
      "Epoch 4/10000\n",
      "113/113 - 7s - loss: 2359426.5000 - accuracy: 0.5740 - val_loss: 1359598.8750 - val_accuracy: 0.5299 - 7s/epoch - 64ms/step\n",
      "Epoch 5/10000\n",
      "113/113 - 7s - loss: 1960101.8750 - accuracy: 0.5775 - val_loss: 4740158.0000 - val_accuracy: 0.4302 - 7s/epoch - 66ms/step\n",
      "Epoch 6/10000\n",
      "113/113 - 7s - loss: 2632451.2500 - accuracy: 0.5651 - val_loss: 1192694.1250 - val_accuracy: 0.6213 - 7s/epoch - 63ms/step\n",
      "Epoch 7/10000\n",
      "113/113 - 7s - loss: 2560594.2500 - accuracy: 0.5731 - val_loss: 1202510.1250 - val_accuracy: 0.5883 - 7s/epoch - 62ms/step\n",
      "Epoch 8/10000\n",
      "113/113 - 7s - loss: 1917518.1250 - accuracy: 0.5796 - val_loss: 908326.0625 - val_accuracy: 0.6310 - 7s/epoch - 61ms/step\n",
      "Epoch 9/10000\n",
      "113/113 - 7s - loss: 1482873.1250 - accuracy: 0.5787 - val_loss: 895458.7500 - val_accuracy: 0.4991 - 7s/epoch - 63ms/step\n",
      "Epoch 10/10000\n",
      "113/113 - 7s - loss: 2311680.5000 - accuracy: 0.5704 - val_loss: 890429.6875 - val_accuracy: 0.6317 - 7s/epoch - 61ms/step\n",
      "Epoch 11/10000\n",
      "113/113 - 7s - loss: 2350205.0000 - accuracy: 0.5684 - val_loss: 1759007.7500 - val_accuracy: 0.6240 - 7s/epoch - 62ms/step\n",
      "Epoch 12/10000\n",
      "113/113 - 7s - loss: 2278622.5000 - accuracy: 0.5795 - val_loss: 846813.9375 - val_accuracy: 0.6230 - 7s/epoch - 63ms/step\n",
      "Epoch 13/10000\n",
      "113/113 - 7s - loss: 1986831.6250 - accuracy: 0.5696 - val_loss: 1415735.7500 - val_accuracy: 0.6215 - 7s/epoch - 61ms/step\n",
      "Epoch 14/10000\n",
      "113/113 - 7s - loss: 2332538.7500 - accuracy: 0.5701 - val_loss: 4562992.0000 - val_accuracy: 0.4107 - 7s/epoch - 60ms/step\n",
      "Epoch 15/10000\n",
      "113/113 - 7s - loss: 2370794.5000 - accuracy: 0.5721 - val_loss: 1872196.5000 - val_accuracy: 0.4717 - 7s/epoch - 60ms/step\n",
      "Epoch 16/10000\n",
      "113/113 - 7s - loss: 2021049.5000 - accuracy: 0.5731 - val_loss: 1267865.7500 - val_accuracy: 0.5655 - 7s/epoch - 59ms/step\n",
      "Epoch 17/10000\n",
      "113/113 - 7s - loss: 2313690.2500 - accuracy: 0.5714 - val_loss: 1424544.6250 - val_accuracy: 0.5357 - 7s/epoch - 60ms/step\n",
      "Epoch 18/10000\n",
      "113/113 - 7s - loss: 1446045.6250 - accuracy: 0.5759 - val_loss: 2095942.6250 - val_accuracy: 0.4671 - 7s/epoch - 61ms/step\n",
      "Epoch 19/10000\n",
      "113/113 - 7s - loss: 1540410.5000 - accuracy: 0.5738 - val_loss: 977855.4375 - val_accuracy: 0.6366 - 7s/epoch - 61ms/step\n",
      "Epoch 20/10000\n",
      "113/113 - 7s - loss: 1538235.3750 - accuracy: 0.5762 - val_loss: 6140081.0000 - val_accuracy: 0.3943 - 7s/epoch - 62ms/step\n",
      "Epoch 21/10000\n",
      "113/113 - 7s - loss: 2748384.0000 - accuracy: 0.5693 - val_loss: 1633617.2500 - val_accuracy: 0.4972 - 7s/epoch - 64ms/step\n",
      "391/391 [==============================] - 3s 7ms/step\n",
      "\n",
      "Iterazione 108/128 (03-11-2023_14-37-01)\n",
      "hidden_layer_size: 1000\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 2\n",
      "activation_hidden: relu\n",
      "activation_output: sigmoid\n",
      "batch_size: 1000\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 20\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 63.67%\n",
      "La recall di questo modello sul validation set è: 20.86%\n",
      "La f1 di questo modello sul validation set è: 30.77%\n",
      "La balanced accuracy di questo modello sul validation set è: 55.78%\n",
      "La precision di questo modello sul validation set è: 58.58%\n",
      "La AUC di questo modello sul validation set è: 55.78%\n",
      "391/391 [==============================] - 3s 7ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.64    0.91      0.75  7,670.00\n",
      "1                  0.59    0.21      0.31  4,841.00\n",
      "accuracy           0.64    0.64      0.64      0.64\n",
      "macro avg          0.62    0.56      0.53 12,511.00\n",
      "weighted avg       0.62    0.64      0.58 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         6956          714\n",
      "Actual 1         3831         1010\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 66.14% con il modello:\n",
      "98 - AC66.14%, L1000, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA20, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_14-02-18).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 63.1% con il modello:\n",
      "98 - BA63.1%, L1000, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA20, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_14-02-18).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 66.75% con il modello:\n",
      "105 - PR66.75%, L1000, mV-10, MV10, P2, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_14-19-26).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "1126/1126 - 32s - loss: 2.9406 - accuracy: 0.5638 - val_loss: 2.2715 - val_accuracy: 0.5977 - 32s/epoch - 28ms/step\n",
      "Epoch 2/10000\n",
      "1126/1126 - 30s - loss: 2.2064 - accuracy: 0.5730 - val_loss: 2.1137 - val_accuracy: 0.5697 - 30s/epoch - 27ms/step\n",
      "Epoch 3/10000\n",
      "1126/1126 - 29s - loss: 2.0365 - accuracy: 0.5755 - val_loss: 2.0286 - val_accuracy: 0.5787 - 29s/epoch - 26ms/step\n",
      "Epoch 4/10000\n",
      "1126/1126 - 28s - loss: 1.9938 - accuracy: 0.5745 - val_loss: 2.0566 - val_accuracy: 0.5779 - 28s/epoch - 25ms/step\n",
      "Epoch 5/10000\n",
      "1126/1126 - 28s - loss: 1.9491 - accuracy: 0.5737 - val_loss: 1.9865 - val_accuracy: 0.5844 - 28s/epoch - 25ms/step\n",
      "Epoch 6/10000\n",
      "1126/1126 - 27s - loss: 1.8432 - accuracy: 0.5766 - val_loss: 1.9126 - val_accuracy: 0.5752 - 27s/epoch - 24ms/step\n",
      "Epoch 7/10000\n",
      "1126/1126 - 27s - loss: 1.8362 - accuracy: 0.5771 - val_loss: 1.8818 - val_accuracy: 0.5447 - 27s/epoch - 24ms/step\n",
      "Epoch 8/10000\n",
      "1126/1126 - 27s - loss: 1.7455 - accuracy: 0.5773 - val_loss: 1.7899 - val_accuracy: 0.5796 - 27s/epoch - 24ms/step\n",
      "Epoch 9/10000\n",
      "1126/1126 - 27s - loss: 1.7186 - accuracy: 0.5795 - val_loss: 1.8116 - val_accuracy: 0.5867 - 27s/epoch - 24ms/step\n",
      "Epoch 10/10000\n",
      "1126/1126 - 28s - loss: 1.6551 - accuracy: 0.5847 - val_loss: 1.6522 - val_accuracy: 0.5690 - 28s/epoch - 25ms/step\n",
      "Epoch 11/10000\n",
      "1126/1126 - 27s - loss: 1.6795 - accuracy: 0.5817 - val_loss: 1.7390 - val_accuracy: 0.5828 - 27s/epoch - 24ms/step\n",
      "391/391 [==============================] - 2s 6ms/step\n",
      "\n",
      "Iterazione 109/128 (03-11-2023_14-42-20)\n",
      "hidden_layer_size: 1000\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 2\n",
      "activation_hidden: sigmoid\n",
      "activation_output: sigmoid\n",
      "batch_size: 100\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 10\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 59.77%\n",
      "La recall di questo modello sul validation set è: 34.91%\n",
      "La f1 di questo modello sul validation set è: 40.18%\n",
      "La balanced accuracy di questo modello sul validation set è: 55.19%\n",
      "La precision di questo modello sul validation set è: 47.31%\n",
      "La AUC di questo modello sul validation set è: 59.63%\n",
      "391/391 [==============================] - 3s 6ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.65    0.75      0.70  7,670.00\n",
      "1                  0.47    0.35      0.40  4,841.00\n",
      "accuracy           0.60    0.60      0.60      0.60\n",
      "macro avg          0.56    0.55      0.55 12,511.00\n",
      "weighted avg       0.58    0.60      0.58 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         5788         1882\n",
      "Actual 1         3151         1690\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 66.14% con il modello:\n",
      "98 - AC66.14%, L1000, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA20, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_14-02-18).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 63.1% con il modello:\n",
      "98 - BA63.1%, L1000, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA20, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_14-02-18).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 66.75% con il modello:\n",
      "105 - PR66.75%, L1000, mV-10, MV10, P2, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_14-19-26).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "1126/1126 - 30s - loss: 2.9406 - accuracy: 0.5638 - val_loss: 2.2715 - val_accuracy: 0.5977 - 30s/epoch - 27ms/step\n",
      "Epoch 2/10000\n",
      "1126/1126 - 30s - loss: 2.2064 - accuracy: 0.5730 - val_loss: 2.1137 - val_accuracy: 0.5697 - 30s/epoch - 27ms/step\n",
      "Epoch 3/10000\n",
      "1126/1126 - 30s - loss: 2.0365 - accuracy: 0.5755 - val_loss: 2.0286 - val_accuracy: 0.5787 - 30s/epoch - 27ms/step\n",
      "Epoch 4/10000\n",
      "1126/1126 - 28s - loss: 1.9938 - accuracy: 0.5745 - val_loss: 2.0566 - val_accuracy: 0.5779 - 28s/epoch - 25ms/step\n",
      "Epoch 5/10000\n",
      "1126/1126 - 28s - loss: 1.9491 - accuracy: 0.5737 - val_loss: 1.9865 - val_accuracy: 0.5844 - 28s/epoch - 25ms/step\n",
      "Epoch 6/10000\n",
      "1126/1126 - 28s - loss: 1.8432 - accuracy: 0.5766 - val_loss: 1.9126 - val_accuracy: 0.5752 - 28s/epoch - 25ms/step\n",
      "Epoch 7/10000\n",
      "1126/1126 - 27s - loss: 1.8362 - accuracy: 0.5771 - val_loss: 1.8818 - val_accuracy: 0.5447 - 27s/epoch - 24ms/step\n",
      "Epoch 8/10000\n",
      "1126/1126 - 27s - loss: 1.7455 - accuracy: 0.5773 - val_loss: 1.7899 - val_accuracy: 0.5796 - 27s/epoch - 24ms/step\n",
      "Epoch 9/10000\n",
      "1126/1126 - 26s - loss: 1.7186 - accuracy: 0.5795 - val_loss: 1.8116 - val_accuracy: 0.5867 - 26s/epoch - 23ms/step\n",
      "Epoch 10/10000\n",
      "1126/1126 - 27s - loss: 1.6551 - accuracy: 0.5847 - val_loss: 1.6522 - val_accuracy: 0.5690 - 27s/epoch - 24ms/step\n",
      "Epoch 11/10000\n",
      "1126/1126 - 27s - loss: 1.6795 - accuracy: 0.5817 - val_loss: 1.7390 - val_accuracy: 0.5828 - 27s/epoch - 24ms/step\n",
      "Epoch 12/10000\n",
      "1126/1126 - 28s - loss: 1.6451 - accuracy: 0.5827 - val_loss: 1.6458 - val_accuracy: 0.5864 - 28s/epoch - 25ms/step\n",
      "Epoch 13/10000\n",
      "1126/1126 - 28s - loss: 1.5921 - accuracy: 0.5822 - val_loss: 1.6395 - val_accuracy: 0.5844 - 28s/epoch - 25ms/step\n",
      "Epoch 14/10000\n",
      "1126/1126 - 29s - loss: 1.5743 - accuracy: 0.5835 - val_loss: 1.7498 - val_accuracy: 0.5880 - 29s/epoch - 25ms/step\n",
      "Epoch 15/10000\n",
      "1126/1126 - 29s - loss: 1.5583 - accuracy: 0.5829 - val_loss: 1.5729 - val_accuracy: 0.5660 - 29s/epoch - 26ms/step\n",
      "Epoch 16/10000\n",
      "1126/1126 - 28s - loss: 1.5326 - accuracy: 0.5806 - val_loss: 1.6272 - val_accuracy: 0.5580 - 28s/epoch - 25ms/step\n",
      "Epoch 17/10000\n",
      "1126/1126 - 38s - loss: 1.5368 - accuracy: 0.5821 - val_loss: 1.5727 - val_accuracy: 0.5840 - 38s/epoch - 34ms/step\n",
      "Epoch 18/10000\n",
      "1126/1126 - 30s - loss: 1.5146 - accuracy: 0.5807 - val_loss: 1.5537 - val_accuracy: 0.5797 - 30s/epoch - 27ms/step\n",
      "Epoch 19/10000\n",
      "1126/1126 - 30s - loss: 1.4884 - accuracy: 0.5823 - val_loss: 1.4941 - val_accuracy: 0.5738 - 30s/epoch - 27ms/step\n",
      "Epoch 20/10000\n",
      "1126/1126 - 31s - loss: 1.4961 - accuracy: 0.5814 - val_loss: 1.5004 - val_accuracy: 0.5820 - 31s/epoch - 28ms/step\n",
      "Epoch 21/10000\n",
      "1126/1126 - 31s - loss: 1.4575 - accuracy: 0.5835 - val_loss: 1.4672 - val_accuracy: 0.5748 - 31s/epoch - 27ms/step\n",
      "391/391 [==============================] - 3s 7ms/step\n",
      "\n",
      "Iterazione 110/128 (03-11-2023_14-52-36)\n",
      "hidden_layer_size: 1000\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 2\n",
      "activation_hidden: sigmoid\n",
      "activation_output: sigmoid\n",
      "batch_size: 100\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 20\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 59.77%\n",
      "La recall di questo modello sul validation set è: 34.91%\n",
      "La f1 di questo modello sul validation set è: 40.18%\n",
      "La balanced accuracy di questo modello sul validation set è: 55.19%\n",
      "La precision di questo modello sul validation set è: 47.31%\n",
      "La AUC di questo modello sul validation set è: 59.63%\n",
      "391/391 [==============================] - 3s 8ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.65    0.75      0.70  7,670.00\n",
      "1                  0.47    0.35      0.40  4,841.00\n",
      "accuracy           0.60    0.60      0.60      0.60\n",
      "macro avg          0.56    0.55      0.55 12,511.00\n",
      "weighted avg       0.58    0.60      0.58 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         5788         1882\n",
      "Actual 1         3151         1690\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 66.14% con il modello:\n",
      "98 - AC66.14%, L1000, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA20, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_14-02-18).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 63.1% con il modello:\n",
      "98 - BA63.1%, L1000, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA20, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_14-02-18).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 66.75% con il modello:\n",
      "105 - PR66.75%, L1000, mV-10, MV10, P2, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_14-19-26).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "113/113 - 16s - loss: 5.1602 - accuracy: 0.5540 - val_loss: 2.8526 - val_accuracy: 0.5827 - 16s/epoch - 137ms/step\n",
      "Epoch 2/10000\n",
      "113/113 - 10s - loss: 2.5427 - accuracy: 0.5769 - val_loss: 2.3805 - val_accuracy: 0.5843 - 10s/epoch - 91ms/step\n",
      "Epoch 3/10000\n",
      "113/113 - 11s - loss: 2.2366 - accuracy: 0.5848 - val_loss: 2.2392 - val_accuracy: 0.5872 - 11s/epoch - 95ms/step\n",
      "Epoch 4/10000\n",
      "113/113 - 9s - loss: 2.0713 - accuracy: 0.5879 - val_loss: 2.0219 - val_accuracy: 0.5865 - 9s/epoch - 84ms/step\n",
      "Epoch 5/10000\n",
      "113/113 - 11s - loss: 1.9643 - accuracy: 0.5868 - val_loss: 1.9769 - val_accuracy: 0.6041 - 11s/epoch - 96ms/step\n",
      "Epoch 6/10000\n",
      "113/113 - 10s - loss: 1.8889 - accuracy: 0.5905 - val_loss: 1.9452 - val_accuracy: 0.5968 - 10s/epoch - 91ms/step\n",
      "Epoch 7/10000\n",
      "113/113 - 11s - loss: 1.8182 - accuracy: 0.5914 - val_loss: 1.9126 - val_accuracy: 0.5498 - 11s/epoch - 96ms/step\n",
      "Epoch 8/10000\n",
      "113/113 - 11s - loss: 1.7624 - accuracy: 0.5925 - val_loss: 1.8452 - val_accuracy: 0.5866 - 11s/epoch - 96ms/step\n",
      "Epoch 9/10000\n",
      "113/113 - 12s - loss: 1.7254 - accuracy: 0.5926 - val_loss: 1.8546 - val_accuracy: 0.5956 - 12s/epoch - 106ms/step\n",
      "Epoch 10/10000\n",
      "113/113 - 10s - loss: 1.6734 - accuracy: 0.5946 - val_loss: 1.8214 - val_accuracy: 0.6022 - 10s/epoch - 93ms/step\n",
      "Epoch 11/10000\n",
      "113/113 - 9s - loss: 1.6460 - accuracy: 0.5943 - val_loss: 1.7141 - val_accuracy: 0.5947 - 9s/epoch - 83ms/step\n",
      "Epoch 12/10000\n",
      "113/113 - 9s - loss: 1.6140 - accuracy: 0.5946 - val_loss: 1.7556 - val_accuracy: 0.5858 - 9s/epoch - 83ms/step\n",
      "Epoch 13/10000\n",
      "113/113 - 10s - loss: 1.6086 - accuracy: 0.5954 - val_loss: 1.7662 - val_accuracy: 0.6094 - 10s/epoch - 92ms/step\n",
      "Epoch 14/10000\n",
      "113/113 - 11s - loss: 1.5640 - accuracy: 0.5985 - val_loss: 1.7052 - val_accuracy: 0.6074 - 11s/epoch - 96ms/step\n",
      "Epoch 15/10000\n",
      "113/113 - 11s - loss: 1.5725 - accuracy: 0.5961 - val_loss: 1.6687 - val_accuracy: 0.5941 - 11s/epoch - 96ms/step\n",
      "Epoch 16/10000\n",
      "113/113 - 8s - loss: 1.5570 - accuracy: 0.5952 - val_loss: 1.5994 - val_accuracy: 0.6004 - 8s/epoch - 73ms/step\n",
      "Epoch 17/10000\n",
      "113/113 - 9s - loss: 1.5207 - accuracy: 0.5951 - val_loss: 1.6149 - val_accuracy: 0.5807 - 9s/epoch - 80ms/step\n",
      "Epoch 18/10000\n",
      "113/113 - 9s - loss: 1.4874 - accuracy: 0.5958 - val_loss: 1.5390 - val_accuracy: 0.6149 - 9s/epoch - 81ms/step\n",
      "Epoch 19/10000\n",
      "113/113 - 9s - loss: 1.4605 - accuracy: 0.5991 - val_loss: 1.5379 - val_accuracy: 0.6045 - 9s/epoch - 77ms/step\n",
      "Epoch 20/10000\n",
      "113/113 - 9s - loss: 1.4515 - accuracy: 0.5965 - val_loss: 1.5331 - val_accuracy: 0.5758 - 9s/epoch - 76ms/step\n",
      "Epoch 21/10000\n",
      "113/113 - 9s - loss: 1.4182 - accuracy: 0.5996 - val_loss: 1.5679 - val_accuracy: 0.5932 - 9s/epoch - 83ms/step\n",
      "Epoch 22/10000\n",
      "113/113 - 9s - loss: 1.4212 - accuracy: 0.5980 - val_loss: 1.5737 - val_accuracy: 0.6206 - 9s/epoch - 78ms/step\n",
      "Epoch 23/10000\n",
      "113/113 - 10s - loss: 1.4084 - accuracy: 0.5994 - val_loss: 1.5802 - val_accuracy: 0.5752 - 10s/epoch - 86ms/step\n",
      "Epoch 24/10000\n",
      "113/113 - 12s - loss: 1.4396 - accuracy: 0.5987 - val_loss: 1.5643 - val_accuracy: 0.5916 - 12s/epoch - 103ms/step\n",
      "Epoch 25/10000\n",
      "113/113 - 10s - loss: 1.4321 - accuracy: 0.5987 - val_loss: 1.5485 - val_accuracy: 0.5962 - 10s/epoch - 85ms/step\n",
      "Epoch 26/10000\n",
      "113/113 - 9s - loss: 1.4091 - accuracy: 0.5998 - val_loss: 1.5960 - val_accuracy: 0.6035 - 9s/epoch - 82ms/step\n",
      "Epoch 27/10000\n",
      "113/113 - 10s - loss: 1.3914 - accuracy: 0.6010 - val_loss: 1.4909 - val_accuracy: 0.6022 - 10s/epoch - 90ms/step\n",
      "Epoch 28/10000\n",
      "113/113 - 11s - loss: 1.3822 - accuracy: 0.6004 - val_loss: 1.5108 - val_accuracy: 0.5839 - 11s/epoch - 96ms/step\n",
      "Epoch 29/10000\n",
      "113/113 - 10s - loss: 1.3864 - accuracy: 0.6005 - val_loss: 1.5189 - val_accuracy: 0.5743 - 10s/epoch - 85ms/step\n",
      "Epoch 30/10000\n",
      "113/113 - 9s - loss: 1.3640 - accuracy: 0.6020 - val_loss: 1.4855 - val_accuracy: 0.5994 - 9s/epoch - 81ms/step\n",
      "Epoch 31/10000\n",
      "113/113 - 10s - loss: 1.3557 - accuracy: 0.6010 - val_loss: 1.4608 - val_accuracy: 0.5984 - 10s/epoch - 86ms/step\n",
      "Epoch 32/10000\n",
      "113/113 - 10s - loss: 1.3522 - accuracy: 0.6012 - val_loss: 1.5248 - val_accuracy: 0.5850 - 10s/epoch - 86ms/step\n",
      "391/391 [==============================] - 5s 11ms/step\n",
      "\n",
      "Iterazione 111/128 (03-11-2023_14-58-11)\n",
      "hidden_layer_size: 1000\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 2\n",
      "activation_hidden: sigmoid\n",
      "activation_output: sigmoid\n",
      "batch_size: 1000\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 10\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 62.06%\n",
      "La recall di questo modello sul validation set è: 35.98%\n",
      "La f1 di questo modello sul validation set è: 42.33%\n",
      "La balanced accuracy di questo modello sul validation set è: 57.25%\n",
      "La precision di questo modello sul validation set è: 51.39%\n",
      "La AUC di questo modello sul validation set è: 61.14%\n",
      "391/391 [==============================] - 4s 9ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.66    0.79      0.72  7,670.00\n",
      "1                  0.51    0.36      0.42  4,841.00\n",
      "accuracy           0.62    0.62      0.62      0.62\n",
      "macro avg          0.59    0.57      0.57 12,511.00\n",
      "weighted avg       0.60    0.62      0.60 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         6022         1648\n",
      "Actual 1         3099         1742\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 66.14% con il modello:\n",
      "98 - AC66.14%, L1000, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA20, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_14-02-18).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 63.1% con il modello:\n",
      "98 - BA63.1%, L1000, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA20, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_14-02-18).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 66.75% con il modello:\n",
      "105 - PR66.75%, L1000, mV-10, MV10, P2, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_14-19-26).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "113/113 - 13s - loss: 5.1602 - accuracy: 0.5540 - val_loss: 2.8526 - val_accuracy: 0.5827 - 13s/epoch - 116ms/step\n",
      "Epoch 2/10000\n",
      "113/113 - 11s - loss: 2.5427 - accuracy: 0.5769 - val_loss: 2.3805 - val_accuracy: 0.5843 - 11s/epoch - 97ms/step\n",
      "Epoch 3/10000\n",
      "113/113 - 11s - loss: 2.2366 - accuracy: 0.5848 - val_loss: 2.2392 - val_accuracy: 0.5872 - 11s/epoch - 100ms/step\n",
      "Epoch 4/10000\n",
      "113/113 - 10s - loss: 2.0713 - accuracy: 0.5879 - val_loss: 2.0219 - val_accuracy: 0.5865 - 10s/epoch - 86ms/step\n",
      "Epoch 5/10000\n",
      "113/113 - 10s - loss: 1.9643 - accuracy: 0.5868 - val_loss: 1.9769 - val_accuracy: 0.6041 - 10s/epoch - 86ms/step\n",
      "Epoch 6/10000\n",
      "113/113 - 10s - loss: 1.8889 - accuracy: 0.5905 - val_loss: 1.9452 - val_accuracy: 0.5968 - 10s/epoch - 86ms/step\n",
      "Epoch 7/10000\n",
      "113/113 - 10s - loss: 1.8182 - accuracy: 0.5914 - val_loss: 1.9126 - val_accuracy: 0.5498 - 10s/epoch - 86ms/step\n",
      "Epoch 8/10000\n",
      "113/113 - 10s - loss: 1.7624 - accuracy: 0.5925 - val_loss: 1.8452 - val_accuracy: 0.5866 - 10s/epoch - 89ms/step\n",
      "Epoch 9/10000\n",
      "113/113 - 10s - loss: 1.7254 - accuracy: 0.5926 - val_loss: 1.8546 - val_accuracy: 0.5956 - 10s/epoch - 87ms/step\n",
      "Epoch 10/10000\n",
      "113/113 - 10s - loss: 1.6734 - accuracy: 0.5946 - val_loss: 1.8214 - val_accuracy: 0.6022 - 10s/epoch - 89ms/step\n",
      "Epoch 11/10000\n",
      "113/113 - 10s - loss: 1.6460 - accuracy: 0.5943 - val_loss: 1.7141 - val_accuracy: 0.5947 - 10s/epoch - 92ms/step\n",
      "Epoch 12/10000\n",
      "113/113 - 10s - loss: 1.6140 - accuracy: 0.5946 - val_loss: 1.7556 - val_accuracy: 0.5858 - 10s/epoch - 89ms/step\n",
      "Epoch 13/10000\n",
      "113/113 - 11s - loss: 1.6086 - accuracy: 0.5954 - val_loss: 1.7662 - val_accuracy: 0.6094 - 11s/epoch - 96ms/step\n",
      "Epoch 14/10000\n",
      "113/113 - 10s - loss: 1.5640 - accuracy: 0.5985 - val_loss: 1.7052 - val_accuracy: 0.6074 - 10s/epoch - 87ms/step\n",
      "Epoch 15/10000\n",
      "113/113 - 10s - loss: 1.5725 - accuracy: 0.5961 - val_loss: 1.6687 - val_accuracy: 0.5941 - 10s/epoch - 89ms/step\n",
      "Epoch 16/10000\n",
      "113/113 - 9s - loss: 1.5570 - accuracy: 0.5952 - val_loss: 1.5994 - val_accuracy: 0.6004 - 9s/epoch - 82ms/step\n",
      "Epoch 17/10000\n",
      "113/113 - 9s - loss: 1.5207 - accuracy: 0.5951 - val_loss: 1.6149 - val_accuracy: 0.5807 - 9s/epoch - 80ms/step\n",
      "Epoch 18/10000\n",
      "113/113 - 9s - loss: 1.4874 - accuracy: 0.5958 - val_loss: 1.5390 - val_accuracy: 0.6149 - 9s/epoch - 80ms/step\n",
      "Epoch 19/10000\n",
      "113/113 - 9s - loss: 1.4605 - accuracy: 0.5991 - val_loss: 1.5379 - val_accuracy: 0.6045 - 9s/epoch - 81ms/step\n",
      "Epoch 20/10000\n",
      "113/113 - 10s - loss: 1.4515 - accuracy: 0.5965 - val_loss: 1.5331 - val_accuracy: 0.5758 - 10s/epoch - 86ms/step\n",
      "Epoch 21/10000\n",
      "113/113 - 9s - loss: 1.4182 - accuracy: 0.5996 - val_loss: 1.5679 - val_accuracy: 0.5932 - 9s/epoch - 83ms/step\n",
      "Epoch 22/10000\n",
      "113/113 - 10s - loss: 1.4212 - accuracy: 0.5980 - val_loss: 1.5737 - val_accuracy: 0.6206 - 10s/epoch - 86ms/step\n",
      "Epoch 23/10000\n",
      "113/113 - 9s - loss: 1.4084 - accuracy: 0.5994 - val_loss: 1.5802 - val_accuracy: 0.5752 - 9s/epoch - 80ms/step\n",
      "Epoch 24/10000\n",
      "113/113 - 9s - loss: 1.4396 - accuracy: 0.5987 - val_loss: 1.5643 - val_accuracy: 0.5916 - 9s/epoch - 78ms/step\n",
      "Epoch 25/10000\n",
      "113/113 - 9s - loss: 1.4321 - accuracy: 0.5987 - val_loss: 1.5485 - val_accuracy: 0.5962 - 9s/epoch - 83ms/step\n",
      "Epoch 26/10000\n",
      "113/113 - 10s - loss: 1.4091 - accuracy: 0.5998 - val_loss: 1.5960 - val_accuracy: 0.6035 - 10s/epoch - 92ms/step\n",
      "Epoch 27/10000\n",
      "113/113 - 10s - loss: 1.3914 - accuracy: 0.6010 - val_loss: 1.4909 - val_accuracy: 0.6022 - 10s/epoch - 88ms/step\n",
      "Epoch 28/10000\n",
      "113/113 - 9s - loss: 1.3822 - accuracy: 0.6004 - val_loss: 1.5108 - val_accuracy: 0.5839 - 9s/epoch - 81ms/step\n",
      "Epoch 29/10000\n",
      "113/113 - 10s - loss: 1.3864 - accuracy: 0.6005 - val_loss: 1.5189 - val_accuracy: 0.5743 - 10s/epoch - 85ms/step\n",
      "Epoch 30/10000\n",
      "113/113 - 10s - loss: 1.3640 - accuracy: 0.6020 - val_loss: 1.4855 - val_accuracy: 0.5994 - 10s/epoch - 89ms/step\n",
      "Epoch 31/10000\n",
      "113/113 - 10s - loss: 1.3557 - accuracy: 0.6010 - val_loss: 1.4608 - val_accuracy: 0.5984 - 10s/epoch - 85ms/step\n",
      "Epoch 32/10000\n",
      "113/113 - 10s - loss: 1.3522 - accuracy: 0.6012 - val_loss: 1.5248 - val_accuracy: 0.5850 - 10s/epoch - 86ms/step\n",
      "Epoch 33/10000\n",
      "113/113 - 10s - loss: 1.3591 - accuracy: 0.5978 - val_loss: 1.4771 - val_accuracy: 0.6060 - 10s/epoch - 86ms/step\n",
      "Epoch 34/10000\n",
      "113/113 - 9s - loss: 1.3393 - accuracy: 0.6003 - val_loss: 1.4571 - val_accuracy: 0.5988 - 9s/epoch - 83ms/step\n",
      "Epoch 35/10000\n",
      "113/113 - 9s - loss: 1.3326 - accuracy: 0.6013 - val_loss: 1.3863 - val_accuracy: 0.5991 - 9s/epoch - 80ms/step\n",
      "Epoch 36/10000\n",
      "113/113 - 9s - loss: 1.3145 - accuracy: 0.6012 - val_loss: 1.4018 - val_accuracy: 0.6010 - 9s/epoch - 82ms/step\n",
      "Epoch 37/10000\n",
      "113/113 - 9s - loss: 1.3029 - accuracy: 0.5981 - val_loss: 1.4063 - val_accuracy: 0.6093 - 9s/epoch - 81ms/step\n",
      "Epoch 38/10000\n",
      "113/113 - 9s - loss: 1.3046 - accuracy: 0.5983 - val_loss: 1.4354 - val_accuracy: 0.5995 - 9s/epoch - 83ms/step\n",
      "Epoch 39/10000\n",
      "113/113 - 9s - loss: 1.2949 - accuracy: 0.5992 - val_loss: 1.3627 - val_accuracy: 0.6018 - 9s/epoch - 83ms/step\n",
      "Epoch 40/10000\n",
      "113/113 - 9s - loss: 1.2668 - accuracy: 0.6017 - val_loss: 1.3582 - val_accuracy: 0.6176 - 9s/epoch - 84ms/step\n",
      "Epoch 41/10000\n",
      "113/113 - 10s - loss: 1.2773 - accuracy: 0.6033 - val_loss: 1.3837 - val_accuracy: 0.5976 - 10s/epoch - 85ms/step\n",
      "Epoch 42/10000\n",
      "113/113 - 10s - loss: 1.2811 - accuracy: 0.6018 - val_loss: 1.3778 - val_accuracy: 0.6033 - 10s/epoch - 88ms/step\n",
      "391/391 [==============================] - 3s 8ms/step\n",
      "\n",
      "Iterazione 112/128 (03-11-2023_15-05-11)\n",
      "hidden_layer_size: 1000\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 2\n",
      "activation_hidden: sigmoid\n",
      "activation_output: sigmoid\n",
      "batch_size: 1000\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 20\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 62.06%\n",
      "La recall di questo modello sul validation set è: 35.98%\n",
      "La f1 di questo modello sul validation set è: 42.33%\n",
      "La balanced accuracy di questo modello sul validation set è: 57.25%\n",
      "La precision di questo modello sul validation set è: 51.39%\n",
      "La AUC di questo modello sul validation set è: 61.14%\n",
      "391/391 [==============================] - 3s 8ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.66    0.79      0.72  7,670.00\n",
      "1                  0.51    0.36      0.42  4,841.00\n",
      "accuracy           0.62    0.62      0.62      0.62\n",
      "macro avg          0.59    0.57      0.57 12,511.00\n",
      "weighted avg       0.60    0.62      0.60 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         6022         1648\n",
      "Actual 1         3099         1742\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 66.14% con il modello:\n",
      "98 - AC66.14%, L1000, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA20, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_14-02-18).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 63.1% con il modello:\n",
      "98 - BA63.1%, L1000, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA20, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_14-02-18).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 66.75% con il modello:\n",
      "105 - PR66.75%, L1000, mV-10, MV10, P2, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_14-19-26).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "1126/1126 - 74s - loss: 682914240.0000 - accuracy: 0.5641 - val_loss: 1643667200.0000 - val_accuracy: 0.6135 - 74s/epoch - 66ms/step\n",
      "Epoch 2/10000\n",
      "1126/1126 - 71s - loss: 440340896.0000 - accuracy: 0.5722 - val_loss: 916649856.0000 - val_accuracy: 0.6157 - 71s/epoch - 63ms/step\n",
      "Epoch 3/10000\n",
      "1126/1126 - 73s - loss: 405314944.0000 - accuracy: 0.5715 - val_loss: 413828224.0000 - val_accuracy: 0.6192 - 73s/epoch - 65ms/step\n",
      "Epoch 4/10000\n",
      "1126/1126 - 78s - loss: 400499520.0000 - accuracy: 0.5703 - val_loss: 481669152.0000 - val_accuracy: 0.6132 - 78s/epoch - 69ms/step\n",
      "Epoch 5/10000\n",
      "1126/1126 - 90s - loss: 376037344.0000 - accuracy: 0.5711 - val_loss: 128531752.0000 - val_accuracy: 0.6358 - 90s/epoch - 80ms/step\n",
      "Epoch 6/10000\n",
      "1126/1126 - 81s - loss: 402098272.0000 - accuracy: 0.5752 - val_loss: 348511104.0000 - val_accuracy: 0.6174 - 81s/epoch - 72ms/step\n",
      "Epoch 7/10000\n",
      "1126/1126 - 74s - loss: 375967712.0000 - accuracy: 0.5763 - val_loss: 98853424.0000 - val_accuracy: 0.6403 - 74s/epoch - 66ms/step\n",
      "Epoch 8/10000\n",
      "1126/1126 - 78s - loss: 414903712.0000 - accuracy: 0.5726 - val_loss: 346430912.0000 - val_accuracy: 0.5710 - 78s/epoch - 69ms/step\n",
      "Epoch 9/10000\n",
      "1126/1126 - 68s - loss: 365650944.0000 - accuracy: 0.5746 - val_loss: 553427328.0000 - val_accuracy: 0.6134 - 68s/epoch - 61ms/step\n",
      "Epoch 10/10000\n",
      "1126/1126 - 52s - loss: 419103232.0000 - accuracy: 0.5734 - val_loss: 734419520.0000 - val_accuracy: 0.4293 - 52s/epoch - 46ms/step\n",
      "Epoch 11/10000\n",
      "1126/1126 - 52s - loss: 395526400.0000 - accuracy: 0.5727 - val_loss: 356978848.0000 - val_accuracy: 0.6175 - 52s/epoch - 46ms/step\n",
      "Epoch 12/10000\n",
      "1126/1126 - 53s - loss: 366587552.0000 - accuracy: 0.5722 - val_loss: 380730848.0000 - val_accuracy: 0.5520 - 53s/epoch - 47ms/step\n",
      "Epoch 13/10000\n",
      "1126/1126 - 51s - loss: 399671808.0000 - accuracy: 0.5773 - val_loss: 406726048.0000 - val_accuracy: 0.5333 - 51s/epoch - 46ms/step\n",
      "Epoch 14/10000\n",
      "1126/1126 - 64s - loss: 361418144.0000 - accuracy: 0.5780 - val_loss: 332421312.0000 - val_accuracy: 0.5473 - 64s/epoch - 57ms/step\n",
      "Epoch 15/10000\n",
      "1126/1126 - 54s - loss: 391249664.0000 - accuracy: 0.5776 - val_loss: 885843840.0000 - val_accuracy: 0.6182 - 54s/epoch - 48ms/step\n",
      "Epoch 16/10000\n",
      "1126/1126 - 57s - loss: 389932928.0000 - accuracy: 0.5778 - val_loss: 486537248.0000 - val_accuracy: 0.6146 - 57s/epoch - 50ms/step\n",
      "Epoch 17/10000\n",
      "1126/1126 - 58s - loss: 333021696.0000 - accuracy: 0.5779 - val_loss: 268012624.0000 - val_accuracy: 0.4912 - 58s/epoch - 51ms/step\n",
      "391/391 [==============================] - 4s 10ms/step\n",
      "\n",
      "Iterazione 113/128 (03-11-2023_15-24-10)\n",
      "hidden_layer_size: 1000\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 3\n",
      "activation_hidden: relu\n",
      "activation_output: sigmoid\n",
      "batch_size: 100\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 10\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 64.03%\n",
      "La recall di questo modello sul validation set è: 28.9%\n",
      "La f1 di questo modello sul validation set è: 38.34%\n",
      "La balanced accuracy di questo modello sul validation set è: 57.55%\n",
      "La precision di questo modello sul validation set è: 56.94%\n",
      "La AUC di questo modello sul validation set è: 57.55%\n",
      "391/391 [==============================] - 4s 9ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.66    0.86      0.75  7,670.00\n",
      "1                  0.57    0.29      0.38  4,841.00\n",
      "accuracy           0.64    0.64      0.64      0.64\n",
      "macro avg          0.61    0.58      0.56 12,511.00\n",
      "weighted avg       0.62    0.64      0.61 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         6612         1058\n",
      "Actual 1         3442         1399\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 66.14% con il modello:\n",
      "98 - AC66.14%, L1000, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA20, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_14-02-18).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 63.1% con il modello:\n",
      "98 - BA63.1%, L1000, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA20, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_14-02-18).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 66.75% con il modello:\n",
      "105 - PR66.75%, L1000, mV-10, MV10, P2, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_14-19-26).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "1126/1126 - 66s - loss: 682914240.0000 - accuracy: 0.5641 - val_loss: 1643667200.0000 - val_accuracy: 0.6135 - 66s/epoch - 58ms/step\n",
      "Epoch 2/10000\n",
      "1126/1126 - 58s - loss: 440340896.0000 - accuracy: 0.5722 - val_loss: 916649856.0000 - val_accuracy: 0.6157 - 58s/epoch - 51ms/step\n",
      "Epoch 3/10000\n",
      "1126/1126 - 58s - loss: 405314944.0000 - accuracy: 0.5715 - val_loss: 413828224.0000 - val_accuracy: 0.6192 - 58s/epoch - 52ms/step\n",
      "Epoch 4/10000\n",
      "1126/1126 - 58s - loss: 400499520.0000 - accuracy: 0.5703 - val_loss: 481669152.0000 - val_accuracy: 0.6132 - 58s/epoch - 52ms/step\n",
      "Epoch 5/10000\n",
      "1126/1126 - 59s - loss: 376037344.0000 - accuracy: 0.5711 - val_loss: 128531752.0000 - val_accuracy: 0.6358 - 59s/epoch - 52ms/step\n",
      "Epoch 6/10000\n",
      "1126/1126 - 55s - loss: 402098272.0000 - accuracy: 0.5752 - val_loss: 348511104.0000 - val_accuracy: 0.6174 - 55s/epoch - 49ms/step\n",
      "Epoch 7/10000\n",
      "1126/1126 - 54s - loss: 375967712.0000 - accuracy: 0.5763 - val_loss: 98853424.0000 - val_accuracy: 0.6403 - 54s/epoch - 48ms/step\n",
      "Epoch 8/10000\n",
      "1126/1126 - 56s - loss: 414903712.0000 - accuracy: 0.5726 - val_loss: 346430912.0000 - val_accuracy: 0.5710 - 56s/epoch - 49ms/step\n",
      "Epoch 9/10000\n",
      "1126/1126 - 56s - loss: 365650944.0000 - accuracy: 0.5746 - val_loss: 553427328.0000 - val_accuracy: 0.6134 - 56s/epoch - 50ms/step\n",
      "Epoch 10/10000\n",
      "1126/1126 - 55s - loss: 419103232.0000 - accuracy: 0.5734 - val_loss: 734419520.0000 - val_accuracy: 0.4293 - 55s/epoch - 49ms/step\n",
      "Epoch 11/10000\n",
      "1126/1126 - 52s - loss: 395526400.0000 - accuracy: 0.5727 - val_loss: 356978848.0000 - val_accuracy: 0.6175 - 52s/epoch - 47ms/step\n",
      "Epoch 12/10000\n",
      "1126/1126 - 57s - loss: 366587552.0000 - accuracy: 0.5722 - val_loss: 380730848.0000 - val_accuracy: 0.5520 - 57s/epoch - 51ms/step\n",
      "Epoch 13/10000\n",
      "1126/1126 - 55s - loss: 399671808.0000 - accuracy: 0.5773 - val_loss: 406726048.0000 - val_accuracy: 0.5333 - 55s/epoch - 49ms/step\n",
      "Epoch 14/10000\n",
      "1126/1126 - 54s - loss: 361418144.0000 - accuracy: 0.5780 - val_loss: 332421312.0000 - val_accuracy: 0.5473 - 54s/epoch - 48ms/step\n",
      "Epoch 15/10000\n",
      "1126/1126 - 57s - loss: 391249664.0000 - accuracy: 0.5776 - val_loss: 885843840.0000 - val_accuracy: 0.6182 - 57s/epoch - 50ms/step\n",
      "Epoch 16/10000\n",
      "1126/1126 - 57s - loss: 389932928.0000 - accuracy: 0.5778 - val_loss: 486537248.0000 - val_accuracy: 0.6146 - 57s/epoch - 51ms/step\n",
      "Epoch 17/10000\n",
      "1126/1126 - 58s - loss: 333021696.0000 - accuracy: 0.5779 - val_loss: 268012624.0000 - val_accuracy: 0.4912 - 58s/epoch - 51ms/step\n",
      "Epoch 18/10000\n",
      "1126/1126 - 53s - loss: 383655712.0000 - accuracy: 0.5756 - val_loss: 107506592.0000 - val_accuracy: 0.6542 - 53s/epoch - 47ms/step\n",
      "Epoch 19/10000\n",
      "1126/1126 - 54s - loss: 357084256.0000 - accuracy: 0.5779 - val_loss: 434105184.0000 - val_accuracy: 0.4131 - 54s/epoch - 48ms/step\n",
      "Epoch 20/10000\n",
      "1126/1126 - 55s - loss: 320010208.0000 - accuracy: 0.5781 - val_loss: 601166016.0000 - val_accuracy: 0.6138 - 55s/epoch - 49ms/step\n",
      "Epoch 21/10000\n",
      "1126/1126 - 55s - loss: 377437920.0000 - accuracy: 0.5760 - val_loss: 109454080.0000 - val_accuracy: 0.6477 - 55s/epoch - 49ms/step\n",
      "Epoch 22/10000\n",
      "1126/1126 - 60s - loss: 358217280.0000 - accuracy: 0.5783 - val_loss: 277792480.0000 - val_accuracy: 0.6344 - 60s/epoch - 53ms/step\n",
      "Epoch 23/10000\n",
      "1126/1126 - 57s - loss: 350008640.0000 - accuracy: 0.5777 - val_loss: 127465000.0000 - val_accuracy: 0.6434 - 57s/epoch - 51ms/step\n",
      "Epoch 24/10000\n",
      "1126/1126 - 52s - loss: 349198784.0000 - accuracy: 0.5759 - val_loss: 208690176.0000 - val_accuracy: 0.6349 - 52s/epoch - 47ms/step\n",
      "Epoch 25/10000\n",
      "1126/1126 - 57s - loss: 343353696.0000 - accuracy: 0.5772 - val_loss: 987845056.0000 - val_accuracy: 0.4548 - 57s/epoch - 50ms/step\n",
      "Epoch 26/10000\n",
      "1126/1126 - 56s - loss: 341026336.0000 - accuracy: 0.5799 - val_loss: 237185360.0000 - val_accuracy: 0.6355 - 56s/epoch - 50ms/step\n",
      "Epoch 27/10000\n",
      "1126/1126 - 55s - loss: 318079424.0000 - accuracy: 0.5802 - val_loss: 306665216.0000 - val_accuracy: 0.4427 - 55s/epoch - 49ms/step\n",
      "Epoch 28/10000\n",
      "1126/1126 - 58s - loss: 338564320.0000 - accuracy: 0.5787 - val_loss: 98245856.0000 - val_accuracy: 0.6494 - 58s/epoch - 51ms/step\n",
      "Epoch 29/10000\n",
      "1126/1126 - 58s - loss: 329762944.0000 - accuracy: 0.5806 - val_loss: 120871464.0000 - val_accuracy: 0.6366 - 58s/epoch - 51ms/step\n",
      "Epoch 30/10000\n",
      "1126/1126 - 58s - loss: 322328288.0000 - accuracy: 0.5791 - val_loss: 207100048.0000 - val_accuracy: 0.4904 - 58s/epoch - 51ms/step\n",
      "Epoch 31/10000\n",
      "1126/1126 - 58s - loss: 323794592.0000 - accuracy: 0.5797 - val_loss: 573053632.0000 - val_accuracy: 0.5020 - 58s/epoch - 52ms/step\n",
      "Epoch 32/10000\n",
      "1126/1126 - 57s - loss: 343622048.0000 - accuracy: 0.5805 - val_loss: 641044608.0000 - val_accuracy: 0.4100 - 57s/epoch - 50ms/step\n",
      "Epoch 33/10000\n",
      "1126/1126 - 59s - loss: 300489600.0000 - accuracy: 0.5830 - val_loss: 143563952.0000 - val_accuracy: 0.5123 - 59s/epoch - 52ms/step\n",
      "Epoch 34/10000\n",
      "1126/1126 - 61s - loss: 315372352.0000 - accuracy: 0.5799 - val_loss: 64407080.0000 - val_accuracy: 0.6432 - 61s/epoch - 54ms/step\n",
      "Epoch 35/10000\n",
      "1126/1126 - 61s - loss: 362102784.0000 - accuracy: 0.5786 - val_loss: 676927616.0000 - val_accuracy: 0.4133 - 61s/epoch - 54ms/step\n",
      "Epoch 36/10000\n",
      "1126/1126 - 62s - loss: 300615616.0000 - accuracy: 0.5812 - val_loss: 367315072.0000 - val_accuracy: 0.6233 - 62s/epoch - 55ms/step\n",
      "Epoch 37/10000\n",
      "1126/1126 - 63s - loss: 291346848.0000 - accuracy: 0.5819 - val_loss: 481924352.0000 - val_accuracy: 0.6136 - 63s/epoch - 56ms/step\n",
      "Epoch 38/10000\n",
      "1126/1126 - 62s - loss: 342183552.0000 - accuracy: 0.5806 - val_loss: 136009968.0000 - val_accuracy: 0.5610 - 62s/epoch - 55ms/step\n",
      "391/391 [==============================] - 5s 11ms/step\n",
      "\n",
      "Iterazione 114/128 (03-11-2023_16-00-36)\n",
      "hidden_layer_size: 1000\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 3\n",
      "activation_hidden: relu\n",
      "activation_output: sigmoid\n",
      "batch_size: 100\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 20\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 65.42%\n",
      "La recall di questo modello sul validation set è: 40.32%\n",
      "La f1 di questo modello sul validation set è: 47.44%\n",
      "La balanced accuracy di questo modello sul validation set è: 60.79%\n",
      "La precision di questo modello sul validation set è: 57.6%\n",
      "La AUC di questo modello sul validation set è: 60.79%\n",
      "391/391 [==============================] - 4s 11ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.68    0.81      0.74  7,670.00\n",
      "1                  0.58    0.40      0.47  4,841.00\n",
      "accuracy           0.65    0.65      0.65      0.65\n",
      "macro avg          0.63    0.61      0.61 12,511.00\n",
      "weighted avg       0.64    0.65      0.64 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         6233         1437\n",
      "Actual 1         2889         1952\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 66.14% con il modello:\n",
      "98 - AC66.14%, L1000, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA20, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_14-02-18).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 63.1% con il modello:\n",
      "98 - BA63.1%, L1000, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA20, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_14-02-18).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 66.75% con il modello:\n",
      "105 - PR66.75%, L1000, mV-10, MV10, P2, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_14-19-26).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "113/113 - 20s - loss: 2651523584.0000 - accuracy: 0.5373 - val_loss: 977148032.0000 - val_accuracy: 0.4239 - 20s/epoch - 177ms/step\n",
      "Epoch 2/10000\n",
      "113/113 - 15s - loss: 417971776.0000 - accuracy: 0.5707 - val_loss: 671898752.0000 - val_accuracy: 0.6117 - 15s/epoch - 129ms/step\n",
      "Epoch 3/10000\n",
      "113/113 - 14s - loss: 341907296.0000 - accuracy: 0.5740 - val_loss: 122915592.0000 - val_accuracy: 0.5577 - 14s/epoch - 127ms/step\n",
      "Epoch 4/10000\n",
      "113/113 - 16s - loss: 390469120.0000 - accuracy: 0.5694 - val_loss: 199159920.0000 - val_accuracy: 0.6252 - 16s/epoch - 142ms/step\n",
      "Epoch 5/10000\n",
      "113/113 - 16s - loss: 315842304.0000 - accuracy: 0.5680 - val_loss: 1050972352.0000 - val_accuracy: 0.6140 - 16s/epoch - 139ms/step\n",
      "Epoch 6/10000\n",
      "113/113 - 15s - loss: 377751264.0000 - accuracy: 0.5733 - val_loss: 310535744.0000 - val_accuracy: 0.6233 - 15s/epoch - 136ms/step\n",
      "Epoch 7/10000\n",
      "113/113 - 15s - loss: 431378560.0000 - accuracy: 0.5742 - val_loss: 95886096.0000 - val_accuracy: 0.5923 - 15s/epoch - 129ms/step\n",
      "Epoch 8/10000\n",
      "113/113 - 16s - loss: 471390752.0000 - accuracy: 0.5704 - val_loss: 249178416.0000 - val_accuracy: 0.6191 - 16s/epoch - 145ms/step\n",
      "Epoch 9/10000\n",
      "113/113 - 16s - loss: 461788000.0000 - accuracy: 0.5719 - val_loss: 107561824.0000 - val_accuracy: 0.6409 - 16s/epoch - 138ms/step\n",
      "Epoch 10/10000\n",
      "113/113 - 15s - loss: 450085184.0000 - accuracy: 0.5718 - val_loss: 205357392.0000 - val_accuracy: 0.6229 - 15s/epoch - 135ms/step\n",
      "Epoch 11/10000\n",
      "113/113 - 15s - loss: 377114560.0000 - accuracy: 0.5707 - val_loss: 199098608.0000 - val_accuracy: 0.5243 - 15s/epoch - 131ms/step\n",
      "Epoch 12/10000\n",
      "113/113 - 15s - loss: 379351040.0000 - accuracy: 0.5696 - val_loss: 175777008.0000 - val_accuracy: 0.5268 - 15s/epoch - 129ms/step\n",
      "Epoch 13/10000\n",
      "113/113 - 14s - loss: 405349536.0000 - accuracy: 0.5696 - val_loss: 226392096.0000 - val_accuracy: 0.5167 - 14s/epoch - 127ms/step\n",
      "Epoch 14/10000\n",
      "113/113 - 15s - loss: 457129120.0000 - accuracy: 0.5744 - val_loss: 96984176.0000 - val_accuracy: 0.6316 - 15s/epoch - 135ms/step\n",
      "Epoch 15/10000\n",
      "113/113 - 15s - loss: 401484224.0000 - accuracy: 0.5655 - val_loss: 971168896.0000 - val_accuracy: 0.6130 - 15s/epoch - 128ms/step\n",
      "Epoch 16/10000\n",
      "113/113 - 14s - loss: 397430464.0000 - accuracy: 0.5745 - val_loss: 217593168.0000 - val_accuracy: 0.6398 - 14s/epoch - 124ms/step\n",
      "Epoch 17/10000\n",
      "113/113 - 15s - loss: 385661440.0000 - accuracy: 0.5732 - val_loss: 455008576.0000 - val_accuracy: 0.6153 - 15s/epoch - 134ms/step\n",
      "Epoch 18/10000\n",
      "113/113 - 15s - loss: 431147712.0000 - accuracy: 0.5701 - val_loss: 123819384.0000 - val_accuracy: 0.6410 - 15s/epoch - 136ms/step\n",
      "Epoch 19/10000\n",
      "113/113 - 15s - loss: 423606944.0000 - accuracy: 0.5717 - val_loss: 737718784.0000 - val_accuracy: 0.6170 - 15s/epoch - 129ms/step\n",
      "Epoch 20/10000\n",
      "113/113 - 15s - loss: 461922944.0000 - accuracy: 0.5730 - val_loss: 379553280.0000 - val_accuracy: 0.4964 - 15s/epoch - 135ms/step\n",
      "Epoch 21/10000\n",
      "113/113 - 16s - loss: 243382928.0000 - accuracy: 0.5809 - val_loss: 923477696.0000 - val_accuracy: 0.3992 - 16s/epoch - 138ms/step\n",
      "Epoch 22/10000\n",
      "113/113 - 16s - loss: 458475392.0000 - accuracy: 0.5717 - val_loss: 76718224.0000 - val_accuracy: 0.5870 - 16s/epoch - 145ms/step\n",
      "Epoch 23/10000\n",
      "113/113 - 16s - loss: 459119648.0000 - accuracy: 0.5681 - val_loss: 321447040.0000 - val_accuracy: 0.4837 - 16s/epoch - 138ms/step\n",
      "Epoch 24/10000\n",
      "113/113 - 15s - loss: 249539328.0000 - accuracy: 0.5807 - val_loss: 358047552.0000 - val_accuracy: 0.6199 - 15s/epoch - 135ms/step\n",
      "Epoch 25/10000\n",
      "113/113 - 16s - loss: 417169056.0000 - accuracy: 0.5710 - val_loss: 98110288.0000 - val_accuracy: 0.6401 - 16s/epoch - 141ms/step\n",
      "Epoch 26/10000\n",
      "113/113 - 15s - loss: 374181920.0000 - accuracy: 0.5691 - val_loss: 589127168.0000 - val_accuracy: 0.4994 - 15s/epoch - 135ms/step\n",
      "Epoch 27/10000\n",
      "113/113 - 15s - loss: 266461520.0000 - accuracy: 0.5798 - val_loss: 132442784.0000 - val_accuracy: 0.5143 - 15s/epoch - 135ms/step\n",
      "Epoch 28/10000\n",
      "113/113 - 15s - loss: 391495488.0000 - accuracy: 0.5744 - val_loss: 504689088.0000 - val_accuracy: 0.4106 - 15s/epoch - 128ms/step\n",
      "391/391 [==============================] - 5s 11ms/step\n",
      "\n",
      "Iterazione 115/128 (03-11-2023_16-07-58)\n",
      "hidden_layer_size: 1000\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 3\n",
      "activation_hidden: relu\n",
      "activation_output: sigmoid\n",
      "batch_size: 1000\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 10\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 64.11%\n",
      "La recall di questo modello sul validation set è: 27.97%\n",
      "La f1 di questo modello sul validation set è: 37.62%\n",
      "La balanced accuracy di questo modello sul validation set è: 57.45%\n",
      "La precision di questo modello sul validation set è: 57.45%\n",
      "La AUC di questo modello sul validation set è: 57.45%\n",
      "391/391 [==============================] - 4s 11ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.66    0.87      0.75  7,670.00\n",
      "1                  0.57    0.28      0.38  4,841.00\n",
      "accuracy           0.64    0.64      0.64      0.64\n",
      "macro avg          0.62    0.57      0.56 12,511.00\n",
      "weighted avg       0.62    0.64      0.60 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         6667         1003\n",
      "Actual 1         3487         1354\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 66.14% con il modello:\n",
      "98 - AC66.14%, L1000, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA20, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_14-02-18).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 63.1% con il modello:\n",
      "98 - BA63.1%, L1000, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA20, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_14-02-18).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 66.75% con il modello:\n",
      "105 - PR66.75%, L1000, mV-10, MV10, P2, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_14-19-26).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "113/113 - 20s - loss: 2651523584.0000 - accuracy: 0.5373 - val_loss: 977148032.0000 - val_accuracy: 0.4239 - 20s/epoch - 179ms/step\n",
      "Epoch 2/10000\n",
      "113/113 - 16s - loss: 417971776.0000 - accuracy: 0.5707 - val_loss: 671898752.0000 - val_accuracy: 0.6117 - 16s/epoch - 141ms/step\n",
      "Epoch 3/10000\n",
      "113/113 - 16s - loss: 341907296.0000 - accuracy: 0.5740 - val_loss: 122915592.0000 - val_accuracy: 0.5577 - 16s/epoch - 143ms/step\n",
      "Epoch 4/10000\n",
      "113/113 - 16s - loss: 390469120.0000 - accuracy: 0.5694 - val_loss: 199159920.0000 - val_accuracy: 0.6252 - 16s/epoch - 142ms/step\n",
      "Epoch 5/10000\n",
      "113/113 - 16s - loss: 315842304.0000 - accuracy: 0.5680 - val_loss: 1050972352.0000 - val_accuracy: 0.6140 - 16s/epoch - 142ms/step\n",
      "Epoch 6/10000\n",
      "113/113 - 15s - loss: 377751264.0000 - accuracy: 0.5733 - val_loss: 310535744.0000 - val_accuracy: 0.6233 - 15s/epoch - 136ms/step\n",
      "Epoch 7/10000\n",
      "113/113 - 15s - loss: 431378560.0000 - accuracy: 0.5742 - val_loss: 95886096.0000 - val_accuracy: 0.5923 - 15s/epoch - 134ms/step\n",
      "Epoch 8/10000\n",
      "113/113 - 15s - loss: 471390752.0000 - accuracy: 0.5704 - val_loss: 249178416.0000 - val_accuracy: 0.6191 - 15s/epoch - 136ms/step\n",
      "Epoch 9/10000\n",
      "113/113 - 16s - loss: 461788000.0000 - accuracy: 0.5719 - val_loss: 107561824.0000 - val_accuracy: 0.6409 - 16s/epoch - 140ms/step\n",
      "Epoch 10/10000\n",
      "113/113 - 15s - loss: 450085184.0000 - accuracy: 0.5718 - val_loss: 205357392.0000 - val_accuracy: 0.6229 - 15s/epoch - 136ms/step\n",
      "Epoch 11/10000\n",
      "113/113 - 15s - loss: 377114560.0000 - accuracy: 0.5707 - val_loss: 199098608.0000 - val_accuracy: 0.5243 - 15s/epoch - 133ms/step\n",
      "Epoch 12/10000\n",
      "113/113 - 15s - loss: 379351040.0000 - accuracy: 0.5696 - val_loss: 175777008.0000 - val_accuracy: 0.5268 - 15s/epoch - 134ms/step\n",
      "Epoch 13/10000\n",
      "113/113 - 16s - loss: 405349536.0000 - accuracy: 0.5696 - val_loss: 226392096.0000 - val_accuracy: 0.5167 - 16s/epoch - 138ms/step\n",
      "Epoch 14/10000\n",
      "113/113 - 14s - loss: 457129120.0000 - accuracy: 0.5744 - val_loss: 96984176.0000 - val_accuracy: 0.6316 - 14s/epoch - 123ms/step\n",
      "Epoch 15/10000\n",
      "113/113 - 15s - loss: 401484224.0000 - accuracy: 0.5655 - val_loss: 971168896.0000 - val_accuracy: 0.6130 - 15s/epoch - 134ms/step\n",
      "Epoch 16/10000\n",
      "113/113 - 14s - loss: 397430464.0000 - accuracy: 0.5745 - val_loss: 217593168.0000 - val_accuracy: 0.6398 - 14s/epoch - 126ms/step\n",
      "Epoch 17/10000\n",
      "113/113 - 15s - loss: 385661440.0000 - accuracy: 0.5732 - val_loss: 455008576.0000 - val_accuracy: 0.6153 - 15s/epoch - 129ms/step\n",
      "Epoch 18/10000\n",
      "113/113 - 14s - loss: 431147712.0000 - accuracy: 0.5701 - val_loss: 123819384.0000 - val_accuracy: 0.6410 - 14s/epoch - 127ms/step\n",
      "Epoch 19/10000\n",
      "113/113 - 14s - loss: 423606944.0000 - accuracy: 0.5717 - val_loss: 737718784.0000 - val_accuracy: 0.6170 - 14s/epoch - 126ms/step\n",
      "Epoch 20/10000\n",
      "113/113 - 15s - loss: 461922944.0000 - accuracy: 0.5730 - val_loss: 379553280.0000 - val_accuracy: 0.4964 - 15s/epoch - 134ms/step\n",
      "Epoch 21/10000\n",
      "113/113 - 15s - loss: 243382928.0000 - accuracy: 0.5809 - val_loss: 923477696.0000 - val_accuracy: 0.3992 - 15s/epoch - 131ms/step\n",
      "Epoch 22/10000\n",
      "113/113 - 14s - loss: 458475392.0000 - accuracy: 0.5717 - val_loss: 76718224.0000 - val_accuracy: 0.5870 - 14s/epoch - 128ms/step\n",
      "Epoch 23/10000\n",
      "113/113 - 15s - loss: 459119648.0000 - accuracy: 0.5681 - val_loss: 321447040.0000 - val_accuracy: 0.4837 - 15s/epoch - 129ms/step\n",
      "Epoch 24/10000\n",
      "113/113 - 15s - loss: 249539328.0000 - accuracy: 0.5807 - val_loss: 358047552.0000 - val_accuracy: 0.6199 - 15s/epoch - 132ms/step\n",
      "Epoch 25/10000\n",
      "113/113 - 14s - loss: 417169056.0000 - accuracy: 0.5710 - val_loss: 98110288.0000 - val_accuracy: 0.6401 - 14s/epoch - 127ms/step\n",
      "Epoch 26/10000\n",
      "113/113 - 14s - loss: 374181920.0000 - accuracy: 0.5691 - val_loss: 589127168.0000 - val_accuracy: 0.4994 - 14s/epoch - 126ms/step\n",
      "Epoch 27/10000\n",
      "113/113 - 14s - loss: 266461520.0000 - accuracy: 0.5798 - val_loss: 132442784.0000 - val_accuracy: 0.5143 - 14s/epoch - 126ms/step\n",
      "Epoch 28/10000\n",
      "113/113 - 15s - loss: 391495488.0000 - accuracy: 0.5744 - val_loss: 504689088.0000 - val_accuracy: 0.4106 - 15s/epoch - 134ms/step\n",
      "Epoch 29/10000\n",
      "113/113 - 15s - loss: 376313792.0000 - accuracy: 0.5713 - val_loss: 99627888.0000 - val_accuracy: 0.5993 - 15s/epoch - 131ms/step\n",
      "Epoch 30/10000\n",
      "113/113 - 15s - loss: 443253888.0000 - accuracy: 0.5708 - val_loss: 128847192.0000 - val_accuracy: 0.5908 - 15s/epoch - 129ms/step\n",
      "Epoch 31/10000\n",
      "113/113 - 17s - loss: 378120256.0000 - accuracy: 0.5747 - val_loss: 548046336.0000 - val_accuracy: 0.6197 - 17s/epoch - 147ms/step\n",
      "Epoch 32/10000\n",
      "113/113 - 17s - loss: 422820896.0000 - accuracy: 0.5728 - val_loss: 109010944.0000 - val_accuracy: 0.6248 - 17s/epoch - 154ms/step\n",
      "Epoch 33/10000\n",
      "113/113 - 16s - loss: 332476512.0000 - accuracy: 0.5756 - val_loss: 177959824.0000 - val_accuracy: 0.5908 - 16s/epoch - 138ms/step\n",
      "Epoch 34/10000\n",
      "113/113 - 15s - loss: 427605824.0000 - accuracy: 0.5782 - val_loss: 79693832.0000 - val_accuracy: 0.6346 - 15s/epoch - 133ms/step\n",
      "Epoch 35/10000\n",
      "113/113 - 15s - loss: 372040192.0000 - accuracy: 0.5732 - val_loss: 98314160.0000 - val_accuracy: 0.6481 - 15s/epoch - 133ms/step\n",
      "Epoch 36/10000\n",
      "113/113 - 15s - loss: 410481440.0000 - accuracy: 0.5706 - val_loss: 388329024.0000 - val_accuracy: 0.6146 - 15s/epoch - 136ms/step\n",
      "Epoch 37/10000\n",
      "113/113 - 15s - loss: 319621408.0000 - accuracy: 0.5774 - val_loss: 298500832.0000 - val_accuracy: 0.6318 - 15s/epoch - 131ms/step\n",
      "Epoch 38/10000\n",
      "113/113 - 15s - loss: 371586368.0000 - accuracy: 0.5754 - val_loss: 187275520.0000 - val_accuracy: 0.6130 - 15s/epoch - 134ms/step\n",
      "Epoch 39/10000\n",
      "113/113 - 15s - loss: 374453632.0000 - accuracy: 0.5744 - val_loss: 104100504.0000 - val_accuracy: 0.5953 - 15s/epoch - 137ms/step\n",
      "Epoch 40/10000\n",
      "113/113 - 14s - loss: 382118880.0000 - accuracy: 0.5757 - val_loss: 81620360.0000 - val_accuracy: 0.6407 - 14s/epoch - 128ms/step\n",
      "Epoch 41/10000\n",
      "113/113 - 14s - loss: 355309664.0000 - accuracy: 0.5732 - val_loss: 215917568.0000 - val_accuracy: 0.5649 - 14s/epoch - 120ms/step\n",
      "Epoch 42/10000\n",
      "113/113 - 14s - loss: 410000960.0000 - accuracy: 0.5720 - val_loss: 882511424.0000 - val_accuracy: 0.4713 - 14s/epoch - 120ms/step\n",
      "Epoch 43/10000\n",
      "113/113 - 14s - loss: 297494912.0000 - accuracy: 0.5823 - val_loss: 90260144.0000 - val_accuracy: 0.6386 - 14s/epoch - 126ms/step\n",
      "Epoch 44/10000\n",
      "113/113 - 15s - loss: 374860928.0000 - accuracy: 0.5745 - val_loss: 289106880.0000 - val_accuracy: 0.6263 - 15s/epoch - 128ms/step\n",
      "Epoch 45/10000\n",
      "113/113 - 14s - loss: 354461760.0000 - accuracy: 0.5758 - val_loss: 83171120.0000 - val_accuracy: 0.5904 - 14s/epoch - 124ms/step\n",
      "Epoch 46/10000\n",
      "113/113 - 15s - loss: 301971872.0000 - accuracy: 0.5759 - val_loss: 166901904.0000 - val_accuracy: 0.6224 - 15s/epoch - 132ms/step\n",
      "Epoch 47/10000\n",
      "113/113 - 15s - loss: 311722880.0000 - accuracy: 0.5760 - val_loss: 305810016.0000 - val_accuracy: 0.6198 - 15s/epoch - 130ms/step\n",
      "Epoch 48/10000\n",
      "113/113 - 16s - loss: 358228224.0000 - accuracy: 0.5727 - val_loss: 374066528.0000 - val_accuracy: 0.5214 - 16s/epoch - 138ms/step\n",
      "Epoch 49/10000\n",
      "113/113 - 15s - loss: 411412640.0000 - accuracy: 0.5756 - val_loss: 1529507328.0000 - val_accuracy: 0.4024 - 15s/epoch - 129ms/step\n",
      "Epoch 50/10000\n",
      "113/113 - 14s - loss: 360227008.0000 - accuracy: 0.5761 - val_loss: 186409008.0000 - val_accuracy: 0.4944 - 14s/epoch - 128ms/step\n",
      "Epoch 51/10000\n",
      "113/113 - 16s - loss: 352160736.0000 - accuracy: 0.5749 - val_loss: 78020320.0000 - val_accuracy: 0.6214 - 16s/epoch - 143ms/step\n",
      "Epoch 52/10000\n",
      "113/113 - 16s - loss: 456208256.0000 - accuracy: 0.5715 - val_loss: 74028016.0000 - val_accuracy: 0.6185 - 16s/epoch - 138ms/step\n",
      "Epoch 53/10000\n",
      "113/113 - 16s - loss: 335800864.0000 - accuracy: 0.5759 - val_loss: 239741136.0000 - val_accuracy: 0.6328 - 16s/epoch - 141ms/step\n",
      "Epoch 54/10000\n",
      "113/113 - 15s - loss: 357308928.0000 - accuracy: 0.5732 - val_loss: 149677056.0000 - val_accuracy: 0.6513 - 15s/epoch - 132ms/step\n",
      "Epoch 55/10000\n",
      "113/113 - 15s - loss: 402601248.0000 - accuracy: 0.5710 - val_loss: 285367040.0000 - val_accuracy: 0.6266 - 15s/epoch - 131ms/step\n",
      "Epoch 56/10000\n",
      "113/113 - 15s - loss: 317491200.0000 - accuracy: 0.5798 - val_loss: 291670112.0000 - val_accuracy: 0.5145 - 15s/epoch - 133ms/step\n",
      "Epoch 57/10000\n",
      "113/113 - 15s - loss: 362317216.0000 - accuracy: 0.5753 - val_loss: 86642608.0000 - val_accuracy: 0.6314 - 15s/epoch - 136ms/step\n",
      "Epoch 58/10000\n",
      "113/113 - 15s - loss: 448761760.0000 - accuracy: 0.5686 - val_loss: 640077440.0000 - val_accuracy: 0.6132 - 15s/epoch - 130ms/step\n",
      "Epoch 59/10000\n",
      "113/113 - 15s - loss: 404468160.0000 - accuracy: 0.5756 - val_loss: 170134656.0000 - val_accuracy: 0.6182 - 15s/epoch - 130ms/step\n",
      "Epoch 60/10000\n",
      "113/113 - 14s - loss: 401602304.0000 - accuracy: 0.5751 - val_loss: 363426464.0000 - val_accuracy: 0.4537 - 14s/epoch - 125ms/step\n",
      "Epoch 61/10000\n",
      "113/113 - 14s - loss: 320332896.0000 - accuracy: 0.5799 - val_loss: 241456576.0000 - val_accuracy: 0.4796 - 14s/epoch - 128ms/step\n",
      "Epoch 62/10000\n",
      "113/113 - 13s - loss: 326994944.0000 - accuracy: 0.5786 - val_loss: 135444208.0000 - val_accuracy: 0.6287 - 13s/epoch - 119ms/step\n",
      "Epoch 63/10000\n",
      "113/113 - 14s - loss: 391952480.0000 - accuracy: 0.5732 - val_loss: 140748304.0000 - val_accuracy: 0.6294 - 14s/epoch - 127ms/step\n",
      "Epoch 64/10000\n",
      "113/113 - 14s - loss: 376093024.0000 - accuracy: 0.5761 - val_loss: 297877312.0000 - val_accuracy: 0.6166 - 14s/epoch - 126ms/step\n",
      "Epoch 65/10000\n",
      "113/113 - 14s - loss: 214086224.0000 - accuracy: 0.5823 - val_loss: 79451224.0000 - val_accuracy: 0.6437 - 14s/epoch - 121ms/step\n",
      "Epoch 66/10000\n",
      "113/113 - 14s - loss: 397975712.0000 - accuracy: 0.5733 - val_loss: 395322272.0000 - val_accuracy: 0.4681 - 14s/epoch - 127ms/step\n",
      "Epoch 67/10000\n",
      "113/113 - 14s - loss: 390044192.0000 - accuracy: 0.5722 - val_loss: 204441328.0000 - val_accuracy: 0.6345 - 14s/epoch - 126ms/step\n",
      "Epoch 68/10000\n",
      "113/113 - 14s - loss: 282005696.0000 - accuracy: 0.5806 - val_loss: 147887776.0000 - val_accuracy: 0.6263 - 14s/epoch - 125ms/step\n",
      "Epoch 69/10000\n",
      "113/113 - 15s - loss: 217846576.0000 - accuracy: 0.5834 - val_loss: 102003144.0000 - val_accuracy: 0.6354 - 15s/epoch - 130ms/step\n",
      "Epoch 70/10000\n",
      "113/113 - 15s - loss: 403010816.0000 - accuracy: 0.5705 - val_loss: 394046592.0000 - val_accuracy: 0.4327 - 15s/epoch - 134ms/step\n",
      "Epoch 71/10000\n",
      "113/113 - 15s - loss: 243348848.0000 - accuracy: 0.5813 - val_loss: 619182848.0000 - val_accuracy: 0.4028 - 15s/epoch - 132ms/step\n",
      "Epoch 72/10000\n",
      "113/113 - 15s - loss: 434301568.0000 - accuracy: 0.5731 - val_loss: 472211008.0000 - val_accuracy: 0.5621 - 15s/epoch - 130ms/step\n",
      "Epoch 73/10000\n",
      "113/113 - 14s - loss: 375965248.0000 - accuracy: 0.5785 - val_loss: 119982344.0000 - val_accuracy: 0.5644 - 14s/epoch - 124ms/step\n",
      "Epoch 74/10000\n",
      "113/113 - 14s - loss: 388945312.0000 - accuracy: 0.5763 - val_loss: 162749552.0000 - val_accuracy: 0.6394 - 14s/epoch - 120ms/step\n",
      "391/391 [==============================] - 4s 10ms/step\n",
      "\n",
      "Iterazione 116/128 (03-11-2023_16-26-35)\n",
      "hidden_layer_size: 1000\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 3\n",
      "activation_hidden: relu\n",
      "activation_output: sigmoid\n",
      "batch_size: 1000\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 20\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 65.13%\n",
      "La recall di questo modello sul validation set è: 32.66%\n",
      "La f1 di questo modello sul validation set è: 42.03%\n",
      "La balanced accuracy di questo modello sul validation set è: 59.15%\n",
      "La precision di questo modello sul validation set è: 58.93%\n",
      "La AUC di questo modello sul validation set è: 59.15%\n",
      "391/391 [==============================] - 4s 10ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.67    0.86      0.75  7,670.00\n",
      "1                  0.59    0.33      0.42  4,841.00\n",
      "accuracy           0.65    0.65      0.65      0.65\n",
      "macro avg          0.63    0.59      0.59 12,511.00\n",
      "weighted avg       0.64    0.65      0.62 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         6568         1102\n",
      "Actual 1         3260         1581\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 66.14% con il modello:\n",
      "98 - AC66.14%, L1000, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA20, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_14-02-18).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 63.1% con il modello:\n",
      "98 - BA63.1%, L1000, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA20, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_14-02-18).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 66.75% con il modello:\n",
      "105 - PR66.75%, L1000, mV-10, MV10, P2, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_14-19-26).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "1126/1126 - 66s - loss: 5.3280 - accuracy: 0.5638 - val_loss: 4.4635 - val_accuracy: 0.5853 - 66s/epoch - 58ms/step\n",
      "Epoch 2/10000\n",
      "1126/1126 - 63s - loss: 4.2264 - accuracy: 0.5623 - val_loss: 3.8673 - val_accuracy: 0.5560 - 63s/epoch - 56ms/step\n",
      "Epoch 3/10000\n",
      "1126/1126 - 63s - loss: 3.7334 - accuracy: 0.5663 - val_loss: 3.4917 - val_accuracy: 0.5448 - 63s/epoch - 56ms/step\n",
      "Epoch 4/10000\n",
      "1126/1126 - 61s - loss: 3.4207 - accuracy: 0.5669 - val_loss: 3.4385 - val_accuracy: 0.5511 - 61s/epoch - 54ms/step\n",
      "Epoch 5/10000\n",
      "1126/1126 - 61s - loss: 3.1109 - accuracy: 0.5688 - val_loss: 3.0533 - val_accuracy: 0.5719 - 61s/epoch - 54ms/step\n",
      "Epoch 6/10000\n",
      "1126/1126 - 59s - loss: 2.8935 - accuracy: 0.5671 - val_loss: 2.7637 - val_accuracy: 0.5774 - 59s/epoch - 52ms/step\n",
      "Epoch 7/10000\n",
      "1126/1126 - 60s - loss: 2.5757 - accuracy: 0.5672 - val_loss: 2.4411 - val_accuracy: 0.5678 - 60s/epoch - 54ms/step\n",
      "Epoch 8/10000\n",
      "1126/1126 - 58s - loss: 2.3924 - accuracy: 0.5664 - val_loss: 2.1907 - val_accuracy: 0.5740 - 58s/epoch - 52ms/step\n",
      "Epoch 9/10000\n",
      "1126/1126 - 63s - loss: 2.2521 - accuracy: 0.5695 - val_loss: 2.1357 - val_accuracy: 0.5848 - 63s/epoch - 56ms/step\n",
      "Epoch 10/10000\n",
      "1126/1126 - 57s - loss: 2.0684 - accuracy: 0.5689 - val_loss: 2.1321 - val_accuracy: 0.5763 - 57s/epoch - 50ms/step\n",
      "Epoch 11/10000\n",
      "1126/1126 - 59s - loss: 1.9861 - accuracy: 0.5685 - val_loss: 1.9904 - val_accuracy: 0.5589 - 59s/epoch - 52ms/step\n",
      "391/391 [==============================] - 4s 10ms/step\n",
      "\n",
      "Iterazione 117/128 (03-11-2023_16-37-56)\n",
      "hidden_layer_size: 1000\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 3\n",
      "activation_hidden: sigmoid\n",
      "activation_output: sigmoid\n",
      "batch_size: 100\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 10\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 58.53%\n",
      "La recall di questo modello sul validation set è: 37.76%\n",
      "La f1 di questo modello sul validation set è: 41.34%\n",
      "La balanced accuracy di questo modello sul validation set è: 54.7%\n",
      "La precision di questo modello sul validation set è: 45.67%\n",
      "La AUC di questo modello sul validation set è: 57.31%\n",
      "391/391 [==============================] - 4s 11ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.65    0.72      0.68  7,670.00\n",
      "1                  0.46    0.38      0.41  4,841.00\n",
      "accuracy           0.59    0.59      0.59      0.59\n",
      "macro avg          0.55    0.55      0.55 12,511.00\n",
      "weighted avg       0.57    0.59      0.58 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         5495         2175\n",
      "Actual 1         3013         1828\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 66.14% con il modello:\n",
      "98 - AC66.14%, L1000, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA20, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_14-02-18).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 63.1% con il modello:\n",
      "98 - BA63.1%, L1000, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA20, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_14-02-18).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 66.75% con il modello:\n",
      "105 - PR66.75%, L1000, mV-10, MV10, P2, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_14-19-26).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "1126/1126 - 65s - loss: 5.3280 - accuracy: 0.5638 - val_loss: 4.4635 - val_accuracy: 0.5853 - 65s/epoch - 58ms/step\n",
      "Epoch 2/10000\n",
      "1126/1126 - 58s - loss: 4.2264 - accuracy: 0.5623 - val_loss: 3.8673 - val_accuracy: 0.5560 - 58s/epoch - 52ms/step\n",
      "Epoch 3/10000\n",
      "1126/1126 - 54s - loss: 3.7334 - accuracy: 0.5663 - val_loss: 3.4917 - val_accuracy: 0.5448 - 54s/epoch - 48ms/step\n",
      "Epoch 4/10000\n",
      "1126/1126 - 60s - loss: 3.4207 - accuracy: 0.5669 - val_loss: 3.4385 - val_accuracy: 0.5511 - 60s/epoch - 53ms/step\n",
      "Epoch 5/10000\n",
      "1126/1126 - 58s - loss: 3.1109 - accuracy: 0.5688 - val_loss: 3.0533 - val_accuracy: 0.5719 - 58s/epoch - 51ms/step\n",
      "Epoch 6/10000\n",
      "1126/1126 - 57s - loss: 2.8935 - accuracy: 0.5671 - val_loss: 2.7637 - val_accuracy: 0.5774 - 57s/epoch - 50ms/step\n",
      "Epoch 7/10000\n",
      "1126/1126 - 55s - loss: 2.5757 - accuracy: 0.5672 - val_loss: 2.4411 - val_accuracy: 0.5678 - 55s/epoch - 49ms/step\n",
      "Epoch 8/10000\n",
      "1126/1126 - 65s - loss: 2.3924 - accuracy: 0.5664 - val_loss: 2.1907 - val_accuracy: 0.5740 - 65s/epoch - 57ms/step\n",
      "Epoch 9/10000\n",
      "1126/1126 - 63s - loss: 2.2521 - accuracy: 0.5695 - val_loss: 2.1357 - val_accuracy: 0.5848 - 63s/epoch - 56ms/step\n",
      "Epoch 10/10000\n",
      "1126/1126 - 61s - loss: 2.0684 - accuracy: 0.5689 - val_loss: 2.1321 - val_accuracy: 0.5763 - 61s/epoch - 54ms/step\n",
      "Epoch 11/10000\n",
      "1126/1126 - 62s - loss: 1.9861 - accuracy: 0.5685 - val_loss: 1.9904 - val_accuracy: 0.5589 - 62s/epoch - 55ms/step\n",
      "Epoch 12/10000\n",
      "1126/1126 - 63s - loss: 1.8399 - accuracy: 0.5703 - val_loss: 1.7222 - val_accuracy: 0.5665 - 63s/epoch - 56ms/step\n",
      "Epoch 13/10000\n",
      "1126/1126 - 67s - loss: 1.7926 - accuracy: 0.5718 - val_loss: 1.6886 - val_accuracy: 0.5795 - 67s/epoch - 60ms/step\n",
      "Epoch 14/10000\n",
      "1126/1126 - 62s - loss: 1.7099 - accuracy: 0.5723 - val_loss: 1.5832 - val_accuracy: 0.5743 - 62s/epoch - 55ms/step\n",
      "Epoch 15/10000\n",
      "1126/1126 - 59s - loss: 1.5987 - accuracy: 0.5688 - val_loss: 1.4767 - val_accuracy: 0.5836 - 59s/epoch - 53ms/step\n",
      "Epoch 16/10000\n",
      "1126/1126 - 61s - loss: 1.4638 - accuracy: 0.5746 - val_loss: 1.4103 - val_accuracy: 0.5745 - 61s/epoch - 54ms/step\n",
      "Epoch 17/10000\n",
      "1126/1126 - 64s - loss: 1.4089 - accuracy: 0.5785 - val_loss: 1.3969 - val_accuracy: 0.5916 - 64s/epoch - 57ms/step\n",
      "Epoch 18/10000\n",
      "1126/1126 - 69s - loss: 1.3499 - accuracy: 0.5791 - val_loss: 1.3160 - val_accuracy: 0.5847 - 69s/epoch - 62ms/step\n",
      "Epoch 19/10000\n",
      "1126/1126 - 62s - loss: 1.3293 - accuracy: 0.5788 - val_loss: 1.3148 - val_accuracy: 0.5804 - 62s/epoch - 55ms/step\n",
      "Epoch 20/10000\n",
      "1126/1126 - 67s - loss: 1.2816 - accuracy: 0.5786 - val_loss: 1.2665 - val_accuracy: 0.5757 - 67s/epoch - 60ms/step\n",
      "Epoch 21/10000\n",
      "1126/1126 - 63s - loss: 1.2811 - accuracy: 0.5765 - val_loss: 1.2263 - val_accuracy: 0.5680 - 63s/epoch - 56ms/step\n",
      "Epoch 22/10000\n",
      "1126/1126 - 70s - loss: 1.2390 - accuracy: 0.5805 - val_loss: 1.1899 - val_accuracy: 0.5761 - 70s/epoch - 62ms/step\n",
      "Epoch 23/10000\n",
      "1126/1126 - 67s - loss: 1.1849 - accuracy: 0.5836 - val_loss: 1.2214 - val_accuracy: 0.5755 - 67s/epoch - 59ms/step\n",
      "Epoch 24/10000\n",
      "1126/1126 - 64s - loss: 1.1946 - accuracy: 0.5842 - val_loss: 1.1622 - val_accuracy: 0.5963 - 64s/epoch - 57ms/step\n",
      "Epoch 25/10000\n",
      "1126/1126 - 70s - loss: 1.1611 - accuracy: 0.5892 - val_loss: 1.1166 - val_accuracy: 0.5792 - 70s/epoch - 62ms/step\n",
      "Epoch 26/10000\n",
      "1126/1126 - 68s - loss: 1.1160 - accuracy: 0.5822 - val_loss: 1.0918 - val_accuracy: 0.5749 - 68s/epoch - 61ms/step\n",
      "Epoch 27/10000\n",
      "1126/1126 - 73s - loss: 1.0734 - accuracy: 0.5870 - val_loss: 1.0588 - val_accuracy: 0.5984 - 73s/epoch - 64ms/step\n",
      "Epoch 28/10000\n",
      "1126/1126 - 69s - loss: 1.0143 - accuracy: 0.5915 - val_loss: 0.9793 - val_accuracy: 0.5917 - 69s/epoch - 61ms/step\n",
      "Epoch 29/10000\n",
      "1126/1126 - 70s - loss: 0.9951 - accuracy: 0.5904 - val_loss: 0.9876 - val_accuracy: 0.5890 - 70s/epoch - 62ms/step\n",
      "Epoch 30/10000\n",
      "1126/1126 - 67s - loss: 0.9551 - accuracy: 0.5933 - val_loss: 0.9885 - val_accuracy: 0.5952 - 67s/epoch - 60ms/step\n",
      "Epoch 31/10000\n",
      "1126/1126 - 70s - loss: 0.9793 - accuracy: 0.5951 - val_loss: 1.0252 - val_accuracy: 0.5715 - 70s/epoch - 62ms/step\n",
      "Epoch 32/10000\n",
      "1126/1126 - 65s - loss: 0.9736 - accuracy: 0.5968 - val_loss: 0.9386 - val_accuracy: 0.6150 - 65s/epoch - 58ms/step\n",
      "Epoch 33/10000\n",
      "1126/1126 - 73s - loss: 0.9409 - accuracy: 0.5973 - val_loss: 0.9023 - val_accuracy: 0.5985 - 73s/epoch - 65ms/step\n",
      "Epoch 34/10000\n",
      "1126/1126 - 65s - loss: 0.9208 - accuracy: 0.6012 - val_loss: 0.8802 - val_accuracy: 0.6123 - 65s/epoch - 58ms/step\n",
      "Epoch 35/10000\n",
      "1126/1126 - 72s - loss: 0.9014 - accuracy: 0.6049 - val_loss: 0.9212 - val_accuracy: 0.5989 - 72s/epoch - 64ms/step\n",
      "Epoch 36/10000\n",
      "1126/1126 - 71s - loss: 0.9117 - accuracy: 0.6038 - val_loss: 0.9048 - val_accuracy: 0.6102 - 71s/epoch - 63ms/step\n",
      "Epoch 37/10000\n",
      "1126/1126 - 68s - loss: 0.9101 - accuracy: 0.6007 - val_loss: 0.9015 - val_accuracy: 0.5902 - 68s/epoch - 60ms/step\n",
      "Epoch 38/10000\n",
      "1126/1126 - 64s - loss: 0.8891 - accuracy: 0.6036 - val_loss: 0.9014 - val_accuracy: 0.5793 - 64s/epoch - 57ms/step\n",
      "Epoch 39/10000\n",
      "1126/1126 - 61s - loss: 0.8913 - accuracy: 0.6045 - val_loss: 0.8772 - val_accuracy: 0.6030 - 61s/epoch - 54ms/step\n",
      "Epoch 40/10000\n",
      "1126/1126 - 70s - loss: 0.8648 - accuracy: 0.6087 - val_loss: 0.8808 - val_accuracy: 0.6026 - 70s/epoch - 62ms/step\n",
      "Epoch 41/10000\n",
      "1126/1126 - 68s - loss: 0.8653 - accuracy: 0.6084 - val_loss: 0.8372 - val_accuracy: 0.6102 - 68s/epoch - 61ms/step\n",
      "Epoch 42/10000\n",
      "1126/1126 - 66s - loss: 0.8511 - accuracy: 0.6102 - val_loss: 0.8562 - val_accuracy: 0.6138 - 66s/epoch - 59ms/step\n",
      "Epoch 43/10000\n",
      "1126/1126 - 65s - loss: 0.8566 - accuracy: 0.6019 - val_loss: 0.8205 - val_accuracy: 0.6041 - 65s/epoch - 58ms/step\n",
      "Epoch 44/10000\n",
      "1126/1126 - 66s - loss: 0.8443 - accuracy: 0.6036 - val_loss: 0.8495 - val_accuracy: 0.6035 - 66s/epoch - 59ms/step\n",
      "Epoch 45/10000\n",
      "1126/1126 - 72s - loss: 0.8472 - accuracy: 0.6045 - val_loss: 0.8815 - val_accuracy: 0.6000 - 72s/epoch - 64ms/step\n",
      "Epoch 46/10000\n",
      "1126/1126 - 64s - loss: 0.8349 - accuracy: 0.6077 - val_loss: 0.8463 - val_accuracy: 0.5952 - 64s/epoch - 57ms/step\n",
      "Epoch 47/10000\n",
      "1126/1126 - 66s - loss: 0.8260 - accuracy: 0.6070 - val_loss: 0.8143 - val_accuracy: 0.6002 - 66s/epoch - 59ms/step\n",
      "Epoch 48/10000\n",
      "1126/1126 - 69s - loss: 0.8116 - accuracy: 0.6081 - val_loss: 0.7951 - val_accuracy: 0.6230 - 69s/epoch - 61ms/step\n",
      "Epoch 49/10000\n",
      "1126/1126 - 68s - loss: 0.8128 - accuracy: 0.6065 - val_loss: 0.8131 - val_accuracy: 0.6000 - 68s/epoch - 60ms/step\n",
      "Epoch 50/10000\n",
      "1126/1126 - 71s - loss: 0.8076 - accuracy: 0.6098 - val_loss: 0.8411 - val_accuracy: 0.6073 - 71s/epoch - 63ms/step\n",
      "Epoch 51/10000\n",
      "1126/1126 - 65s - loss: 0.7967 - accuracy: 0.6103 - val_loss: 0.8229 - val_accuracy: 0.6034 - 65s/epoch - 58ms/step\n",
      "Epoch 52/10000\n",
      "1126/1126 - 69s - loss: 0.8121 - accuracy: 0.6118 - val_loss: 0.8065 - val_accuracy: 0.6145 - 69s/epoch - 61ms/step\n",
      "Epoch 53/10000\n",
      "1126/1126 - 75s - loss: 0.8192 - accuracy: 0.6112 - val_loss: 0.8044 - val_accuracy: 0.6080 - 75s/epoch - 66ms/step\n",
      "Epoch 54/10000\n",
      "1126/1126 - 72s - loss: 0.7904 - accuracy: 0.6116 - val_loss: 0.7792 - val_accuracy: 0.6154 - 72s/epoch - 64ms/step\n",
      "Epoch 55/10000\n",
      "1126/1126 - 71s - loss: 0.7873 - accuracy: 0.6109 - val_loss: 0.7607 - val_accuracy: 0.6157 - 71s/epoch - 63ms/step\n",
      "Epoch 56/10000\n",
      "1126/1126 - 72s - loss: 0.7884 - accuracy: 0.6103 - val_loss: 0.7884 - val_accuracy: 0.6155 - 72s/epoch - 64ms/step\n",
      "Epoch 57/10000\n",
      "1126/1126 - 61s - loss: 0.7797 - accuracy: 0.6087 - val_loss: 0.7680 - val_accuracy: 0.6145 - 61s/epoch - 54ms/step\n",
      "Epoch 58/10000\n",
      "1126/1126 - 68s - loss: 0.7822 - accuracy: 0.6072 - val_loss: 0.7754 - val_accuracy: 0.6095 - 68s/epoch - 60ms/step\n",
      "Epoch 59/10000\n",
      "1126/1126 - 68s - loss: 0.7891 - accuracy: 0.6056 - val_loss: 0.8155 - val_accuracy: 0.6077 - 68s/epoch - 60ms/step\n",
      "Epoch 60/10000\n",
      "1126/1126 - 67s - loss: 0.7676 - accuracy: 0.6091 - val_loss: 0.7714 - val_accuracy: 0.5914 - 67s/epoch - 60ms/step\n",
      "Epoch 61/10000\n",
      "1126/1126 - 66s - loss: 0.7561 - accuracy: 0.6118 - val_loss: 0.7628 - val_accuracy: 0.6111 - 66s/epoch - 59ms/step\n",
      "Epoch 62/10000\n",
      "1126/1126 - 65s - loss: 0.7707 - accuracy: 0.6088 - val_loss: 0.7526 - val_accuracy: 0.6085 - 65s/epoch - 58ms/step\n",
      "Epoch 63/10000\n",
      "1126/1126 - 64s - loss: 0.7731 - accuracy: 0.6078 - val_loss: 0.7972 - val_accuracy: 0.6155 - 64s/epoch - 57ms/step\n",
      "Epoch 64/10000\n",
      "1126/1126 - 69s - loss: 0.7610 - accuracy: 0.6111 - val_loss: 0.7631 - val_accuracy: 0.6159 - 69s/epoch - 61ms/step\n",
      "Epoch 65/10000\n",
      "1126/1126 - 68s - loss: 0.7686 - accuracy: 0.6131 - val_loss: 0.7676 - val_accuracy: 0.6099 - 68s/epoch - 60ms/step\n",
      "Epoch 66/10000\n",
      "1126/1126 - 71s - loss: 0.7623 - accuracy: 0.6124 - val_loss: 0.7519 - val_accuracy: 0.6176 - 71s/epoch - 63ms/step\n",
      "Epoch 67/10000\n",
      "1126/1126 - 74s - loss: 0.7614 - accuracy: 0.6109 - val_loss: 0.7512 - val_accuracy: 0.6151 - 74s/epoch - 66ms/step\n",
      "Epoch 68/10000\n",
      "1126/1126 - 76s - loss: 0.7488 - accuracy: 0.6124 - val_loss: 0.7422 - val_accuracy: 0.6023 - 76s/epoch - 67ms/step\n",
      "391/391 [==============================] - 6s 14ms/step\n",
      "\n",
      "Iterazione 118/128 (03-11-2023_17-53-24)\n",
      "hidden_layer_size: 1000\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 3\n",
      "activation_hidden: sigmoid\n",
      "activation_output: sigmoid\n",
      "batch_size: 100\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 20\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 62.3%\n",
      "La recall di questo modello sul validation set è: 26.59%\n",
      "La f1 di questo modello sul validation set è: 35.3%\n",
      "La balanced accuracy di questo modello sul validation set è: 55.71%\n",
      "La precision di questo modello sul validation set è: 52.53%\n",
      "La AUC di questo modello sul validation set è: 61.14%\n",
      "391/391 [==============================] - 6s 14ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.65    0.85      0.73  7,670.00\n",
      "1                  0.53    0.27      0.35  4,841.00\n",
      "accuracy           0.62    0.62      0.62      0.62\n",
      "macro avg          0.59    0.56      0.54 12,511.00\n",
      "weighted avg       0.60    0.62      0.59 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         6507         1163\n",
      "Actual 1         3554         1287\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 66.14% con il modello:\n",
      "98 - AC66.14%, L1000, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA20, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_14-02-18).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 63.1% con il modello:\n",
      "98 - BA63.1%, L1000, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA20, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_14-02-18).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 66.75% con il modello:\n",
      "105 - PR66.75%, L1000, mV-10, MV10, P2, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_14-19-26).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "113/113 - 26s - loss: 5.7626 - accuracy: 0.5617 - val_loss: 3.9353 - val_accuracy: 0.5799 - 26s/epoch - 230ms/step\n",
      "Epoch 2/10000\n",
      "113/113 - 20s - loss: 3.6009 - accuracy: 0.5744 - val_loss: 3.3807 - val_accuracy: 0.5754 - 20s/epoch - 174ms/step\n",
      "Epoch 3/10000\n",
      "113/113 - 18s - loss: 3.2461 - accuracy: 0.5736 - val_loss: 3.3202 - val_accuracy: 0.5669 - 18s/epoch - 162ms/step\n",
      "Epoch 4/10000\n",
      "113/113 - 19s - loss: 3.1044 - accuracy: 0.5777 - val_loss: 3.1915 - val_accuracy: 0.5888 - 19s/epoch - 170ms/step\n",
      "Epoch 5/10000\n",
      "113/113 - 19s - loss: 2.9276 - accuracy: 0.5789 - val_loss: 3.0606 - val_accuracy: 0.5714 - 19s/epoch - 164ms/step\n",
      "Epoch 6/10000\n",
      "113/113 - 21s - loss: 2.8748 - accuracy: 0.5799 - val_loss: 2.9841 - val_accuracy: 0.5761 - 21s/epoch - 182ms/step\n",
      "Epoch 7/10000\n",
      "113/113 - 19s - loss: 2.7929 - accuracy: 0.5777 - val_loss: 2.8799 - val_accuracy: 0.5733 - 19s/epoch - 169ms/step\n",
      "Epoch 8/10000\n",
      "113/113 - 21s - loss: 2.7751 - accuracy: 0.5768 - val_loss: 3.0655 - val_accuracy: 0.5512 - 21s/epoch - 183ms/step\n",
      "Epoch 9/10000\n",
      "113/113 - 21s - loss: 2.7539 - accuracy: 0.5802 - val_loss: 2.7881 - val_accuracy: 0.5836 - 21s/epoch - 183ms/step\n",
      "Epoch 10/10000\n",
      "113/113 - 18s - loss: 2.6650 - accuracy: 0.5781 - val_loss: 2.7982 - val_accuracy: 0.5976 - 18s/epoch - 160ms/step\n",
      "Epoch 11/10000\n",
      "113/113 - 19s - loss: 2.6324 - accuracy: 0.5795 - val_loss: 2.7805 - val_accuracy: 0.5715 - 19s/epoch - 165ms/step\n",
      "Epoch 12/10000\n",
      "113/113 - 18s - loss: 2.6442 - accuracy: 0.5799 - val_loss: 2.8320 - val_accuracy: 0.5832 - 18s/epoch - 157ms/step\n",
      "Epoch 13/10000\n",
      "113/113 - 19s - loss: 2.6550 - accuracy: 0.5777 - val_loss: 2.7522 - val_accuracy: 0.5845 - 19s/epoch - 166ms/step\n",
      "Epoch 14/10000\n",
      "113/113 - 20s - loss: 2.5685 - accuracy: 0.5793 - val_loss: 2.6504 - val_accuracy: 0.5697 - 20s/epoch - 181ms/step\n",
      "Epoch 15/10000\n",
      "113/113 - 18s - loss: 2.5263 - accuracy: 0.5821 - val_loss: 2.5961 - val_accuracy: 0.5837 - 18s/epoch - 162ms/step\n",
      "Epoch 16/10000\n",
      "113/113 - 19s - loss: 2.4938 - accuracy: 0.5815 - val_loss: 2.6120 - val_accuracy: 0.5691 - 19s/epoch - 168ms/step\n",
      "Epoch 17/10000\n",
      "113/113 - 19s - loss: 2.5174 - accuracy: 0.5804 - val_loss: 2.6099 - val_accuracy: 0.5881 - 19s/epoch - 165ms/step\n",
      "Epoch 18/10000\n",
      "113/113 - 22s - loss: 2.5141 - accuracy: 0.5797 - val_loss: 2.6344 - val_accuracy: 0.5765 - 22s/epoch - 196ms/step\n",
      "Epoch 19/10000\n",
      "113/113 - 21s - loss: 2.5456 - accuracy: 0.5795 - val_loss: 2.6683 - val_accuracy: 0.5717 - 21s/epoch - 187ms/step\n",
      "Epoch 20/10000\n",
      "113/113 - 23s - loss: 2.4414 - accuracy: 0.5818 - val_loss: 2.5006 - val_accuracy: 0.5467 - 23s/epoch - 200ms/step\n",
      "391/391 [==============================] - 7s 16ms/step\n",
      "\n",
      "Iterazione 119/128 (03-11-2023_18-00-22)\n",
      "hidden_layer_size: 1000\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 3\n",
      "activation_hidden: sigmoid\n",
      "activation_output: sigmoid\n",
      "batch_size: 1000\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 10\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 59.76%\n",
      "La recall di questo modello sul validation set è: 30.63%\n",
      "La f1 di questo modello sul validation set è: 37.08%\n",
      "La balanced accuracy di questo modello sul validation set è: 54.39%\n",
      "La precision di questo modello sul validation set è: 46.95%\n",
      "La AUC di questo modello sul validation set è: 58.96%\n",
      "391/391 [==============================] - 6s 15ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.64    0.78      0.70  7,670.00\n",
      "1                  0.47    0.31      0.37  4,841.00\n",
      "accuracy           0.60    0.60      0.60      0.60\n",
      "macro avg          0.56    0.54      0.54 12,511.00\n",
      "weighted avg       0.57    0.60      0.58 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         5994         1676\n",
      "Actual 1         3358         1483\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 66.14% con il modello:\n",
      "98 - AC66.14%, L1000, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA20, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_14-02-18).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 63.1% con il modello:\n",
      "98 - BA63.1%, L1000, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA20, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_14-02-18).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 66.75% con il modello:\n",
      "105 - PR66.75%, L1000, mV-10, MV10, P2, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_14-19-26).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "113/113 - 30s - loss: 5.7626 - accuracy: 0.5617 - val_loss: 3.9353 - val_accuracy: 0.5799 - 30s/epoch - 268ms/step\n",
      "Epoch 2/10000\n",
      "113/113 - 22s - loss: 3.6009 - accuracy: 0.5744 - val_loss: 3.3807 - val_accuracy: 0.5754 - 22s/epoch - 197ms/step\n",
      "Epoch 3/10000\n",
      "113/113 - 21s - loss: 3.2461 - accuracy: 0.5736 - val_loss: 3.3202 - val_accuracy: 0.5669 - 21s/epoch - 185ms/step\n",
      "Epoch 4/10000\n",
      "113/113 - 20s - loss: 3.1044 - accuracy: 0.5777 - val_loss: 3.1915 - val_accuracy: 0.5888 - 20s/epoch - 179ms/step\n",
      "Epoch 5/10000\n",
      "113/113 - 23s - loss: 2.9276 - accuracy: 0.5789 - val_loss: 3.0606 - val_accuracy: 0.5714 - 23s/epoch - 208ms/step\n",
      "Epoch 6/10000\n",
      "113/113 - 21s - loss: 2.8748 - accuracy: 0.5799 - val_loss: 2.9841 - val_accuracy: 0.5761 - 21s/epoch - 190ms/step\n",
      "Epoch 7/10000\n",
      "113/113 - 22s - loss: 2.7929 - accuracy: 0.5777 - val_loss: 2.8799 - val_accuracy: 0.5733 - 22s/epoch - 192ms/step\n",
      "Epoch 8/10000\n",
      "113/113 - 22s - loss: 2.7751 - accuracy: 0.5768 - val_loss: 3.0655 - val_accuracy: 0.5512 - 22s/epoch - 193ms/step\n",
      "Epoch 9/10000\n",
      "113/113 - 22s - loss: 2.7539 - accuracy: 0.5802 - val_loss: 2.7881 - val_accuracy: 0.5836 - 22s/epoch - 192ms/step\n",
      "Epoch 10/10000\n",
      "113/113 - 21s - loss: 2.6650 - accuracy: 0.5781 - val_loss: 2.7982 - val_accuracy: 0.5976 - 21s/epoch - 183ms/step\n",
      "Epoch 11/10000\n",
      "113/113 - 24s - loss: 2.6324 - accuracy: 0.5795 - val_loss: 2.7805 - val_accuracy: 0.5715 - 24s/epoch - 211ms/step\n",
      "Epoch 12/10000\n",
      "113/113 - 18s - loss: 2.6442 - accuracy: 0.5799 - val_loss: 2.8320 - val_accuracy: 0.5832 - 18s/epoch - 161ms/step\n",
      "Epoch 13/10000\n",
      "113/113 - 19s - loss: 2.6550 - accuracy: 0.5777 - val_loss: 2.7522 - val_accuracy: 0.5845 - 19s/epoch - 168ms/step\n",
      "Epoch 14/10000\n",
      "113/113 - 19s - loss: 2.5685 - accuracy: 0.5793 - val_loss: 2.6504 - val_accuracy: 0.5697 - 19s/epoch - 164ms/step\n",
      "Epoch 15/10000\n",
      "113/113 - 21s - loss: 2.5263 - accuracy: 0.5821 - val_loss: 2.5961 - val_accuracy: 0.5837 - 21s/epoch - 185ms/step\n",
      "Epoch 16/10000\n",
      "113/113 - 24s - loss: 2.4938 - accuracy: 0.5815 - val_loss: 2.6120 - val_accuracy: 0.5691 - 24s/epoch - 214ms/step\n",
      "Epoch 17/10000\n",
      "113/113 - 21s - loss: 2.5174 - accuracy: 0.5804 - val_loss: 2.6099 - val_accuracy: 0.5881 - 21s/epoch - 187ms/step\n",
      "Epoch 18/10000\n",
      "113/113 - 26s - loss: 2.5141 - accuracy: 0.5797 - val_loss: 2.6344 - val_accuracy: 0.5765 - 26s/epoch - 228ms/step\n",
      "Epoch 19/10000\n",
      "113/113 - 19s - loss: 2.5456 - accuracy: 0.5795 - val_loss: 2.6683 - val_accuracy: 0.5717 - 19s/epoch - 171ms/step\n",
      "Epoch 20/10000\n",
      "113/113 - 20s - loss: 2.4414 - accuracy: 0.5818 - val_loss: 2.5006 - val_accuracy: 0.5467 - 20s/epoch - 177ms/step\n",
      "Epoch 21/10000\n",
      "113/113 - 19s - loss: 2.4324 - accuracy: 0.5784 - val_loss: 2.5047 - val_accuracy: 0.5883 - 19s/epoch - 164ms/step\n",
      "Epoch 22/10000\n",
      "113/113 - 19s - loss: 2.3904 - accuracy: 0.5848 - val_loss: 2.5418 - val_accuracy: 0.5734 - 19s/epoch - 172ms/step\n",
      "Epoch 23/10000\n",
      "113/113 - 19s - loss: 2.3459 - accuracy: 0.5821 - val_loss: 2.4655 - val_accuracy: 0.5617 - 19s/epoch - 165ms/step\n",
      "Epoch 24/10000\n",
      "113/113 - 18s - loss: 2.3635 - accuracy: 0.5821 - val_loss: 2.4545 - val_accuracy: 0.5917 - 18s/epoch - 157ms/step\n",
      "Epoch 25/10000\n",
      "113/113 - 18s - loss: 2.3152 - accuracy: 0.5838 - val_loss: 2.4551 - val_accuracy: 0.5908 - 18s/epoch - 156ms/step\n",
      "Epoch 26/10000\n",
      "113/113 - 18s - loss: 2.3285 - accuracy: 0.5841 - val_loss: 2.5300 - val_accuracy: 0.5657 - 18s/epoch - 163ms/step\n",
      "Epoch 27/10000\n",
      "113/113 - 18s - loss: 2.3025 - accuracy: 0.5805 - val_loss: 2.3599 - val_accuracy: 0.5620 - 18s/epoch - 158ms/step\n",
      "Epoch 28/10000\n",
      "113/113 - 18s - loss: 2.2632 - accuracy: 0.5830 - val_loss: 2.3534 - val_accuracy: 0.5665 - 18s/epoch - 163ms/step\n",
      "Epoch 29/10000\n",
      "113/113 - 20s - loss: 2.2851 - accuracy: 0.5810 - val_loss: 2.3822 - val_accuracy: 0.5811 - 20s/epoch - 175ms/step\n",
      "Epoch 30/10000\n",
      "113/113 - 19s - loss: 2.2286 - accuracy: 0.5827 - val_loss: 2.4609 - val_accuracy: 0.5729 - 19s/epoch - 170ms/step\n",
      "391/391 [==============================] - 7s 16ms/step\n",
      "\n",
      "Iterazione 120/128 (03-11-2023_18-11-02)\n",
      "hidden_layer_size: 1000\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 3\n",
      "activation_hidden: sigmoid\n",
      "activation_output: sigmoid\n",
      "batch_size: 1000\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 20\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 59.76%\n",
      "La recall di questo modello sul validation set è: 30.63%\n",
      "La f1 di questo modello sul validation set è: 37.08%\n",
      "La balanced accuracy di questo modello sul validation set è: 54.39%\n",
      "La precision di questo modello sul validation set è: 46.95%\n",
      "La AUC di questo modello sul validation set è: 58.96%\n",
      "391/391 [==============================] - 5s 13ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.64    0.78      0.70  7,670.00\n",
      "1                  0.47    0.31      0.37  4,841.00\n",
      "accuracy           0.60    0.60      0.60      0.60\n",
      "macro avg          0.56    0.54      0.54 12,511.00\n",
      "weighted avg       0.57    0.60      0.58 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         5994         1676\n",
      "Actual 1         3358         1483\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 66.14% con il modello:\n",
      "98 - AC66.14%, L1000, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA20, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_14-02-18).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 63.1% con il modello:\n",
      "98 - BA63.1%, L1000, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA20, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_14-02-18).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 66.75% con il modello:\n",
      "105 - PR66.75%, L1000, mV-10, MV10, P2, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_14-19-26).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "1126/1126 - 123s - loss: 88188485632.0000 - accuracy: 0.5700 - val_loss: 252961783808.0000 - val_accuracy: 0.6148 - 123s/epoch - 110ms/step\n",
      "Epoch 2/10000\n",
      "1126/1126 - 116s - loss: 74176757760.0000 - accuracy: 0.5708 - val_loss: 120514445312.0000 - val_accuracy: 0.4162 - 116s/epoch - 103ms/step\n",
      "Epoch 3/10000\n",
      "1126/1126 - 117s - loss: 69512855552.0000 - accuracy: 0.5722 - val_loss: 46077431808.0000 - val_accuracy: 0.6274 - 117s/epoch - 104ms/step\n",
      "Epoch 4/10000\n",
      "1126/1126 - 108s - loss: 74315292672.0000 - accuracy: 0.5723 - val_loss: 36818882560.0000 - val_accuracy: 0.5432 - 108s/epoch - 96ms/step\n",
      "Epoch 5/10000\n",
      "1126/1126 - 105s - loss: 67756613632.0000 - accuracy: 0.5744 - val_loss: 16768441344.0000 - val_accuracy: 0.6210 - 105s/epoch - 93ms/step\n",
      "Epoch 6/10000\n",
      "1126/1126 - 104s - loss: 86060687360.0000 - accuracy: 0.5726 - val_loss: 71598170112.0000 - val_accuracy: 0.6173 - 104s/epoch - 93ms/step\n",
      "Epoch 7/10000\n",
      "1126/1126 - 107s - loss: 66826055680.0000 - accuracy: 0.5759 - val_loss: 126089879552.0000 - val_accuracy: 0.6148 - 107s/epoch - 95ms/step\n",
      "Epoch 8/10000\n",
      "1126/1126 - 106s - loss: 64624181248.0000 - accuracy: 0.5751 - val_loss: 187954626560.0000 - val_accuracy: 0.6132 - 106s/epoch - 94ms/step\n",
      "Epoch 9/10000\n",
      "1126/1126 - 107s - loss: 71578419200.0000 - accuracy: 0.5766 - val_loss: 24784291840.0000 - val_accuracy: 0.6410 - 107s/epoch - 95ms/step\n",
      "Epoch 10/10000\n",
      "1126/1126 - 105s - loss: 68669079552.0000 - accuracy: 0.5758 - val_loss: 20208400384.0000 - val_accuracy: 0.6509 - 105s/epoch - 93ms/step\n",
      "Epoch 11/10000\n",
      "1126/1126 - 125s - loss: 62719729664.0000 - accuracy: 0.5760 - val_loss: 59084390400.0000 - val_accuracy: 0.6315 - 125s/epoch - 111ms/step\n",
      "Epoch 12/10000\n",
      "1126/1126 - 114s - loss: 60505657344.0000 - accuracy: 0.5746 - val_loss: 63831080960.0000 - val_accuracy: 0.5613 - 114s/epoch - 101ms/step\n",
      "Epoch 13/10000\n",
      "1126/1126 - 106s - loss: 65869824000.0000 - accuracy: 0.5753 - val_loss: 31637196800.0000 - val_accuracy: 0.6505 - 106s/epoch - 95ms/step\n",
      "Epoch 14/10000\n",
      "1126/1126 - 108s - loss: 65800687616.0000 - accuracy: 0.5753 - val_loss: 26945241088.0000 - val_accuracy: 0.5736 - 108s/epoch - 96ms/step\n",
      "Epoch 15/10000\n",
      "1126/1126 - 121s - loss: 59713683456.0000 - accuracy: 0.5758 - val_loss: 59283668992.0000 - val_accuracy: 0.6234 - 121s/epoch - 107ms/step\n",
      "Epoch 16/10000\n",
      "1126/1126 - 125s - loss: 61382742016.0000 - accuracy: 0.5768 - val_loss: 19780665344.0000 - val_accuracy: 0.6223 - 125s/epoch - 111ms/step\n",
      "Epoch 17/10000\n",
      "1126/1126 - 122s - loss: 57488871424.0000 - accuracy: 0.5775 - val_loss: 24855490560.0000 - val_accuracy: 0.5651 - 122s/epoch - 109ms/step\n",
      "Epoch 18/10000\n",
      "1126/1126 - 129s - loss: 58979594240.0000 - accuracy: 0.5776 - val_loss: 18782638080.0000 - val_accuracy: 0.5926 - 129s/epoch - 114ms/step\n",
      "Epoch 19/10000\n",
      "1126/1126 - 119s - loss: 59426811904.0000 - accuracy: 0.5752 - val_loss: 45401546752.0000 - val_accuracy: 0.5014 - 119s/epoch - 105ms/step\n",
      "Epoch 20/10000\n",
      "1126/1126 - 119s - loss: 56604241920.0000 - accuracy: 0.5743 - val_loss: 81907597312.0000 - val_accuracy: 0.6138 - 119s/epoch - 105ms/step\n",
      "391/391 [==============================] - 9s 20ms/step\n",
      "\n",
      "Iterazione 121/128 (03-11-2023_18-49-32)\n",
      "hidden_layer_size: 1000\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 4\n",
      "activation_hidden: relu\n",
      "activation_output: sigmoid\n",
      "batch_size: 100\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 10\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 65.09%\n",
      "La recall di questo modello sul validation set è: 27.47%\n",
      "La f1 di questo modello sul validation set è: 37.85%\n",
      "La balanced accuracy di questo modello sul validation set è: 58.16%\n",
      "La precision di questo modello sul validation set è: 60.84%\n",
      "La AUC di questo modello sul validation set è: 58.16%\n",
      "391/391 [==============================] - 9s 22ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.66    0.89      0.76  7,670.00\n",
      "1                  0.61    0.27      0.38  4,841.00\n",
      "accuracy           0.65    0.65      0.65      0.65\n",
      "macro avg          0.63    0.58      0.57 12,511.00\n",
      "weighted avg       0.64    0.65      0.61 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         6814          856\n",
      "Actual 1         3511         1330\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 66.14% con il modello:\n",
      "98 - AC66.14%, L1000, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA20, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_14-02-18).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 63.1% con il modello:\n",
      "98 - BA63.1%, L1000, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA20, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_14-02-18).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 66.75% con il modello:\n",
      "105 - PR66.75%, L1000, mV-10, MV10, P2, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_14-19-26).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "1126/1126 - 151s - loss: 88188485632.0000 - accuracy: 0.5700 - val_loss: 252961783808.0000 - val_accuracy: 0.6148 - 151s/epoch - 134ms/step\n",
      "Epoch 2/10000\n",
      "1126/1126 - 124s - loss: 74176757760.0000 - accuracy: 0.5708 - val_loss: 120514445312.0000 - val_accuracy: 0.4162 - 124s/epoch - 111ms/step\n",
      "Epoch 3/10000\n",
      "1126/1126 - 125s - loss: 69512855552.0000 - accuracy: 0.5722 - val_loss: 46077431808.0000 - val_accuracy: 0.6274 - 125s/epoch - 111ms/step\n",
      "Epoch 4/10000\n",
      "1126/1126 - 128s - loss: 74315292672.0000 - accuracy: 0.5723 - val_loss: 36818882560.0000 - val_accuracy: 0.5432 - 128s/epoch - 113ms/step\n",
      "Epoch 5/10000\n",
      "1126/1126 - 123s - loss: 67756613632.0000 - accuracy: 0.5744 - val_loss: 16768441344.0000 - val_accuracy: 0.6210 - 123s/epoch - 109ms/step\n",
      "Epoch 6/10000\n",
      "1126/1126 - 117s - loss: 86060687360.0000 - accuracy: 0.5726 - val_loss: 71598170112.0000 - val_accuracy: 0.6173 - 117s/epoch - 104ms/step\n",
      "Epoch 7/10000\n",
      "1126/1126 - 122s - loss: 66826055680.0000 - accuracy: 0.5759 - val_loss: 126089879552.0000 - val_accuracy: 0.6148 - 122s/epoch - 109ms/step\n",
      "Epoch 8/10000\n",
      "1126/1126 - 123s - loss: 64624181248.0000 - accuracy: 0.5751 - val_loss: 187954626560.0000 - val_accuracy: 0.6132 - 123s/epoch - 109ms/step\n",
      "Epoch 9/10000\n",
      "1126/1126 - 126s - loss: 71578419200.0000 - accuracy: 0.5766 - val_loss: 24784291840.0000 - val_accuracy: 0.6410 - 126s/epoch - 112ms/step\n",
      "Epoch 10/10000\n",
      "1126/1126 - 119s - loss: 68669079552.0000 - accuracy: 0.5758 - val_loss: 20208400384.0000 - val_accuracy: 0.6509 - 119s/epoch - 106ms/step\n",
      "Epoch 11/10000\n",
      "1126/1126 - 124s - loss: 62719729664.0000 - accuracy: 0.5760 - val_loss: 59084390400.0000 - val_accuracy: 0.6315 - 124s/epoch - 110ms/step\n",
      "Epoch 12/10000\n",
      "1126/1126 - 121s - loss: 60505657344.0000 - accuracy: 0.5746 - val_loss: 63831080960.0000 - val_accuracy: 0.5613 - 121s/epoch - 107ms/step\n",
      "Epoch 13/10000\n",
      "1126/1126 - 114s - loss: 65869824000.0000 - accuracy: 0.5753 - val_loss: 31637196800.0000 - val_accuracy: 0.6505 - 114s/epoch - 102ms/step\n",
      "Epoch 14/10000\n",
      "1126/1126 - 137s - loss: 65800687616.0000 - accuracy: 0.5753 - val_loss: 26945241088.0000 - val_accuracy: 0.5736 - 137s/epoch - 122ms/step\n",
      "Epoch 15/10000\n",
      "1126/1126 - 127s - loss: 59713683456.0000 - accuracy: 0.5758 - val_loss: 59283668992.0000 - val_accuracy: 0.6234 - 127s/epoch - 113ms/step\n",
      "Epoch 16/10000\n",
      "1126/1126 - 129s - loss: 61382742016.0000 - accuracy: 0.5768 - val_loss: 19780665344.0000 - val_accuracy: 0.6223 - 129s/epoch - 115ms/step\n",
      "Epoch 17/10000\n",
      "1126/1126 - 131s - loss: 57488871424.0000 - accuracy: 0.5775 - val_loss: 24855490560.0000 - val_accuracy: 0.5651 - 131s/epoch - 117ms/step\n",
      "Epoch 18/10000\n",
      "1126/1126 - 123s - loss: 58979594240.0000 - accuracy: 0.5776 - val_loss: 18782638080.0000 - val_accuracy: 0.5926 - 123s/epoch - 109ms/step\n",
      "Epoch 19/10000\n",
      "1126/1126 - 117s - loss: 59426811904.0000 - accuracy: 0.5752 - val_loss: 45401546752.0000 - val_accuracy: 0.5014 - 117s/epoch - 104ms/step\n",
      "Epoch 20/10000\n",
      "1126/1126 - 115s - loss: 56604241920.0000 - accuracy: 0.5743 - val_loss: 81907597312.0000 - val_accuracy: 0.6138 - 115s/epoch - 102ms/step\n",
      "Epoch 21/10000\n",
      "1126/1126 - 116s - loss: 57884917760.0000 - accuracy: 0.5778 - val_loss: 51120611328.0000 - val_accuracy: 0.6260 - 116s/epoch - 103ms/step\n",
      "Epoch 22/10000\n",
      "1126/1126 - 120s - loss: 53652791296.0000 - accuracy: 0.5796 - val_loss: 49203752960.0000 - val_accuracy: 0.6233 - 120s/epoch - 107ms/step\n",
      "Epoch 23/10000\n",
      "1126/1126 - 116s - loss: 56484962304.0000 - accuracy: 0.5768 - val_loss: 200638349312.0000 - val_accuracy: 0.6179 - 116s/epoch - 103ms/step\n",
      "Epoch 24/10000\n",
      "1126/1126 - 119s - loss: 51125645312.0000 - accuracy: 0.5786 - val_loss: 54289805312.0000 - val_accuracy: 0.5016 - 119s/epoch - 105ms/step\n",
      "Epoch 25/10000\n",
      "1126/1126 - 117s - loss: 49495638016.0000 - accuracy: 0.5777 - val_loss: 59522318336.0000 - val_accuracy: 0.6215 - 117s/epoch - 104ms/step\n",
      "Epoch 26/10000\n",
      "1126/1126 - 115s - loss: 50674171904.0000 - accuracy: 0.5790 - val_loss: 21809035264.0000 - val_accuracy: 0.6421 - 115s/epoch - 102ms/step\n",
      "Epoch 27/10000\n",
      "1126/1126 - 118s - loss: 50487230464.0000 - accuracy: 0.5788 - val_loss: 22031931392.0000 - val_accuracy: 0.6357 - 118s/epoch - 105ms/step\n",
      "Epoch 28/10000\n",
      "1126/1126 - 118s - loss: 46013480960.0000 - accuracy: 0.5799 - val_loss: 124159328256.0000 - val_accuracy: 0.6133 - 118s/epoch - 105ms/step\n",
      "Epoch 29/10000\n",
      "1126/1126 - 113s - loss: 48422539264.0000 - accuracy: 0.5806 - val_loss: 36400123904.0000 - val_accuracy: 0.6407 - 113s/epoch - 101ms/step\n",
      "Epoch 30/10000\n",
      "1126/1126 - 118s - loss: 48354861056.0000 - accuracy: 0.5786 - val_loss: 23027417088.0000 - val_accuracy: 0.5316 - 118s/epoch - 104ms/step\n",
      "391/391 [==============================] - 8s 18ms/step\n",
      "\n",
      "Iterazione 122/128 (03-11-2023_19-51-04)\n",
      "hidden_layer_size: 1000\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 4\n",
      "activation_hidden: relu\n",
      "activation_output: sigmoid\n",
      "batch_size: 100\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 20\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 65.09%\n",
      "La recall di questo modello sul validation set è: 27.47%\n",
      "La f1 di questo modello sul validation set è: 37.85%\n",
      "La balanced accuracy di questo modello sul validation set è: 58.16%\n",
      "La precision di questo modello sul validation set è: 60.84%\n",
      "La AUC di questo modello sul validation set è: 58.16%\n",
      "391/391 [==============================] - 7s 17ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.66    0.89      0.76  7,670.00\n",
      "1                  0.61    0.27      0.38  4,841.00\n",
      "accuracy           0.65    0.65      0.65      0.65\n",
      "macro avg          0.63    0.58      0.57 12,511.00\n",
      "weighted avg       0.64    0.65      0.61 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         6814          856\n",
      "Actual 1         3511         1330\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 66.14% con il modello:\n",
      "98 - AC66.14%, L1000, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA20, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_14-02-18).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 63.1% con il modello:\n",
      "98 - BA63.1%, L1000, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA20, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_14-02-18).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 66.75% con il modello:\n",
      "105 - PR66.75%, L1000, mV-10, MV10, P2, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_14-19-26).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "113/113 - 33s - loss: 120271151104.0000 - accuracy: 0.5618 - val_loss: 108867674112.0000 - val_accuracy: 0.4584 - 33s/epoch - 293ms/step\n",
      "Epoch 2/10000\n",
      "113/113 - 28s - loss: 73741492224.0000 - accuracy: 0.5689 - val_loss: 61295947776.0000 - val_accuracy: 0.6260 - 28s/epoch - 246ms/step\n",
      "Epoch 3/10000\n",
      "113/113 - 26s - loss: 63206285312.0000 - accuracy: 0.5732 - val_loss: 56688082944.0000 - val_accuracy: 0.4758 - 26s/epoch - 232ms/step\n",
      "Epoch 4/10000\n",
      "113/113 - 29s - loss: 72667299840.0000 - accuracy: 0.5682 - val_loss: 34428358656.0000 - val_accuracy: 0.6289 - 29s/epoch - 259ms/step\n",
      "Epoch 5/10000\n",
      "113/113 - 26s - loss: 53943697408.0000 - accuracy: 0.5751 - val_loss: 14965233664.0000 - val_accuracy: 0.6376 - 26s/epoch - 234ms/step\n",
      "Epoch 6/10000\n",
      "113/113 - 27s - loss: 72641839104.0000 - accuracy: 0.5687 - val_loss: 52249677824.0000 - val_accuracy: 0.5151 - 27s/epoch - 238ms/step\n",
      "Epoch 7/10000\n",
      "113/113 - 29s - loss: 54420140032.0000 - accuracy: 0.5726 - val_loss: 98068545536.0000 - val_accuracy: 0.6173 - 29s/epoch - 253ms/step\n",
      "Epoch 8/10000\n",
      "113/113 - 27s - loss: 65426763776.0000 - accuracy: 0.5768 - val_loss: 78695071744.0000 - val_accuracy: 0.6135 - 27s/epoch - 241ms/step\n",
      "Epoch 9/10000\n",
      "113/113 - 30s - loss: 66232090624.0000 - accuracy: 0.5703 - val_loss: 20727236608.0000 - val_accuracy: 0.6374 - 30s/epoch - 265ms/step\n",
      "Epoch 10/10000\n",
      "113/113 - 31s - loss: 52319289344.0000 - accuracy: 0.5760 - val_loss: 22524403712.0000 - val_accuracy: 0.6393 - 31s/epoch - 270ms/step\n",
      "Epoch 11/10000\n",
      "113/113 - 31s - loss: 58366107648.0000 - accuracy: 0.5749 - val_loss: 47313235968.0000 - val_accuracy: 0.6143 - 31s/epoch - 277ms/step\n",
      "Epoch 12/10000\n",
      "113/113 - 38s - loss: 98184388608.0000 - accuracy: 0.5675 - val_loss: 45869846528.0000 - val_accuracy: 0.6372 - 38s/epoch - 333ms/step\n",
      "Epoch 13/10000\n",
      "113/113 - 30s - loss: 63578984448.0000 - accuracy: 0.5756 - val_loss: 158711808000.0000 - val_accuracy: 0.4044 - 30s/epoch - 268ms/step\n",
      "Epoch 14/10000\n",
      "113/113 - 31s - loss: 83819642880.0000 - accuracy: 0.5688 - val_loss: 45348761600.0000 - val_accuracy: 0.5656 - 31s/epoch - 275ms/step\n",
      "Epoch 15/10000\n",
      "113/113 - 29s - loss: 56886931456.0000 - accuracy: 0.5757 - val_loss: 41236197376.0000 - val_accuracy: 0.6192 - 29s/epoch - 261ms/step\n",
      "Epoch 16/10000\n",
      "113/113 - 30s - loss: 54600527872.0000 - accuracy: 0.5788 - val_loss: 37675016192.0000 - val_accuracy: 0.6210 - 30s/epoch - 267ms/step\n",
      "Epoch 17/10000\n",
      "113/113 - 30s - loss: 56322834432.0000 - accuracy: 0.5760 - val_loss: 16785423360.0000 - val_accuracy: 0.5540 - 30s/epoch - 270ms/step\n",
      "Epoch 18/10000\n",
      "113/113 - 30s - loss: 82238005248.0000 - accuracy: 0.5683 - val_loss: 59303510016.0000 - val_accuracy: 0.6250 - 30s/epoch - 268ms/step\n",
      "Epoch 19/10000\n",
      "113/113 - 27s - loss: 62922084352.0000 - accuracy: 0.5742 - val_loss: 84330881024.0000 - val_accuracy: 0.4154 - 27s/epoch - 235ms/step\n",
      "Epoch 20/10000\n",
      "113/113 - 28s - loss: 53680021504.0000 - accuracy: 0.5775 - val_loss: 73415524352.0000 - val_accuracy: 0.6205 - 28s/epoch - 248ms/step\n",
      "391/391 [==============================] - 8s 18ms/step\n",
      "\n",
      "Iterazione 123/128 (03-11-2023_20-01-15)\n",
      "hidden_layer_size: 1000\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 4\n",
      "activation_hidden: relu\n",
      "activation_output: sigmoid\n",
      "batch_size: 1000\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 10\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 63.93%\n",
      "La recall di questo modello sul validation set è: 21.59%\n",
      "La f1 di questo modello sul validation set è: 31.65%\n",
      "La balanced accuracy di questo modello sul validation set è: 56.12%\n",
      "La precision di questo modello sul validation set è: 59.31%\n",
      "La AUC di questo modello sul validation set è: 56.12%\n",
      "391/391 [==============================] - 6s 16ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.65    0.91      0.75  7,670.00\n",
      "1                  0.59    0.22      0.32  4,841.00\n",
      "accuracy           0.64    0.64      0.64      0.64\n",
      "macro avg          0.62    0.56      0.54 12,511.00\n",
      "weighted avg       0.63    0.64      0.59 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         6953          717\n",
      "Actual 1         3796         1045\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 66.14% con il modello:\n",
      "98 - AC66.14%, L1000, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA20, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_14-02-18).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 63.1% con il modello:\n",
      "98 - BA63.1%, L1000, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA20, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_14-02-18).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 66.75% con il modello:\n",
      "105 - PR66.75%, L1000, mV-10, MV10, P2, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_14-19-26).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "113/113 - 42s - loss: 120271151104.0000 - accuracy: 0.5618 - val_loss: 108867674112.0000 - val_accuracy: 0.4584 - 42s/epoch - 368ms/step\n",
      "Epoch 2/10000\n",
      "113/113 - 31s - loss: 73741492224.0000 - accuracy: 0.5689 - val_loss: 61295947776.0000 - val_accuracy: 0.6260 - 31s/epoch - 278ms/step\n",
      "Epoch 3/10000\n",
      "113/113 - 32s - loss: 63206285312.0000 - accuracy: 0.5732 - val_loss: 56688082944.0000 - val_accuracy: 0.4758 - 32s/epoch - 285ms/step\n",
      "Epoch 4/10000\n",
      "113/113 - 31s - loss: 72667299840.0000 - accuracy: 0.5682 - val_loss: 34428358656.0000 - val_accuracy: 0.6289 - 31s/epoch - 271ms/step\n",
      "Epoch 5/10000\n",
      "113/113 - 32s - loss: 53943697408.0000 - accuracy: 0.5751 - val_loss: 14965233664.0000 - val_accuracy: 0.6376 - 32s/epoch - 285ms/step\n",
      "Epoch 6/10000\n",
      "113/113 - 31s - loss: 72641839104.0000 - accuracy: 0.5687 - val_loss: 52249677824.0000 - val_accuracy: 0.5151 - 31s/epoch - 272ms/step\n",
      "Epoch 7/10000\n",
      "113/113 - 31s - loss: 54420140032.0000 - accuracy: 0.5726 - val_loss: 98068545536.0000 - val_accuracy: 0.6173 - 31s/epoch - 274ms/step\n",
      "Epoch 8/10000\n",
      "113/113 - 31s - loss: 65426763776.0000 - accuracy: 0.5768 - val_loss: 78695071744.0000 - val_accuracy: 0.6135 - 31s/epoch - 278ms/step\n",
      "Epoch 9/10000\n",
      "113/113 - 31s - loss: 66232090624.0000 - accuracy: 0.5703 - val_loss: 20727236608.0000 - val_accuracy: 0.6374 - 31s/epoch - 272ms/step\n",
      "Epoch 10/10000\n",
      "113/113 - 30s - loss: 52319289344.0000 - accuracy: 0.5760 - val_loss: 22524403712.0000 - val_accuracy: 0.6393 - 30s/epoch - 263ms/step\n",
      "Epoch 11/10000\n",
      "113/113 - 30s - loss: 58366107648.0000 - accuracy: 0.5749 - val_loss: 47313235968.0000 - val_accuracy: 0.6143 - 30s/epoch - 262ms/step\n",
      "Epoch 12/10000\n",
      "113/113 - 28s - loss: 98184388608.0000 - accuracy: 0.5675 - val_loss: 45869846528.0000 - val_accuracy: 0.6372 - 28s/epoch - 248ms/step\n",
      "Epoch 13/10000\n",
      "113/113 - 29s - loss: 63578984448.0000 - accuracy: 0.5756 - val_loss: 158711808000.0000 - val_accuracy: 0.4044 - 29s/epoch - 253ms/step\n",
      "Epoch 14/10000\n",
      "113/113 - 29s - loss: 83819642880.0000 - accuracy: 0.5688 - val_loss: 45348761600.0000 - val_accuracy: 0.5656 - 29s/epoch - 261ms/step\n",
      "Epoch 15/10000\n",
      "113/113 - 30s - loss: 56886931456.0000 - accuracy: 0.5757 - val_loss: 41236197376.0000 - val_accuracy: 0.6192 - 30s/epoch - 264ms/step\n",
      "Epoch 16/10000\n",
      "113/113 - 30s - loss: 54600527872.0000 - accuracy: 0.5788 - val_loss: 37675016192.0000 - val_accuracy: 0.6210 - 30s/epoch - 267ms/step\n",
      "Epoch 17/10000\n",
      "113/113 - 30s - loss: 56322834432.0000 - accuracy: 0.5760 - val_loss: 16785423360.0000 - val_accuracy: 0.5540 - 30s/epoch - 266ms/step\n",
      "Epoch 18/10000\n",
      "113/113 - 29s - loss: 82238005248.0000 - accuracy: 0.5683 - val_loss: 59303510016.0000 - val_accuracy: 0.6250 - 29s/epoch - 253ms/step\n",
      "Epoch 19/10000\n",
      "113/113 - 27s - loss: 62922084352.0000 - accuracy: 0.5742 - val_loss: 84330881024.0000 - val_accuracy: 0.4154 - 27s/epoch - 240ms/step\n",
      "Epoch 20/10000\n",
      "113/113 - 27s - loss: 53680021504.0000 - accuracy: 0.5775 - val_loss: 73415524352.0000 - val_accuracy: 0.6205 - 27s/epoch - 243ms/step\n",
      "Epoch 21/10000\n",
      "113/113 - 28s - loss: 53863084032.0000 - accuracy: 0.5747 - val_loss: 27292792832.0000 - val_accuracy: 0.6426 - 28s/epoch - 248ms/step\n",
      "Epoch 22/10000\n",
      "113/113 - 27s - loss: 71437066240.0000 - accuracy: 0.5740 - val_loss: 39042445312.0000 - val_accuracy: 0.6350 - 27s/epoch - 242ms/step\n",
      "Epoch 23/10000\n",
      "113/113 - 28s - loss: 54992908288.0000 - accuracy: 0.5735 - val_loss: 29848309760.0000 - val_accuracy: 0.6358 - 28s/epoch - 252ms/step\n",
      "Epoch 24/10000\n",
      "113/113 - 28s - loss: 70514155520.0000 - accuracy: 0.5757 - val_loss: 59131359232.0000 - val_accuracy: 0.6155 - 28s/epoch - 251ms/step\n",
      "Epoch 25/10000\n",
      "113/113 - 28s - loss: 65504927744.0000 - accuracy: 0.5761 - val_loss: 148834680832.0000 - val_accuracy: 0.6131 - 28s/epoch - 244ms/step\n",
      "Epoch 26/10000\n",
      "113/113 - 28s - loss: 78725005312.0000 - accuracy: 0.5706 - val_loss: 69521547264.0000 - val_accuracy: 0.5789 - 28s/epoch - 247ms/step\n",
      "Epoch 27/10000\n",
      "113/113 - 28s - loss: 66260336640.0000 - accuracy: 0.5767 - val_loss: 37482659840.0000 - val_accuracy: 0.5039 - 28s/epoch - 249ms/step\n",
      "Epoch 28/10000\n",
      "113/113 - 28s - loss: 69819383808.0000 - accuracy: 0.5750 - val_loss: 23532544000.0000 - val_accuracy: 0.6415 - 28s/epoch - 250ms/step\n",
      "Epoch 29/10000\n",
      "113/113 - 29s - loss: 55687225344.0000 - accuracy: 0.5779 - val_loss: 54704652288.0000 - val_accuracy: 0.4486 - 29s/epoch - 255ms/step\n",
      "Epoch 30/10000\n",
      "113/113 - 29s - loss: 61224169472.0000 - accuracy: 0.5700 - val_loss: 35932684288.0000 - val_accuracy: 0.6347 - 29s/epoch - 261ms/step\n",
      "Epoch 31/10000\n",
      "113/113 - 29s - loss: 58019434496.0000 - accuracy: 0.5734 - val_loss: 151101489152.0000 - val_accuracy: 0.4311 - 29s/epoch - 257ms/step\n",
      "Epoch 32/10000\n",
      "113/113 - 28s - loss: 86915334144.0000 - accuracy: 0.5734 - val_loss: 68412317696.0000 - val_accuracy: 0.4705 - 28s/epoch - 250ms/step\n",
      "Epoch 33/10000\n",
      "113/113 - 28s - loss: 77624098816.0000 - accuracy: 0.5764 - val_loss: 18111426560.0000 - val_accuracy: 0.6106 - 28s/epoch - 247ms/step\n",
      "Epoch 34/10000\n",
      "113/113 - 27s - loss: 61576421376.0000 - accuracy: 0.5739 - val_loss: 77730422784.0000 - val_accuracy: 0.4631 - 27s/epoch - 243ms/step\n",
      "Epoch 35/10000\n",
      "113/113 - 37s - loss: 74685628416.0000 - accuracy: 0.5703 - val_loss: 44978466816.0000 - val_accuracy: 0.6336 - 37s/epoch - 330ms/step\n",
      "Epoch 36/10000\n",
      "113/113 - 37s - loss: 66357673984.0000 - accuracy: 0.5763 - val_loss: 60082302976.0000 - val_accuracy: 0.4769 - 37s/epoch - 326ms/step\n",
      "Epoch 37/10000\n",
      "113/113 - 31s - loss: 53281378304.0000 - accuracy: 0.5752 - val_loss: 50939645952.0000 - val_accuracy: 0.5402 - 31s/epoch - 271ms/step\n",
      "Epoch 38/10000\n",
      "113/113 - 32s - loss: 65561731072.0000 - accuracy: 0.5760 - val_loss: 89836019712.0000 - val_accuracy: 0.4100 - 32s/epoch - 286ms/step\n",
      "Epoch 39/10000\n",
      "113/113 - 29s - loss: 71535345664.0000 - accuracy: 0.5717 - val_loss: 49895903232.0000 - val_accuracy: 0.6213 - 29s/epoch - 260ms/step\n",
      "Epoch 40/10000\n",
      "113/113 - 29s - loss: 71798702080.0000 - accuracy: 0.5699 - val_loss: 34382725120.0000 - val_accuracy: 0.5815 - 29s/epoch - 257ms/step\n",
      "Epoch 41/10000\n",
      "113/113 - 33s - loss: 70309740544.0000 - accuracy: 0.5729 - val_loss: 49431060480.0000 - val_accuracy: 0.6231 - 33s/epoch - 295ms/step\n",
      "391/391 [==============================] - 8s 19ms/step\n",
      "\n",
      "Iterazione 124/128 (03-11-2023_20-22-11)\n",
      "hidden_layer_size: 1000\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 4\n",
      "activation_hidden: relu\n",
      "activation_output: sigmoid\n",
      "batch_size: 1000\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 20\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 64.26%\n",
      "La recall di questo modello sul validation set è: 23.18%\n",
      "La f1 di questo modello sul validation set è: 33.42%\n",
      "La balanced accuracy di questo modello sul validation set è: 56.69%\n",
      "La precision di questo modello sul validation set è: 59.87%\n",
      "La AUC di questo modello sul validation set è: 56.69%\n",
      "391/391 [==============================] - 7s 17ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.65    0.90      0.76  7,670.00\n",
      "1                  0.60    0.23      0.33  4,841.00\n",
      "accuracy           0.64    0.64      0.64      0.64\n",
      "macro avg          0.62    0.57      0.54 12,511.00\n",
      "weighted avg       0.63    0.64      0.59 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         6918          752\n",
      "Actual 1         3719         1122\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 66.14% con il modello:\n",
      "98 - AC66.14%, L1000, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA20, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_14-02-18).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 63.1% con il modello:\n",
      "98 - BA63.1%, L1000, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA20, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_14-02-18).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 66.75% con il modello:\n",
      "105 - PR66.75%, L1000, mV-10, MV10, P2, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_14-19-26).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "1126/1126 - 122s - loss: 9.0201 - accuracy: 0.5585 - val_loss: 7.9046 - val_accuracy: 0.5840 - 122s/epoch - 109ms/step\n",
      "Epoch 2/10000\n",
      "1126/1126 - 106s - loss: 7.2237 - accuracy: 0.5559 - val_loss: 6.5445 - val_accuracy: 0.5733 - 106s/epoch - 94ms/step\n",
      "Epoch 3/10000\n",
      "1126/1126 - 93s - loss: 5.6344 - accuracy: 0.5568 - val_loss: 4.6986 - val_accuracy: 0.5512 - 93s/epoch - 82ms/step\n",
      "Epoch 4/10000\n",
      "1126/1126 - 94s - loss: 4.6351 - accuracy: 0.5588 - val_loss: 4.1006 - val_accuracy: 0.5708 - 94s/epoch - 83ms/step\n",
      "Epoch 5/10000\n",
      "1126/1126 - 91s - loss: 3.6739 - accuracy: 0.5600 - val_loss: 3.1165 - val_accuracy: 0.5518 - 91s/epoch - 81ms/step\n",
      "Epoch 6/10000\n",
      "1126/1126 - 91s - loss: 2.7865 - accuracy: 0.5642 - val_loss: 2.3842 - val_accuracy: 0.5699 - 91s/epoch - 81ms/step\n",
      "Epoch 7/10000\n",
      "1126/1126 - 87s - loss: 2.2215 - accuracy: 0.5596 - val_loss: 2.0334 - val_accuracy: 0.5413 - 87s/epoch - 78ms/step\n",
      "Epoch 8/10000\n",
      "1126/1126 - 88s - loss: 1.7886 - accuracy: 0.5609 - val_loss: 1.8403 - val_accuracy: 0.5442 - 88s/epoch - 78ms/step\n",
      "Epoch 9/10000\n",
      "1126/1126 - 86s - loss: 1.4890 - accuracy: 0.5690 - val_loss: 1.4137 - val_accuracy: 0.5775 - 86s/epoch - 76ms/step\n",
      "Epoch 10/10000\n",
      "1126/1126 - 92s - loss: 1.2573 - accuracy: 0.5693 - val_loss: 1.1569 - val_accuracy: 0.5594 - 92s/epoch - 81ms/step\n",
      "Epoch 11/10000\n",
      "1126/1126 - 102s - loss: 1.0745 - accuracy: 0.5758 - val_loss: 0.9876 - val_accuracy: 0.5904 - 102s/epoch - 91ms/step\n",
      "Epoch 12/10000\n",
      "1126/1126 - 97s - loss: 0.9466 - accuracy: 0.5877 - val_loss: 0.8592 - val_accuracy: 0.6105 - 97s/epoch - 86ms/step\n",
      "Epoch 13/10000\n",
      "1126/1126 - 95s - loss: 0.8390 - accuracy: 0.5998 - val_loss: 0.7870 - val_accuracy: 0.6161 - 95s/epoch - 84ms/step\n",
      "Epoch 14/10000\n",
      "1126/1126 - 92s - loss: 0.7866 - accuracy: 0.6014 - val_loss: 0.7501 - val_accuracy: 0.6098 - 92s/epoch - 82ms/step\n",
      "Epoch 15/10000\n",
      "1126/1126 - 89s - loss: 0.7462 - accuracy: 0.6046 - val_loss: 0.7177 - val_accuracy: 0.6128 - 89s/epoch - 79ms/step\n",
      "Epoch 16/10000\n",
      "1126/1126 - 87s - loss: 0.7285 - accuracy: 0.6078 - val_loss: 0.6920 - val_accuracy: 0.6153 - 87s/epoch - 77ms/step\n",
      "Epoch 17/10000\n",
      "1126/1126 - 90s - loss: 0.7152 - accuracy: 0.6123 - val_loss: 0.7000 - val_accuracy: 0.6247 - 90s/epoch - 80ms/step\n",
      "Epoch 18/10000\n",
      "1126/1126 - 92s - loss: 0.7103 - accuracy: 0.6087 - val_loss: 0.6962 - val_accuracy: 0.6166 - 92s/epoch - 81ms/step\n",
      "Epoch 19/10000\n",
      "1126/1126 - 150s - loss: 0.7033 - accuracy: 0.6112 - val_loss: 0.6895 - val_accuracy: 0.6107 - 150s/epoch - 133ms/step\n",
      "Epoch 20/10000\n",
      "1126/1126 - 139s - loss: 0.6983 - accuracy: 0.6128 - val_loss: 0.6835 - val_accuracy: 0.6224 - 139s/epoch - 123ms/step\n",
      "Epoch 21/10000\n",
      "1126/1126 - 90s - loss: 0.6927 - accuracy: 0.6135 - val_loss: 0.7127 - val_accuracy: 0.6174 - 90s/epoch - 80ms/step\n",
      "Epoch 22/10000\n",
      "1126/1126 - 94s - loss: 0.6966 - accuracy: 0.6134 - val_loss: 0.6879 - val_accuracy: 0.6268 - 94s/epoch - 83ms/step\n",
      "Epoch 23/10000\n",
      "1126/1126 - 91s - loss: 0.6842 - accuracy: 0.6163 - val_loss: 0.6817 - val_accuracy: 0.6085 - 91s/epoch - 81ms/step\n",
      "Epoch 24/10000\n",
      "1126/1126 - 92s - loss: 0.6840 - accuracy: 0.6131 - val_loss: 0.6746 - val_accuracy: 0.6050 - 92s/epoch - 82ms/step\n",
      "Epoch 25/10000\n",
      "1126/1126 - 103s - loss: 0.6762 - accuracy: 0.6185 - val_loss: 0.6790 - val_accuracy: 0.6255 - 103s/epoch - 92ms/step\n",
      "Epoch 26/10000\n",
      "1126/1126 - 95s - loss: 0.6783 - accuracy: 0.6200 - val_loss: 0.6771 - val_accuracy: 0.6212 - 95s/epoch - 84ms/step\n",
      "Epoch 27/10000\n",
      "1126/1126 - 91s - loss: 0.6719 - accuracy: 0.6196 - val_loss: 0.6827 - val_accuracy: 0.6192 - 91s/epoch - 81ms/step\n",
      "Epoch 28/10000\n",
      "1126/1126 - 93s - loss: 0.6685 - accuracy: 0.6181 - val_loss: 0.6639 - val_accuracy: 0.6250 - 93s/epoch - 83ms/step\n",
      "Epoch 29/10000\n",
      "1126/1126 - 92s - loss: 0.6728 - accuracy: 0.6183 - val_loss: 0.6632 - val_accuracy: 0.6222 - 92s/epoch - 82ms/step\n",
      "Epoch 30/10000\n",
      "1126/1126 - 87s - loss: 0.6712 - accuracy: 0.6175 - val_loss: 0.6756 - val_accuracy: 0.6160 - 87s/epoch - 77ms/step\n",
      "Epoch 31/10000\n",
      "1126/1126 - 92s - loss: 0.6743 - accuracy: 0.6180 - val_loss: 0.6607 - val_accuracy: 0.6175 - 92s/epoch - 82ms/step\n",
      "Epoch 32/10000\n",
      "1126/1126 - 90s - loss: 0.6648 - accuracy: 0.6221 - val_loss: 0.6666 - val_accuracy: 0.6232 - 90s/epoch - 80ms/step\n",
      "391/391 [==============================] - 6s 14ms/step\n",
      "\n",
      "Iterazione 125/128 (03-11-2023_21-14-02)\n",
      "hidden_layer_size: 1000\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 4\n",
      "activation_hidden: sigmoid\n",
      "activation_output: sigmoid\n",
      "batch_size: 100\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 10\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 62.68%\n",
      "La recall di questo modello sul validation set è: 25.57%\n",
      "La f1 di questo modello sul validation set è: 34.65%\n",
      "La balanced accuracy di questo modello sul validation set è: 55.84%\n",
      "La precision di questo modello sul validation set è: 53.73%\n",
      "La AUC di questo modello sul validation set è: 60.86%\n",
      "391/391 [==============================] - 5s 14ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.65    0.86      0.74  7,670.00\n",
      "1                  0.54    0.26      0.35  4,841.00\n",
      "accuracy           0.63    0.63      0.63      0.63\n",
      "macro avg          0.59    0.56      0.54 12,511.00\n",
      "weighted avg       0.60    0.63      0.59 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         6604         1066\n",
      "Actual 1         3603         1238\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 66.14% con il modello:\n",
      "98 - AC66.14%, L1000, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA20, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_14-02-18).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 63.1% con il modello:\n",
      "98 - BA63.1%, L1000, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA20, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_14-02-18).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 66.75% con il modello:\n",
      "105 - PR66.75%, L1000, mV-10, MV10, P2, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_14-19-26).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "1126/1126 - 106s - loss: 9.0201 - accuracy: 0.5585 - val_loss: 7.9046 - val_accuracy: 0.5840 - 106s/epoch - 94ms/step\n",
      "Epoch 2/10000\n",
      "1126/1126 - 93s - loss: 7.2237 - accuracy: 0.5559 - val_loss: 6.5445 - val_accuracy: 0.5733 - 93s/epoch - 82ms/step\n",
      "Epoch 3/10000\n",
      "1126/1126 - 98s - loss: 5.6344 - accuracy: 0.5568 - val_loss: 4.6986 - val_accuracy: 0.5512 - 98s/epoch - 87ms/step\n",
      "Epoch 4/10000\n",
      "1126/1126 - 90s - loss: 4.6351 - accuracy: 0.5588 - val_loss: 4.1006 - val_accuracy: 0.5708 - 90s/epoch - 80ms/step\n",
      "Epoch 5/10000\n",
      "1126/1126 - 88s - loss: 3.6739 - accuracy: 0.5600 - val_loss: 3.1165 - val_accuracy: 0.5518 - 88s/epoch - 78ms/step\n",
      "Epoch 6/10000\n",
      "1126/1126 - 88s - loss: 2.7865 - accuracy: 0.5642 - val_loss: 2.3842 - val_accuracy: 0.5699 - 88s/epoch - 78ms/step\n",
      "Epoch 7/10000\n",
      "1126/1126 - 90s - loss: 2.2215 - accuracy: 0.5596 - val_loss: 2.0334 - val_accuracy: 0.5413 - 90s/epoch - 80ms/step\n",
      "Epoch 8/10000\n",
      "1126/1126 - 88s - loss: 1.7886 - accuracy: 0.5609 - val_loss: 1.8403 - val_accuracy: 0.5442 - 88s/epoch - 78ms/step\n",
      "Epoch 9/10000\n",
      "1126/1126 - 87s - loss: 1.4890 - accuracy: 0.5690 - val_loss: 1.4137 - val_accuracy: 0.5775 - 87s/epoch - 78ms/step\n",
      "Epoch 10/10000\n",
      "1126/1126 - 91s - loss: 1.2573 - accuracy: 0.5693 - val_loss: 1.1569 - val_accuracy: 0.5594 - 91s/epoch - 81ms/step\n",
      "Epoch 11/10000\n",
      "1126/1126 - 90s - loss: 1.0745 - accuracy: 0.5758 - val_loss: 0.9876 - val_accuracy: 0.5904 - 90s/epoch - 80ms/step\n",
      "Epoch 12/10000\n",
      "1126/1126 - 87s - loss: 0.9466 - accuracy: 0.5877 - val_loss: 0.8592 - val_accuracy: 0.6105 - 87s/epoch - 77ms/step\n",
      "Epoch 13/10000\n",
      "1126/1126 - 88s - loss: 0.8390 - accuracy: 0.5998 - val_loss: 0.7870 - val_accuracy: 0.6161 - 88s/epoch - 78ms/step\n",
      "Epoch 14/10000\n",
      "1126/1126 - 89s - loss: 0.7866 - accuracy: 0.6014 - val_loss: 0.7501 - val_accuracy: 0.6098 - 89s/epoch - 79ms/step\n",
      "Epoch 15/10000\n",
      "1126/1126 - 89s - loss: 0.7462 - accuracy: 0.6046 - val_loss: 0.7177 - val_accuracy: 0.6128 - 89s/epoch - 79ms/step\n",
      "Epoch 16/10000\n",
      "1126/1126 - 87s - loss: 0.7285 - accuracy: 0.6078 - val_loss: 0.6920 - val_accuracy: 0.6153 - 87s/epoch - 77ms/step\n",
      "Epoch 17/10000\n",
      "1126/1126 - 92s - loss: 0.7152 - accuracy: 0.6123 - val_loss: 0.7000 - val_accuracy: 0.6247 - 92s/epoch - 81ms/step\n",
      "Epoch 18/10000\n",
      "1126/1126 - 92s - loss: 0.7103 - accuracy: 0.6087 - val_loss: 0.6962 - val_accuracy: 0.6166 - 92s/epoch - 82ms/step\n",
      "Epoch 19/10000\n",
      "1126/1126 - 88s - loss: 0.7033 - accuracy: 0.6112 - val_loss: 0.6895 - val_accuracy: 0.6107 - 88s/epoch - 78ms/step\n",
      "Epoch 20/10000\n",
      "1126/1126 - 89s - loss: 0.6983 - accuracy: 0.6128 - val_loss: 0.6835 - val_accuracy: 0.6224 - 89s/epoch - 79ms/step\n",
      "Epoch 21/10000\n",
      "1126/1126 - 88s - loss: 0.6927 - accuracy: 0.6135 - val_loss: 0.7127 - val_accuracy: 0.6174 - 88s/epoch - 78ms/step\n",
      "Epoch 22/10000\n",
      "1126/1126 - 91s - loss: 0.6966 - accuracy: 0.6134 - val_loss: 0.6879 - val_accuracy: 0.6268 - 91s/epoch - 81ms/step\n",
      "Epoch 23/10000\n",
      "1126/1126 - 86s - loss: 0.6842 - accuracy: 0.6163 - val_loss: 0.6817 - val_accuracy: 0.6085 - 86s/epoch - 76ms/step\n",
      "Epoch 24/10000\n",
      "1126/1126 - 89s - loss: 0.6840 - accuracy: 0.6131 - val_loss: 0.6746 - val_accuracy: 0.6050 - 89s/epoch - 79ms/step\n",
      "Epoch 25/10000\n",
      "1126/1126 - 92s - loss: 0.6762 - accuracy: 0.6185 - val_loss: 0.6790 - val_accuracy: 0.6255 - 92s/epoch - 81ms/step\n",
      "Epoch 26/10000\n",
      "1126/1126 - 96s - loss: 0.6783 - accuracy: 0.6200 - val_loss: 0.6771 - val_accuracy: 0.6212 - 96s/epoch - 85ms/step\n",
      "Epoch 27/10000\n",
      "1126/1126 - 91s - loss: 0.6719 - accuracy: 0.6196 - val_loss: 0.6827 - val_accuracy: 0.6192 - 91s/epoch - 81ms/step\n",
      "Epoch 28/10000\n",
      "1126/1126 - 94s - loss: 0.6685 - accuracy: 0.6181 - val_loss: 0.6639 - val_accuracy: 0.6250 - 94s/epoch - 84ms/step\n",
      "Epoch 29/10000\n",
      "1126/1126 - 95s - loss: 0.6728 - accuracy: 0.6183 - val_loss: 0.6632 - val_accuracy: 0.6222 - 95s/epoch - 85ms/step\n",
      "Epoch 30/10000\n",
      "1126/1126 - 93s - loss: 0.6712 - accuracy: 0.6175 - val_loss: 0.6756 - val_accuracy: 0.6160 - 93s/epoch - 83ms/step\n",
      "Epoch 31/10000\n",
      "1126/1126 - 92s - loss: 0.6743 - accuracy: 0.6180 - val_loss: 0.6607 - val_accuracy: 0.6175 - 92s/epoch - 82ms/step\n",
      "Epoch 32/10000\n",
      "1126/1126 - 95s - loss: 0.6648 - accuracy: 0.6221 - val_loss: 0.6666 - val_accuracy: 0.6232 - 95s/epoch - 85ms/step\n",
      "Epoch 33/10000\n",
      "1126/1126 - 91s - loss: 0.6677 - accuracy: 0.6159 - val_loss: 0.6615 - val_accuracy: 0.6343 - 91s/epoch - 81ms/step\n",
      "Epoch 34/10000\n",
      "1126/1126 - 90s - loss: 0.6711 - accuracy: 0.6165 - val_loss: 0.6804 - val_accuracy: 0.6211 - 90s/epoch - 80ms/step\n",
      "Epoch 35/10000\n",
      "1126/1126 - 90s - loss: 0.6701 - accuracy: 0.6166 - val_loss: 0.6794 - val_accuracy: 0.6077 - 90s/epoch - 80ms/step\n",
      "Epoch 36/10000\n",
      "1126/1126 - 90s - loss: 0.6697 - accuracy: 0.6157 - val_loss: 0.6619 - val_accuracy: 0.6213 - 90s/epoch - 80ms/step\n",
      "Epoch 37/10000\n",
      "1126/1126 - 90s - loss: 0.6715 - accuracy: 0.6144 - val_loss: 0.6693 - val_accuracy: 0.5969 - 90s/epoch - 80ms/step\n",
      "Epoch 38/10000\n",
      "1126/1126 - 88s - loss: 0.6685 - accuracy: 0.6156 - val_loss: 0.6770 - val_accuracy: 0.5989 - 88s/epoch - 79ms/step\n",
      "Epoch 39/10000\n",
      "1126/1126 - 88s - loss: 0.6685 - accuracy: 0.6223 - val_loss: 0.6596 - val_accuracy: 0.6200 - 88s/epoch - 78ms/step\n",
      "Epoch 40/10000\n",
      "1126/1126 - 88s - loss: 0.6646 - accuracy: 0.6208 - val_loss: 0.6622 - val_accuracy: 0.6231 - 88s/epoch - 78ms/step\n",
      "Epoch 41/10000\n",
      "1126/1126 - 87s - loss: 0.6620 - accuracy: 0.6214 - val_loss: 0.6579 - val_accuracy: 0.6254 - 87s/epoch - 77ms/step\n",
      "Epoch 42/10000\n",
      "1126/1126 - 91s - loss: 0.6620 - accuracy: 0.6218 - val_loss: 0.6532 - val_accuracy: 0.6319 - 91s/epoch - 81ms/step\n",
      "Epoch 43/10000\n",
      "1126/1126 - 87s - loss: 0.6612 - accuracy: 0.6210 - val_loss: 0.6573 - val_accuracy: 0.6290 - 87s/epoch - 77ms/step\n",
      "Epoch 44/10000\n",
      "1126/1126 - 92s - loss: 0.6603 - accuracy: 0.6217 - val_loss: 0.6478 - val_accuracy: 0.6297 - 92s/epoch - 82ms/step\n",
      "Epoch 45/10000\n",
      "1126/1126 - 87s - loss: 0.6523 - accuracy: 0.6245 - val_loss: 0.6621 - val_accuracy: 0.6238 - 87s/epoch - 77ms/step\n",
      "Epoch 46/10000\n",
      "1126/1126 - 86s - loss: 0.6543 - accuracy: 0.6219 - val_loss: 0.6523 - val_accuracy: 0.6223 - 86s/epoch - 77ms/step\n",
      "Epoch 47/10000\n",
      "1126/1126 - 90s - loss: 0.6551 - accuracy: 0.6156 - val_loss: 0.6479 - val_accuracy: 0.6276 - 90s/epoch - 80ms/step\n",
      "Epoch 48/10000\n",
      "1126/1126 - 88s - loss: 0.6594 - accuracy: 0.6158 - val_loss: 0.6472 - val_accuracy: 0.6253 - 88s/epoch - 78ms/step\n",
      "Epoch 49/10000\n",
      "1126/1126 - 84s - loss: 0.6584 - accuracy: 0.6172 - val_loss: 0.6523 - val_accuracy: 0.6284 - 84s/epoch - 75ms/step\n",
      "Epoch 50/10000\n",
      "1126/1126 - 86s - loss: 0.6552 - accuracy: 0.6220 - val_loss: 0.6647 - val_accuracy: 0.6133 - 86s/epoch - 77ms/step\n",
      "Epoch 51/10000\n",
      "1126/1126 - 86s - loss: 0.6585 - accuracy: 0.6171 - val_loss: 0.6538 - val_accuracy: 0.6272 - 86s/epoch - 76ms/step\n",
      "Epoch 52/10000\n",
      "1126/1126 - 85s - loss: 0.6572 - accuracy: 0.6211 - val_loss: 0.6471 - val_accuracy: 0.6318 - 85s/epoch - 75ms/step\n",
      "Epoch 53/10000\n",
      "1126/1126 - 81s - loss: 0.6548 - accuracy: 0.6227 - val_loss: 0.6547 - val_accuracy: 0.6211 - 81s/epoch - 72ms/step\n",
      "391/391 [==============================] - 5s 13ms/step\n",
      "\n",
      "Iterazione 126/128 (03-11-2023_22-33-38)\n",
      "hidden_layer_size: 1000\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 4\n",
      "activation_hidden: sigmoid\n",
      "activation_output: sigmoid\n",
      "batch_size: 100\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 20\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 63.43%\n",
      "La recall di questo modello sul validation set è: 20.26%\n",
      "La f1 di questo modello sul validation set è: 30.01%\n",
      "La balanced accuracy di questo modello sul validation set è: 55.47%\n",
      "La precision di questo modello sul validation set è: 57.84%\n",
      "La AUC di questo modello sul validation set è: 62.06%\n",
      "391/391 [==============================] - 5s 12ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.64    0.91      0.75  7,670.00\n",
      "1                  0.58    0.20      0.30  4,841.00\n",
      "accuracy           0.63    0.63      0.63      0.63\n",
      "macro avg          0.61    0.55      0.53 12,511.00\n",
      "weighted avg       0.62    0.63      0.58 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         6955          715\n",
      "Actual 1         3860          981\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 66.14% con il modello:\n",
      "98 - AC66.14%, L1000, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA20, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_14-02-18).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 63.1% con il modello:\n",
      "98 - BA63.1%, L1000, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA20, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_14-02-18).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 66.75% con il modello:\n",
      "105 - PR66.75%, L1000, mV-10, MV10, P2, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_14-19-26).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "113/113 - 24s - loss: 8.0289 - accuracy: 0.5596 - val_loss: 7.0477 - val_accuracy: 0.5645 - 24s/epoch - 216ms/step\n",
      "Epoch 2/10000\n",
      "113/113 - 20s - loss: 6.7917 - accuracy: 0.5606 - val_loss: 6.4549 - val_accuracy: 0.5422 - 20s/epoch - 179ms/step\n",
      "Epoch 3/10000\n",
      "113/113 - 21s - loss: 6.2406 - accuracy: 0.5640 - val_loss: 6.2020 - val_accuracy: 0.5302 - 21s/epoch - 183ms/step\n",
      "Epoch 4/10000\n",
      "113/113 - 21s - loss: 5.9295 - accuracy: 0.5624 - val_loss: 5.8576 - val_accuracy: 0.5679 - 21s/epoch - 188ms/step\n",
      "Epoch 5/10000\n",
      "113/113 - 21s - loss: 5.7733 - accuracy: 0.5607 - val_loss: 5.6113 - val_accuracy: 0.5449 - 21s/epoch - 188ms/step\n",
      "Epoch 6/10000\n",
      "113/113 - 21s - loss: 5.4208 - accuracy: 0.5618 - val_loss: 5.4381 - val_accuracy: 0.5550 - 21s/epoch - 183ms/step\n",
      "Epoch 7/10000\n",
      "113/113 - 21s - loss: 5.2039 - accuracy: 0.5627 - val_loss: 5.0993 - val_accuracy: 0.5458 - 21s/epoch - 189ms/step\n",
      "Epoch 8/10000\n",
      "113/113 - 22s - loss: 4.9721 - accuracy: 0.5654 - val_loss: 4.9496 - val_accuracy: 0.5669 - 22s/epoch - 194ms/step\n",
      "Epoch 9/10000\n",
      "113/113 - 21s - loss: 4.8725 - accuracy: 0.5665 - val_loss: 4.9658 - val_accuracy: 0.5583 - 21s/epoch - 186ms/step\n",
      "Epoch 10/10000\n",
      "113/113 - 19s - loss: 5.0234 - accuracy: 0.5662 - val_loss: 5.2778 - val_accuracy: 0.5690 - 19s/epoch - 171ms/step\n",
      "Epoch 11/10000\n",
      "113/113 - 19s - loss: 4.9282 - accuracy: 0.5667 - val_loss: 4.8051 - val_accuracy: 0.5663 - 19s/epoch - 171ms/step\n",
      "Epoch 12/10000\n",
      "113/113 - 19s - loss: 4.9265 - accuracy: 0.5655 - val_loss: 5.2038 - val_accuracy: 0.5733 - 19s/epoch - 168ms/step\n",
      "Epoch 13/10000\n",
      "113/113 - 19s - loss: 5.0024 - accuracy: 0.5647 - val_loss: 4.8982 - val_accuracy: 0.5726 - 19s/epoch - 167ms/step\n",
      "Epoch 14/10000\n",
      "113/113 - 19s - loss: 4.7856 - accuracy: 0.5649 - val_loss: 4.6129 - val_accuracy: 0.5690 - 19s/epoch - 167ms/step\n",
      "Epoch 15/10000\n",
      "113/113 - 19s - loss: 4.5959 - accuracy: 0.5656 - val_loss: 4.5554 - val_accuracy: 0.5511 - 19s/epoch - 171ms/step\n",
      "Epoch 16/10000\n",
      "113/113 - 19s - loss: 4.4779 - accuracy: 0.5652 - val_loss: 4.6321 - val_accuracy: 0.5562 - 19s/epoch - 168ms/step\n",
      "Epoch 17/10000\n",
      "113/113 - 19s - loss: 4.4156 - accuracy: 0.5651 - val_loss: 4.5500 - val_accuracy: 0.5657 - 19s/epoch - 169ms/step\n",
      "Epoch 18/10000\n",
      "113/113 - 19s - loss: 4.2495 - accuracy: 0.5679 - val_loss: 4.2250 - val_accuracy: 0.5760 - 19s/epoch - 168ms/step\n",
      "Epoch 19/10000\n",
      "113/113 - 19s - loss: 4.1217 - accuracy: 0.5671 - val_loss: 4.2432 - val_accuracy: 0.5577 - 19s/epoch - 168ms/step\n",
      "Epoch 20/10000\n",
      "113/113 - 21s - loss: 4.1236 - accuracy: 0.5649 - val_loss: 4.1107 - val_accuracy: 0.5520 - 21s/epoch - 182ms/step\n",
      "Epoch 21/10000\n",
      "113/113 - 31s - loss: 4.2014 - accuracy: 0.5631 - val_loss: 4.2634 - val_accuracy: 0.5364 - 31s/epoch - 276ms/step\n",
      "Epoch 22/10000\n",
      "113/113 - 23s - loss: 4.0188 - accuracy: 0.5664 - val_loss: 4.3920 - val_accuracy: 0.5642 - 23s/epoch - 204ms/step\n",
      "Epoch 23/10000\n",
      "113/113 - 22s - loss: 3.9902 - accuracy: 0.5661 - val_loss: 3.7719 - val_accuracy: 0.5751 - 22s/epoch - 191ms/step\n",
      "Epoch 24/10000\n",
      "113/113 - 21s - loss: 3.8310 - accuracy: 0.5660 - val_loss: 3.8052 - val_accuracy: 0.5761 - 21s/epoch - 184ms/step\n",
      "Epoch 25/10000\n",
      "113/113 - 21s - loss: 3.7366 - accuracy: 0.5658 - val_loss: 3.9083 - val_accuracy: 0.5593 - 21s/epoch - 182ms/step\n",
      "Epoch 26/10000\n",
      "113/113 - 20s - loss: 3.6095 - accuracy: 0.5670 - val_loss: 3.5745 - val_accuracy: 0.5737 - 20s/epoch - 181ms/step\n",
      "Epoch 27/10000\n",
      "113/113 - 20s - loss: 3.5343 - accuracy: 0.5706 - val_loss: 3.5841 - val_accuracy: 0.5501 - 20s/epoch - 175ms/step\n",
      "Epoch 28/10000\n",
      "113/113 - 20s - loss: 3.6442 - accuracy: 0.5694 - val_loss: 3.6419 - val_accuracy: 0.5741 - 20s/epoch - 175ms/step\n",
      "Epoch 29/10000\n",
      "113/113 - 20s - loss: 3.5384 - accuracy: 0.5665 - val_loss: 3.2858 - val_accuracy: 0.5678 - 20s/epoch - 175ms/step\n",
      "Epoch 30/10000\n",
      "113/113 - 20s - loss: 3.4840 - accuracy: 0.5699 - val_loss: 3.6472 - val_accuracy: 0.5662 - 20s/epoch - 176ms/step\n",
      "Epoch 31/10000\n",
      "113/113 - 22s - loss: 3.5805 - accuracy: 0.5708 - val_loss: 3.4369 - val_accuracy: 0.5842 - 22s/epoch - 195ms/step\n",
      "Epoch 32/10000\n",
      "113/113 - 19s - loss: 3.3469 - accuracy: 0.5693 - val_loss: 3.1291 - val_accuracy: 0.5653 - 19s/epoch - 170ms/step\n",
      "Epoch 33/10000\n",
      "113/113 - 19s - loss: 3.2253 - accuracy: 0.5699 - val_loss: 3.1164 - val_accuracy: 0.5699 - 19s/epoch - 169ms/step\n",
      "Epoch 34/10000\n",
      "113/113 - 19s - loss: 3.1788 - accuracy: 0.5724 - val_loss: 3.1013 - val_accuracy: 0.5665 - 19s/epoch - 171ms/step\n",
      "Epoch 35/10000\n",
      "113/113 - 19s - loss: 3.1527 - accuracy: 0.5691 - val_loss: 2.8761 - val_accuracy: 0.5670 - 19s/epoch - 170ms/step\n",
      "Epoch 36/10000\n",
      "113/113 - 19s - loss: 3.0004 - accuracy: 0.5723 - val_loss: 3.1260 - val_accuracy: 0.5769 - 19s/epoch - 171ms/step\n",
      "Epoch 37/10000\n",
      "113/113 - 19s - loss: 2.9881 - accuracy: 0.5706 - val_loss: 2.9246 - val_accuracy: 0.5787 - 19s/epoch - 171ms/step\n",
      "Epoch 38/10000\n",
      "113/113 - 19s - loss: 2.8640 - accuracy: 0.5720 - val_loss: 2.9586 - val_accuracy: 0.5769 - 19s/epoch - 170ms/step\n",
      "Epoch 39/10000\n",
      "113/113 - 19s - loss: 2.8705 - accuracy: 0.5716 - val_loss: 2.8784 - val_accuracy: 0.5504 - 19s/epoch - 172ms/step\n",
      "Epoch 40/10000\n",
      "113/113 - 20s - loss: 2.9555 - accuracy: 0.5697 - val_loss: 2.9155 - val_accuracy: 0.5801 - 20s/epoch - 179ms/step\n",
      "Epoch 41/10000\n",
      "113/113 - 19s - loss: 2.8222 - accuracy: 0.5730 - val_loss: 2.8120 - val_accuracy: 0.5710 - 19s/epoch - 171ms/step\n",
      "391/391 [==============================] - 5s 12ms/step\n",
      "\n",
      "Iterazione 127/128 (03-11-2023_22-47-48)\n",
      "hidden_layer_size: 1000\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 4\n",
      "activation_hidden: sigmoid\n",
      "activation_output: sigmoid\n",
      "batch_size: 1000\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 10\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 58.42%\n",
      "La recall di questo modello sul validation set è: 44.76%\n",
      "La f1 di questo modello sul validation set è: 45.45%\n",
      "La balanced accuracy di questo modello sul validation set è: 55.9%\n",
      "La precision di questo modello sul validation set è: 46.16%\n",
      "La AUC di questo modello sul validation set è: 57.93%\n",
      "391/391 [==============================] - 5s 11ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.66    0.67      0.66  7,670.00\n",
      "1                  0.46    0.45      0.45  4,841.00\n",
      "accuracy           0.58    0.58      0.58      0.58\n",
      "macro avg          0.56    0.56      0.56 12,511.00\n",
      "weighted avg       0.58    0.58      0.58 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         5142         2528\n",
      "Actual 1         2674         2167\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 66.14% con il modello:\n",
      "98 - AC66.14%, L1000, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA20, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_14-02-18).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 63.1% con il modello:\n",
      "98 - BA63.1%, L1000, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA20, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_14-02-18).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 66.75% con il modello:\n",
      "105 - PR66.75%, L1000, mV-10, MV10, P2, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_14-19-26).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n",
      "Epoch 1/10000\n",
      "113/113 - 25s - loss: 8.0289 - accuracy: 0.5596 - val_loss: 7.0477 - val_accuracy: 0.5645 - 25s/epoch - 224ms/step\n",
      "Epoch 2/10000\n",
      "113/113 - 22s - loss: 6.7917 - accuracy: 0.5606 - val_loss: 6.4549 - val_accuracy: 0.5422 - 22s/epoch - 196ms/step\n",
      "Epoch 3/10000\n",
      "113/113 - 21s - loss: 6.2406 - accuracy: 0.5640 - val_loss: 6.2020 - val_accuracy: 0.5302 - 21s/epoch - 187ms/step\n",
      "Epoch 4/10000\n",
      "113/113 - 22s - loss: 5.9295 - accuracy: 0.5624 - val_loss: 5.8576 - val_accuracy: 0.5679 - 22s/epoch - 192ms/step\n",
      "Epoch 5/10000\n",
      "113/113 - 22s - loss: 5.7733 - accuracy: 0.5607 - val_loss: 5.6113 - val_accuracy: 0.5449 - 22s/epoch - 191ms/step\n",
      "Epoch 6/10000\n",
      "113/113 - 21s - loss: 5.4208 - accuracy: 0.5618 - val_loss: 5.4381 - val_accuracy: 0.5550 - 21s/epoch - 188ms/step\n",
      "Epoch 7/10000\n",
      "113/113 - 21s - loss: 5.2039 - accuracy: 0.5627 - val_loss: 5.0993 - val_accuracy: 0.5458 - 21s/epoch - 185ms/step\n",
      "Epoch 8/10000\n",
      "113/113 - 20s - loss: 4.9721 - accuracy: 0.5654 - val_loss: 4.9496 - val_accuracy: 0.5669 - 20s/epoch - 181ms/step\n",
      "Epoch 9/10000\n",
      "113/113 - 19s - loss: 4.8725 - accuracy: 0.5665 - val_loss: 4.9658 - val_accuracy: 0.5583 - 19s/epoch - 171ms/step\n",
      "Epoch 10/10000\n",
      "113/113 - 19s - loss: 5.0234 - accuracy: 0.5662 - val_loss: 5.2778 - val_accuracy: 0.5690 - 19s/epoch - 170ms/step\n",
      "Epoch 11/10000\n",
      "113/113 - 19s - loss: 4.9282 - accuracy: 0.5667 - val_loss: 4.8051 - val_accuracy: 0.5663 - 19s/epoch - 169ms/step\n",
      "Epoch 12/10000\n",
      "113/113 - 19s - loss: 4.9265 - accuracy: 0.5655 - val_loss: 5.2038 - val_accuracy: 0.5733 - 19s/epoch - 169ms/step\n",
      "Epoch 13/10000\n",
      "113/113 - 19s - loss: 5.0024 - accuracy: 0.5647 - val_loss: 4.8982 - val_accuracy: 0.5726 - 19s/epoch - 171ms/step\n",
      "Epoch 14/10000\n",
      "113/113 - 19s - loss: 4.7856 - accuracy: 0.5649 - val_loss: 4.6129 - val_accuracy: 0.5690 - 19s/epoch - 168ms/step\n",
      "Epoch 15/10000\n",
      "113/113 - 19s - loss: 4.5959 - accuracy: 0.5656 - val_loss: 4.5554 - val_accuracy: 0.5511 - 19s/epoch - 171ms/step\n",
      "Epoch 16/10000\n",
      "113/113 - 19s - loss: 4.4779 - accuracy: 0.5652 - val_loss: 4.6321 - val_accuracy: 0.5562 - 19s/epoch - 170ms/step\n",
      "Epoch 17/10000\n",
      "113/113 - 19s - loss: 4.4156 - accuracy: 0.5651 - val_loss: 4.5500 - val_accuracy: 0.5657 - 19s/epoch - 169ms/step\n",
      "Epoch 18/10000\n",
      "113/113 - 19s - loss: 4.2495 - accuracy: 0.5679 - val_loss: 4.2250 - val_accuracy: 0.5760 - 19s/epoch - 172ms/step\n",
      "Epoch 19/10000\n",
      "113/113 - 19s - loss: 4.1217 - accuracy: 0.5671 - val_loss: 4.2432 - val_accuracy: 0.5577 - 19s/epoch - 170ms/step\n",
      "Epoch 20/10000\n",
      "113/113 - 19s - loss: 4.1236 - accuracy: 0.5649 - val_loss: 4.1107 - val_accuracy: 0.5520 - 19s/epoch - 169ms/step\n",
      "Epoch 21/10000\n",
      "113/113 - 20s - loss: 4.2014 - accuracy: 0.5631 - val_loss: 4.2634 - val_accuracy: 0.5364 - 20s/epoch - 180ms/step\n",
      "Epoch 22/10000\n",
      "113/113 - 20s - loss: 4.0188 - accuracy: 0.5664 - val_loss: 4.3920 - val_accuracy: 0.5642 - 20s/epoch - 178ms/step\n",
      "Epoch 23/10000\n",
      "113/113 - 19s - loss: 3.9902 - accuracy: 0.5661 - val_loss: 3.7719 - val_accuracy: 0.5751 - 19s/epoch - 171ms/step\n",
      "Epoch 24/10000\n",
      "113/113 - 20s - loss: 3.8310 - accuracy: 0.5660 - val_loss: 3.8052 - val_accuracy: 0.5761 - 20s/epoch - 173ms/step\n",
      "Epoch 25/10000\n",
      "113/113 - 19s - loss: 3.7366 - accuracy: 0.5658 - val_loss: 3.9083 - val_accuracy: 0.5593 - 19s/epoch - 171ms/step\n",
      "Epoch 26/10000\n",
      "113/113 - 19s - loss: 3.6095 - accuracy: 0.5670 - val_loss: 3.5745 - val_accuracy: 0.5737 - 19s/epoch - 168ms/step\n",
      "Epoch 27/10000\n",
      "113/113 - 19s - loss: 3.5343 - accuracy: 0.5706 - val_loss: 3.5841 - val_accuracy: 0.5501 - 19s/epoch - 168ms/step\n",
      "Epoch 28/10000\n",
      "113/113 - 20s - loss: 3.6442 - accuracy: 0.5694 - val_loss: 3.6419 - val_accuracy: 0.5741 - 20s/epoch - 173ms/step\n",
      "Epoch 29/10000\n",
      "113/113 - 19s - loss: 3.5384 - accuracy: 0.5665 - val_loss: 3.2858 - val_accuracy: 0.5678 - 19s/epoch - 171ms/step\n",
      "Epoch 30/10000\n",
      "113/113 - 19s - loss: 3.4840 - accuracy: 0.5699 - val_loss: 3.6472 - val_accuracy: 0.5662 - 19s/epoch - 170ms/step\n",
      "Epoch 31/10000\n",
      "113/113 - 19s - loss: 3.5805 - accuracy: 0.5708 - val_loss: 3.4369 - val_accuracy: 0.5842 - 19s/epoch - 169ms/step\n",
      "Epoch 32/10000\n",
      "113/113 - 19s - loss: 3.3469 - accuracy: 0.5693 - val_loss: 3.1291 - val_accuracy: 0.5653 - 19s/epoch - 170ms/step\n",
      "Epoch 33/10000\n",
      "113/113 - 19s - loss: 3.2253 - accuracy: 0.5699 - val_loss: 3.1164 - val_accuracy: 0.5699 - 19s/epoch - 167ms/step\n",
      "Epoch 34/10000\n",
      "113/113 - 19s - loss: 3.1788 - accuracy: 0.5724 - val_loss: 3.1013 - val_accuracy: 0.5665 - 19s/epoch - 169ms/step\n",
      "Epoch 35/10000\n",
      "113/113 - 19s - loss: 3.1527 - accuracy: 0.5691 - val_loss: 2.8761 - val_accuracy: 0.5670 - 19s/epoch - 172ms/step\n",
      "Epoch 36/10000\n",
      "113/113 - 20s - loss: 3.0004 - accuracy: 0.5723 - val_loss: 3.1260 - val_accuracy: 0.5769 - 20s/epoch - 178ms/step\n",
      "Epoch 37/10000\n",
      "113/113 - 20s - loss: 2.9881 - accuracy: 0.5706 - val_loss: 2.9246 - val_accuracy: 0.5787 - 20s/epoch - 175ms/step\n",
      "Epoch 38/10000\n",
      "113/113 - 21s - loss: 2.8640 - accuracy: 0.5720 - val_loss: 2.9586 - val_accuracy: 0.5769 - 21s/epoch - 184ms/step\n",
      "Epoch 39/10000\n",
      "113/113 - 20s - loss: 2.8705 - accuracy: 0.5716 - val_loss: 2.8784 - val_accuracy: 0.5504 - 20s/epoch - 178ms/step\n",
      "Epoch 40/10000\n",
      "113/113 - 21s - loss: 2.9555 - accuracy: 0.5697 - val_loss: 2.9155 - val_accuracy: 0.5801 - 21s/epoch - 184ms/step\n",
      "Epoch 41/10000\n",
      "113/113 - 19s - loss: 2.8222 - accuracy: 0.5730 - val_loss: 2.8120 - val_accuracy: 0.5710 - 19s/epoch - 172ms/step\n",
      "Epoch 42/10000\n",
      "113/113 - 19s - loss: 2.8938 - accuracy: 0.5700 - val_loss: 2.7506 - val_accuracy: 0.5782 - 19s/epoch - 168ms/step\n",
      "Epoch 43/10000\n",
      "113/113 - 19s - loss: 2.8504 - accuracy: 0.5704 - val_loss: 2.8636 - val_accuracy: 0.5781 - 19s/epoch - 172ms/step\n",
      "Epoch 44/10000\n",
      "113/113 - 19s - loss: 2.8162 - accuracy: 0.5695 - val_loss: 2.9387 - val_accuracy: 0.5623 - 19s/epoch - 167ms/step\n",
      "Epoch 45/10000\n",
      "113/113 - 19s - loss: 2.8564 - accuracy: 0.5706 - val_loss: 2.7958 - val_accuracy: 0.5672 - 19s/epoch - 167ms/step\n",
      "Epoch 46/10000\n",
      "113/113 - 19s - loss: 2.7167 - accuracy: 0.5758 - val_loss: 2.5140 - val_accuracy: 0.5761 - 19s/epoch - 168ms/step\n",
      "Epoch 47/10000\n",
      "113/113 - 19s - loss: 2.5825 - accuracy: 0.5714 - val_loss: 2.5359 - val_accuracy: 0.5676 - 19s/epoch - 166ms/step\n",
      "Epoch 48/10000\n",
      "113/113 - 19s - loss: 2.5580 - accuracy: 0.5688 - val_loss: 2.6369 - val_accuracy: 0.5762 - 19s/epoch - 167ms/step\n",
      "Epoch 49/10000\n",
      "113/113 - 19s - loss: 2.5066 - accuracy: 0.5724 - val_loss: 2.3552 - val_accuracy: 0.5677 - 19s/epoch - 167ms/step\n",
      "Epoch 50/10000\n",
      "113/113 - 19s - loss: 2.3835 - accuracy: 0.5716 - val_loss: 2.5054 - val_accuracy: 0.5814 - 19s/epoch - 166ms/step\n",
      "Epoch 51/10000\n",
      "113/113 - 19s - loss: 2.3899 - accuracy: 0.5750 - val_loss: 2.3289 - val_accuracy: 0.5810 - 19s/epoch - 167ms/step\n",
      "391/391 [==============================] - 5s 12ms/step\n",
      "\n",
      "Iterazione 128/128 (03-11-2023_23-04-45)\n",
      "hidden_layer_size: 1000\n",
      "min_val: -10\n",
      "max_val: 10\n",
      "num_hidden_layers: 4\n",
      "activation_hidden: sigmoid\n",
      "activation_output: sigmoid\n",
      "batch_size: 1000\n",
      "max_epochs: 10000\n",
      "monitor: val_accuracy\n",
      "patience: 20\n",
      "optimizer: adam\n",
      "loss: binary_crossentropy\n",
      "metrics: accuracy\n",
      "seed_value: 42\n",
      "\n",
      "La accuracy di questo modello sul validation set è: 58.42%\n",
      "La recall di questo modello sul validation set è: 44.76%\n",
      "La f1 di questo modello sul validation set è: 45.45%\n",
      "La balanced accuracy di questo modello sul validation set è: 55.9%\n",
      "La precision di questo modello sul validation set è: 46.16%\n",
      "La AUC di questo modello sul validation set è: 57.93%\n",
      "391/391 [==============================] - 5s 12ms/step\n",
      "\n",
      "Classification report per questo modello:\n",
      "              precision  recall  f1-score   support\n",
      "0                  0.66    0.67      0.66  7,670.00\n",
      "1                  0.46    0.45      0.45  4,841.00\n",
      "accuracy           0.58    0.58      0.58      0.58\n",
      "macro avg          0.56    0.56      0.56 12,511.00\n",
      "weighted avg       0.58    0.58      0.58 12,511.00\n",
      "\n",
      "Matrice di confusione:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         5142         2528\n",
      "Actual 1         2674         2167\n",
      "\n",
      "La accuracy migliore sul validation set trovata finora è 66.14% con il modello:\n",
      "98 - AC66.14%, L1000, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA20, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_14-02-18).pkl\n",
      "\n",
      "La recall migliore sul validation set trovata finora è 99.28% con il modello:\n",
      "7 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La f1 migliore sul validation set trovata finora è 55.56% con il modello:\n",
      "7 - F155.56%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-20-27).pkl\n",
      "\n",
      "La balanced accuracy migliore sul validation set trovata finora è 63.1% con il modello:\n",
      "98 - BA63.1%, L1000, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA20, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_14-02-18).pkl\n",
      "\n",
      "La precision migliore sul validation set trovata finora è 66.75% con il modello:\n",
      "105 - PR66.75%, L1000, mV-10, MV10, P2, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_14-19-26).pkl\n",
      "\n",
      "La AUC migliore sul validation set trovata finora è 68.11% con il modello:\n",
      "1 - AU68.11%, L20, mV-10, MV10, P1, AHrelu, AOsigmoid, BS100, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (03-11-2023_10-15-30).pkl\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output_size = 1\n",
    "\n",
    "# Parametri della griglia\n",
    "param_grid = {\n",
    "    'hidden_layer_size': [20, 100, 500, 1000],\n",
    "    'minval': [-10],\n",
    "    'maxval': [10],\n",
    "    'num_hidden_layers': [1, 2, 3, 4],\n",
    "    'activation_hidden': ['relu', 'sigmoid'],\n",
    "    'activation_output': ['sigmoid'],\n",
    "    'batch_size': [100, 1000],\n",
    "    'max_epochs': [10000],\n",
    "    'monitor': ['val_accuracy'],\n",
    "    'patience': [10, 20],\n",
    "    'optimizer': ['adam'],\n",
    "    'loss': ['binary_crossentropy'],\n",
    "    'metrics': ['accuracy'],\n",
    "    'seed_value': [42],\n",
    "}\n",
    "\n",
    "\n",
    "# Creo una nuova cartella per salvare i modelli, con un nuovo nome se la cartella esiste già\n",
    "current_time = datetime.now().strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
    "model_dir = 'Modelli con reti neurali'\n",
    "counter = 1\n",
    "\n",
    "# Elenco di tutte le directory nella directory corrente\n",
    "existing_dirs = os.listdir()\n",
    "\n",
    "while any(name.startswith(f\"{counter} - Modelli con reti neurali\") for name in existing_dirs):\n",
    "    counter += 1\n",
    "\n",
    "new_dir = f'{counter} - {model_dir} {current_time} (custom+)'\n",
    "\n",
    "os.makedirs(new_dir)\n",
    "\n",
    "# Creo un txt in cui salvare le stampe a video (in questo caso ne creo uno vuoto)\n",
    "txt_output = os.path.join(new_dir, 'prints.txt')\n",
    "with open(txt_output, 'w'):\n",
    "    pass\n",
    "\n",
    "print('Short list usata:')\n",
    "print(colonne_da_usare)\n",
    "print('')\n",
    "# Salvo la stessa stampa nel txt di output\n",
    "with open(txt_output, 'a') as file:\n",
    "    print('Short list usata:', file=file)\n",
    "    print(colonne_da_usare, file=file)\n",
    "    print('', file=file)\n",
    "\n",
    "print('Grid search usata:')\n",
    "print(param_grid)\n",
    "print('')\n",
    "# Salvo la stessa stampa nel txt di output\n",
    "with open(txt_output, 'a') as file:\n",
    "    print('Grid search usata:', file=file)\n",
    "    print(param_grid, file=file)\n",
    "    print('', file=file)\n",
    "\n",
    "# Creo nella cartella appena creata una nuova cartella per ogni metrica che voglio monitorare\n",
    "dir_accuracy = os.path.join(new_dir, 'Accuracy')\n",
    "dir_recall = os.path.join(new_dir, 'Recall')\n",
    "dir_f1 = os.path.join(new_dir, 'F1')\n",
    "dir_balanced_accuracy = os.path.join(new_dir, 'Balanced accuracy')\n",
    "dir_precision = os.path.join(new_dir, 'Precision')\n",
    "dir_AUC = os.path.join(new_dir, 'AUC')\n",
    "\n",
    "dirs_monitoraggio = [dir_accuracy, dir_recall, dir_f1, dir_balanced_accuracy, dir_precision, dir_AUC]\n",
    "\n",
    "for dir in dirs_monitoraggio:\n",
    "    os.makedirs(dir)\n",
    "\n",
    "\n",
    "# Mi creo delle variabili fittizie solo perchè altrimenti nella lista successiva non sa cosa siano queste variabili (lo 0 può essere in realtà qualunque valore, tanto vengono sovrascritti subito dopo il primo fit)\n",
    "accuracy = 0\n",
    "recall = 0\n",
    "f1 = 0\n",
    "bas = 0\n",
    "precision = 0\n",
    "AUC = 0\n",
    "\n",
    "# Inizializzo a 0 i valori massimi di ogni variabile che voglio monitorare\n",
    "variabili_da_monitorare = [accuracy, recall, f1, bas, precision, AUC]\n",
    "max_variabili_da_monitorare = [0]*len(variabili_da_monitorare)\n",
    "modelli_migliori = ['']*len(variabili_da_monitorare)\n",
    "nomi_variabili = ['accuracy', 'recall', 'f1', 'balanced accuracy', 'precision', 'AUC']\n",
    "\n",
    "# Itera su tutte le combinazioni dei parametri\n",
    "all_params = [dict(zip(param_grid.keys(), v)) for v in itertools.product(*param_grid.values())]\n",
    "# all_params = [dict(zip(param_grid.keys(), v)) for v in itertools.product(*param_grid.values())][::-1]\n",
    "\n",
    "# for i in range(len(all_params) - 1, -1, -1):\n",
    "#     params = all_params[len(all_params) - i - 1]\n",
    "for i, params in enumerate(all_params):\n",
    "    hidden_layer_size, minval, maxval, num_hidden_layers, activation_hidden, activation_output, batch_size, max_epochs, monitor, patience, optimizer, loss, metrics, seed_value = params.values()\n",
    "\n",
    "    tf.random.set_seed(seed_value)\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "    for _ in range(num_hidden_layers):\n",
    "        model.add(tf.keras.layers.Dense(hidden_layer_size,\n",
    "                                        activation=activation_hidden,\n",
    "                                        kernel_initializer=tf.random_uniform_initializer(minval=minval, maxval=maxval),\n",
    "                                        bias_initializer=tf.random_uniform_initializer(minval=minval, maxval=maxval)))\n",
    "    model.add(tf.keras.layers.Dense(output_size, activation=activation_output,\n",
    "                                    kernel_initializer=tf.random_uniform_initializer(minval=minval, maxval=maxval),\n",
    "                                    bias_initializer=tf.random_uniform_initializer(minval=minval, maxval=maxval)))\n",
    "\n",
    "    # Compilazione del modello\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=[metrics])\n",
    "\n",
    "    # Callback per l'early stopping\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor=monitor,patience=patience, restore_best_weights=True)\n",
    "\n",
    "    # Allenamento del modello\n",
    "    model.fit(X_train,\n",
    "            y_train,\n",
    "            batch_size=batch_size,\n",
    "            epochs=max_epochs,\n",
    "            callbacks=[early_stopping],\n",
    "            validation_data=(X_val, y_val),\n",
    "            verbose=2)\n",
    "\n",
    "    # Calcolo delle metriche sul set di test\n",
    "    val_predictions = model.predict(X_val)\n",
    "    val_predictions_rounded = np.round(val_predictions)\n",
    "\n",
    "    val_accuracy = accuracy_score(y_val, val_predictions_rounded)\n",
    "    val_recall = recall_score(y_val, val_predictions_rounded)\n",
    "    val_f1 = f1_score(y_val, val_predictions_rounded)\n",
    "    val_balanced_accuracy = balanced_accuracy_score(y_val, val_predictions_rounded)\n",
    "    val_precision = precision_score(y_val, val_predictions_rounded)\n",
    "    val_auc = roc_auc_score(y_val, val_predictions)\n",
    "\n",
    "    # test_loss, test_metrics = model.evaluate(X_test, y_test)\n",
    "    # print(f\"Larghezza: {hidden_layer_size} \\nProfondità: {num_hidden_layers} \\nDimensione del batch: {batch_size}\\nTest loss ({loss}): {test_loss}\\nTest metrics ({metrics}): {test_metrics}\")\n",
    "    \n",
    "\n",
    "    # Aggiorno la lista delle variabili da monitorare con i nuovi valori appena calcolati\n",
    "    variabili_da_monitorare = [val_accuracy, val_recall, val_f1, val_balanced_accuracy, val_precision, val_auc]\n",
    "\n",
    "\n",
    "    print(f\"\\nIterazione {i+1}/{len(all_params)} ({datetime.now().strftime('%d-%m-%Y_%H-%M-%S')})\\nhidden_layer_size: {hidden_layer_size}\\nmin_val: {minval}\\nmax_val: {maxval}\\nnum_hidden_layers: {num_hidden_layers}\\nactivation_hidden: {activation_hidden}\\nactivation_output: {activation_output}\\nbatch_size: {batch_size}\\nmax_epochs: {max_epochs}\\nmonitor: {monitor}\\npatience: {patience}\\noptimizer: {optimizer}\\nloss: {loss}\\nmetrics: {metrics}\\nseed_value: {seed_value}\\n\")    \n",
    "    for j in range(len(variabili_da_monitorare)):\n",
    "        print(f'La {nomi_variabili[j]} di questo modello sul validation set è: {round(variabili_da_monitorare[j]*100,2)}%')\n",
    "    # Salvo la stessa stampa nel txt di output\n",
    "    with open(txt_output, 'a') as file:\n",
    "        print(f\"Iterazione {i+1}/{len(all_params)} ({datetime.now().strftime('%d-%m-%Y_%H-%M-%S')})\\nhidden_layer_size: {hidden_layer_size}\\nmin_val: {minval}\\nmax_val: {maxval}\\nnum_hidden_layers: {num_hidden_layers}\\nactivation_hidden: {activation_hidden}\\nactivation_output: {activation_output}\\nbatch_size: {batch_size}\\nmax_epochs: {max_epochs}\\nmonitor: {monitor}\\npatience: {patience}\\noptimizer: {optimizer}\\nloss: {loss}\\nmetrics: {metrics}\\nseed_value: {seed_value}\\n\", file=file)    \n",
    "        for j in range(len(variabili_da_monitorare)):\n",
    "            print(f'La {nomi_variabili[j]} di questo modello sul validation set è: {round(variabili_da_monitorare[j]*100,2)}%', file=file)\n",
    "\n",
    "\n",
    "    val_predictions = model.predict(X_val)\n",
    "    val_predictions_rounded = np.round(val_predictions)\n",
    "    # Genero il classification report\n",
    "    report = classification_report(y_val, val_predictions_rounded, output_dict=True)\n",
    "    df = pd.DataFrame(report).transpose()\n",
    "\n",
    "    print('\\nClassification report per questo modello:')\n",
    "    print(df)\n",
    "    print('')\n",
    "    # Salvo la stessa stampa nel txt di output\n",
    "    with open(txt_output, 'a') as file:\n",
    "        print('\\nClassification report per questo modello:', file=file)\n",
    "        print(df, file=file)\n",
    "        print('', file=file)\n",
    "\n",
    "\n",
    "    # Calcolo la matrice di confusione \n",
    "    confusion_val = confusion_matrix(y_val, val_predictions_rounded)\n",
    "    confusion_df_val = pd.DataFrame(confusion_val, columns=['Predicted 0', 'Predicted 1'], index=['Actual 0', 'Actual 1'])\n",
    "\n",
    "    print('Matrice di confusione:')\n",
    "    print(confusion_df_val)\n",
    "    print('')\n",
    "    # Salvo la stessa stampa nel txt di output\n",
    "    with open(txt_output, 'a') as file:\n",
    "        print('Matrice di confusione:', file=file)\n",
    "        print(confusion_df_val, file=file)\n",
    "        print('', file=file)\n",
    "\n",
    "    # Mi salvo il tempo attuale come stringa\n",
    "    current_time = datetime.now().strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
    "    \n",
    "    # Salvo solo i modelli che migliorano la metrica in questione rispetto ai precedenti modelli della stessa cartella\n",
    "    for k in range(len(variabili_da_monitorare)):\n",
    "        if variabili_da_monitorare[k] > max_variabili_da_monitorare[k]:\n",
    "            max_variabili_da_monitorare[k] = variabili_da_monitorare[k]\n",
    "\n",
    "            # Do un nome al modello\n",
    "            model_name = f\"{i+1} - {nomi_variabili[k][0:2].upper()}{round(variabili_da_monitorare[k]*100,2)}%, L{hidden_layer_size}, mV{minval}, MV{maxval}, P{num_hidden_layers}, AH{activation_hidden}, AO{activation_output}, BS{batch_size}, ME{max_epochs}, MO{monitor}, PA{patience}, OP{optimizer}, LO{loss}, MET{metrics}, SV{seed_value} ({current_time}).pkl\"\n",
    "\n",
    "            modelli_migliori[k] = model_name\n",
    "\n",
    "            # Salvo il modello in un file\n",
    "            model_path = os.path.join(dirs_monitoraggio[k], model_name)\n",
    "            with open(model_path, 'wb') as file:\n",
    "                pickle.dump(model, file)\n",
    "        \n",
    "\n",
    "        print(f'La {nomi_variabili[k]} migliore sul validation set trovata finora è {round(max_variabili_da_monitorare[k]*100,2)}% con il modello:\\n{modelli_migliori[k]}\\n')\n",
    "        # Salvo la stessa stampa nel txt di output\n",
    "        with open(txt_output, 'a') as file:\n",
    "            print(f'La {nomi_variabili[k]} migliore sul validation set trovata finora è {round(max_variabili_da_monitorare[k]*100,2)}% con il modello:\\n{modelli_migliori[k]}\\n', file=file)\n",
    "\n",
    "    print('')\n",
    "    # Salvo la stessa stampa nel txt di output\n",
    "    with open(txt_output, 'a') as file:\n",
    "        print('', file=file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "No file or directory found at C:\\Users\\qj1aleimbria\\Desktop\\File VS Code\\11 - 16 10 23 (ML su Prestitempo)\\3 - Modelli con reti neurali 02-11-2023_19-03-52 (custom+)\\Recall\\13 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (02-11-2023_19-21-14).h5",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\qj1aleimbria\\Desktop\\File VS Code\\11 - 16 10 23 (ML su Prestitempo)\\Reti neurali custom con più variabili.ipynb Cell 102\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/qj1aleimbria/Desktop/File%20VS%20Code/11%20-%2016%2010%2023%20%28ML%20su%20Prestitempo%29/Reti%20neurali%20custom%20con%20pi%C3%B9%20variabili.ipynb#Y200sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m modello_scelto \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mkeras\u001b[39m.\u001b[39;49mmodels\u001b[39m.\u001b[39;49mload_model(\u001b[39mr\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mC:\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mUsers\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mqj1aleimbria\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mDesktop\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mFile VS Code\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39m11 - 16 10 23 (ML su Prestitempo)\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39m3 - Modelli con reti neurali 02-11-2023_19-03-52 (custom+)\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mRecall\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39m13 - RE99.28\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39m, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (02-11-2023_19-21-14).h5\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\qj1aleimbria\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\saving\\saving_api.py:262\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, safe_mode, **kwargs)\u001b[0m\n\u001b[0;32m    254\u001b[0m     \u001b[39mreturn\u001b[39;00m saving_lib\u001b[39m.\u001b[39mload_model(\n\u001b[0;32m    255\u001b[0m         filepath,\n\u001b[0;32m    256\u001b[0m         custom_objects\u001b[39m=\u001b[39mcustom_objects,\n\u001b[0;32m    257\u001b[0m         \u001b[39mcompile\u001b[39m\u001b[39m=\u001b[39m\u001b[39mcompile\u001b[39m,\n\u001b[0;32m    258\u001b[0m         safe_mode\u001b[39m=\u001b[39msafe_mode,\n\u001b[0;32m    259\u001b[0m     )\n\u001b[0;32m    261\u001b[0m \u001b[39m# Legacy case.\u001b[39;00m\n\u001b[1;32m--> 262\u001b[0m \u001b[39mreturn\u001b[39;00m legacy_sm_saving_lib\u001b[39m.\u001b[39mload_model(\n\u001b[0;32m    263\u001b[0m     filepath, custom_objects\u001b[39m=\u001b[39mcustom_objects, \u001b[39mcompile\u001b[39m\u001b[39m=\u001b[39m\u001b[39mcompile\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    264\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\qj1aleimbria\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\qj1aleimbria\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\saving\\legacy\\save.py:234\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(filepath_str, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    233\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m tf\u001b[39m.\u001b[39mio\u001b[39m.\u001b[39mgfile\u001b[39m.\u001b[39mexists(filepath_str):\n\u001b[1;32m--> 234\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mIOError\u001b[39;00m(\n\u001b[0;32m    235\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNo file or directory found at \u001b[39m\u001b[39m{\u001b[39;00mfilepath_str\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    236\u001b[0m         )\n\u001b[0;32m    238\u001b[0m     \u001b[39mif\u001b[39;00m tf\u001b[39m.\u001b[39mio\u001b[39m.\u001b[39mgfile\u001b[39m.\u001b[39misdir(filepath_str):\n\u001b[0;32m    239\u001b[0m         \u001b[39mreturn\u001b[39;00m saved_model_load\u001b[39m.\u001b[39mload(\n\u001b[0;32m    240\u001b[0m             filepath_str, \u001b[39mcompile\u001b[39m, options\n\u001b[0;32m    241\u001b[0m         )\n",
      "\u001b[1;31mOSError\u001b[0m: No file or directory found at C:\\Users\\qj1aleimbria\\Desktop\\File VS Code\\11 - 16 10 23 (ML su Prestitempo)\\3 - Modelli con reti neurali 02-11-2023_19-03-52 (custom+)\\Recall\\13 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (02-11-2023_19-21-14).h5"
     ]
    }
   ],
   "source": [
    "modello_scelto = tf.keras.models.load_model(r\"C:\\Users\\qj1aleimbria\\Desktop\\File VS Code\\11 - 16 10 23 (ML su Prestitempo)\\3 - Modelli con reti neurali 02-11-2023_19-03-52 (custom+)\\Recall\\13 - RE99.28%, L20, mV-10, MV10, P1, AHsigmoid, AOsigmoid, BS1000, ME10000, MOval_accuracy, PA10, OPadam, LObinary_crossentropy, METaccuracy, SV42 (02-11-2023_19-21-14).pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 1.0452582 ],\n",
       "        [28.009459  ],\n",
       "        [ 0.05059099],\n",
       "        [ 0.03503529],\n",
       "        [-0.15071137],\n",
       "        [-0.3865301 ],\n",
       "        [ 3.5427334 ],\n",
       "        [-7.4607806 ],\n",
       "        [ 0.25772637],\n",
       "        [-4.086055  ],\n",
       "        [-0.6518401 ],\n",
       "        [-0.28280306]], dtype=float32),\n",
       " array([-8.483837], dtype=float32)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "modello_scelto.layers[0].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print('Performance sul training set:')\n",
    "# Get model predictions\n",
    "predictions = modello_scelto.predict(train_inputs)\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(train_targets, predictions, output_dict=True)\n",
    "\n",
    "df = pd.DataFrame(report).transpose()\n",
    "print('Report senza round:')\n",
    "print(df)\n",
    "\n",
    "\n",
    "predictions = np.round(predictions)\n",
    "report = classification_report(train_targets, predictions, output_dict=True)\n",
    "df = pd.DataFrame(report).transpose()\n",
    "print('Report con round:')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print('Performance sul validation set:')\n",
    "# Get model predictions\n",
    "predictions = modello_scelto.predict(validation_inputs)\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(validation_targets, predictions, output_dict=True)\n",
    "\n",
    "df = pd.DataFrame(report).transpose()\n",
    "print('Report senza round:')\n",
    "print(df)\n",
    "\n",
    "\n",
    "predictions = np.round(predictions)\n",
    "report = classification_report(validation_targets, predictions, output_dict=True)\n",
    "df = pd.DataFrame(report).transpose()\n",
    "print('Report con round:')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print('Performance sul test set:')\n",
    "# Get model predictions\n",
    "predictions = modello_scelto.predict(test_inputs)\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(test_targets, predictions, output_dict=True)\n",
    "\n",
    "df = pd.DataFrame(report).transpose()\n",
    "print('Report senza round:')\n",
    "print(df)\n",
    "\n",
    "\n",
    "predictions = np.round(predictions)\n",
    "report = classification_report(test_targets, predictions, output_dict=True)\n",
    "df = pd.DataFrame(report).transpose()\n",
    "print('Report con round:')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> Disegno la curva ROC e calcolo l'AUC </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# predictions dovrebbe essere una matrice (array) di probabilità con shape (n_samples, n_classes).\n",
    "predictions = modello_scelto.predict(test_inputs)\n",
    "\n",
    "# Calcola la curva ROC e l'AUC\n",
    "fpr, tpr, _ = roc_curve(test_targets, predictions)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Disegna la curva ROC\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = {:.2f})'.format(roc_auc))\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "# Stampa l'AUC\n",
    "print(\"Area Under the Curve (AUC): {:.2f}\".format(roc_auc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
